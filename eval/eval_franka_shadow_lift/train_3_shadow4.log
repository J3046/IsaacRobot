################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 9694 steps/s (collection: 9.609s, learning 0.531s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 36.9363
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0008
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 10.14s
                      Time elapsed: 00:00:10
                               ETA: 05:38:00

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14262 steps/s (collection: 6.740s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 37.0333
                       Mean reward: 0.00
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0023
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0011
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.89s
                      Time elapsed: 00:00:17
                               ETA: 04:43:44

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 16015 steps/s (collection: 5.996s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 37.0976
                       Mean reward: 0.01
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0039
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.14s
                      Time elapsed: 00:00:23
                               ETA: 04:17:11

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 15885 steps/s (collection: 6.064s, learning 0.125s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 37.1068
                       Mean reward: 0.01
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0052
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.19s
                      Time elapsed: 00:00:29
                               ETA: 04:04:17

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 13782 steps/s (collection: 6.965s, learning 0.168s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.1146
                       Mean reward: 0.01
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.13s
                      Time elapsed: 00:00:36
                               ETA: 04:02:47

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 16166 steps/s (collection: 5.924s, learning 0.157s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 37.1153
                       Mean reward: 0.01
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0078
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0040
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.08s
                      Time elapsed: 00:00:42
                               ETA: 03:55:55

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 15493 steps/s (collection: 6.205s, learning 0.140s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 37.1114
                       Mean reward: 0.02
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0093
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.34s
                      Time elapsed: 00:00:48
                               ETA: 03:52:14

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14976 steps/s (collection: 6.429s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 37.0948
                       Mean reward: 0.02
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0107
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.56s
                      Time elapsed: 00:00:55
                               ETA: 03:50:21

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 16892 steps/s (collection: 5.705s, learning 0.115s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 37.0903
                       Mean reward: 0.02
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0120
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.82s
                      Time elapsed: 00:01:01
                               ETA: 03:46:07

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 51928 steps/s (collection: 1.796s, learning 0.097s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 37.0896
                       Mean reward: 0.03
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0150
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.89s
                      Time elapsed: 00:01:03
                               ETA: 03:29:41

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 53773 steps/s (collection: 1.721s, learning 0.107s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 37.0793
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0166
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.83s
                      Time elapsed: 00:01:05
                               ETA: 03:16:03

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 50333 steps/s (collection: 1.843s, learning 0.110s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 37.0750
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0187
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.95s
                      Time elapsed: 00:01:06
                               ETA: 03:05:01

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 55966 steps/s (collection: 1.662s, learning 0.094s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 37.1067
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0204
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.76s
                      Time elapsed: 00:01:08
                               ETA: 02:55:10

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 55886 steps/s (collection: 1.665s, learning 0.094s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 37.1219
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0259
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.76s
                      Time elapsed: 00:01:10
                               ETA: 02:46:44

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 52348 steps/s (collection: 1.786s, learning 0.092s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 37.1581
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0307
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.88s
                      Time elapsed: 00:01:12
                               ETA: 02:39:41

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 54773 steps/s (collection: 1.689s, learning 0.106s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 37.2171
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0415
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.79s
                      Time elapsed: 00:01:14
                               ETA: 02:33:20

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 54319 steps/s (collection: 1.709s, learning 0.101s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 37.2656
                       Mean reward: 0.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0569
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.81s
                      Time elapsed: 00:01:15
                               ETA: 02:27:46

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 50802 steps/s (collection: 1.832s, learning 0.103s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 37.2715
                       Mean reward: 0.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0735
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.94s
                      Time elapsed: 00:01:17
                               ETA: 02:23:02

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 47134 steps/s (collection: 1.998s, learning 0.088s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.8739
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 37.3618
                       Mean reward: 0.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0975
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 2.09s
                      Time elapsed: 00:01:19
                               ETA: 02:19:04

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 52091 steps/s (collection: 1.785s, learning 0.103s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1488
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.4463
                       Mean reward: 0.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1279
     Episode_Reward/lifting_object: -0.0895
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.89s
                      Time elapsed: 00:01:21
                               ETA: 02:15:10

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 48745 steps/s (collection: 1.920s, learning 0.097s)
             Mean action noise std: 1.03
          Mean value_function loss: 7.4122
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 37.6682
                       Mean reward: -4.56
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.1683
     Episode_Reward/lifting_object: -0.4094
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.02s
                      Time elapsed: 00:01:23
                               ETA: 02:11:50

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 45187 steps/s (collection: 2.061s, learning 0.115s)
             Mean action noise std: 1.04
          Mean value_function loss: 5.4800
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 37.8197
                       Mean reward: -0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2127
     Episode_Reward/lifting_object: -0.4728
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0073
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.18s
                      Time elapsed: 00:01:26
                               ETA: 02:09:02

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 43610 steps/s (collection: 2.103s, learning 0.151s)
             Mean action noise std: 1.05
          Mean value_function loss: 11.7265
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 37.9835
                       Mean reward: -4.29
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.2408
     Episode_Reward/lifting_object: -0.8525
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.25s
                      Time elapsed: 00:01:28
                               ETA: 02:06:36

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 45619 steps/s (collection: 2.037s, learning 0.118s)
             Mean action noise std: 1.05
          Mean value_function loss: 11.3359
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 38.1168
                       Mean reward: -8.07
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.2785
     Episode_Reward/lifting_object: -1.4436
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.15s
                      Time elapsed: 00:01:30
                               ETA: 02:04:13

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 48271 steps/s (collection: 1.924s, learning 0.112s)
             Mean action noise std: 1.06
          Mean value_function loss: 3.3942
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 38.2464
                       Mean reward: -2.75
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.3051
     Episode_Reward/lifting_object: -0.8127
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.04s
                      Time elapsed: 00:01:32
                               ETA: 02:01:52

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 49248 steps/s (collection: 1.903s, learning 0.093s)
             Mean action noise std: 1.06
          Mean value_function loss: 2.7487
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.3730
                       Mean reward: 0.03
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: -0.2535
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.00s
                      Time elapsed: 00:01:34
                               ETA: 01:59:39

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 49197 steps/s (collection: 1.906s, learning 0.092s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.8496
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.4912
                       Mean reward: 0.47
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: -0.1054
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.00s
                      Time elapsed: 00:01:36
                               ETA: 01:57:36

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 50194 steps/s (collection: 1.865s, learning 0.094s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.6647
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.5960
                       Mean reward: 0.77
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.3293
     Episode_Reward/lifting_object: -0.1783
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.96s
                      Time elapsed: 00:01:38
                               ETA: 01:55:38

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 50155 steps/s (collection: 1.866s, learning 0.094s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.2406
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.7148
                       Mean reward: 1.40
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.3020
     Episode_Reward/lifting_object: -0.0951
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.96s
                      Time elapsed: 00:01:40
                               ETA: 01:53:49

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 52320 steps/s (collection: 1.788s, learning 0.091s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 38.8336
                       Mean reward: 1.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2461
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.88s
                      Time elapsed: 00:01:42
                               ETA: 01:52:01

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 52495 steps/s (collection: 1.776s, learning 0.096s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 39.0046
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2153
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.87s
                      Time elapsed: 00:01:44
                               ETA: 01:50:20

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 52795 steps/s (collection: 1.765s, learning 0.097s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 39.1282
                       Mean reward: 0.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1769
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.86s
                      Time elapsed: 00:01:46
                               ETA: 01:48:45

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 53197 steps/s (collection: 1.743s, learning 0.105s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 39.1699
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1441
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.85s
                      Time elapsed: 00:01:47
                               ETA: 01:47:14

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 52202 steps/s (collection: 1.779s, learning 0.105s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 39.2300
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1282
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.88s
                      Time elapsed: 00:01:49
                               ETA: 01:45:50

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 51954 steps/s (collection: 1.792s, learning 0.100s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 39.2625
                       Mean reward: 0.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1127
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.89s
                      Time elapsed: 00:01:51
                               ETA: 01:44:32

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 52589 steps/s (collection: 1.769s, learning 0.100s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 39.2997
                       Mean reward: 0.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1002
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.87s
                      Time elapsed: 00:01:53
                               ETA: 01:43:17

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 50978 steps/s (collection: 1.833s, learning 0.095s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 39.3303
                       Mean reward: 0.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.93s
                      Time elapsed: 00:01:55
                               ETA: 01:42:09

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 48534 steps/s (collection: 1.921s, learning 0.105s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 39.3454
                       Mean reward: 0.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 2.03s
                      Time elapsed: 00:01:57
                               ETA: 01:41:09

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 50003 steps/s (collection: 1.848s, learning 0.118s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 39.3583
                       Mean reward: 0.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1161
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.97s
                      Time elapsed: 00:01:59
                               ETA: 01:40:09

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 49534 steps/s (collection: 1.876s, learning 0.109s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.7711
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.4245
                       Mean reward: 0.55
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.1417
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.98s
                      Time elapsed: 00:02:01
                               ETA: 01:39:13

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 48285 steps/s (collection: 1.935s, learning 0.101s)
             Mean action noise std: 1.11
          Mean value_function loss: 7.8461
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.4865
                       Mean reward: -2.29
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.1842
     Episode_Reward/lifting_object: -0.2860
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 2.04s
                      Time elapsed: 00:02:03
                               ETA: 01:38:22

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 44492 steps/s (collection: 2.110s, learning 0.099s)
             Mean action noise std: 1.11
          Mean value_function loss: 11.4840
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 39.6065
                       Mean reward: 0.15
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.2320
     Episode_Reward/lifting_object: -1.0182
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.21s
                      Time elapsed: 00:02:05
                               ETA: 01:37:42

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 47265 steps/s (collection: 1.971s, learning 0.109s)
             Mean action noise std: 1.12
          Mean value_function loss: 21.8335
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 39.6924
                       Mean reward: -6.34
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.2856
     Episode_Reward/lifting_object: -1.6715
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.08s
                      Time elapsed: 00:02:07
                               ETA: 01:36:57

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 46795 steps/s (collection: 1.993s, learning 0.107s)
             Mean action noise std: 1.12
          Mean value_function loss: 17.8971
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 39.7942
                       Mean reward: -9.52
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.3356
     Episode_Reward/lifting_object: -1.8806
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 2.10s
                      Time elapsed: 00:02:09
                               ETA: 01:36:16

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 48099 steps/s (collection: 1.946s, learning 0.098s)
             Mean action noise std: 1.12
          Mean value_function loss: 9.3025
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 39.8913
                       Mean reward: -2.74
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.3697
     Episode_Reward/lifting_object: -1.5506
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.04s
                      Time elapsed: 00:02:11
                               ETA: 01:35:33

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 48816 steps/s (collection: 1.913s, learning 0.101s)
             Mean action noise std: 1.13
          Mean value_function loss: 3.6586
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 39.9852
                       Mean reward: -1.69
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.3886
     Episode_Reward/lifting_object: -0.7990
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.01s
                      Time elapsed: 00:02:13
                               ETA: 01:34:51

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 44867 steps/s (collection: 2.093s, learning 0.098s)
             Mean action noise std: 1.13
          Mean value_function loss: 1.5549
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.0856
                       Mean reward: 0.95
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.4074
     Episode_Reward/lifting_object: -0.4114
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.19s
                      Time elapsed: 00:02:16
                               ETA: 01:34:18

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 45612 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.5843
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.1948
                       Mean reward: 1.91
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.4100
     Episode_Reward/lifting_object: -0.2116
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.16s
                      Time elapsed: 00:02:18
                               ETA: 01:33:45

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 47992 steps/s (collection: 1.937s, learning 0.111s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.8039
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.2921
                       Mean reward: 1.75
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.3769
     Episode_Reward/lifting_object: -0.0980
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.05s
                      Time elapsed: 00:02:20
                               ETA: 01:33:09

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 50462 steps/s (collection: 1.840s, learning 0.108s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0057
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 40.3809
                       Mean reward: 1.64
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.3533
     Episode_Reward/lifting_object: -0.0714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.95s
                      Time elapsed: 00:02:22
                               ETA: 01:32:31

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52464 steps/s (collection: 1.773s, learning 0.101s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 40.5069
                       Mean reward: 1.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.87s
                      Time elapsed: 00:02:24
                               ETA: 01:31:51

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 53187 steps/s (collection: 1.745s, learning 0.104s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 40.6764
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2122
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.85s
                      Time elapsed: 00:02:25
                               ETA: 01:31:11

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 53623 steps/s (collection: 1.715s, learning 0.118s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 40.8019
                       Mean reward: 0.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1576
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.83s
                      Time elapsed: 00:02:27
                               ETA: 01:30:33

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 40530 steps/s (collection: 2.232s, learning 0.194s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 40.9305
                       Mean reward: 0.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1308
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.43s
                      Time elapsed: 00:02:30
                               ETA: 01:30:17

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 42725 steps/s (collection: 2.154s, learning 0.147s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 41.0625
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0968
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.30s
                      Time elapsed: 00:02:32
                               ETA: 01:29:57

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 45640 steps/s (collection: 2.011s, learning 0.143s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 41.1678
                       Mean reward: 0.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0739
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.15s
                      Time elapsed: 00:02:34
                               ETA: 01:29:33

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 49272 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 41.2632
                       Mean reward: 0.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0604
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.00s
                      Time elapsed: 00:02:36
                               ETA: 01:29:04

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 51844 steps/s (collection: 1.792s, learning 0.104s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 41.3139
                       Mean reward: 0.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0502
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.90s
                      Time elapsed: 00:02:38
                               ETA: 01:28:32

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 52695 steps/s (collection: 1.755s, learning 0.110s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 41.3421
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0427
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.87s
                      Time elapsed: 00:02:40
                               ETA: 01:28:01

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 53399 steps/s (collection: 1.739s, learning 0.102s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 41.3638
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0373
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.84s
                      Time elapsed: 00:02:42
                               ETA: 01:27:30

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52785 steps/s (collection: 1.765s, learning 0.098s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 41.3793
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0321
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.86s
                      Time elapsed: 00:02:44
                               ETA: 01:27:00

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52910 steps/s (collection: 1.757s, learning 0.101s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 41.3997
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0290
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.86s
                      Time elapsed: 00:02:46
                               ETA: 01:26:32

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 51352 steps/s (collection: 1.814s, learning 0.100s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 41.4032
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0279
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.91s
                      Time elapsed: 00:02:47
                               ETA: 01:26:05

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 52758 steps/s (collection: 1.766s, learning 0.098s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 41.4149
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0273
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 19.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.86s
                      Time elapsed: 00:02:49
                               ETA: 01:25:39

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 53182 steps/s (collection: 1.754s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 41.4149
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0272
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.85s
                      Time elapsed: 00:02:51
                               ETA: 01:25:12

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 53852 steps/s (collection: 1.715s, learning 0.110s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 41.4210
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0288
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.83s
                      Time elapsed: 00:02:53
                               ETA: 01:24:45

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 52453 steps/s (collection: 1.766s, learning 0.109s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 41.4366
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0330
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.87s
                      Time elapsed: 00:02:55
                               ETA: 01:24:21

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 54784 steps/s (collection: 1.698s, learning 0.097s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 41.4425
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0407
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.79s
                      Time elapsed: 00:02:57
                               ETA: 01:23:55

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 52442 steps/s (collection: 1.766s, learning 0.109s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 41.4477
                       Mean reward: 0.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0467
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.87s
                      Time elapsed: 00:02:59
                               ETA: 01:23:32

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 52681 steps/s (collection: 1.757s, learning 0.109s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 41.4708
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0648
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.87s
                      Time elapsed: 00:03:00
                               ETA: 01:23:09

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52494 steps/s (collection: 1.767s, learning 0.106s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0293
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 41.5286
                       Mean reward: 0.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0841
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.87s
                      Time elapsed: 00:03:02
                               ETA: 01:22:47

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 50629 steps/s (collection: 1.845s, learning 0.097s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.5740
                       Mean reward: 0.49
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.1040
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.94s
                      Time elapsed: 00:03:04
                               ETA: 01:22:28

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 49910 steps/s (collection: 1.860s, learning 0.110s)
             Mean action noise std: 1.20
          Mean value_function loss: 3.8981
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 41.6676
                       Mean reward: -0.49
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.1461
     Episode_Reward/lifting_object: -0.1792
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.97s
                      Time elapsed: 00:03:06
                               ETA: 01:22:09

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 49996 steps/s (collection: 1.868s, learning 0.098s)
             Mean action noise std: 1.21
          Mean value_function loss: 5.6788
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.7478
                       Mean reward: -1.96
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.1803
     Episode_Reward/lifting_object: -0.5514
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.97s
                      Time elapsed: 00:03:08
                               ETA: 01:21:52

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 49404 steps/s (collection: 1.890s, learning 0.100s)
             Mean action noise std: 1.21
          Mean value_function loss: 5.6296
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 41.8594
                       Mean reward: -4.47
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.2261
     Episode_Reward/lifting_object: -0.5623
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.99s
                      Time elapsed: 00:03:10
                               ETA: 01:21:35

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 49577 steps/s (collection: 1.883s, learning 0.100s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.7571
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.9272
                       Mean reward: 0.77
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.2483
     Episode_Reward/lifting_object: -0.3565
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.98s
                      Time elapsed: 00:03:12
                               ETA: 01:21:18

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 49604 steps/s (collection: 1.880s, learning 0.102s)
             Mean action noise std: 1.22
          Mean value_function loss: 1.0080
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.0329
                       Mean reward: 0.37
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.2777
     Episode_Reward/lifting_object: -0.1912
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.98s
                      Time elapsed: 00:03:14
                               ETA: 01:21:02

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 47540 steps/s (collection: 1.964s, learning 0.104s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.5943
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.1164
                       Mean reward: 1.38
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: -0.1258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.07s
                      Time elapsed: 00:03:16
                               ETA: 01:20:48

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 40937 steps/s (collection: 2.274s, learning 0.127s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1873
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.2533
                       Mean reward: 1.32
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.3035
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.40s
                      Time elapsed: 00:03:19
                               ETA: 01:20:42

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 44540 steps/s (collection: 2.108s, learning 0.100s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.4432
                       Mean reward: 1.48
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.3174
     Episode_Reward/lifting_object: 0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.21s
                      Time elapsed: 00:03:21
                               ETA: 01:20:32

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 37357 steps/s (collection: 2.444s, learning 0.187s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 42.6716
                       Mean reward: 1.45
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.3073
     Episode_Reward/lifting_object: -0.0392
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.63s
                      Time elapsed: 00:03:23
                               ETA: 01:20:33

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 38402 steps/s (collection: 2.356s, learning 0.204s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0282
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 42.6455
                       Mean reward: 1.21
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.2678
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.56s
                      Time elapsed: 00:03:26
                               ETA: 01:20:31

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 45378 steps/s (collection: 1.975s, learning 0.191s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 42.6732
                       Mean reward: 1.00
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.2304
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.17s
                      Time elapsed: 00:03:28
                               ETA: 01:20:20

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 48583 steps/s (collection: 1.883s, learning 0.140s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 42.7665
                       Mean reward: 0.88
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.1973
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.02s
                      Time elapsed: 00:03:30
                               ETA: 01:20:07

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 47225 steps/s (collection: 1.983s, learning 0.099s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0288
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 42.8109
                       Mean reward: 0.77
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.1916
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 19.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.08s
                      Time elapsed: 00:03:32
                               ETA: 01:19:55

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 46110 steps/s (collection: 2.027s, learning 0.105s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1324
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.8534
                       Mean reward: 0.52
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.1619
     Episode_Reward/lifting_object: -0.0308
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.13s
                      Time elapsed: 00:03:34
                               ETA: 01:19:44

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 45436 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.2018
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.9668
                       Mean reward: 0.26
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.1512
     Episode_Reward/lifting_object: -0.0375
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.16s
                      Time elapsed: 00:03:37
                               ETA: 01:19:34

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 50842 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.0604
                       Mean reward: 0.61
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.1438
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.93s
                      Time elapsed: 00:03:38
                               ETA: 01:19:19

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 51753 steps/s (collection: 1.808s, learning 0.091s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0770
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.1579
                       Mean reward: 0.35
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.1579
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.90s
                      Time elapsed: 00:03:40
                               ETA: 01:19:04

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 49949 steps/s (collection: 1.874s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2944
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.2788
                       Mean reward: 0.65
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.1714
     Episode_Reward/lifting_object: -0.0994
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.97s
                      Time elapsed: 00:03:42
                               ETA: 01:18:51

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 47984 steps/s (collection: 1.934s, learning 0.115s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.8074
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.3770
                       Mean reward: -0.13
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.1966
     Episode_Reward/lifting_object: -0.1968
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.05s
                      Time elapsed: 00:03:44
                               ETA: 01:18:39

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 46464 steps/s (collection: 2.014s, learning 0.102s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.2093
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.4804
                       Mean reward: -0.88
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.2330
     Episode_Reward/lifting_object: -0.1921
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.12s
                      Time elapsed: 00:03:46
                               ETA: 01:18:29

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 47173 steps/s (collection: 1.963s, learning 0.121s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4990
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.5656
                       Mean reward: 0.64
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.2580
     Episode_Reward/lifting_object: -0.1338
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.08s
                      Time elapsed: 00:03:49
                               ETA: 01:18:19

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 47893 steps/s (collection: 1.958s, learning 0.094s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.8438
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.6488
                       Mean reward: -0.72
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: -0.1718
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.05s
                      Time elapsed: 00:03:51
                               ETA: 01:18:08

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 42146 steps/s (collection: 2.111s, learning 0.222s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1902
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.7394
                       Mean reward: 1.45
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 0.2934
     Episode_Reward/lifting_object: -0.0591
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.33s
                      Time elapsed: 00:03:53
                               ETA: 01:18:03

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 42830 steps/s (collection: 2.170s, learning 0.126s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.1764
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.8524
                       Mean reward: 0.75
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: -0.0485
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.30s
                      Time elapsed: 00:03:55
                               ETA: 01:17:58

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 47351 steps/s (collection: 1.974s, learning 0.102s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.4236
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.9384
                       Mean reward: 1.52
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.08s
                      Time elapsed: 00:03:57
                               ETA: 01:17:48

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 49285 steps/s (collection: 1.901s, learning 0.094s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0652
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.0335
                       Mean reward: 1.38
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.3124
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.99s
                      Time elapsed: 00:03:59
                               ETA: 01:17:36

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 46952 steps/s (collection: 1.999s, learning 0.095s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1870
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.2209
                       Mean reward: 1.48
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.3132
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 2.09s
                      Time elapsed: 00:04:01
                               ETA: 01:17:27

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 41747 steps/s (collection: 2.247s, learning 0.108s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0507
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.2703
                       Mean reward: 1.27
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.2959
     Episode_Reward/lifting_object: -0.0907
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.35s
                      Time elapsed: 00:04:04
                               ETA: 01:17:23

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 48635 steps/s (collection: 1.912s, learning 0.109s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.4104
                       Mean reward: 1.01
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.2737
     Episode_Reward/lifting_object: -0.0190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.02s
                      Time elapsed: 00:04:06
                               ETA: 01:17:13

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 45095 steps/s (collection: 2.078s, learning 0.102s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 44.5514
                       Mean reward: 1.17
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.2696
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.18s
                      Time elapsed: 00:04:08
                               ETA: 01:17:05

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 41988 steps/s (collection: 2.156s, learning 0.186s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.4057
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.5238
                       Mean reward: 1.12
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.2424
     Episode_Reward/lifting_object: -0.0716
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.34s
                      Time elapsed: 00:04:10
                               ETA: 01:17:01

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 36532 steps/s (collection: 2.502s, learning 0.189s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2031
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.5531
                       Mean reward: 1.18
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.2483
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.69s
                      Time elapsed: 00:04:13
                               ETA: 01:17:03

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 37879 steps/s (collection: 2.401s, learning 0.194s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.5082
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.6465
                       Mean reward: 1.23
               Mean episode length: 245.63
    Episode_Reward/reaching_object: 0.2735
     Episode_Reward/lifting_object: -0.0853
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.60s
                      Time elapsed: 00:04:16
                               ETA: 01:17:04

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 42042 steps/s (collection: 2.223s, learning 0.115s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2285
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.7160
                       Mean reward: 0.83
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.2678
     Episode_Reward/lifting_object: -0.0487
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.34s
                      Time elapsed: 00:04:18
                               ETA: 01:17:00

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 41821 steps/s (collection: 2.180s, learning 0.171s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3353
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.7925
                       Mean reward: 0.77
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.2596
     Episode_Reward/lifting_object: -0.1117
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.35s
                      Time elapsed: 00:04:20
                               ETA: 01:16:56

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 42873 steps/s (collection: 2.176s, learning 0.117s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 44.8511
                       Mean reward: 1.23
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.2700
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.29s
                      Time elapsed: 00:04:23
                               ETA: 01:16:51

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 43464 steps/s (collection: 2.132s, learning 0.130s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.0521
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.9221
                       Mean reward: -0.11
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: -0.1193
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.26s
                      Time elapsed: 00:04:25
                               ETA: 01:16:45

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 41984 steps/s (collection: 2.180s, learning 0.161s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.4947
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.9598
                       Mean reward: -1.21
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 0.2987
     Episode_Reward/lifting_object: -0.2944
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.34s
                      Time elapsed: 00:04:27
                               ETA: 01:16:41

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 43023 steps/s (collection: 2.156s, learning 0.129s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.0048
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.0417
                       Mean reward: 1.13
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 0.3261
     Episode_Reward/lifting_object: -0.1953
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.28s
                      Time elapsed: 00:04:29
                               ETA: 01:16:36

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 42660 steps/s (collection: 2.190s, learning 0.115s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3136
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.1333
                       Mean reward: 1.65
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.3384
     Episode_Reward/lifting_object: -0.0663
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.30s
                      Time elapsed: 00:04:32
                               ETA: 01:16:32

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 44566 steps/s (collection: 2.098s, learning 0.108s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2942
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.1899
                       Mean reward: 1.65
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: -0.1071
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.21s
                      Time elapsed: 00:04:34
                               ETA: 01:16:25

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 45416 steps/s (collection: 2.011s, learning 0.154s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1275
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.2491
                       Mean reward: 1.42
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 0.3526
     Episode_Reward/lifting_object: -0.0415
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.16s
                      Time elapsed: 00:04:36
                               ETA: 01:16:19

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 43530 steps/s (collection: 2.114s, learning 0.144s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1135
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.3173
                       Mean reward: 1.76
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.3668
     Episode_Reward/lifting_object: -0.0485
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.26s
                      Time elapsed: 00:04:38
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 44278 steps/s (collection: 2.105s, learning 0.115s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0337
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.4557
                       Mean reward: 1.76
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.22s
                      Time elapsed: 00:04:41
                               ETA: 01:16:08

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 43954 steps/s (collection: 2.111s, learning 0.125s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.5094
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.5181
                       Mean reward: 0.98
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.3771
     Episode_Reward/lifting_object: -0.1190
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.24s
                      Time elapsed: 00:04:43
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 42476 steps/s (collection: 2.133s, learning 0.182s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0588
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.5522
                       Mean reward: 1.51
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 0.3827
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.31s
                      Time elapsed: 00:04:45
                               ETA: 01:15:58

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 44626 steps/s (collection: 2.065s, learning 0.137s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.6734
                       Mean reward: 1.81
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.3745
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.20s
                      Time elapsed: 00:04:47
                               ETA: 01:15:52

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 39482 steps/s (collection: 2.350s, learning 0.140s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0569
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.7595
                       Mean reward: 1.45
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 0.3732
     Episode_Reward/lifting_object: -0.0439
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.49s
                      Time elapsed: 00:04:50
                               ETA: 01:15:51

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 44789 steps/s (collection: 2.084s, learning 0.111s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0961
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.8223
                       Mean reward: 1.51
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.3549
     Episode_Reward/lifting_object: -0.0337
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.19s
                      Time elapsed: 00:04:52
                               ETA: 01:15:45

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 42172 steps/s (collection: 2.200s, learning 0.131s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.6209
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.8876
                       Mean reward: 1.65
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 0.3454
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.33s
                      Time elapsed: 00:04:54
                               ETA: 01:15:41

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 45557 steps/s (collection: 2.040s, learning 0.118s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0465
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.9254
                       Mean reward: 1.13
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.3551
     Episode_Reward/lifting_object: -0.0729
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.16s
                      Time elapsed: 00:04:57
                               ETA: 01:15:35

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 40930 steps/s (collection: 2.302s, learning 0.100s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5332
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.9938
                       Mean reward: 0.52
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.3504
     Episode_Reward/lifting_object: -0.1346
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.40s
                      Time elapsed: 00:04:59
                               ETA: 01:15:32

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 44233 steps/s (collection: 2.086s, learning 0.137s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.0247
                       Mean reward: 1.73
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 0.3768
     Episode_Reward/lifting_object: -0.0118
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.22s
                      Time elapsed: 00:05:01
                               ETA: 01:15:27

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 44625 steps/s (collection: 2.083s, learning 0.120s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.4189
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 46.1019
                       Mean reward: 1.46
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 0.3647
     Episode_Reward/lifting_object: -0.0320
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.20s
                      Time elapsed: 00:05:03
                               ETA: 01:15:21

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 41119 steps/s (collection: 2.260s, learning 0.131s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0462
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.1350
                       Mean reward: 1.65
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 0.3839
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.39s
                      Time elapsed: 00:05:06
                               ETA: 01:15:19

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 38558 steps/s (collection: 2.345s, learning 0.205s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1064
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.2319
                       Mean reward: 1.85
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 0.3774
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.55s
                      Time elapsed: 00:05:08
                               ETA: 01:15:18

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 42671 steps/s (collection: 2.139s, learning 0.165s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1395
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.2685
                       Mean reward: 1.17
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 0.3823
     Episode_Reward/lifting_object: -0.0346
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.30s
                      Time elapsed: 00:05:11
                               ETA: 01:15:14

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 41412 steps/s (collection: 2.264s, learning 0.110s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.4613
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.3291
                       Mean reward: 0.96
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: -0.0835
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.37s
                      Time elapsed: 00:05:13
                               ETA: 01:15:11

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 38326 steps/s (collection: 2.383s, learning 0.182s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.4454
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.3740
                       Mean reward: 1.71
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 0.3765
     Episode_Reward/lifting_object: -0.0475
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.56s
                      Time elapsed: 00:05:16
                               ETA: 01:15:11

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 43470 steps/s (collection: 2.115s, learning 0.146s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1642
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 46.4226
                       Mean reward: 1.72
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 0.3678
     Episode_Reward/lifting_object: -0.0561
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.26s
                      Time elapsed: 00:05:18
                               ETA: 01:15:07

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 41370 steps/s (collection: 2.244s, learning 0.133s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3977
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.4757
                       Mean reward: 1.40
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.3476
     Episode_Reward/lifting_object: -0.0677
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.38s
                      Time elapsed: 00:05:20
                               ETA: 01:15:04

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 43341 steps/s (collection: 2.123s, learning 0.145s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.5295
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.5167
                       Mean reward: 1.62
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 0.3432
     Episode_Reward/lifting_object: -0.0517
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.27s
                      Time elapsed: 00:05:22
                               ETA: 01:14:59

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 43499 steps/s (collection: 2.134s, learning 0.126s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.5506
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5685
                       Mean reward: 0.63
               Mean episode length: 217.12
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: -0.0377
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.26s
                      Time elapsed: 00:05:25
                               ETA: 01:14:55

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 44184 steps/s (collection: 2.113s, learning 0.112s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1740
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.6203
                       Mean reward: 1.69
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 0.3449
     Episode_Reward/lifting_object: -0.0288
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.22s
                      Time elapsed: 00:05:27
                               ETA: 01:14:50

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 42216 steps/s (collection: 2.132s, learning 0.196s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 46.6994
                       Mean reward: 1.74
               Mean episode length: 212.71
    Episode_Reward/reaching_object: 0.3555
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.33s
                      Time elapsed: 00:05:29
                               ETA: 01:14:46

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 40645 steps/s (collection: 2.285s, learning 0.134s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1147
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 46.8071
                       Mean reward: 1.62
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 0.3476
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.42s
                      Time elapsed: 00:05:32
                               ETA: 01:14:44

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 35091 steps/s (collection: 2.579s, learning 0.223s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0889
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.8378
                       Mean reward: 1.84
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 0.3793
     Episode_Reward/lifting_object: -0.0310
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.80s
                      Time elapsed: 00:05:34
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 44081 steps/s (collection: 2.096s, learning 0.134s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 46.9361
                       Mean reward: 2.02
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 0.4024
     Episode_Reward/lifting_object: -0.0050
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.23s
                      Time elapsed: 00:05:37
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 42098 steps/s (collection: 2.216s, learning 0.119s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0379
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.0154
                       Mean reward: 1.90
               Mean episode length: 223.12
    Episode_Reward/reaching_object: 0.4160
     Episode_Reward/lifting_object: 0.0018
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.34s
                      Time elapsed: 00:05:39
                               ETA: 01:14:39

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 41186 steps/s (collection: 2.192s, learning 0.195s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0558
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.0522
                       Mean reward: 1.94
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.4449
     Episode_Reward/lifting_object: -0.0255
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.39s
                      Time elapsed: 00:05:41
                               ETA: 01:14:36

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 44941 steps/s (collection: 2.067s, learning 0.120s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0683
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.0934
                       Mean reward: 1.83
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.4424
     Episode_Reward/lifting_object: -0.0156
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.19s
                      Time elapsed: 00:05:44
                               ETA: 01:14:31

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 43926 steps/s (collection: 2.114s, learning 0.124s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.2335
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.1333
                       Mean reward: 2.04
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 0.4359
     Episode_Reward/lifting_object: -0.0155
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.24s
                      Time elapsed: 00:05:46
                               ETA: 01:14:26

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 43566 steps/s (collection: 2.122s, learning 0.134s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4983
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.1656
                       Mean reward: 1.11
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 0.4668
     Episode_Reward/lifting_object: -0.0662
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.26s
                      Time elapsed: 00:05:48
                               ETA: 01:14:22

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 41679 steps/s (collection: 2.218s, learning 0.141s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2067
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.2131
                       Mean reward: 2.22
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 0.4628
     Episode_Reward/lifting_object: -0.0794
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.36s
                      Time elapsed: 00:05:50
                               ETA: 01:14:19

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 43641 steps/s (collection: 2.130s, learning 0.122s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1269
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.2433
                       Mean reward: 2.14
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 0.4714
     Episode_Reward/lifting_object: 0.0184
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.25s
                      Time elapsed: 00:05:53
                               ETA: 01:14:15

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 40003 steps/s (collection: 2.302s, learning 0.155s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1654
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.2966
                       Mean reward: 2.29
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 0.4716
     Episode_Reward/lifting_object: -0.0040
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.46s
                      Time elapsed: 00:05:55
                               ETA: 01:14:13

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 39456 steps/s (collection: 2.371s, learning 0.121s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3787
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.3642
                       Mean reward: 1.79
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 0.4784
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.49s
                      Time elapsed: 00:05:58
                               ETA: 01:14:12

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 41705 steps/s (collection: 2.215s, learning 0.142s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.7393
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.3945
                       Mean reward: 1.15
               Mean episode length: 225.99
    Episode_Reward/reaching_object: 0.5014
     Episode_Reward/lifting_object: -0.0948
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.36s
                      Time elapsed: 00:06:00
                               ETA: 01:14:09

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 40481 steps/s (collection: 2.298s, learning 0.130s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.7728
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.4255
                       Mean reward: 2.46
               Mean episode length: 214.24
    Episode_Reward/reaching_object: 0.5013
     Episode_Reward/lifting_object: -0.0097
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.43s
                      Time elapsed: 00:06:02
                               ETA: 01:14:07

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 40320 steps/s (collection: 2.298s, learning 0.140s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1217
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.4669
                       Mean reward: 2.53
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 0.5271
     Episode_Reward/lifting_object: -0.1246
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.44s
                      Time elapsed: 00:06:05
                               ETA: 01:14:05

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 39373 steps/s (collection: 2.254s, learning 0.243s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1316
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 47.5752
                       Mean reward: 2.31
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 0.5381
     Episode_Reward/lifting_object: -0.0049
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.50s
                      Time elapsed: 00:06:07
                               ETA: 01:14:03

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 45226 steps/s (collection: 2.079s, learning 0.095s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.2063
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.6861
                       Mean reward: 3.39
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 0.5673
     Episode_Reward/lifting_object: -0.0048
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.17s
                      Time elapsed: 00:06:10
                               ETA: 01:13:58

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 44302 steps/s (collection: 2.122s, learning 0.097s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.5160
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.7139
                       Mean reward: 2.78
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 0.5862
     Episode_Reward/lifting_object: -0.0817
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.22s
                      Time elapsed: 00:06:12
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 42359 steps/s (collection: 2.223s, learning 0.098s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1780
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.7495
                       Mean reward: 2.68
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.6188
     Episode_Reward/lifting_object: -0.0019
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.32s
                      Time elapsed: 00:06:14
                               ETA: 01:13:50

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 41452 steps/s (collection: 2.226s, learning 0.146s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0660
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.8405
                       Mean reward: 2.70
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.6244
     Episode_Reward/lifting_object: -0.0019
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.37s
                      Time elapsed: 00:06:16
                               ETA: 01:13:47

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 45040 steps/s (collection: 2.083s, learning 0.100s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3007
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.9159
                       Mean reward: 2.72
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 0.6316
     Episode_Reward/lifting_object: -0.0588
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.18s
                      Time elapsed: 00:06:19
                               ETA: 01:13:42

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 45164 steps/s (collection: 2.065s, learning 0.112s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3244
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.9442
                       Mean reward: 2.32
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.6554
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.18s
                      Time elapsed: 00:06:21
                               ETA: 01:13:37

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 39313 steps/s (collection: 2.380s, learning 0.120s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.4115
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.9824
                       Mean reward: 2.41
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: -0.1091
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.50s
                      Time elapsed: 00:06:23
                               ETA: 01:13:36

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 44627 steps/s (collection: 2.054s, learning 0.149s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.7084
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.0270
                       Mean reward: 3.02
               Mean episode length: 237.81
    Episode_Reward/reaching_object: 0.6383
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.20s
                      Time elapsed: 00:06:26
                               ETA: 01:13:32

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 45429 steps/s (collection: 2.041s, learning 0.123s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.5543
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.0542
                       Mean reward: 3.31
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.6427
     Episode_Reward/lifting_object: -0.0096
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.16s
                      Time elapsed: 00:06:28
                               ETA: 01:13:27

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 45361 steps/s (collection: 2.072s, learning 0.095s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1466
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.0850
                       Mean reward: 3.74
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.6527
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.17s
                      Time elapsed: 00:06:30
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 41483 steps/s (collection: 2.172s, learning 0.198s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.5548
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.1233
                       Mean reward: 3.20
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 0.6308
     Episode_Reward/lifting_object: 0.0178
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.37s
                      Time elapsed: 00:06:32
                               ETA: 01:13:19

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 43695 steps/s (collection: 2.137s, learning 0.113s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2710
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.1625
                       Mean reward: 3.20
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: -0.0746
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.25s
                      Time elapsed: 00:06:35
                               ETA: 01:13:15

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 42524 steps/s (collection: 2.153s, learning 0.159s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.7563
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.1938
                       Mean reward: 2.95
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.6512
     Episode_Reward/lifting_object: -0.0810
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.31s
                      Time elapsed: 00:06:37
                               ETA: 01:13:12

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 44958 steps/s (collection: 2.075s, learning 0.111s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2315
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.2208
                       Mean reward: 2.87
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 0.6545
     Episode_Reward/lifting_object: -0.1580
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.19s
                      Time elapsed: 00:06:39
                               ETA: 01:13:07

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 44010 steps/s (collection: 2.127s, learning 0.106s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2931
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.2608
                       Mean reward: 3.29
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.6664
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.23s
                      Time elapsed: 00:06:41
                               ETA: 01:13:03

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 41986 steps/s (collection: 2.170s, learning 0.172s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1489
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.2874
                       Mean reward: 2.78
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 0.6421
     Episode_Reward/lifting_object: -0.0274
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.34s
                      Time elapsed: 00:06:44
                               ETA: 01:13:00

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 43959 steps/s (collection: 2.113s, learning 0.123s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0871
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.3436
                       Mean reward: 3.01
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 0.6618
     Episode_Reward/lifting_object: -0.0433
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.24s
                      Time elapsed: 00:06:46
                               ETA: 01:12:56

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 42185 steps/s (collection: 2.189s, learning 0.142s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2170
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.4005
                       Mean reward: 2.76
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.6890
     Episode_Reward/lifting_object: -0.0011
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.33s
                      Time elapsed: 00:06:48
                               ETA: 01:12:53

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 42593 steps/s (collection: 2.192s, learning 0.116s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.3291
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.4929
                       Mean reward: 3.60
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.6957
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.31s
                      Time elapsed: 00:06:50
                               ETA: 01:12:49

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 45573 steps/s (collection: 2.020s, learning 0.138s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1402
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.5182
                       Mean reward: 3.50
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 0.7151
     Episode_Reward/lifting_object: 0.0265
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.16s
                      Time elapsed: 00:06:53
                               ETA: 01:12:45

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 43686 steps/s (collection: 2.143s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1799
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.5626
                       Mean reward: 2.87
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.7202
     Episode_Reward/lifting_object: -0.0868
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.25s
                      Time elapsed: 00:06:55
                               ETA: 01:12:41

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 31654 steps/s (collection: 2.807s, learning 0.299s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2170
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.5918
                       Mean reward: 3.37
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.6831
     Episode_Reward/lifting_object: 0.0336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 3.11s
                      Time elapsed: 00:06:58
                               ETA: 01:12:46

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 27865 steps/s (collection: 3.380s, learning 0.148s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4536
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.6107
                       Mean reward: 3.34
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.7098
     Episode_Reward/lifting_object: 0.0593
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 3.53s
                      Time elapsed: 00:07:01
                               ETA: 01:12:55

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 43577 steps/s (collection: 2.131s, learning 0.125s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1003
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 48.6415
                       Mean reward: 2.85
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 0.6830
     Episode_Reward/lifting_object: 0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.26s
                      Time elapsed: 00:07:04
                               ETA: 01:12:51

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 43735 steps/s (collection: 2.111s, learning 0.137s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3309
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.6720
                       Mean reward: 3.82
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.7175
     Episode_Reward/lifting_object: 0.0732
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.25s
                      Time elapsed: 00:07:06
                               ETA: 01:12:48

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 43699 steps/s (collection: 2.135s, learning 0.115s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.4668
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.6962
                       Mean reward: 4.28
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.7023
     Episode_Reward/lifting_object: 0.1168
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.25s
                      Time elapsed: 00:07:08
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 45217 steps/s (collection: 2.051s, learning 0.123s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.0947
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.7289
                       Mean reward: 3.33
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 0.7133
     Episode_Reward/lifting_object: 0.0714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.17s
                      Time elapsed: 00:07:10
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 45696 steps/s (collection: 2.040s, learning 0.111s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3561
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.7671
                       Mean reward: 3.88
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 0.0369
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.15s
                      Time elapsed: 00:07:13
                               ETA: 01:12:34

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 45243 steps/s (collection: 2.054s, learning 0.119s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.6556
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.8010
                       Mean reward: 3.76
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.6928
     Episode_Reward/lifting_object: 0.0553
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.17s
                      Time elapsed: 00:07:15
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 45757 steps/s (collection: 2.042s, learning 0.107s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2961
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.8287
                       Mean reward: 3.41
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 0.7036
     Episode_Reward/lifting_object: 0.0539
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.15s
                      Time elapsed: 00:07:17
                               ETA: 01:12:25

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 44270 steps/s (collection: 2.119s, learning 0.102s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.5202
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.8507
                       Mean reward: 3.69
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 0.0456
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.22s
                      Time elapsed: 00:07:19
                               ETA: 01:12:21

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 44892 steps/s (collection: 2.068s, learning 0.122s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.6356
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.8740
                       Mean reward: 4.45
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 0.7289
     Episode_Reward/lifting_object: 0.0894
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.19s
                      Time elapsed: 00:07:21
                               ETA: 01:12:16

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 45452 steps/s (collection: 2.052s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4348
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.8995
                       Mean reward: 4.49
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.7273
     Episode_Reward/lifting_object: 0.0333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.16s
                      Time elapsed: 00:07:23
                               ETA: 01:12:12

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 46580 steps/s (collection: 2.010s, learning 0.100s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2724
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 48.9711
                       Mean reward: 3.19
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 0.7175
     Episode_Reward/lifting_object: -0.0220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.11s
                      Time elapsed: 00:07:26
                               ETA: 01:12:07

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 46345 steps/s (collection: 2.020s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2902
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.0667
                       Mean reward: 4.06
               Mean episode length: 232.62
    Episode_Reward/reaching_object: 0.7161
     Episode_Reward/lifting_object: 0.0419
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.12s
                      Time elapsed: 00:07:28
                               ETA: 01:12:02

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 45728 steps/s (collection: 2.037s, learning 0.113s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3527
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.0949
                       Mean reward: 4.19
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.7399
     Episode_Reward/lifting_object: 0.0256
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.15s
                      Time elapsed: 00:07:30
                               ETA: 01:11:57

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 44323 steps/s (collection: 2.111s, learning 0.107s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.8404
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.1154
                       Mean reward: 3.90
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 0.0933
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.22s
                      Time elapsed: 00:07:32
                               ETA: 01:11:53

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 45261 steps/s (collection: 2.044s, learning 0.128s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2559
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.1430
                       Mean reward: 4.17
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 0.7166
     Episode_Reward/lifting_object: 0.1565
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.17s
                      Time elapsed: 00:07:34
                               ETA: 01:11:49

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 45303 steps/s (collection: 2.056s, learning 0.114s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.6629
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.2037
                       Mean reward: 4.62
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.7042
     Episode_Reward/lifting_object: 0.1166
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.17s
                      Time elapsed: 00:07:36
                               ETA: 01:11:44

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 45027 steps/s (collection: 2.050s, learning 0.134s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5196
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.2283
                       Mean reward: 4.48
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 0.1174
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.18s
                      Time elapsed: 00:07:39
                               ETA: 01:11:40

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 44107 steps/s (collection: 2.122s, learning 0.107s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.4168
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 49.2820
                       Mean reward: 5.08
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.7341
     Episode_Reward/lifting_object: 0.1847
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.23s
                      Time elapsed: 00:07:41
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 43469 steps/s (collection: 2.138s, learning 0.124s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.8639
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.3101
                       Mean reward: 3.70
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.7136
     Episode_Reward/lifting_object: 0.0667
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.26s
                      Time elapsed: 00:07:43
                               ETA: 01:11:33

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 46408 steps/s (collection: 2.021s, learning 0.098s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3558
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 49.3321
                       Mean reward: 2.54
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 0.0421
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.12s
                      Time elapsed: 00:07:45
                               ETA: 01:11:28

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 46940 steps/s (collection: 1.991s, learning 0.104s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4051
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.3809
                       Mean reward: 4.13
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 0.1071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.09s
                      Time elapsed: 00:07:47
                               ETA: 01:11:23

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 47350 steps/s (collection: 1.965s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.8585
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.4018
                       Mean reward: 3.69
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.7010
     Episode_Reward/lifting_object: 0.0295
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.08s
                      Time elapsed: 00:07:49
                               ETA: 01:11:18

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 44650 steps/s (collection: 2.091s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4134
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.4215
                       Mean reward: 3.34
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.7006
     Episode_Reward/lifting_object: 0.1132
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.20s
                      Time elapsed: 00:07:52
                               ETA: 01:11:14

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 45331 steps/s (collection: 2.066s, learning 0.103s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.4679
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 49.4823
                       Mean reward: 4.20
               Mean episode length: 234.21
    Episode_Reward/reaching_object: 0.6886
     Episode_Reward/lifting_object: 0.1120
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.17s
                      Time elapsed: 00:07:54
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 45312 steps/s (collection: 2.062s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.6636
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 49.5335
                       Mean reward: 4.10
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.6826
     Episode_Reward/lifting_object: 0.1611
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.17s
                      Time elapsed: 00:07:56
                               ETA: 01:11:06

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 44466 steps/s (collection: 2.109s, learning 0.102s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.7909
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.5694
                       Mean reward: 4.71
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 0.1812
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.21s
                      Time elapsed: 00:07:58
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 45643 steps/s (collection: 2.048s, learning 0.106s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.2885
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.5947
                       Mean reward: 4.49
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: 0.0691
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.15s
                      Time elapsed: 00:08:00
                               ETA: 01:10:58

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 47299 steps/s (collection: 1.979s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.6588
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 49.6525
                       Mean reward: 4.34
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 0.1927
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.08s
                      Time elapsed: 00:08:02
                               ETA: 01:10:53

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 47261 steps/s (collection: 1.971s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.7287
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.7083
                       Mean reward: 5.12
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 0.1789
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.08s
                      Time elapsed: 00:08:04
                               ETA: 01:10:48

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 47811 steps/s (collection: 1.932s, learning 0.124s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.8788
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.7283
                       Mean reward: 4.74
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 0.2871
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.06s
                      Time elapsed: 00:08:06
                               ETA: 01:10:43

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 46964 steps/s (collection: 1.975s, learning 0.118s)
             Mean action noise std: 1.64
          Mean value_function loss: 2.3494
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.7804
                       Mean reward: 4.90
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 0.1398
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.09s
                      Time elapsed: 00:08:09
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 44238 steps/s (collection: 2.105s, learning 0.117s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.6149
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.8043
                       Mean reward: 4.53
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 0.1233
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.22s
                      Time elapsed: 00:08:11
                               ETA: 01:10:35

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 45534 steps/s (collection: 2.060s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.7646
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 49.8552
                       Mean reward: 4.56
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 0.1752
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.16s
                      Time elapsed: 00:08:13
                               ETA: 01:10:31

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 44091 steps/s (collection: 2.111s, learning 0.118s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.5204
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 49.9328
                       Mean reward: 5.03
               Mean episode length: 233.40
    Episode_Reward/reaching_object: 0.7398
     Episode_Reward/lifting_object: 0.1865
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.23s
                      Time elapsed: 00:08:15
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 42881 steps/s (collection: 2.167s, learning 0.125s)
             Mean action noise std: 1.66
          Mean value_function loss: 2.8300
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.9670
                       Mean reward: 5.04
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 0.2721
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.29s
                      Time elapsed: 00:08:17
                               ETA: 01:10:24

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 39712 steps/s (collection: 2.251s, learning 0.224s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.5206
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 49.9897
                       Mean reward: 5.34
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 0.2051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.48s
                      Time elapsed: 00:08:20
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 31471 steps/s (collection: 2.995s, learning 0.129s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.6336
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.0480
                       Mean reward: 2.42
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 0.1094
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 3.12s
                      Time elapsed: 00:08:23
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 45084 steps/s (collection: 2.078s, learning 0.103s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.6341
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.0712
                       Mean reward: 3.57
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 0.2525
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.18s
                      Time elapsed: 00:08:25
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 45291 steps/s (collection: 2.069s, learning 0.101s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.6723
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 50.1154
                       Mean reward: 5.01
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 0.2815
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.17s
                      Time elapsed: 00:08:27
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 45426 steps/s (collection: 2.052s, learning 0.112s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.6087
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 50.1486
                       Mean reward: 5.63
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 0.2991
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.16s
                      Time elapsed: 00:08:30
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 43286 steps/s (collection: 2.123s, learning 0.148s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.6590
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.1714
                       Mean reward: 4.28
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 0.1664
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.27s
                      Time elapsed: 00:08:32
                               ETA: 01:10:12

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 41150 steps/s (collection: 2.269s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.8179
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.2010
                       Mean reward: 5.10
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 0.3042
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.39s
                      Time elapsed: 00:08:34
                               ETA: 01:10:10

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 43891 steps/s (collection: 2.115s, learning 0.125s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.3188
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.2298
                       Mean reward: 5.36
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 0.1927
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.24s
                      Time elapsed: 00:08:36
                               ETA: 01:10:06

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 42656 steps/s (collection: 2.194s, learning 0.111s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.0311
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.2532
                       Mean reward: 4.40
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 0.2488
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.30s
                      Time elapsed: 00:08:39
                               ETA: 01:10:03

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 42354 steps/s (collection: 2.195s, learning 0.126s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2095
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.3019
                       Mean reward: 5.37
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.7132
     Episode_Reward/lifting_object: 0.3041
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.32s
                      Time elapsed: 00:08:41
                               ETA: 01:10:01

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 44023 steps/s (collection: 2.111s, learning 0.122s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.9525
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.3433
                       Mean reward: 3.98
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 0.2668
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.23s
                      Time elapsed: 00:08:43
                               ETA: 01:09:57

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 43138 steps/s (collection: 2.140s, learning 0.139s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.5178
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.3678
                       Mean reward: 2.48
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 0.6999
     Episode_Reward/lifting_object: 0.1706
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.28s
                      Time elapsed: 00:08:46
                               ETA: 01:09:54

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 44720 steps/s (collection: 2.081s, learning 0.117s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.8130
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.3917
                       Mean reward: 3.91
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: 0.2482
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.20s
                      Time elapsed: 00:08:48
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 37931 steps/s (collection: 2.472s, learning 0.120s)
             Mean action noise std: 1.69
          Mean value_function loss: 4.8302
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.4543
                       Mean reward: 2.67
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 0.0640
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.59s
                      Time elapsed: 00:08:50
                               ETA: 01:09:50

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 40745 steps/s (collection: 2.277s, learning 0.136s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.1335
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.4789
                       Mean reward: 4.83
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.7273
     Episode_Reward/lifting_object: 0.2532
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.41s
                      Time elapsed: 00:08:53
                               ETA: 01:09:48

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 40582 steps/s (collection: 2.320s, learning 0.103s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.3186
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 50.5361
                       Mean reward: 4.05
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.7268
     Episode_Reward/lifting_object: 0.3778
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.42s
                      Time elapsed: 00:08:55
                               ETA: 01:09:46

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 38699 steps/s (collection: 2.400s, learning 0.140s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.7328
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.5708
                       Mean reward: 4.02
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: 0.2290
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.54s
                      Time elapsed: 00:08:58
                               ETA: 01:09:45

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 39531 steps/s (collection: 2.326s, learning 0.161s)
             Mean action noise std: 1.70
          Mean value_function loss: 7.0712
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.5879
                       Mean reward: 5.62
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 0.7205
     Episode_Reward/lifting_object: 0.4750
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.49s
                      Time elapsed: 00:09:00
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 44284 steps/s (collection: 2.119s, learning 0.101s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.2095
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 50.6099
                       Mean reward: 4.32
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.7346
     Episode_Reward/lifting_object: 0.2783
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.22s
                      Time elapsed: 00:09:03
                               ETA: 01:09:41

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 44981 steps/s (collection: 2.081s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.6543
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.6387
                       Mean reward: 4.52
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 0.6982
     Episode_Reward/lifting_object: 0.1941
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.19s
                      Time elapsed: 00:09:05
                               ETA: 01:09:37

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 43346 steps/s (collection: 2.149s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.7969
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.6625
                       Mean reward: 2.71
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 0.6722
     Episode_Reward/lifting_object: 0.1360
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.27s
                      Time elapsed: 00:09:07
                               ETA: 01:09:34

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 40344 steps/s (collection: 2.309s, learning 0.128s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.0757
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 50.6978
                       Mean reward: 3.60
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.7034
     Episode_Reward/lifting_object: 0.3772
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.44s
                      Time elapsed: 00:09:09
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 43225 steps/s (collection: 2.142s, learning 0.132s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.8519
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 50.7319
                       Mean reward: 4.50
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 0.6883
     Episode_Reward/lifting_object: 0.3298
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.27s
                      Time elapsed: 00:09:12
                               ETA: 01:09:29

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 37591 steps/s (collection: 2.445s, learning 0.170s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.9540
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.8036
                       Mean reward: 5.71
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 0.2705
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.62s
                      Time elapsed: 00:09:14
                               ETA: 01:09:29

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 34564 steps/s (collection: 2.681s, learning 0.164s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.9242
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.8282
                       Mean reward: 3.56
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 0.6601
     Episode_Reward/lifting_object: 0.1751
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.84s
                      Time elapsed: 00:09:17
                               ETA: 01:09:30

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 37311 steps/s (collection: 2.503s, learning 0.132s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.2744
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 50.8874
                       Mean reward: 4.64
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 0.6559
     Episode_Reward/lifting_object: 0.3709
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.63s
                      Time elapsed: 00:09:20
                               ETA: 01:09:30

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 40259 steps/s (collection: 2.318s, learning 0.124s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.9011
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 50.9244
                       Mean reward: 4.12
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 0.6255
     Episode_Reward/lifting_object: 0.2675
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.44s
                      Time elapsed: 00:09:22
                               ETA: 01:09:28

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 42697 steps/s (collection: 2.171s, learning 0.131s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.5570
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.9740
                       Mean reward: 5.47
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 0.6378
     Episode_Reward/lifting_object: 0.3422
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.30s
                      Time elapsed: 00:09:25
                               ETA: 01:09:25

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 40501 steps/s (collection: 2.307s, learning 0.120s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.8752
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.9980
                       Mean reward: 5.33
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.6472
     Episode_Reward/lifting_object: 0.4226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.43s
                      Time elapsed: 00:09:27
                               ETA: 01:09:23

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 40280 steps/s (collection: 2.328s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.1603
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 51.0675
                       Mean reward: 5.00
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 0.6628
     Episode_Reward/lifting_object: 0.3896
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.44s
                      Time elapsed: 00:09:29
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 44450 steps/s (collection: 2.095s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.8067
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.1527
                       Mean reward: 5.79
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 0.6352
     Episode_Reward/lifting_object: 0.3509
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.21s
                      Time elapsed: 00:09:32
                               ETA: 01:09:18

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 41697 steps/s (collection: 2.247s, learning 0.111s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.7847
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.1727
                       Mean reward: 5.30
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.6406
     Episode_Reward/lifting_object: 0.3043
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.36s
                      Time elapsed: 00:09:34
                               ETA: 01:09:15

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 43827 steps/s (collection: 2.134s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 2.0736
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.2207
                       Mean reward: 5.86
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.6293
     Episode_Reward/lifting_object: 0.3705
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.24s
                      Time elapsed: 00:09:36
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 43681 steps/s (collection: 2.124s, learning 0.127s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.8853
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 51.2634
                       Mean reward: 6.36
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.6380
     Episode_Reward/lifting_object: 0.2435
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.25s
                      Time elapsed: 00:09:38
                               ETA: 01:09:09

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 45696 steps/s (collection: 2.035s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.9229
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.2856
                       Mean reward: 6.50
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 0.6437
     Episode_Reward/lifting_object: 0.5029
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.15s
                      Time elapsed: 00:09:41
                               ETA: 01:09:05

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 43547 steps/s (collection: 2.124s, learning 0.134s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.7764
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 51.3364
                       Mean reward: 3.72
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 0.6492
     Episode_Reward/lifting_object: 0.3770
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.26s
                      Time elapsed: 00:09:43
                               ETA: 01:09:02

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 44858 steps/s (collection: 2.079s, learning 0.113s)
             Mean action noise std: 1.75
          Mean value_function loss: 2.0706
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.3803
                       Mean reward: 4.73
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 0.5636
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.19s
                      Time elapsed: 00:09:45
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 44999 steps/s (collection: 2.075s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.9243
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.3997
                       Mean reward: 4.96
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 0.3917
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.18s
                      Time elapsed: 00:09:47
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 45167 steps/s (collection: 2.074s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 2.0965
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.4512
                       Mean reward: 3.54
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 0.6389
     Episode_Reward/lifting_object: 0.2883
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.18s
                      Time elapsed: 00:09:49
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 45007 steps/s (collection: 2.089s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 2.0951
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.5105
                       Mean reward: 6.65
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 0.4525
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.18s
                      Time elapsed: 00:09:52
                               ETA: 01:08:48

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 42988 steps/s (collection: 2.172s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 2.0713
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.5364
                       Mean reward: 5.42
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 0.4846
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.29s
                      Time elapsed: 00:09:54
                               ETA: 01:08:45

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 44945 steps/s (collection: 2.069s, learning 0.118s)
             Mean action noise std: 1.76
          Mean value_function loss: 3.5859
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.5722
                       Mean reward: 5.24
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 0.4681
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.19s
                      Time elapsed: 00:09:56
                               ETA: 01:08:41

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 45235 steps/s (collection: 2.060s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 2.5108
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.6312
                       Mean reward: 7.71
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 0.6362
     Episode_Reward/lifting_object: 0.5056
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.17s
                      Time elapsed: 00:09:58
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 44066 steps/s (collection: 2.099s, learning 0.132s)
             Mean action noise std: 1.77
          Mean value_function loss: 2.2420
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.6740
                       Mean reward: 4.76
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 0.6336
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.23s
                      Time elapsed: 00:10:00
                               ETA: 01:08:34

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 44419 steps/s (collection: 2.108s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.6956
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.6903
                       Mean reward: 6.93
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.6654
     Episode_Reward/lifting_object: 0.6214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.21s
                      Time elapsed: 00:10:03
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 44450 steps/s (collection: 2.093s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.8074
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.7272
                       Mean reward: 5.59
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 0.4761
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.21s
                      Time elapsed: 00:10:05
                               ETA: 01:08:28

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 45794 steps/s (collection: 2.049s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 2.2708
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.7507
                       Mean reward: 3.35
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 0.3116
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.15s
                      Time elapsed: 00:10:07
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 45777 steps/s (collection: 2.039s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 3.8555
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.7759
                       Mean reward: 4.99
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 0.6474
     Episode_Reward/lifting_object: 0.5264
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.15s
                      Time elapsed: 00:10:09
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 46547 steps/s (collection: 1.996s, learning 0.116s)
             Mean action noise std: 1.78
          Mean value_function loss: 3.8502
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.8015
                       Mean reward: 4.75
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 0.5282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.11s
                      Time elapsed: 00:10:11
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 44167 steps/s (collection: 2.093s, learning 0.133s)
             Mean action noise std: 1.78
          Mean value_function loss: 5.1238
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.8226
                       Mean reward: 6.62
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 0.6669
     Episode_Reward/lifting_object: 0.4218
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.23s
                      Time elapsed: 00:10:14
                               ETA: 01:08:13

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 45239 steps/s (collection: 2.061s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 7.7299
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.8794
                       Mean reward: -0.46
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 0.4320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.17s
                      Time elapsed: 00:10:16
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 44415 steps/s (collection: 2.088s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 3.0977
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.9016
                       Mean reward: 5.65
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 0.6855
     Episode_Reward/lifting_object: 0.4358
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.21s
                      Time elapsed: 00:10:18
                               ETA: 01:08:06

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 44173 steps/s (collection: 2.116s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.6556
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.9487
                       Mean reward: 6.61
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 0.6966
     Episode_Reward/lifting_object: 0.5799
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.23s
                      Time elapsed: 00:10:20
                               ETA: 01:08:03

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 43127 steps/s (collection: 2.162s, learning 0.118s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.2818
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.9711
                       Mean reward: 4.70
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 0.3275
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.28s
                      Time elapsed: 00:10:22
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 43580 steps/s (collection: 2.153s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.6761
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.0212
                       Mean reward: 5.14
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.6812
     Episode_Reward/lifting_object: 0.3731
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.26s
                      Time elapsed: 00:10:25
                               ETA: 01:07:57

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 44999 steps/s (collection: 2.085s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 4.6875
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.0396
                       Mean reward: 3.68
               Mean episode length: 226.62
    Episode_Reward/reaching_object: 0.6536
     Episode_Reward/lifting_object: 0.4827
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.18s
                      Time elapsed: 00:10:27
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 41837 steps/s (collection: 2.231s, learning 0.119s)
             Mean action noise std: 1.80
          Mean value_function loss: 2.7986
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.0827
                       Mean reward: 5.03
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 0.5547
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.35s
                      Time elapsed: 00:10:29
                               ETA: 01:07:51

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 41160 steps/s (collection: 2.253s, learning 0.135s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.7134
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.1086
                       Mean reward: 5.39
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 0.6644
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.39s
                      Time elapsed: 00:10:32
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 42958 steps/s (collection: 2.177s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 5.1176
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.1346
                       Mean reward: 6.99
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 0.6697
     Episode_Reward/lifting_object: 0.6287
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.29s
                      Time elapsed: 00:10:34
                               ETA: 01:07:47

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 44991 steps/s (collection: 2.073s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 2.7483
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.1506
                       Mean reward: 6.41
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 0.6479
     Episode_Reward/lifting_object: 0.3857
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.18s
                      Time elapsed: 00:10:36
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 44176 steps/s (collection: 2.109s, learning 0.116s)
             Mean action noise std: 1.80
          Mean value_function loss: 3.8485
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.1727
                       Mean reward: 4.90
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 0.5929
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.23s
                      Time elapsed: 00:10:38
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 45215 steps/s (collection: 2.040s, learning 0.134s)
             Mean action noise std: 1.80
          Mean value_function loss: 2.2469
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 52.1943
                       Mean reward: 5.06
               Mean episode length: 221.30
    Episode_Reward/reaching_object: 0.6494
     Episode_Reward/lifting_object: 0.4418
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.17s
                      Time elapsed: 00:10:40
                               ETA: 01:07:37

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 46697 steps/s (collection: 2.012s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 5.4217
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.2313
                       Mean reward: 4.83
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 0.6112
     Episode_Reward/lifting_object: 0.4985
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.11s
                      Time elapsed: 00:10:43
                               ETA: 01:07:33

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 45943 steps/s (collection: 2.036s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 2.4793
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.2481
                       Mean reward: 2.84
               Mean episode length: 223.37
    Episode_Reward/reaching_object: 0.6208
     Episode_Reward/lifting_object: 0.4333
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.14s
                      Time elapsed: 00:10:45
                               ETA: 01:07:29

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 45278 steps/s (collection: 2.061s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 3.1919
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.2844
                       Mean reward: 5.18
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.6402
     Episode_Reward/lifting_object: 0.4542
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.17s
                      Time elapsed: 00:10:47
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 44709 steps/s (collection: 2.090s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 2.0087
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.3130
                       Mean reward: 6.48
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: 0.6282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.20s
                      Time elapsed: 00:10:49
                               ETA: 01:07:22

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 44964 steps/s (collection: 2.064s, learning 0.122s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.3502
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.3592
                       Mean reward: 4.78
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 0.4955
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.19s
                      Time elapsed: 00:10:51
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 45901 steps/s (collection: 2.029s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 3.4728
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.3761
                       Mean reward: 6.51
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 0.4516
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.14s
                      Time elapsed: 00:10:53
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 44736 steps/s (collection: 2.085s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.7966
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.3980
                       Mean reward: 6.18
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 0.6108
     Episode_Reward/lifting_object: 0.5129
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.20s
                      Time elapsed: 00:10:56
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 45969 steps/s (collection: 2.035s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 3.1702
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.4412
                       Mean reward: 3.86
               Mean episode length: 236.70
    Episode_Reward/reaching_object: 0.6426
     Episode_Reward/lifting_object: 0.3266
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.14s
                      Time elapsed: 00:10:58
                               ETA: 01:07:09

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 45093 steps/s (collection: 2.072s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 4.3431
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.4580
                       Mean reward: 3.94
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.6529
     Episode_Reward/lifting_object: 0.6173
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.18s
                      Time elapsed: 00:11:00
                               ETA: 01:07:05

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 45684 steps/s (collection: 2.047s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.3167
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 52.5003
                       Mean reward: 7.01
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 0.6602
     Episode_Reward/lifting_object: 0.7814
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.15s
                      Time elapsed: 00:11:02
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 46417 steps/s (collection: 2.017s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.1682
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.5425
                       Mean reward: 7.31
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.6543
     Episode_Reward/lifting_object: 0.7456
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.12s
                      Time elapsed: 00:11:04
                               ETA: 01:06:58

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 44890 steps/s (collection: 2.070s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 2.3482
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.5908
                       Mean reward: 5.62
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 0.6794
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.19s
                      Time elapsed: 00:11:06
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 44678 steps/s (collection: 2.092s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 5.8961
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.6077
                       Mean reward: 5.54
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.6371
     Episode_Reward/lifting_object: 0.6291
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.20s
                      Time elapsed: 00:11:09
                               ETA: 01:06:52

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 44863 steps/s (collection: 2.075s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 3.5206
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.6430
                       Mean reward: 6.19
               Mean episode length: 228.02
    Episode_Reward/reaching_object: 0.6359
     Episode_Reward/lifting_object: 0.7280
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.19s
                      Time elapsed: 00:11:11
                               ETA: 01:06:48

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 44514 steps/s (collection: 2.093s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 10.2978
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 52.6561
                       Mean reward: 5.74
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 0.6387
     Episode_Reward/lifting_object: 0.5208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.21s
                      Time elapsed: 00:11:13
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 44843 steps/s (collection: 2.057s, learning 0.136s)
             Mean action noise std: 1.84
          Mean value_function loss: 3.5562
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.6678
                       Mean reward: 6.11
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 0.5501
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.19s
                      Time elapsed: 00:11:15
                               ETA: 01:06:42

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 46295 steps/s (collection: 2.008s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 3.8943
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.7047
                       Mean reward: 6.67
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 0.6514
     Episode_Reward/lifting_object: 0.7476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.12s
                      Time elapsed: 00:11:17
                               ETA: 01:06:38

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 44570 steps/s (collection: 2.074s, learning 0.132s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.9879
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.7178
                       Mean reward: 6.51
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.6180
     Episode_Reward/lifting_object: 0.6317
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.21s
                      Time elapsed: 00:11:19
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 45518 steps/s (collection: 2.044s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.6581
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 52.7428
                       Mean reward: 4.68
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 0.6489
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.16s
                      Time elapsed: 00:11:22
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 44900 steps/s (collection: 2.083s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 11.4553
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 52.7543
                       Mean reward: 1.10
               Mean episode length: 220.98
    Episode_Reward/reaching_object: 0.6093
     Episode_Reward/lifting_object: 0.3561
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.19s
                      Time elapsed: 00:11:24
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 44454 steps/s (collection: 2.107s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 2.8791
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.7652
                       Mean reward: 5.48
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 0.5484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.21s
                      Time elapsed: 00:11:26
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 46516 steps/s (collection: 2.012s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 5.0884
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.7840
                       Mean reward: 4.81
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.5848
     Episode_Reward/lifting_object: 0.5229
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.11s
                      Time elapsed: 00:11:28
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 45184 steps/s (collection: 2.070s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 4.0888
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.8017
                       Mean reward: 4.14
               Mean episode length: 219.24
    Episode_Reward/reaching_object: 0.5993
     Episode_Reward/lifting_object: 0.2512
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.18s
                      Time elapsed: 00:11:30
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 45972 steps/s (collection: 2.021s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 5.2253
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.8383
                       Mean reward: 7.34
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 0.4845
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.14s
                      Time elapsed: 00:11:32
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 45021 steps/s (collection: 2.066s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 5.3886
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.8579
                       Mean reward: 7.68
               Mean episode length: 211.01
    Episode_Reward/reaching_object: 0.5900
     Episode_Reward/lifting_object: 0.6693
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.18s
                      Time elapsed: 00:11:35
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 43430 steps/s (collection: 2.146s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 3.8235
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.8782
                       Mean reward: 7.06
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 0.5698
     Episode_Reward/lifting_object: 0.5981
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.26s
                      Time elapsed: 00:11:37
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 43495 steps/s (collection: 2.156s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 4.0680
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 52.8974
                       Mean reward: 2.69
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 0.6817
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.26s
                      Time elapsed: 00:11:39
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 42690 steps/s (collection: 2.162s, learning 0.141s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.8450
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.9097
                       Mean reward: 5.37
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 0.3870
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.30s
                      Time elapsed: 00:11:41
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 43072 steps/s (collection: 2.159s, learning 0.123s)
             Mean action noise std: 1.86
          Mean value_function loss: 6.7316
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 52.9360
                       Mean reward: 3.97
               Mean episode length: 222.50
    Episode_Reward/reaching_object: 0.6190
     Episode_Reward/lifting_object: 0.5091
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.28s
                      Time elapsed: 00:11:44
                               ETA: 01:06:02

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 44411 steps/s (collection: 2.076s, learning 0.138s)
             Mean action noise std: 1.86
          Mean value_function loss: 6.6069
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 52.9463
                       Mean reward: 4.60
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 0.5966
     Episode_Reward/lifting_object: 0.4881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.21s
                      Time elapsed: 00:11:46
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 44041 steps/s (collection: 2.111s, learning 0.122s)
             Mean action noise std: 1.86
          Mean value_function loss: 6.9640
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.9622
                       Mean reward: 5.71
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.6212
     Episode_Reward/lifting_object: 0.3662
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.23s
                      Time elapsed: 00:11:48
                               ETA: 01:05:56

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 45169 steps/s (collection: 2.067s, learning 0.110s)
             Mean action noise std: 1.86
          Mean value_function loss: 3.0157
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.9855
                       Mean reward: 4.77
               Mean episode length: 222.40
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 0.5920
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.18s
                      Time elapsed: 00:11:50
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 44447 steps/s (collection: 2.082s, learning 0.130s)
             Mean action noise std: 1.86
          Mean value_function loss: 10.1738
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.0220
                       Mean reward: 5.32
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.6096
     Episode_Reward/lifting_object: 0.8987
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.21s
                      Time elapsed: 00:11:53
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 45603 steps/s (collection: 2.053s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 4.3656
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.0371
                       Mean reward: 6.07
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 0.6165
     Episode_Reward/lifting_object: 0.5155
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.16s
                      Time elapsed: 00:11:55
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 44242 steps/s (collection: 2.126s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 3.8988
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.0796
                       Mean reward: 5.66
               Mean episode length: 217.34
    Episode_Reward/reaching_object: 0.5653
     Episode_Reward/lifting_object: 0.4515
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.22s
                      Time elapsed: 00:11:57
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 43719 steps/s (collection: 2.139s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 6.9117
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.1289
                       Mean reward: 5.31
               Mean episode length: 208.63
    Episode_Reward/reaching_object: 0.5721
     Episode_Reward/lifting_object: 0.7103
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.25s
                      Time elapsed: 00:11:59
                               ETA: 01:05:41

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 45691 steps/s (collection: 2.047s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 4.6572
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.1544
                       Mean reward: 7.92
               Mean episode length: 213.43
    Episode_Reward/reaching_object: 0.5968
     Episode_Reward/lifting_object: 0.5486
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.15s
                      Time elapsed: 00:12:01
                               ETA: 01:05:37

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 46223 steps/s (collection: 2.034s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 3.7180
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.1660
                       Mean reward: 6.43
               Mean episode length: 214.63
    Episode_Reward/reaching_object: 0.5829
     Episode_Reward/lifting_object: 0.6957
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.13s
                      Time elapsed: 00:12:04
                               ETA: 01:05:34

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 44776 steps/s (collection: 2.083s, learning 0.113s)
             Mean action noise std: 1.87
          Mean value_function loss: 9.9592
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 53.1919
                       Mean reward: 5.19
               Mean episode length: 213.88
    Episode_Reward/reaching_object: 0.5858
     Episode_Reward/lifting_object: 0.7895
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.20s
                      Time elapsed: 00:12:06
                               ETA: 01:05:31

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 44842 steps/s (collection: 2.082s, learning 0.111s)
             Mean action noise std: 1.88
          Mean value_function loss: 4.4602
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.2019
                       Mean reward: 7.08
               Mean episode length: 217.29
    Episode_Reward/reaching_object: 0.5896
     Episode_Reward/lifting_object: 0.9181
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.19s
                      Time elapsed: 00:12:08
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 44087 steps/s (collection: 2.117s, learning 0.113s)
             Mean action noise std: 1.88
          Mean value_function loss: 3.9631
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2297
                       Mean reward: 4.64
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 0.5792
     Episode_Reward/lifting_object: 0.6517
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.23s
                      Time elapsed: 00:12:10
                               ETA: 01:05:25

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 45819 steps/s (collection: 2.039s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 4.3982
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.2629
                       Mean reward: 3.88
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 0.5864
     Episode_Reward/lifting_object: 0.5207
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.15s
                      Time elapsed: 00:12:12
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 45100 steps/s (collection: 2.068s, learning 0.112s)
             Mean action noise std: 1.88
          Mean value_function loss: 2.8500
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.3010
                       Mean reward: 8.07
               Mean episode length: 220.27
    Episode_Reward/reaching_object: 0.5851
     Episode_Reward/lifting_object: 0.8693
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.18s
                      Time elapsed: 00:12:14
                               ETA: 01:05:19

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 44684 steps/s (collection: 2.087s, learning 0.113s)
             Mean action noise std: 1.89
          Mean value_function loss: 2.4219
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 53.3324
                       Mean reward: 9.11
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 0.6108
     Episode_Reward/lifting_object: 0.4955
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.20s
                      Time elapsed: 00:12:17
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 44514 steps/s (collection: 2.110s, learning 0.098s)
             Mean action noise std: 1.89
          Mean value_function loss: 6.2447
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 53.3679
                       Mean reward: 4.83
               Mean episode length: 223.25
    Episode_Reward/reaching_object: 0.5944
     Episode_Reward/lifting_object: 0.7008
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.21s
                      Time elapsed: 00:12:19
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 45935 steps/s (collection: 2.044s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 5.7437
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.3766
                       Mean reward: 6.72
               Mean episode length: 220.12
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 0.7194
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.14s
                      Time elapsed: 00:12:21
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 45912 steps/s (collection: 2.046s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 4.2310
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 53.4011
                       Mean reward: 7.20
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 0.5888
     Episode_Reward/lifting_object: 0.5537
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.14s
                      Time elapsed: 00:12:23
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 45770 steps/s (collection: 2.053s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 4.0314
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.4181
                       Mean reward: 3.65
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 0.6347
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.15s
                      Time elapsed: 00:12:25
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 44515 steps/s (collection: 2.111s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 3.0836
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 53.4547
                       Mean reward: 6.47
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 0.6043
     Episode_Reward/lifting_object: 0.7335
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.21s
                      Time elapsed: 00:12:28
                               ETA: 01:05:00

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 45904 steps/s (collection: 2.043s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 4.0113
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 53.4880
                       Mean reward: 8.56
               Mean episode length: 215.05
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 1.0357
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.14s
                      Time elapsed: 00:12:30
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 45587 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 7.7116
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.5381
                       Mean reward: 6.87
               Mean episode length: 210.90
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 0.9627
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.16s
                      Time elapsed: 00:12:32
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 45540 steps/s (collection: 2.059s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 8.5951
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.5876
                       Mean reward: 3.71
               Mean episode length: 208.60
    Episode_Reward/reaching_object: 0.5745
     Episode_Reward/lifting_object: 0.6701
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.16s
                      Time elapsed: 00:12:34
                               ETA: 01:04:50

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 45502 steps/s (collection: 2.060s, learning 0.101s)
             Mean action noise std: 1.91
          Mean value_function loss: 5.7020
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 53.6310
                       Mean reward: 6.35
               Mean episode length: 216.89
    Episode_Reward/reaching_object: 0.5914
     Episode_Reward/lifting_object: 0.7044
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.16s
                      Time elapsed: 00:12:36
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 45777 steps/s (collection: 2.056s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 4.3260
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 53.6765
                       Mean reward: 4.32
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 0.9749
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.15s
                      Time elapsed: 00:12:38
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 44924 steps/s (collection: 2.088s, learning 0.100s)
             Mean action noise std: 1.91
          Mean value_function loss: 10.8666
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 53.6995
                       Mean reward: 7.06
               Mean episode length: 222.02
    Episode_Reward/reaching_object: 0.6015
     Episode_Reward/lifting_object: 0.8608
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.19s
                      Time elapsed: 00:12:40
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 45677 steps/s (collection: 2.051s, learning 0.101s)
             Mean action noise std: 1.91
          Mean value_function loss: 4.1525
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.7100
                       Mean reward: 8.39
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 0.6231
     Episode_Reward/lifting_object: 0.6018
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.15s
                      Time elapsed: 00:12:43
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 43724 steps/s (collection: 2.120s, learning 0.129s)
             Mean action noise std: 1.92
          Mean value_function loss: 5.1902
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.7467
                       Mean reward: 4.33
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 0.6200
     Episode_Reward/lifting_object: 0.6930
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.25s
                      Time elapsed: 00:12:45
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 43793 steps/s (collection: 2.108s, learning 0.137s)
             Mean action noise std: 1.92
          Mean value_function loss: 9.6154
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.7917
                       Mean reward: 0.86
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 0.6473
     Episode_Reward/lifting_object: 0.5384
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.24s
                      Time elapsed: 00:12:47
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 43000 steps/s (collection: 2.180s, learning 0.106s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.5249
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.8033
                       Mean reward: 4.23
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 0.6266
     Episode_Reward/lifting_object: 0.6051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.29s
                      Time elapsed: 00:12:49
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 44584 steps/s (collection: 2.102s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.5307
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 53.8382
                       Mean reward: 5.06
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 0.9314
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.20s
                      Time elapsed: 00:12:52
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18118 steps/s (collection: 5.262s, learning 0.163s)
             Mean action noise std: 1.92
          Mean value_function loss: 12.2276
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.8731
                       Mean reward: 4.75
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 0.6181
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.43s
                      Time elapsed: 00:12:57
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14346 steps/s (collection: 6.713s, learning 0.139s)
             Mean action noise std: 1.93
          Mean value_function loss: 4.9543
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.8854
                       Mean reward: 0.40
               Mean episode length: 220.91
    Episode_Reward/reaching_object: 0.6223
     Episode_Reward/lifting_object: 0.5039
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.85s
                      Time elapsed: 00:13:04
                               ETA: 01:05:00

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14164 steps/s (collection: 6.817s, learning 0.123s)
             Mean action noise std: 1.93
          Mean value_function loss: 6.1396
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9121
                       Mean reward: 8.83
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 0.8420
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.94s
                      Time elapsed: 00:13:11
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 13916 steps/s (collection: 6.933s, learning 0.131s)
             Mean action noise std: 1.93
          Mean value_function loss: 4.8092
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.9361
                       Mean reward: 8.83
               Mean episode length: 220.37
    Episode_Reward/reaching_object: 0.6457
     Episode_Reward/lifting_object: 0.8834
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.06s
                      Time elapsed: 00:13:18
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14312 steps/s (collection: 6.730s, learning 0.139s)
             Mean action noise std: 1.93
          Mean value_function loss: 4.7895
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.9447
                       Mean reward: 7.59
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 0.6037
     Episode_Reward/lifting_object: 0.9475
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.87s
                      Time elapsed: 00:13:25
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 13587 steps/s (collection: 7.105s, learning 0.130s)
             Mean action noise std: 1.93
          Mean value_function loss: 6.2599
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 53.9682
                       Mean reward: 4.91
               Mean episode length: 216.81
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 0.8377
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 7.23s
                      Time elapsed: 00:13:32
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14283 steps/s (collection: 6.759s, learning 0.124s)
             Mean action noise std: 1.93
          Mean value_function loss: 8.1916
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.9774
                       Mean reward: 6.82
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 0.6119
     Episode_Reward/lifting_object: 0.8362
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.88s
                      Time elapsed: 00:13:39
                               ETA: 01:06:42

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13759 steps/s (collection: 7.020s, learning 0.125s)
             Mean action noise std: 1.93
          Mean value_function loss: 6.9303
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9913
                       Mean reward: 5.06
               Mean episode length: 219.55
    Episode_Reward/reaching_object: 0.6254
     Episode_Reward/lifting_object: 0.6438
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.14s
                      Time elapsed: 00:13:46
                               ETA: 01:07:03

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13210 steps/s (collection: 7.339s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 5.3164
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 54.0205
                       Mean reward: 11.04
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 0.6075
     Episode_Reward/lifting_object: 0.9892
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.44s
                      Time elapsed: 00:13:53
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 47856 steps/s (collection: 1.962s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 6.7239
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.0536
                       Mean reward: 5.88
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 0.5964
     Episode_Reward/lifting_object: 0.7573
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.05s
                      Time elapsed: 00:13:56
                               ETA: 01:07:21

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 48317 steps/s (collection: 1.927s, learning 0.108s)
             Mean action noise std: 1.94
          Mean value_function loss: 5.1554
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.0675
                       Mean reward: 8.36
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 0.6056
     Episode_Reward/lifting_object: 0.8071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.03s
                      Time elapsed: 00:13:58
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 48525 steps/s (collection: 1.916s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.5530
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.0876
                       Mean reward: 5.93
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 0.7028
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.03s
                      Time elapsed: 00:14:00
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 46718 steps/s (collection: 1.990s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 4.4701
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.1276
                       Mean reward: 5.72
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 0.6065
     Episode_Reward/lifting_object: 0.5549
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.10s
                      Time elapsed: 00:14:02
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.946s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 7.2446
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.1485
                       Mean reward: 6.88
               Mean episode length: 223.30
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 0.8951
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.04s
                      Time elapsed: 00:14:04
                               ETA: 01:07:03

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 48391 steps/s (collection: 1.937s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 8.0412
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.1838
                       Mean reward: 6.77
               Mean episode length: 214.23
    Episode_Reward/reaching_object: 0.6089
     Episode_Reward/lifting_object: 0.7487
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.03s
                      Time elapsed: 00:14:06
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 48772 steps/s (collection: 1.925s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 6.2193
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.2224
                       Mean reward: 7.11
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.6074
     Episode_Reward/lifting_object: 1.0245
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.02s
                      Time elapsed: 00:14:08
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 48524 steps/s (collection: 1.936s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 7.1627
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 54.2566
                       Mean reward: 5.28
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: 0.9245
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.03s
                      Time elapsed: 00:14:10
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 46377 steps/s (collection: 2.024s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 4.4785
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.2688
                       Mean reward: 5.14
               Mean episode length: 218.53
    Episode_Reward/reaching_object: 0.6069
     Episode_Reward/lifting_object: 0.6776
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.12s
                      Time elapsed: 00:14:12
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 44476 steps/s (collection: 2.097s, learning 0.113s)
             Mean action noise std: 1.96
          Mean value_function loss: 9.3663
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.2978
                       Mean reward: 6.38
               Mean episode length: 209.78
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 0.8264
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.21s
                      Time elapsed: 00:14:14
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 46410 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 4.8641
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 54.3212
                       Mean reward: 4.92
               Mean episode length: 216.77
    Episode_Reward/reaching_object: 0.6045
     Episode_Reward/lifting_object: 0.7700
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.12s
                      Time elapsed: 00:14:16
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 47406 steps/s (collection: 1.980s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 6.1378
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.3574
                       Mean reward: 4.61
               Mean episode length: 218.66
    Episode_Reward/reaching_object: 0.5908
     Episode_Reward/lifting_object: 0.7194
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.07s
                      Time elapsed: 00:14:18
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 47389 steps/s (collection: 1.984s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 20.1619
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3791
                       Mean reward: 6.17
               Mean episode length: 209.63
    Episode_Reward/reaching_object: 0.5865
     Episode_Reward/lifting_object: 0.7578
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.07s
                      Time elapsed: 00:14:20
                               ETA: 01:06:31

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 47102 steps/s (collection: 1.986s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 7.9266
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.3981
                       Mean reward: 11.17
               Mean episode length: 204.60
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 0.6598
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.09s
                      Time elapsed: 00:14:22
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 47585 steps/s (collection: 1.966s, learning 0.100s)
             Mean action noise std: 1.97
          Mean value_function loss: 4.0364
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.4294
                       Mean reward: 9.52
               Mean episode length: 207.90
    Episode_Reward/reaching_object: 0.5900
     Episode_Reward/lifting_object: 1.0565
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.07s
                      Time elapsed: 00:14:25
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 46108 steps/s (collection: 2.020s, learning 0.112s)
             Mean action noise std: 1.97
          Mean value_function loss: 15.8410
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.4441
                       Mean reward: 8.05
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 0.6118
     Episode_Reward/lifting_object: 0.5017
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.13s
                      Time elapsed: 00:14:27
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 46475 steps/s (collection: 2.015s, learning 0.101s)
             Mean action noise std: 1.97
          Mean value_function loss: 8.2866
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.4624
                       Mean reward: 7.70
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 0.5928
     Episode_Reward/lifting_object: 0.7670
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.12s
                      Time elapsed: 00:14:29
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 46446 steps/s (collection: 2.002s, learning 0.114s)
             Mean action noise std: 1.97
          Mean value_function loss: 7.7594
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.4896
                       Mean reward: 5.88
               Mean episode length: 214.98
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 0.5292
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.12s
                      Time elapsed: 00:14:31
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 46142 steps/s (collection: 2.015s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 9.4283
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.5080
                       Mean reward: 5.77
               Mean episode length: 212.24
    Episode_Reward/reaching_object: 0.5777
     Episode_Reward/lifting_object: 1.0145
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.13s
                      Time elapsed: 00:14:33
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 45233 steps/s (collection: 2.061s, learning 0.113s)
             Mean action noise std: 1.97
          Mean value_function loss: 6.8668
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.5221
                       Mean reward: 5.17
               Mean episode length: 213.98
    Episode_Reward/reaching_object: 0.6008
     Episode_Reward/lifting_object: 0.7081
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.17s
                      Time elapsed: 00:14:35
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 43331 steps/s (collection: 2.135s, learning 0.134s)
             Mean action noise std: 1.97
          Mean value_function loss: 10.8007
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 54.5359
                       Mean reward: 5.95
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 0.5988
     Episode_Reward/lifting_object: 0.8621
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.27s
                      Time elapsed: 00:14:37
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 41674 steps/s (collection: 2.250s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 10.0754
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.5435
                       Mean reward: 5.69
               Mean episode length: 211.71
    Episode_Reward/reaching_object: 0.6166
     Episode_Reward/lifting_object: 0.6086
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.36s
                      Time elapsed: 00:14:40
                               ETA: 01:05:59

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 44357 steps/s (collection: 2.121s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 6.1167
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 54.5723
                       Mean reward: 3.41
               Mean episode length: 216.93
    Episode_Reward/reaching_object: 0.6092
     Episode_Reward/lifting_object: 0.7677
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.22s
                      Time elapsed: 00:14:42
                               ETA: 01:05:55

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 43677 steps/s (collection: 2.151s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 9.8939
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.6115
                       Mean reward: 4.81
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 0.4584
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.25s
                      Time elapsed: 00:14:44
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 43016 steps/s (collection: 2.155s, learning 0.130s)
             Mean action noise std: 1.98
          Mean value_function loss: 7.8914
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.6377
                       Mean reward: 6.14
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 0.6146
     Episode_Reward/lifting_object: 0.8738
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.29s
                      Time elapsed: 00:14:47
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 42469 steps/s (collection: 2.191s, learning 0.124s)
             Mean action noise std: 1.98
          Mean value_function loss: 16.7805
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.6536
                       Mean reward: 4.17
               Mean episode length: 223.14
    Episode_Reward/reaching_object: 0.6241
     Episode_Reward/lifting_object: 0.6243
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.31s
                      Time elapsed: 00:14:49
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 43477 steps/s (collection: 2.156s, learning 0.105s)
             Mean action noise std: 1.99
          Mean value_function loss: 6.6958
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.6843
                       Mean reward: 8.83
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 0.6742
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.26s
                      Time elapsed: 00:14:51
                               ETA: 01:05:43

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 44635 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 15.1643
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.6980
                       Mean reward: 8.16
               Mean episode length: 216.98
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 1.1558
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.20s
                      Time elapsed: 00:14:53
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 45341 steps/s (collection: 2.073s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 7.2902
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7140
                       Mean reward: 8.27
               Mean episode length: 216.43
    Episode_Reward/reaching_object: 0.6402
     Episode_Reward/lifting_object: 1.2977
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.17s
                      Time elapsed: 00:14:56
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 45272 steps/s (collection: 2.060s, learning 0.112s)
             Mean action noise std: 1.99
          Mean value_function loss: 12.6954
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.7403
                       Mean reward: 7.09
               Mean episode length: 215.94
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: 0.7162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.17s
                      Time elapsed: 00:14:58
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 45670 steps/s (collection: 2.052s, learning 0.100s)
             Mean action noise std: 1.99
          Mean value_function loss: 6.1432
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.7529
                       Mean reward: 7.99
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 0.6301
     Episode_Reward/lifting_object: 0.3749
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.15s
                      Time elapsed: 00:15:00
                               ETA: 01:05:29

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 45705 steps/s (collection: 2.046s, learning 0.105s)
             Mean action noise std: 1.99
          Mean value_function loss: 5.7330
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7790
                       Mean reward: 8.60
               Mean episode length: 216.25
    Episode_Reward/reaching_object: 0.6259
     Episode_Reward/lifting_object: 1.1143
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.15s
                      Time elapsed: 00:15:02
                               ETA: 01:05:26

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 43801 steps/s (collection: 2.127s, learning 0.117s)
             Mean action noise std: 1.99
          Mean value_function loss: 12.8001
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.7996
                       Mean reward: 6.47
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 0.8748
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.24s
                      Time elapsed: 00:15:04
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 42752 steps/s (collection: 2.190s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 15.6863
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.8274
                       Mean reward: 6.12
               Mean episode length: 214.97
    Episode_Reward/reaching_object: 0.6088
     Episode_Reward/lifting_object: 0.9712
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.30s
                      Time elapsed: 00:15:07
                               ETA: 01:05:20

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 44826 steps/s (collection: 2.092s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 9.6970
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.8483
                       Mean reward: 8.52
               Mean episode length: 208.76
    Episode_Reward/reaching_object: 0.6061
     Episode_Reward/lifting_object: 0.9254
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.19s
                      Time elapsed: 00:15:09
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 45810 steps/s (collection: 2.046s, learning 0.100s)
             Mean action noise std: 2.00
          Mean value_function loss: 5.6396
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 54.8860
                       Mean reward: 5.16
               Mean episode length: 211.45
    Episode_Reward/reaching_object: 0.6222
     Episode_Reward/lifting_object: 1.0048
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.15s
                      Time elapsed: 00:15:11
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 45957 steps/s (collection: 2.038s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 11.4686
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.9235
                       Mean reward: 6.04
               Mean episode length: 207.32
    Episode_Reward/reaching_object: 0.6034
     Episode_Reward/lifting_object: 1.0601
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.14s
                      Time elapsed: 00:15:13
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 46959 steps/s (collection: 1.997s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 5.1871
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 54.9442
                       Mean reward: 7.42
               Mean episode length: 217.40
    Episode_Reward/reaching_object: 0.5894
     Episode_Reward/lifting_object: 1.1136
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.09s
                      Time elapsed: 00:15:15
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 46551 steps/s (collection: 2.013s, learning 0.099s)
             Mean action noise std: 2.01
          Mean value_function loss: 6.0692
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 54.9858
                       Mean reward: 4.33
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 0.6019
     Episode_Reward/lifting_object: 1.1730
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.11s
                      Time elapsed: 00:15:17
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 46514 steps/s (collection: 2.017s, learning 0.096s)
             Mean action noise std: 2.01
          Mean value_function loss: 9.6497
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.0201
                       Mean reward: 7.24
               Mean episode length: 218.66
    Episode_Reward/reaching_object: 0.5929
     Episode_Reward/lifting_object: 1.0160
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.11s
                      Time elapsed: 00:15:19
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 45674 steps/s (collection: 2.052s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 9.9682
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.0345
                       Mean reward: 9.07
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 0.5897
     Episode_Reward/lifting_object: 0.8933
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.15s
                      Time elapsed: 00:15:21
                               ETA: 01:04:54

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 45559 steps/s (collection: 2.064s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 10.4917
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.0513
                       Mean reward: 7.68
               Mean episode length: 206.52
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 1.1699
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.16s
                      Time elapsed: 00:15:24
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 46233 steps/s (collection: 2.030s, learning 0.096s)
             Mean action noise std: 2.02
          Mean value_function loss: 11.9284
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.0781
                       Mean reward: 5.13
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 0.5921
     Episode_Reward/lifting_object: 0.8412
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.13s
                      Time elapsed: 00:15:26
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 45229 steps/s (collection: 2.059s, learning 0.115s)
             Mean action noise std: 2.02
          Mean value_function loss: 12.0018
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.0934
                       Mean reward: 4.82
               Mean episode length: 206.62
    Episode_Reward/reaching_object: 0.5927
     Episode_Reward/lifting_object: 1.1899
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.17s
                      Time elapsed: 00:15:28
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 45724 steps/s (collection: 2.026s, learning 0.124s)
             Mean action noise std: 2.02
          Mean value_function loss: 27.7229
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.1081
                       Mean reward: 9.83
               Mean episode length: 206.37
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 0.6969
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.15s
                      Time elapsed: 00:15:30
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 46781 steps/s (collection: 1.993s, learning 0.108s)
             Mean action noise std: 2.02
          Mean value_function loss: 7.5297
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.1171
                       Mean reward: 7.22
               Mean episode length: 209.02
    Episode_Reward/reaching_object: 0.5870
     Episode_Reward/lifting_object: 0.9391
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.10s
                      Time elapsed: 00:15:32
                               ETA: 01:04:37

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 45741 steps/s (collection: 2.035s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 10.9986
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.1408
                       Mean reward: 9.25
               Mean episode length: 203.17
    Episode_Reward/reaching_object: 0.5853
     Episode_Reward/lifting_object: 1.1882
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.15s
                      Time elapsed: 00:15:34
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 46572 steps/s (collection: 2.008s, learning 0.103s)
             Mean action noise std: 2.02
          Mean value_function loss: 10.3172
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.1504
                       Mean reward: 8.09
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 0.5923
     Episode_Reward/lifting_object: 0.8826
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.11s
                      Time elapsed: 00:15:36
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 46205 steps/s (collection: 2.017s, learning 0.111s)
             Mean action noise std: 2.02
          Mean value_function loss: 9.3722
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.1633
                       Mean reward: 5.81
               Mean episode length: 200.31
    Episode_Reward/reaching_object: 0.5861
     Episode_Reward/lifting_object: 0.6197
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.13s
                      Time elapsed: 00:15:39
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 43652 steps/s (collection: 2.145s, learning 0.107s)
             Mean action noise std: 2.03
          Mean value_function loss: 7.0996
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.1948
                       Mean reward: 10.81
               Mean episode length: 213.49
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 1.2300
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.25s
                      Time elapsed: 00:15:41
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 44824 steps/s (collection: 2.099s, learning 0.095s)
             Mean action noise std: 2.03
          Mean value_function loss: 7.1564
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.2334
                       Mean reward: 6.39
               Mean episode length: 207.11
    Episode_Reward/reaching_object: 0.5969
     Episode_Reward/lifting_object: 1.1447
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.19s
                      Time elapsed: 00:15:43
                               ETA: 01:04:20

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 44648 steps/s (collection: 2.092s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 5.6971
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.2503
                       Mean reward: 10.25
               Mean episode length: 213.36
    Episode_Reward/reaching_object: 0.6057
     Episode_Reward/lifting_object: 1.2743
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.20s
                      Time elapsed: 00:15:45
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 44525 steps/s (collection: 2.104s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 9.3809
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.2597
                       Mean reward: 7.39
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 0.9683
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.21s
                      Time elapsed: 00:15:47
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 45596 steps/s (collection: 2.048s, learning 0.108s)
             Mean action noise std: 2.03
          Mean value_function loss: 6.4229
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2672
                       Mean reward: 9.35
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 1.4523
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.16s
                      Time elapsed: 00:15:50
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 46107 steps/s (collection: 2.026s, learning 0.106s)
             Mean action noise std: 2.03
          Mean value_function loss: 14.2010
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 55.2823
                       Mean reward: 9.43
               Mean episode length: 210.63
    Episode_Reward/reaching_object: 0.5956
     Episode_Reward/lifting_object: 1.3079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.13s
                      Time elapsed: 00:15:52
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 44729 steps/s (collection: 2.094s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 6.0750
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.2883
                       Mean reward: 5.78
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.5845
     Episode_Reward/lifting_object: 1.0980
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.20s
                      Time elapsed: 00:15:54
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 42978 steps/s (collection: 2.172s, learning 0.116s)
             Mean action noise std: 2.03
          Mean value_function loss: 11.2685
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.3081
                       Mean reward: 6.10
               Mean episode length: 208.89
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: 1.0983
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.29s
                      Time elapsed: 00:15:56
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 42959 steps/s (collection: 2.154s, learning 0.135s)
             Mean action noise std: 2.03
          Mean value_function loss: 12.5997
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.3198
                       Mean reward: 7.49
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 0.5917
     Episode_Reward/lifting_object: 1.1380
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.29s
                      Time elapsed: 00:15:59
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 42397 steps/s (collection: 2.200s, learning 0.119s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.5338
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.3312
                       Mean reward: 1.78
               Mean episode length: 214.30
    Episode_Reward/reaching_object: 0.6262
     Episode_Reward/lifting_object: 0.6506
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.32s
                      Time elapsed: 00:16:01
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 42169 steps/s (collection: 2.192s, learning 0.139s)
             Mean action noise std: 2.04
          Mean value_function loss: 9.6430
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.3527
                       Mean reward: 7.64
               Mean episode length: 217.14
    Episode_Reward/reaching_object: 0.6152
     Episode_Reward/lifting_object: 1.3157
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.33s
                      Time elapsed: 00:16:03
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 43880 steps/s (collection: 2.128s, learning 0.112s)
             Mean action noise std: 2.04
          Mean value_function loss: 11.5858
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.3597
                       Mean reward: 10.15
               Mean episode length: 207.19
    Episode_Reward/reaching_object: 0.5868
     Episode_Reward/lifting_object: 1.2914
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.24s
                      Time elapsed: 00:16:05
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 44100 steps/s (collection: 2.118s, learning 0.111s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.0396
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.3684
                       Mean reward: 9.37
               Mean episode length: 205.29
    Episode_Reward/reaching_object: 0.5862
     Episode_Reward/lifting_object: 1.2514
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.23s
                      Time elapsed: 00:16:08
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 42961 steps/s (collection: 2.158s, learning 0.131s)
             Mean action noise std: 2.04
          Mean value_function loss: 10.3436
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.3909
                       Mean reward: 5.95
               Mean episode length: 213.04
    Episode_Reward/reaching_object: 0.6131
     Episode_Reward/lifting_object: 1.2400
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.29s
                      Time elapsed: 00:16:10
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 40133 steps/s (collection: 2.347s, learning 0.103s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.2858
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 55.4280
                       Mean reward: 7.90
               Mean episode length: 206.15
    Episode_Reward/reaching_object: 0.6136
     Episode_Reward/lifting_object: 1.3221
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.45s
                      Time elapsed: 00:16:12
                               ETA: 01:03:41

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 42660 steps/s (collection: 2.198s, learning 0.106s)
             Mean action noise std: 2.05
          Mean value_function loss: 8.9433
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 55.4800
                       Mean reward: 6.92
               Mean episode length: 214.39
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 1.2895
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.30s
                      Time elapsed: 00:16:15
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 42461 steps/s (collection: 2.212s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 6.8929
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.5376
                       Mean reward: 14.18
               Mean episode length: 212.40
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 1.4182
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.32s
                      Time elapsed: 00:16:17
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 42250 steps/s (collection: 2.192s, learning 0.135s)
             Mean action noise std: 2.05
          Mean value_function loss: 16.7271
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.5665
                       Mean reward: 6.92
               Mean episode length: 206.77
    Episode_Reward/reaching_object: 0.6066
     Episode_Reward/lifting_object: 1.0857
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.33s
                      Time elapsed: 00:16:19
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 42224 steps/s (collection: 2.227s, learning 0.101s)
             Mean action noise std: 2.06
          Mean value_function loss: 15.6847
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.5948
                       Mean reward: 8.19
               Mean episode length: 220.00
    Episode_Reward/reaching_object: 0.6340
     Episode_Reward/lifting_object: 1.4962
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.33s
                      Time elapsed: 00:16:22
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 43891 steps/s (collection: 2.129s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 10.7590
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.6206
                       Mean reward: 9.43
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 0.6124
     Episode_Reward/lifting_object: 1.4963
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.24s
                      Time elapsed: 00:16:24
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 44507 steps/s (collection: 2.101s, learning 0.108s)
             Mean action noise std: 2.06
          Mean value_function loss: 6.2646
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 55.6561
                       Mean reward: 11.70
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 0.5913
     Episode_Reward/lifting_object: 0.7905
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.21s
                      Time elapsed: 00:16:26
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 44272 steps/s (collection: 2.102s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 7.9853
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 55.7098
                       Mean reward: 10.38
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.6114
     Episode_Reward/lifting_object: 1.6443
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.22s
                      Time elapsed: 00:16:28
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 42022 steps/s (collection: 2.212s, learning 0.127s)
             Mean action noise std: 2.07
          Mean value_function loss: 11.2289
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.7366
                       Mean reward: 11.36
               Mean episode length: 211.07
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 1.3191
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.34s
                      Time elapsed: 00:16:31
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 42536 steps/s (collection: 2.204s, learning 0.107s)
             Mean action noise std: 2.07
          Mean value_function loss: 14.7185
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.7591
                       Mean reward: 12.72
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 0.6110
     Episode_Reward/lifting_object: 1.2875
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.31s
                      Time elapsed: 00:16:33
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 44503 steps/s (collection: 2.106s, learning 0.103s)
             Mean action noise std: 2.07
          Mean value_function loss: 12.2859
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.7731
                       Mean reward: 11.19
               Mean episode length: 214.89
    Episode_Reward/reaching_object: 0.6089
     Episode_Reward/lifting_object: 1.5631
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.21s
                      Time elapsed: 00:16:35
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 45315 steps/s (collection: 2.071s, learning 0.098s)
             Mean action noise std: 2.07
          Mean value_function loss: 15.2708
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 55.8043
                       Mean reward: 7.97
               Mean episode length: 219.48
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 0.7689
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.17s
                      Time elapsed: 00:16:37
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 45153 steps/s (collection: 2.060s, learning 0.117s)
             Mean action noise std: 2.08
          Mean value_function loss: 15.6748
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8389
                       Mean reward: 8.16
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 1.1134
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.18s
                      Time elapsed: 00:16:40
                               ETA: 01:03:07

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 45219 steps/s (collection: 2.066s, learning 0.108s)
             Mean action noise std: 2.08
          Mean value_function loss: 7.6628
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.8571
                       Mean reward: 7.53
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 0.5945
     Episode_Reward/lifting_object: 1.2837
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.17s
                      Time elapsed: 00:16:42
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 45456 steps/s (collection: 2.062s, learning 0.101s)
             Mean action noise std: 2.08
          Mean value_function loss: 9.3443
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 55.8941
                       Mean reward: 7.74
               Mean episode length: 217.40
    Episode_Reward/reaching_object: 0.5778
     Episode_Reward/lifting_object: 1.3334
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.16s
                      Time elapsed: 00:16:44
                               ETA: 01:03:00

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 44800 steps/s (collection: 2.071s, learning 0.123s)
             Mean action noise std: 2.08
          Mean value_function loss: 10.9249
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.9363
                       Mean reward: 9.57
               Mean episode length: 210.64
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 1.6140
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.19s
                      Time elapsed: 00:16:46
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 45901 steps/s (collection: 2.046s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 9.6029
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.9531
                       Mean reward: 10.02
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 0.5552
     Episode_Reward/lifting_object: 1.5837
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.14s
                      Time elapsed: 00:16:48
                               ETA: 01:02:54

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 45541 steps/s (collection: 2.057s, learning 0.102s)
             Mean action noise std: 2.09
          Mean value_function loss: 13.0934
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 55.9658
                       Mean reward: 9.10
               Mean episode length: 213.98
    Episode_Reward/reaching_object: 0.5549
     Episode_Reward/lifting_object: 0.9909
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.16s
                      Time elapsed: 00:16:50
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 45681 steps/s (collection: 2.052s, learning 0.100s)
             Mean action noise std: 2.09
          Mean value_function loss: 7.8894
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9738
                       Mean reward: 11.65
               Mean episode length: 219.83
    Episode_Reward/reaching_object: 0.5711
     Episode_Reward/lifting_object: 1.1952
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.15s
                      Time elapsed: 00:16:52
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 43994 steps/s (collection: 2.125s, learning 0.109s)
             Mean action noise std: 2.09
          Mean value_function loss: 9.5538
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.9932
                       Mean reward: 9.82
               Mean episode length: 208.20
    Episode_Reward/reaching_object: 0.5626
     Episode_Reward/lifting_object: 0.9594
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.23s
                      Time elapsed: 00:16:55
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 44643 steps/s (collection: 2.110s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 8.0810
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 56.0252
                       Mean reward: 12.95
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 1.6828
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.20s
                      Time elapsed: 00:16:57
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 44430 steps/s (collection: 2.096s, learning 0.117s)
             Mean action noise std: 2.09
          Mean value_function loss: 10.6943
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 56.0584
                       Mean reward: 7.38
               Mean episode length: 217.69
    Episode_Reward/reaching_object: 0.5529
     Episode_Reward/lifting_object: 1.2131
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.21s
                      Time elapsed: 00:16:59
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 43297 steps/s (collection: 2.145s, learning 0.125s)
             Mean action noise std: 2.10
          Mean value_function loss: 15.9897
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.0769
                       Mean reward: 12.31
               Mean episode length: 215.72
    Episode_Reward/reaching_object: 0.5724
     Episode_Reward/lifting_object: 1.6735
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.27s
                      Time elapsed: 00:17:01
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 43507 steps/s (collection: 2.136s, learning 0.124s)
             Mean action noise std: 2.10
          Mean value_function loss: 8.2152
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 56.1084
                       Mean reward: 6.90
               Mean episode length: 198.79
    Episode_Reward/reaching_object: 0.5606
     Episode_Reward/lifting_object: 0.9247
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.26s
                      Time elapsed: 00:17:04
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 43802 steps/s (collection: 2.115s, learning 0.129s)
             Mean action noise std: 2.10
          Mean value_function loss: 9.6843
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.1274
                       Mean reward: 12.43
               Mean episode length: 193.69
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: 1.6372
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.24s
                      Time elapsed: 00:17:06
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 42809 steps/s (collection: 2.195s, learning 0.101s)
             Mean action noise std: 2.10
          Mean value_function loss: 15.4884
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.1597
                       Mean reward: 11.05
               Mean episode length: 206.37
    Episode_Reward/reaching_object: 0.5640
     Episode_Reward/lifting_object: 1.5367
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.30s
                      Time elapsed: 00:17:08
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 43601 steps/s (collection: 2.145s, learning 0.110s)
             Mean action noise std: 2.11
          Mean value_function loss: 13.1687
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 56.2000
                       Mean reward: 8.18
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 1.4862
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.25s
                      Time elapsed: 00:17:10
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 43677 steps/s (collection: 2.146s, learning 0.105s)
             Mean action noise std: 2.11
          Mean value_function loss: 14.3213
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.2341
                       Mean reward: 10.94
               Mean episode length: 198.55
    Episode_Reward/reaching_object: 0.5661
     Episode_Reward/lifting_object: 1.0082
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.25s
                      Time elapsed: 00:17:13
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 42809 steps/s (collection: 2.153s, learning 0.144s)
             Mean action noise std: 2.11
          Mean value_function loss: 10.0205
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.2601
                       Mean reward: 6.97
               Mean episode length: 208.13
    Episode_Reward/reaching_object: 0.5796
     Episode_Reward/lifting_object: 1.3671
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.30s
                      Time elapsed: 00:17:15
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 42801 steps/s (collection: 2.188s, learning 0.108s)
             Mean action noise std: 2.11
          Mean value_function loss: 9.9547
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.2898
                       Mean reward: 6.03
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 1.3815
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.30s
                      Time elapsed: 00:17:17
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 43550 steps/s (collection: 2.138s, learning 0.119s)
             Mean action noise std: 2.11
          Mean value_function loss: 6.9473
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 56.3097
                       Mean reward: 11.22
               Mean episode length: 195.61
    Episode_Reward/reaching_object: 0.5511
     Episode_Reward/lifting_object: 1.3154
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.26s
                      Time elapsed: 00:17:20
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 43459 steps/s (collection: 2.148s, learning 0.114s)
             Mean action noise std: 2.12
          Mean value_function loss: 10.7059
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.3413
                       Mean reward: 12.71
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 0.5763
     Episode_Reward/lifting_object: 1.4487
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.26s
                      Time elapsed: 00:17:22
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 41572 steps/s (collection: 2.254s, learning 0.111s)
             Mean action noise std: 2.12
          Mean value_function loss: 11.2765
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 56.3590
                       Mean reward: 10.73
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 0.5805
     Episode_Reward/lifting_object: 1.1207
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.36s
                      Time elapsed: 00:17:24
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 41130 steps/s (collection: 2.287s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 25.6064
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.3961
                       Mean reward: 13.10
               Mean episode length: 198.86
    Episode_Reward/reaching_object: 0.5724
     Episode_Reward/lifting_object: 1.7432
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.39s
                      Time elapsed: 00:17:27
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 42834 steps/s (collection: 2.165s, learning 0.130s)
             Mean action noise std: 2.12
          Mean value_function loss: 20.7764
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.4191
                       Mean reward: 11.15
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 1.5306
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.29s
                      Time elapsed: 00:17:29
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 42818 steps/s (collection: 2.168s, learning 0.128s)
             Mean action noise std: 2.12
          Mean value_function loss: 24.2501
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 56.4493
                       Mean reward: 10.73
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 1.4571
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.30s
                      Time elapsed: 00:17:31
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 42242 steps/s (collection: 2.213s, learning 0.115s)
             Mean action noise std: 2.13
          Mean value_function loss: 19.3591
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.4573
                       Mean reward: 8.66
               Mean episode length: 206.73
    Episode_Reward/reaching_object: 0.5665
     Episode_Reward/lifting_object: 1.5656
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.33s
                      Time elapsed: 00:17:33
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 42044 steps/s (collection: 2.205s, learning 0.133s)
             Mean action noise std: 2.13
          Mean value_function loss: 21.6704
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.4859
                       Mean reward: 13.40
               Mean episode length: 204.10
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 2.0428
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.34s
                      Time elapsed: 00:17:36
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 42685 steps/s (collection: 2.190s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 16.9558
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.5237
                       Mean reward: 8.26
               Mean episode length: 201.96
    Episode_Reward/reaching_object: 0.5857
     Episode_Reward/lifting_object: 1.2414
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.30s
                      Time elapsed: 00:17:38
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 41223 steps/s (collection: 2.252s, learning 0.133s)
             Mean action noise std: 2.13
          Mean value_function loss: 11.0206
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.5580
                       Mean reward: 8.23
               Mean episode length: 206.62
    Episode_Reward/reaching_object: 0.5522
     Episode_Reward/lifting_object: 1.6398
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.38s
                      Time elapsed: 00:17:41
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 41245 steps/s (collection: 2.268s, learning 0.116s)
             Mean action noise std: 2.14
          Mean value_function loss: 16.8963
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 56.6059
                       Mean reward: 9.96
               Mean episode length: 204.61
    Episode_Reward/reaching_object: 0.5638
     Episode_Reward/lifting_object: 1.5344
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.38s
                      Time elapsed: 00:17:43
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 39321 steps/s (collection: 2.369s, learning 0.131s)
             Mean action noise std: 2.14
          Mean value_function loss: 16.4930
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.6563
                       Mean reward: 5.91
               Mean episode length: 207.90
    Episode_Reward/reaching_object: 0.5586
     Episode_Reward/lifting_object: 1.2138
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.50s
                      Time elapsed: 00:17:45
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 42982 steps/s (collection: 2.187s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 9.6684
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 56.6914
                       Mean reward: 11.48
               Mean episode length: 194.76
    Episode_Reward/reaching_object: 0.5717
     Episode_Reward/lifting_object: 1.0695
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.29s
                      Time elapsed: 00:17:48
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 44089 steps/s (collection: 2.127s, learning 0.103s)
             Mean action noise std: 2.15
          Mean value_function loss: 12.9219
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 56.7226
                       Mean reward: 12.28
               Mean episode length: 209.39
    Episode_Reward/reaching_object: 0.5608
     Episode_Reward/lifting_object: 1.1025
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.23s
                      Time elapsed: 00:17:50
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 43963 steps/s (collection: 2.137s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 9.4209
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.7503
                       Mean reward: 10.79
               Mean episode length: 203.26
    Episode_Reward/reaching_object: 0.5743
     Episode_Reward/lifting_object: 1.4403
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.24s
                      Time elapsed: 00:17:52
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 43871 steps/s (collection: 2.134s, learning 0.107s)
             Mean action noise std: 2.15
          Mean value_function loss: 21.2246
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 56.7683
                       Mean reward: 10.41
               Mean episode length: 199.57
    Episode_Reward/reaching_object: 0.5563
     Episode_Reward/lifting_object: 1.0196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.24s
                      Time elapsed: 00:17:54
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 44046 steps/s (collection: 2.126s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 10.8966
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.7826
                       Mean reward: 7.96
               Mean episode length: 209.89
    Episode_Reward/reaching_object: 0.5588
     Episode_Reward/lifting_object: 1.1665
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.23s
                      Time elapsed: 00:17:57
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 43536 steps/s (collection: 2.136s, learning 0.122s)
             Mean action noise std: 2.16
          Mean value_function loss: 15.5263
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.8144
                       Mean reward: 12.96
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 0.5396
     Episode_Reward/lifting_object: 1.6897
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.26s
                      Time elapsed: 00:17:59
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 43559 steps/s (collection: 2.128s, learning 0.129s)
             Mean action noise std: 2.16
          Mean value_function loss: 10.3906
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.8317
                       Mean reward: 11.99
               Mean episode length: 207.56
    Episode_Reward/reaching_object: 0.5630
     Episode_Reward/lifting_object: 1.5404
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.26s
                      Time elapsed: 00:18:01
                               ETA: 01:01:25

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 43691 steps/s (collection: 2.133s, learning 0.117s)
             Mean action noise std: 2.16
          Mean value_function loss: 12.2070
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 56.8595
                       Mean reward: 10.69
               Mean episode length: 196.51
    Episode_Reward/reaching_object: 0.5410
     Episode_Reward/lifting_object: 1.6231
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.25s
                      Time elapsed: 00:18:03
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 42975 steps/s (collection: 2.179s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 7.5578
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 56.8985
                       Mean reward: 12.05
               Mean episode length: 210.90
    Episode_Reward/reaching_object: 0.5827
     Episode_Reward/lifting_object: 1.7418
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.29s
                      Time elapsed: 00:18:06
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 43484 steps/s (collection: 2.123s, learning 0.138s)
             Mean action noise std: 2.17
          Mean value_function loss: 15.9657
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 56.9284
                       Mean reward: 11.80
               Mean episode length: 210.20
    Episode_Reward/reaching_object: 0.5346
     Episode_Reward/lifting_object: 1.6727
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.26s
                      Time elapsed: 00:18:08
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 44739 steps/s (collection: 2.065s, learning 0.132s)
             Mean action noise std: 2.17
          Mean value_function loss: 10.3722
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.9714
                       Mean reward: 8.75
               Mean episode length: 211.22
    Episode_Reward/reaching_object: 0.5876
     Episode_Reward/lifting_object: 1.6110
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.20s
                      Time elapsed: 00:18:10
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 44843 steps/s (collection: 2.089s, learning 0.103s)
             Mean action noise std: 2.17
          Mean value_function loss: 6.3765
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.9779
                       Mean reward: 12.58
               Mean episode length: 198.90
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 1.9063
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.19s
                      Time elapsed: 00:18:12
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 44013 steps/s (collection: 2.136s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 17.1189
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 56.9920
                       Mean reward: 5.06
               Mean episode length: 200.47
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 1.1616
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.23s
                      Time elapsed: 00:18:15
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 43671 steps/s (collection: 2.150s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 16.1886
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9983
                       Mean reward: 10.60
               Mean episode length: 208.05
    Episode_Reward/reaching_object: 0.5614
     Episode_Reward/lifting_object: 1.7695
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.25s
                      Time elapsed: 00:18:17
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 44534 steps/s (collection: 2.087s, learning 0.121s)
             Mean action noise std: 2.17
          Mean value_function loss: 14.5545
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 57.0233
                       Mean reward: 13.87
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 2.0832
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.21s
                      Time elapsed: 00:18:19
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 44922 steps/s (collection: 2.085s, learning 0.104s)
             Mean action noise std: 2.18
          Mean value_function loss: 14.4744
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.0597
                       Mean reward: 12.03
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 0.5592
     Episode_Reward/lifting_object: 1.8222
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.19s
                      Time elapsed: 00:18:21
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 43411 steps/s (collection: 2.152s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.3747
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 57.0878
                       Mean reward: 10.99
               Mean episode length: 191.57
    Episode_Reward/reaching_object: 0.5861
     Episode_Reward/lifting_object: 1.8340
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.26s
                      Time elapsed: 00:18:23
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 43145 steps/s (collection: 2.169s, learning 0.109s)
             Mean action noise std: 2.18
          Mean value_function loss: 22.4599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.1182
                       Mean reward: 12.12
               Mean episode length: 194.93
    Episode_Reward/reaching_object: 0.5739
     Episode_Reward/lifting_object: 1.5705
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.28s
                      Time elapsed: 00:18:26
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 43708 steps/s (collection: 2.142s, learning 0.108s)
             Mean action noise std: 2.18
          Mean value_function loss: 16.2896
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.1386
                       Mean reward: 11.06
               Mean episode length: 203.46
    Episode_Reward/reaching_object: 0.5800
     Episode_Reward/lifting_object: 1.7265
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.25s
                      Time elapsed: 00:18:28
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 43633 steps/s (collection: 2.135s, learning 0.118s)
             Mean action noise std: 2.18
          Mean value_function loss: 18.0746
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.1582
                       Mean reward: 11.29
               Mean episode length: 207.85
    Episode_Reward/reaching_object: 0.5852
     Episode_Reward/lifting_object: 1.9005
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.25s
                      Time elapsed: 00:18:30
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 43778 steps/s (collection: 2.131s, learning 0.115s)
             Mean action noise std: 2.19
          Mean value_function loss: 14.9572
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.1734
                       Mean reward: 17.66
               Mean episode length: 201.89
    Episode_Reward/reaching_object: 0.6070
     Episode_Reward/lifting_object: 2.4737
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.25s
                      Time elapsed: 00:18:33
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 43022 steps/s (collection: 2.176s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 15.9826
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.1950
                       Mean reward: 13.65
               Mean episode length: 204.28
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 1.9663
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.28s
                      Time elapsed: 00:18:35
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 42376 steps/s (collection: 2.217s, learning 0.103s)
             Mean action noise std: 2.19
          Mean value_function loss: 16.6678
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 57.2101
                       Mean reward: 12.93
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 1.9533
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.32s
                      Time elapsed: 00:18:37
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 44868 steps/s (collection: 2.087s, learning 0.104s)
             Mean action noise std: 2.19
          Mean value_function loss: 13.0471
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 57.2369
                       Mean reward: 11.74
               Mean episode length: 209.83
    Episode_Reward/reaching_object: 0.5818
     Episode_Reward/lifting_object: 1.9800
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.19s
                      Time elapsed: 00:18:39
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 44814 steps/s (collection: 2.100s, learning 0.094s)
             Mean action noise std: 2.19
          Mean value_function loss: 20.8037
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.2588
                       Mean reward: 11.10
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 0.5662
     Episode_Reward/lifting_object: 1.6486
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.19s
                      Time elapsed: 00:18:41
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 44954 steps/s (collection: 2.082s, learning 0.105s)
             Mean action noise std: 2.20
          Mean value_function loss: 9.4466
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 57.2837
                       Mean reward: 13.58
               Mean episode length: 210.37
    Episode_Reward/reaching_object: 0.5935
     Episode_Reward/lifting_object: 1.7162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.19s
                      Time elapsed: 00:18:44
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 44181 steps/s (collection: 2.098s, learning 0.127s)
             Mean action noise std: 2.20
          Mean value_function loss: 17.3946
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.3244
                       Mean reward: 15.42
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 0.5959
     Episode_Reward/lifting_object: 1.5784
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.22s
                      Time elapsed: 00:18:46
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 44738 steps/s (collection: 2.093s, learning 0.105s)
             Mean action noise std: 2.20
          Mean value_function loss: 11.7698
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 57.3736
                       Mean reward: 12.97
               Mean episode length: 204.04
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 1.5226
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.20s
                      Time elapsed: 00:18:48
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 45061 steps/s (collection: 2.074s, learning 0.107s)
             Mean action noise std: 2.21
          Mean value_function loss: 26.9676
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 57.4121
                       Mean reward: 10.94
               Mean episode length: 201.25
    Episode_Reward/reaching_object: 0.5696
     Episode_Reward/lifting_object: 1.8111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.18s
                      Time elapsed: 00:18:50
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 37863 steps/s (collection: 2.417s, learning 0.179s)
             Mean action noise std: 2.21
          Mean value_function loss: 24.4457
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.4191
                       Mean reward: 12.65
               Mean episode length: 195.87
    Episode_Reward/reaching_object: 0.5722
     Episode_Reward/lifting_object: 2.2515
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.60s
                      Time elapsed: 00:18:53
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 41827 steps/s (collection: 2.244s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 24.0082
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.4497
                       Mean reward: 11.11
               Mean episode length: 203.62
    Episode_Reward/reaching_object: 0.5474
     Episode_Reward/lifting_object: 1.3486
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.35s
                      Time elapsed: 00:18:55
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 42615 steps/s (collection: 2.175s, learning 0.132s)
             Mean action noise std: 2.21
          Mean value_function loss: 22.0332
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.4799
                       Mean reward: 10.73
               Mean episode length: 206.95
    Episode_Reward/reaching_object: 0.5585
     Episode_Reward/lifting_object: 1.2423
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.31s
                      Time elapsed: 00:18:58
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 44226 steps/s (collection: 2.109s, learning 0.114s)
             Mean action noise std: 2.21
          Mean value_function loss: 10.6528
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.4940
                       Mean reward: 16.13
               Mean episode length: 219.19
    Episode_Reward/reaching_object: 0.5518
     Episode_Reward/lifting_object: 2.1264
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.22s
                      Time elapsed: 00:19:00
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 43372 steps/s (collection: 2.159s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 11.9253
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 57.5199
                       Mean reward: 6.77
               Mean episode length: 194.72
    Episode_Reward/reaching_object: 0.5344
     Episode_Reward/lifting_object: 1.3913
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.27s
                      Time elapsed: 00:19:02
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 39399 steps/s (collection: 2.323s, learning 0.172s)
             Mean action noise std: 2.22
          Mean value_function loss: 17.2053
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 57.5616
                       Mean reward: 8.63
               Mean episode length: 194.78
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 1.6602
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.50s
                      Time elapsed: 00:19:05
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 41666 steps/s (collection: 2.261s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 11.5717
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 57.6011
                       Mean reward: 12.42
               Mean episode length: 195.15
    Episode_Reward/reaching_object: 0.5141
     Episode_Reward/lifting_object: 1.7593
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.36s
                      Time elapsed: 00:19:07
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 43628 steps/s (collection: 2.124s, learning 0.129s)
             Mean action noise std: 2.22
          Mean value_function loss: 27.3917
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.6274
                       Mean reward: 4.86
               Mean episode length: 201.27
    Episode_Reward/reaching_object: 0.5424
     Episode_Reward/lifting_object: 1.3469
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.25s
                      Time elapsed: 00:19:09
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 38593 steps/s (collection: 2.425s, learning 0.122s)
             Mean action noise std: 2.23
          Mean value_function loss: 12.7346
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 57.6522
                       Mean reward: 17.39
               Mean episode length: 205.45
    Episode_Reward/reaching_object: 0.5549
     Episode_Reward/lifting_object: 2.0653
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.55s
                      Time elapsed: 00:19:12
                               ETA: 01:00:01

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 43748 steps/s (collection: 2.126s, learning 0.121s)
             Mean action noise std: 2.23
          Mean value_function loss: 16.0278
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.6738
                       Mean reward: 13.71
               Mean episode length: 197.11
    Episode_Reward/reaching_object: 0.5381
     Episode_Reward/lifting_object: 2.1880
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.25s
                      Time elapsed: 00:19:14
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 41896 steps/s (collection: 2.221s, learning 0.126s)
             Mean action noise std: 2.23
          Mean value_function loss: 18.2317
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.7183
                       Mean reward: 14.80
               Mean episode length: 189.23
    Episode_Reward/reaching_object: 0.5503
     Episode_Reward/lifting_object: 1.9078
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.35s
                      Time elapsed: 00:19:16
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 43014 steps/s (collection: 2.174s, learning 0.112s)
             Mean action noise std: 2.24
          Mean value_function loss: 15.1079
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.7606
                       Mean reward: 13.22
               Mean episode length: 187.26
    Episode_Reward/reaching_object: 0.5493
     Episode_Reward/lifting_object: 1.9277
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.29s
                      Time elapsed: 00:19:19
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 42021 steps/s (collection: 2.227s, learning 0.112s)
             Mean action noise std: 2.24
          Mean value_function loss: 22.7791
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 57.7868
                       Mean reward: 10.66
               Mean episode length: 203.16
    Episode_Reward/reaching_object: 0.5460
     Episode_Reward/lifting_object: 1.8462
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.34s
                      Time elapsed: 00:19:21
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 34455 steps/s (collection: 2.573s, learning 0.280s)
             Mean action noise std: 2.24
          Mean value_function loss: 21.2000
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 57.8292
                       Mean reward: 10.65
               Mean episode length: 203.27
    Episode_Reward/reaching_object: 0.5565
     Episode_Reward/lifting_object: 1.4965
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.85s
                      Time elapsed: 00:19:24
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 35122 steps/s (collection: 2.625s, learning 0.174s)
             Mean action noise std: 2.25
          Mean value_function loss: 8.5982
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 57.8729
                       Mean reward: 14.41
               Mean episode length: 182.43
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 2.1947
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.80s
                      Time elapsed: 00:19:27
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 36068 steps/s (collection: 2.531s, learning 0.195s)
             Mean action noise std: 2.25
          Mean value_function loss: 16.2968
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 57.9063
                       Mean reward: 16.78
               Mean episode length: 196.37
    Episode_Reward/reaching_object: 0.5606
     Episode_Reward/lifting_object: 2.4556
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.73s
                      Time elapsed: 00:19:29
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 36279 steps/s (collection: 2.480s, learning 0.230s)
             Mean action noise std: 2.25
          Mean value_function loss: 37.3514
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 57.9337
                       Mean reward: 14.35
               Mean episode length: 189.25
    Episode_Reward/reaching_object: 0.5364
     Episode_Reward/lifting_object: 1.8895
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.71s
                      Time elapsed: 00:19:32
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 35767 steps/s (collection: 2.629s, learning 0.119s)
             Mean action noise std: 2.25
          Mean value_function loss: 18.1173
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.9551
                       Mean reward: 12.05
               Mean episode length: 204.53
    Episode_Reward/reaching_object: 0.5609
     Episode_Reward/lifting_object: 2.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.75s
                      Time elapsed: 00:19:35
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 34551 steps/s (collection: 2.592s, learning 0.253s)
             Mean action noise std: 2.26
          Mean value_function loss: 18.3368
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 57.9957
                       Mean reward: 16.29
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 0.5575
     Episode_Reward/lifting_object: 2.3932
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.85s
                      Time elapsed: 00:19:38
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 35301 steps/s (collection: 2.562s, learning 0.223s)
             Mean action noise std: 2.26
          Mean value_function loss: 25.9125
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.0491
                       Mean reward: 12.31
               Mean episode length: 205.34
    Episode_Reward/reaching_object: 0.5621
     Episode_Reward/lifting_object: 2.3777
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.78s
                      Time elapsed: 00:19:40
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 35606 steps/s (collection: 2.575s, learning 0.186s)
             Mean action noise std: 2.27
          Mean value_function loss: 17.7524
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 58.0993
                       Mean reward: 12.70
               Mean episode length: 208.19
    Episode_Reward/reaching_object: 0.5244
     Episode_Reward/lifting_object: 2.3954
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.76s
                      Time elapsed: 00:19:43
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 30757 steps/s (collection: 2.967s, learning 0.229s)
             Mean action noise std: 2.27
          Mean value_function loss: 13.0377
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.1466
                       Mean reward: 11.86
               Mean episode length: 200.53
    Episode_Reward/reaching_object: 0.5704
     Episode_Reward/lifting_object: 2.4598
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 3.20s
                      Time elapsed: 00:19:46
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 28212 steps/s (collection: 3.190s, learning 0.294s)
             Mean action noise std: 2.27
          Mean value_function loss: 12.9088
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 58.1716
                       Mean reward: 17.31
               Mean episode length: 196.69
    Episode_Reward/reaching_object: 0.5249
     Episode_Reward/lifting_object: 2.4834
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 3.48s
                      Time elapsed: 00:19:50
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 31660 steps/s (collection: 2.903s, learning 0.202s)
             Mean action noise std: 2.27
          Mean value_function loss: 16.8037
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.2010
                       Mean reward: 11.90
               Mean episode length: 187.74
    Episode_Reward/reaching_object: 0.5044
     Episode_Reward/lifting_object: 2.3102
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 3.10s
                      Time elapsed: 00:19:53
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 38151 steps/s (collection: 2.432s, learning 0.145s)
             Mean action noise std: 2.28
          Mean value_function loss: 17.4508
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.2235
                       Mean reward: 9.02
               Mean episode length: 193.23
    Episode_Reward/reaching_object: 0.5236
     Episode_Reward/lifting_object: 1.9910
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.58s
                      Time elapsed: 00:19:55
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 38124 steps/s (collection: 2.437s, learning 0.142s)
             Mean action noise std: 2.28
          Mean value_function loss: 18.0806
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.2558
                       Mean reward: 15.17
               Mean episode length: 199.89
    Episode_Reward/reaching_object: 0.5227
     Episode_Reward/lifting_object: 2.1498
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.58s
                      Time elapsed: 00:19:58
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 38402 steps/s (collection: 2.312s, learning 0.248s)
             Mean action noise std: 2.28
          Mean value_function loss: 12.5605
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 58.2824
                       Mean reward: 14.06
               Mean episode length: 190.96
    Episode_Reward/reaching_object: 0.5159
     Episode_Reward/lifting_object: 1.8589
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.56s
                      Time elapsed: 00:20:01
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 32905 steps/s (collection: 2.698s, learning 0.290s)
             Mean action noise std: 2.28
          Mean value_function loss: 14.8958
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.3053
                       Mean reward: 13.55
               Mean episode length: 211.14
    Episode_Reward/reaching_object: 0.5376
     Episode_Reward/lifting_object: 2.0580
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.99s
                      Time elapsed: 00:20:04
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 35807 steps/s (collection: 2.596s, learning 0.150s)
             Mean action noise std: 2.29
          Mean value_function loss: 21.6943
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 58.3320
                       Mean reward: 18.41
               Mean episode length: 195.62
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: 2.6818
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.75s
                      Time elapsed: 00:20:06
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 35302 steps/s (collection: 2.616s, learning 0.169s)
             Mean action noise std: 2.29
          Mean value_function loss: 13.7752
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 58.3722
                       Mean reward: 16.74
               Mean episode length: 202.26
    Episode_Reward/reaching_object: 0.5297
     Episode_Reward/lifting_object: 2.1150
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.78s
                      Time elapsed: 00:20:09
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 36379 steps/s (collection: 2.551s, learning 0.152s)
             Mean action noise std: 2.29
          Mean value_function loss: 16.4835
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.4083
                       Mean reward: 14.05
               Mean episode length: 196.60
    Episode_Reward/reaching_object: 0.5119
     Episode_Reward/lifting_object: 2.1945
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.70s
                      Time elapsed: 00:20:12
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 34199 steps/s (collection: 2.594s, learning 0.281s)
             Mean action noise std: 2.29
          Mean value_function loss: 33.2797
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.4354
                       Mean reward: 12.50
               Mean episode length: 185.97
    Episode_Reward/reaching_object: 0.5157
     Episode_Reward/lifting_object: 2.1635
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.87s
                      Time elapsed: 00:20:15
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 34382 steps/s (collection: 2.679s, learning 0.180s)
             Mean action noise std: 2.30
          Mean value_function loss: 25.8862
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 58.4578
                       Mean reward: 18.62
               Mean episode length: 192.27
    Episode_Reward/reaching_object: 0.5353
     Episode_Reward/lifting_object: 2.2433
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.86s
                      Time elapsed: 00:20:18
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 33773 steps/s (collection: 2.626s, learning 0.284s)
             Mean action noise std: 2.30
          Mean value_function loss: 18.8720
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.4905
                       Mean reward: 14.16
               Mean episode length: 196.68
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 2.2178
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.91s
                      Time elapsed: 00:20:20
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 34827 steps/s (collection: 2.610s, learning 0.213s)
             Mean action noise std: 2.30
          Mean value_function loss: 36.6227
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.5203
                       Mean reward: 14.28
               Mean episode length: 183.47
    Episode_Reward/reaching_object: 0.5792
     Episode_Reward/lifting_object: 3.0104
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.82s
                      Time elapsed: 00:20:23
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 36527 steps/s (collection: 2.516s, learning 0.176s)
             Mean action noise std: 2.31
          Mean value_function loss: 25.8709
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.5469
                       Mean reward: 14.93
               Mean episode length: 186.38
    Episode_Reward/reaching_object: 0.5383
     Episode_Reward/lifting_object: 2.0289
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.69s
                      Time elapsed: 00:20:26
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 38649 steps/s (collection: 2.389s, learning 0.154s)
             Mean action noise std: 2.31
          Mean value_function loss: 20.5622
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.5767
                       Mean reward: 13.06
               Mean episode length: 191.81
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 2.2961
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.54s
                      Time elapsed: 00:20:29
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 35056 steps/s (collection: 2.631s, learning 0.174s)
             Mean action noise std: 2.31
          Mean value_function loss: 30.8494
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.5937
                       Mean reward: 16.59
               Mean episode length: 180.21
    Episode_Reward/reaching_object: 0.5716
     Episode_Reward/lifting_object: 2.8020
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.80s
                      Time elapsed: 00:20:31
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 37190 steps/s (collection: 2.440s, learning 0.203s)
             Mean action noise std: 2.31
          Mean value_function loss: 20.9727
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 58.6178
                       Mean reward: 19.02
               Mean episode length: 191.59
    Episode_Reward/reaching_object: 0.5553
     Episode_Reward/lifting_object: 2.6719
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.64s
                      Time elapsed: 00:20:34
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 40896 steps/s (collection: 2.299s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 40.2580
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.6481
                       Mean reward: 12.41
               Mean episode length: 199.72
    Episode_Reward/reaching_object: 0.5670
     Episode_Reward/lifting_object: 2.1448
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.40s
                      Time elapsed: 00:20:36
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 41515 steps/s (collection: 2.165s, learning 0.203s)
             Mean action noise std: 2.32
          Mean value_function loss: 26.6243
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.6785
                       Mean reward: 15.79
               Mean episode length: 199.94
    Episode_Reward/reaching_object: 0.5683
     Episode_Reward/lifting_object: 1.9930
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.37s
                      Time elapsed: 00:20:39
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 39951 steps/s (collection: 2.259s, learning 0.202s)
             Mean action noise std: 2.32
          Mean value_function loss: 28.4458
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 58.7061
                       Mean reward: 8.63
               Mean episode length: 194.97
    Episode_Reward/reaching_object: 0.5741
     Episode_Reward/lifting_object: 2.2998
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.46s
                      Time elapsed: 00:20:41
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 39945 steps/s (collection: 2.266s, learning 0.195s)
             Mean action noise std: 2.32
          Mean value_function loss: 28.4493
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.7391
                       Mean reward: 10.01
               Mean episode length: 187.66
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 2.5885
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.46s
                      Time elapsed: 00:20:44
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 42358 steps/s (collection: 2.210s, learning 0.111s)
             Mean action noise std: 2.33
          Mean value_function loss: 33.2970
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.7717
                       Mean reward: 9.92
               Mean episode length: 185.36
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 2.7638
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.32s
                      Time elapsed: 00:20:46
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 45605 steps/s (collection: 2.060s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 29.5933
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.8119
                       Mean reward: 8.45
               Mean episode length: 180.18
    Episode_Reward/reaching_object: 0.5519
     Episode_Reward/lifting_object: 2.0380
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.16s
                      Time elapsed: 00:20:48
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 45360 steps/s (collection: 2.064s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 34.3503
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 58.8445
                       Mean reward: 18.32
               Mean episode length: 185.35
    Episode_Reward/reaching_object: 0.5521
     Episode_Reward/lifting_object: 2.4611
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.17s
                      Time elapsed: 00:20:50
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 43995 steps/s (collection: 2.099s, learning 0.135s)
             Mean action noise std: 2.34
          Mean value_function loss: 29.8461
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 58.8988
                       Mean reward: 17.97
               Mean episode length: 180.15
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 2.9519
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.23s
                      Time elapsed: 00:20:53
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 39896 steps/s (collection: 2.352s, learning 0.112s)
             Mean action noise std: 2.34
          Mean value_function loss: 49.9036
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 58.9429
                       Mean reward: 17.25
               Mean episode length: 196.62
    Episode_Reward/reaching_object: 0.5647
     Episode_Reward/lifting_object: 3.0096
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.46s
                      Time elapsed: 00:20:55
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 43484 steps/s (collection: 2.136s, learning 0.124s)
             Mean action noise std: 2.34
          Mean value_function loss: 55.0072
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.9800
                       Mean reward: 18.42
               Mean episode length: 193.13
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: 2.8698
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.26s
                      Time elapsed: 00:20:57
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 43588 steps/s (collection: 2.119s, learning 0.136s)
             Mean action noise std: 2.35
          Mean value_function loss: 50.3799
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.0196
                       Mean reward: 18.10
               Mean episode length: 191.88
    Episode_Reward/reaching_object: 0.5623
     Episode_Reward/lifting_object: 2.1703
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.26s
                      Time elapsed: 00:21:00
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 41466 steps/s (collection: 2.264s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 31.4836
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.0450
                       Mean reward: 19.21
               Mean episode length: 201.03
    Episode_Reward/reaching_object: 0.5628
     Episode_Reward/lifting_object: 2.5909
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.37s
                      Time elapsed: 00:21:02
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 44737 steps/s (collection: 2.090s, learning 0.108s)
             Mean action noise std: 2.35
          Mean value_function loss: 23.6967
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.0723
                       Mean reward: 21.01
               Mean episode length: 189.08
    Episode_Reward/reaching_object: 0.5721
     Episode_Reward/lifting_object: 2.6781
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.20s
                      Time elapsed: 00:21:04
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 45959 steps/s (collection: 2.040s, learning 0.099s)
             Mean action noise std: 2.35
          Mean value_function loss: 22.1743
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 59.0962
                       Mean reward: 16.04
               Mean episode length: 187.65
    Episode_Reward/reaching_object: 0.5313
     Episode_Reward/lifting_object: 2.5971
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.14s
                      Time elapsed: 00:21:06
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 45384 steps/s (collection: 2.061s, learning 0.105s)
             Mean action noise std: 2.36
          Mean value_function loss: 39.1598
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.1223
                       Mean reward: 15.05
               Mean episode length: 182.73
    Episode_Reward/reaching_object: 0.5585
     Episode_Reward/lifting_object: 2.8788
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.17s
                      Time elapsed: 00:21:08
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 40608 steps/s (collection: 2.262s, learning 0.159s)
             Mean action noise std: 2.36
          Mean value_function loss: 24.2061
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.1472
                       Mean reward: 12.64
               Mean episode length: 196.03
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: 2.3605
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.42s
                      Time elapsed: 00:21:11
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 42853 steps/s (collection: 2.164s, learning 0.130s)
             Mean action noise std: 2.36
          Mean value_function loss: 24.0154
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.1757
                       Mean reward: 18.73
               Mean episode length: 187.93
    Episode_Reward/reaching_object: 0.5810
     Episode_Reward/lifting_object: 2.6114
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.29s
                      Time elapsed: 00:21:13
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 42745 steps/s (collection: 2.185s, learning 0.115s)
             Mean action noise std: 2.36
          Mean value_function loss: 41.2897
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.2056
                       Mean reward: 17.89
               Mean episode length: 192.20
    Episode_Reward/reaching_object: 0.5709
     Episode_Reward/lifting_object: 2.5477
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.30s
                      Time elapsed: 00:21:15
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 42296 steps/s (collection: 2.173s, learning 0.151s)
             Mean action noise std: 2.37
          Mean value_function loss: 25.8128
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 59.2366
                       Mean reward: 14.38
               Mean episode length: 168.25
    Episode_Reward/reaching_object: 0.5689
     Episode_Reward/lifting_object: 3.2276
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.32s
                      Time elapsed: 00:21:18
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 43447 steps/s (collection: 2.159s, learning 0.104s)
             Mean action noise std: 2.37
          Mean value_function loss: 22.0153
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 59.2704
                       Mean reward: 15.80
               Mean episode length: 180.70
    Episode_Reward/reaching_object: 0.5144
     Episode_Reward/lifting_object: 2.5373
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.26s
                      Time elapsed: 00:21:20
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 42728 steps/s (collection: 2.179s, learning 0.122s)
             Mean action noise std: 2.37
          Mean value_function loss: 27.0488
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 59.2963
                       Mean reward: 12.10
               Mean episode length: 190.96
    Episode_Reward/reaching_object: 0.5365
     Episode_Reward/lifting_object: 2.9149
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.30s
                      Time elapsed: 00:21:22
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 41597 steps/s (collection: 2.218s, learning 0.146s)
             Mean action noise std: 2.38
          Mean value_function loss: 18.9307
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 59.3213
                       Mean reward: 19.14
               Mean episode length: 181.82
    Episode_Reward/reaching_object: 0.5486
     Episode_Reward/lifting_object: 3.2037
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.36s
                      Time elapsed: 00:21:25
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 42065 steps/s (collection: 2.224s, learning 0.113s)
             Mean action noise std: 2.38
          Mean value_function loss: 39.8855
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 59.3548
                       Mean reward: 21.18
               Mean episode length: 180.64
    Episode_Reward/reaching_object: 0.5514
     Episode_Reward/lifting_object: 3.1495
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.34s
                      Time elapsed: 00:21:27
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 42314 steps/s (collection: 2.184s, learning 0.139s)
             Mean action noise std: 2.38
          Mean value_function loss: 28.5605
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 59.3776
                       Mean reward: 17.41
               Mean episode length: 192.57
    Episode_Reward/reaching_object: 0.5269
     Episode_Reward/lifting_object: 3.2220
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.32s
                      Time elapsed: 00:21:29
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 44162 steps/s (collection: 2.122s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 28.1532
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.4071
                       Mean reward: 19.74
               Mean episode length: 169.89
    Episode_Reward/reaching_object: 0.5151
     Episode_Reward/lifting_object: 2.8158
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.23s
                      Time elapsed: 00:21:32
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 42582 steps/s (collection: 2.170s, learning 0.138s)
             Mean action noise std: 2.38
          Mean value_function loss: 45.1705
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.4275
                       Mean reward: 8.85
               Mean episode length: 189.22
    Episode_Reward/reaching_object: 0.5215
     Episode_Reward/lifting_object: 2.2711
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.31s
                      Time elapsed: 00:21:34
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 43376 steps/s (collection: 2.164s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 28.6232
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.4489
                       Mean reward: 17.80
               Mean episode length: 195.91
    Episode_Reward/reaching_object: 0.5525
     Episode_Reward/lifting_object: 3.5335
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.27s
                      Time elapsed: 00:21:36
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 41226 steps/s (collection: 2.222s, learning 0.163s)
             Mean action noise std: 2.39
          Mean value_function loss: 27.9154
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 59.4851
                       Mean reward: 19.91
               Mean episode length: 199.00
    Episode_Reward/reaching_object: 0.5452
     Episode_Reward/lifting_object: 2.6947
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.38s
                      Time elapsed: 00:21:39
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 42047 steps/s (collection: 2.215s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 20.9139
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.5118
                       Mean reward: 19.57
               Mean episode length: 175.82
    Episode_Reward/reaching_object: 0.5326
     Episode_Reward/lifting_object: 3.0177
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.34s
                      Time elapsed: 00:21:41
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 40150 steps/s (collection: 2.270s, learning 0.178s)
             Mean action noise std: 2.40
          Mean value_function loss: 35.7924
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 59.5345
                       Mean reward: 18.74
               Mean episode length: 188.52
    Episode_Reward/reaching_object: 0.5257
     Episode_Reward/lifting_object: 3.5265
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.45s
                      Time elapsed: 00:21:43
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 43083 steps/s (collection: 2.167s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.1662
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 59.5697
                       Mean reward: 15.03
               Mean episode length: 179.89
    Episode_Reward/reaching_object: 0.5319
     Episode_Reward/lifting_object: 2.7039
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.28s
                      Time elapsed: 00:21:46
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 42024 steps/s (collection: 2.178s, learning 0.161s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.9517
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 59.5990
                       Mean reward: 16.60
               Mean episode length: 172.25
    Episode_Reward/reaching_object: 0.5071
     Episode_Reward/lifting_object: 2.6580
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.34s
                      Time elapsed: 00:21:48
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 46048 steps/s (collection: 2.034s, learning 0.101s)
             Mean action noise std: 2.40
          Mean value_function loss: 47.2905
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.6239
                       Mean reward: 20.79
               Mean episode length: 188.54
    Episode_Reward/reaching_object: 0.5278
     Episode_Reward/lifting_object: 3.4055
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.13s
                      Time elapsed: 00:21:50
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 45299 steps/s (collection: 2.068s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 32.3244
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 59.6464
                       Mean reward: 19.89
               Mean episode length: 166.74
    Episode_Reward/reaching_object: 0.5189
     Episode_Reward/lifting_object: 2.9667
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.17s
                      Time elapsed: 00:21:52
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 44927 steps/s (collection: 2.082s, learning 0.106s)
             Mean action noise std: 2.41
          Mean value_function loss: 40.3143
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.6821
                       Mean reward: 17.66
               Mean episode length: 178.08
    Episode_Reward/reaching_object: 0.5086
     Episode_Reward/lifting_object: 2.8479
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.19s
                      Time elapsed: 00:21:54
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 44328 steps/s (collection: 2.076s, learning 0.142s)
             Mean action noise std: 2.41
          Mean value_function loss: 38.4848
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.7125
                       Mean reward: 15.16
               Mean episode length: 188.85
    Episode_Reward/reaching_object: 0.5230
     Episode_Reward/lifting_object: 2.8643
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.22s
                      Time elapsed: 00:21:57
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 41610 steps/s (collection: 2.184s, learning 0.178s)
             Mean action noise std: 2.42
          Mean value_function loss: 31.2284
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 59.7525
                       Mean reward: 19.10
               Mean episode length: 171.33
    Episode_Reward/reaching_object: 0.5257
     Episode_Reward/lifting_object: 3.5256
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.36s
                      Time elapsed: 00:21:59
                               ETA: 00:57:43

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 40114 steps/s (collection: 2.352s, learning 0.099s)
             Mean action noise std: 2.42
          Mean value_function loss: 26.5470
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 59.7789
                       Mean reward: 13.74
               Mean episode length: 171.31
    Episode_Reward/reaching_object: 0.5111
     Episode_Reward/lifting_object: 2.6781
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.45s
                      Time elapsed: 00:22:01
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 42660 steps/s (collection: 2.185s, learning 0.119s)
             Mean action noise std: 2.42
          Mean value_function loss: 23.8268
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.8048
                       Mean reward: 26.78
               Mean episode length: 163.06
    Episode_Reward/reaching_object: 0.5164
     Episode_Reward/lifting_object: 3.6174
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.30s
                      Time elapsed: 00:22:04
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 42738 steps/s (collection: 2.185s, learning 0.115s)
             Mean action noise std: 2.42
          Mean value_function loss: 30.1446
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.8151
                       Mean reward: 21.26
               Mean episode length: 163.09
    Episode_Reward/reaching_object: 0.5019
     Episode_Reward/lifting_object: 3.3093
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.30s
                      Time elapsed: 00:22:06
                               ETA: 00:57:36

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 45271 steps/s (collection: 2.059s, learning 0.113s)
             Mean action noise std: 2.42
          Mean value_function loss: 35.2826
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.8396
                       Mean reward: 21.88
               Mean episode length: 176.32
    Episode_Reward/reaching_object: 0.5353
     Episode_Reward/lifting_object: 3.9360
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.17s
                      Time elapsed: 00:22:08
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 43375 steps/s (collection: 2.144s, learning 0.122s)
             Mean action noise std: 2.43
          Mean value_function loss: 40.9404
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.8629
                       Mean reward: 16.22
               Mean episode length: 172.25
    Episode_Reward/reaching_object: 0.5367
     Episode_Reward/lifting_object: 3.7064
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.27s
                      Time elapsed: 00:22:11
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 44596 steps/s (collection: 2.085s, learning 0.120s)
             Mean action noise std: 2.43
          Mean value_function loss: 28.8490
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 59.8848
                       Mean reward: 19.38
               Mean episode length: 177.05
    Episode_Reward/reaching_object: 0.5096
     Episode_Reward/lifting_object: 2.9301
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.20s
                      Time elapsed: 00:22:13
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 43120 steps/s (collection: 2.099s, learning 0.181s)
             Mean action noise std: 2.43
          Mean value_function loss: 34.1318
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 59.9041
                       Mean reward: 26.65
               Mean episode length: 170.15
    Episode_Reward/reaching_object: 0.5203
     Episode_Reward/lifting_object: 3.6714
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.28s
                      Time elapsed: 00:22:15
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 44292 steps/s (collection: 2.116s, learning 0.104s)
             Mean action noise std: 2.43
          Mean value_function loss: 39.5851
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.9222
                       Mean reward: 26.76
               Mean episode length: 177.72
    Episode_Reward/reaching_object: 0.5339
     Episode_Reward/lifting_object: 3.6094
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.22s
                      Time elapsed: 00:22:17
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 43214 steps/s (collection: 2.161s, learning 0.114s)
             Mean action noise std: 2.43
          Mean value_function loss: 46.5106
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.9506
                       Mean reward: 20.06
               Mean episode length: 168.42
    Episode_Reward/reaching_object: 0.5087
     Episode_Reward/lifting_object: 2.8957
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.27s
                      Time elapsed: 00:22:19
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 43007 steps/s (collection: 2.166s, learning 0.120s)
             Mean action noise std: 2.44
          Mean value_function loss: 29.0958
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.9747
                       Mean reward: 18.38
               Mean episode length: 165.47
    Episode_Reward/reaching_object: 0.4898
     Episode_Reward/lifting_object: 2.7343
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.29s
                      Time elapsed: 00:22:22
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 43122 steps/s (collection: 2.129s, learning 0.151s)
             Mean action noise std: 2.44
          Mean value_function loss: 36.6330
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.9970
                       Mean reward: 18.19
               Mean episode length: 160.81
    Episode_Reward/reaching_object: 0.4962
     Episode_Reward/lifting_object: 3.4066
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.28s
                      Time elapsed: 00:22:24
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 42633 steps/s (collection: 2.166s, learning 0.140s)
             Mean action noise std: 2.44
          Mean value_function loss: 23.2418
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 60.0115
                       Mean reward: 21.03
               Mean episode length: 169.26
    Episode_Reward/reaching_object: 0.5003
     Episode_Reward/lifting_object: 3.3383
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.31s
                      Time elapsed: 00:22:26
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 41848 steps/s (collection: 2.214s, learning 0.135s)
             Mean action noise std: 2.44
          Mean value_function loss: 33.8341
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.0374
                       Mean reward: 19.39
               Mean episode length: 170.30
    Episode_Reward/reaching_object: 0.5017
     Episode_Reward/lifting_object: 3.8093
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.35s
                      Time elapsed: 00:22:29
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 40436 steps/s (collection: 2.265s, learning 0.166s)
             Mean action noise std: 2.44
          Mean value_function loss: 63.4392
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.0668
                       Mean reward: 19.74
               Mean episode length: 156.19
    Episode_Reward/reaching_object: 0.4885
     Episode_Reward/lifting_object: 3.0270
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.43s
                      Time elapsed: 00:22:31
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 43697 steps/s (collection: 2.118s, learning 0.132s)
             Mean action noise std: 2.45
          Mean value_function loss: 33.9816
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 60.0898
                       Mean reward: 16.64
               Mean episode length: 164.31
    Episode_Reward/reaching_object: 0.4932
     Episode_Reward/lifting_object: 3.3596
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.25s
                      Time elapsed: 00:22:33
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 40850 steps/s (collection: 2.252s, learning 0.155s)
             Mean action noise std: 2.45
          Mean value_function loss: 56.9314
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.1133
                       Mean reward: 15.08
               Mean episode length: 164.20
    Episode_Reward/reaching_object: 0.4825
     Episode_Reward/lifting_object: 3.6319
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.41s
                      Time elapsed: 00:22:36
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 42988 steps/s (collection: 2.126s, learning 0.161s)
             Mean action noise std: 2.45
          Mean value_function loss: 40.6671
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.1381
                       Mean reward: 25.37
               Mean episode length: 180.91
    Episode_Reward/reaching_object: 0.5071
     Episode_Reward/lifting_object: 3.1999
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.29s
                      Time elapsed: 00:22:38
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 44095 steps/s (collection: 2.120s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 37.1264
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 60.1537
                       Mean reward: 19.69
               Mean episode length: 159.03
    Episode_Reward/reaching_object: 0.4899
     Episode_Reward/lifting_object: 3.5538
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.23s
                      Time elapsed: 00:22:40
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 43961 steps/s (collection: 2.137s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 52.7706
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 60.1824
                       Mean reward: 20.24
               Mean episode length: 162.44
    Episode_Reward/reaching_object: 0.4983
     Episode_Reward/lifting_object: 3.6522
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.24s
                      Time elapsed: 00:22:43
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 44698 steps/s (collection: 2.092s, learning 0.107s)
             Mean action noise std: 2.46
          Mean value_function loss: 38.4824
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 60.2056
                       Mean reward: 22.00
               Mean episode length: 154.64
    Episode_Reward/reaching_object: 0.4634
     Episode_Reward/lifting_object: 2.8483
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.20s
                      Time elapsed: 00:22:45
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 38710 steps/s (collection: 2.426s, learning 0.114s)
             Mean action noise std: 2.46
          Mean value_function loss: 53.9279
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 60.2290
                       Mean reward: 19.24
               Mean episode length: 181.00
    Episode_Reward/reaching_object: 0.5122
     Episode_Reward/lifting_object: 3.7232
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.54s
                      Time elapsed: 00:22:47
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 42884 steps/s (collection: 2.185s, learning 0.107s)
             Mean action noise std: 2.46
          Mean value_function loss: 49.8054
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.2545
                       Mean reward: 28.07
               Mean episode length: 175.15
    Episode_Reward/reaching_object: 0.5123
     Episode_Reward/lifting_object: 3.7337
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.29s
                      Time elapsed: 00:22:50
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 43361 steps/s (collection: 2.151s, learning 0.116s)
             Mean action noise std: 2.46
          Mean value_function loss: 36.1984
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.2761
                       Mean reward: 14.04
               Mean episode length: 155.87
    Episode_Reward/reaching_object: 0.4928
     Episode_Reward/lifting_object: 3.3298
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.27s
                      Time elapsed: 00:22:52
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 43179 steps/s (collection: 2.129s, learning 0.148s)
             Mean action noise std: 2.47
          Mean value_function loss: 42.9763
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.2868
                       Mean reward: 16.18
               Mean episode length: 154.69
    Episode_Reward/reaching_object: 0.4912
     Episode_Reward/lifting_object: 3.5234
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.28s
                      Time elapsed: 00:22:54
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 43482 steps/s (collection: 2.154s, learning 0.106s)
             Mean action noise std: 2.47
          Mean value_function loss: 64.5130
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 60.2992
                       Mean reward: 16.76
               Mean episode length: 172.63
    Episode_Reward/reaching_object: 0.5018
     Episode_Reward/lifting_object: 3.8660
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.26s
                      Time elapsed: 00:22:56
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.113s, learning 0.120s)
             Mean action noise std: 2.47
          Mean value_function loss: 40.4303
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 60.3301
                       Mean reward: 30.80
               Mean episode length: 166.06
    Episode_Reward/reaching_object: 0.5090
     Episode_Reward/lifting_object: 3.6315
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.23s
                      Time elapsed: 00:22:59
                               ETA: 00:56:35

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 41401 steps/s (collection: 2.224s, learning 0.150s)
             Mean action noise std: 2.47
          Mean value_function loss: 47.2710
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 60.3678
                       Mean reward: 23.83
               Mean episode length: 167.45
    Episode_Reward/reaching_object: 0.4941
     Episode_Reward/lifting_object: 3.8728
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.37s
                      Time elapsed: 00:23:01
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 43300 steps/s (collection: 2.159s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 54.5549
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.3935
                       Mean reward: 23.73
               Mean episode length: 170.74
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 4.2790
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.27s
                      Time elapsed: 00:23:03
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 42814 steps/s (collection: 2.181s, learning 0.116s)
             Mean action noise std: 2.48
          Mean value_function loss: 45.9565
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 60.4187
                       Mean reward: 26.15
               Mean episode length: 167.40
    Episode_Reward/reaching_object: 0.5187
     Episode_Reward/lifting_object: 4.2342
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.30s
                      Time elapsed: 00:23:06
                               ETA: 00:56:27

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 43127 steps/s (collection: 2.138s, learning 0.141s)
             Mean action noise std: 2.48
          Mean value_function loss: 48.7756
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.4531
                       Mean reward: 27.38
               Mean episode length: 159.27
    Episode_Reward/reaching_object: 0.4849
     Episode_Reward/lifting_object: 3.7823
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.28s
                      Time elapsed: 00:23:08
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 39699 steps/s (collection: 2.325s, learning 0.151s)
             Mean action noise std: 2.48
          Mean value_function loss: 40.5816
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.4779
                       Mean reward: 16.43
               Mean episode length: 150.07
    Episode_Reward/reaching_object: 0.4889
     Episode_Reward/lifting_object: 3.6659
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.48s
                      Time elapsed: 00:23:10
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 41838 steps/s (collection: 2.208s, learning 0.141s)
             Mean action noise std: 2.49
          Mean value_function loss: 47.2598
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 60.4953
                       Mean reward: 29.98
               Mean episode length: 165.37
    Episode_Reward/reaching_object: 0.5093
     Episode_Reward/lifting_object: 4.3145
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.35s
                      Time elapsed: 00:23:13
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 41475 steps/s (collection: 2.246s, learning 0.124s)
             Mean action noise std: 2.49
          Mean value_function loss: 45.2132
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.5180
                       Mean reward: 25.69
               Mean episode length: 153.37
    Episode_Reward/reaching_object: 0.4732
     Episode_Reward/lifting_object: 4.4057
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.37s
                      Time elapsed: 00:23:15
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 42641 steps/s (collection: 2.153s, learning 0.153s)
             Mean action noise std: 2.49
          Mean value_function loss: 43.1392
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.5442
                       Mean reward: 30.42
               Mean episode length: 164.10
    Episode_Reward/reaching_object: 0.5145
     Episode_Reward/lifting_object: 4.8413
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.31s
                      Time elapsed: 00:23:17
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 43563 steps/s (collection: 2.152s, learning 0.105s)
             Mean action noise std: 2.49
          Mean value_function loss: 29.9976
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 60.5627
                       Mean reward: 25.80
               Mean episode length: 155.44
    Episode_Reward/reaching_object: 0.4911
     Episode_Reward/lifting_object: 4.0526
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.26s
                      Time elapsed: 00:23:20
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 42213 steps/s (collection: 2.210s, learning 0.119s)
             Mean action noise std: 2.49
          Mean value_function loss: 34.3259
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 60.5830
                       Mean reward: 27.23
               Mean episode length: 169.80
    Episode_Reward/reaching_object: 0.5123
     Episode_Reward/lifting_object: 4.5230
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.33s
                      Time elapsed: 00:23:22
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 41117 steps/s (collection: 2.274s, learning 0.117s)
             Mean action noise std: 2.50
          Mean value_function loss: 37.8699
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 60.6081
                       Mean reward: 25.64
               Mean episode length: 150.56
    Episode_Reward/reaching_object: 0.4888
     Episode_Reward/lifting_object: 4.2549
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.39s
                      Time elapsed: 00:23:24
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 41405 steps/s (collection: 2.275s, learning 0.099s)
             Mean action noise std: 2.50
          Mean value_function loss: 60.8600
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.6260
                       Mean reward: 22.31
               Mean episode length: 161.19
    Episode_Reward/reaching_object: 0.4794
     Episode_Reward/lifting_object: 3.9257
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.37s
                      Time elapsed: 00:23:27
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 43652 steps/s (collection: 2.144s, learning 0.108s)
             Mean action noise std: 2.50
          Mean value_function loss: 65.9291
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.6441
                       Mean reward: 25.59
               Mean episode length: 169.27
    Episode_Reward/reaching_object: 0.4917
     Episode_Reward/lifting_object: 4.3166
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.25s
                      Time elapsed: 00:23:29
                               ETA: 00:56:02

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 42684 steps/s (collection: 2.142s, learning 0.162s)
             Mean action noise std: 2.50
          Mean value_function loss: 54.3795
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 60.6666
                       Mean reward: 22.44
               Mean episode length: 147.88
    Episode_Reward/reaching_object: 0.4902
     Episode_Reward/lifting_object: 4.3246
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.30s
                      Time elapsed: 00:23:31
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 43308 steps/s (collection: 2.172s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 51.9620
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 60.6947
                       Mean reward: 26.82
               Mean episode length: 155.96
    Episode_Reward/reaching_object: 0.4858
     Episode_Reward/lifting_object: 3.4505
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.27s
                      Time elapsed: 00:23:34
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 44258 steps/s (collection: 2.111s, learning 0.111s)
             Mean action noise std: 2.51
          Mean value_function loss: 54.5900
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 60.7243
                       Mean reward: 14.23
               Mean episode length: 170.12
    Episode_Reward/reaching_object: 0.5028
     Episode_Reward/lifting_object: 4.5298
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.22s
                      Time elapsed: 00:23:36
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 43701 steps/s (collection: 2.141s, learning 0.108s)
             Mean action noise std: 2.51
          Mean value_function loss: 37.9687
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 60.7436
                       Mean reward: 27.26
               Mean episode length: 151.81
    Episode_Reward/reaching_object: 0.5000
     Episode_Reward/lifting_object: 4.9531
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.25s
                      Time elapsed: 00:23:38
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 42336 steps/s (collection: 2.180s, learning 0.142s)
             Mean action noise std: 2.51
          Mean value_function loss: 41.1130
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.7676
                       Mean reward: 23.81
               Mean episode length: 165.66
    Episode_Reward/reaching_object: 0.4944
     Episode_Reward/lifting_object: 4.3695
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.32s
                      Time elapsed: 00:23:40
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 43050 steps/s (collection: 2.181s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 56.8801
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 60.7870
                       Mean reward: 24.15
               Mean episode length: 151.72
    Episode_Reward/reaching_object: 0.4976
     Episode_Reward/lifting_object: 4.6654
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.28s
                      Time elapsed: 00:23:43
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 42486 steps/s (collection: 2.199s, learning 0.115s)
             Mean action noise std: 2.52
          Mean value_function loss: 53.8150
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.8059
                       Mean reward: 22.38
               Mean episode length: 165.70
    Episode_Reward/reaching_object: 0.5036
     Episode_Reward/lifting_object: 3.9501
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.31s
                      Time elapsed: 00:23:45
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 42626 steps/s (collection: 2.140s, learning 0.166s)
             Mean action noise std: 2.52
          Mean value_function loss: 43.6547
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 60.8195
                       Mean reward: 28.56
               Mean episode length: 159.93
    Episode_Reward/reaching_object: 0.5083
     Episode_Reward/lifting_object: 4.6650
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.31s
                      Time elapsed: 00:23:47
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 43293 steps/s (collection: 2.164s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 34.3670
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.8361
                       Mean reward: 26.11
               Mean episode length: 149.75
    Episode_Reward/reaching_object: 0.4804
     Episode_Reward/lifting_object: 4.2803
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.27s
                      Time elapsed: 00:23:49
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 43551 steps/s (collection: 2.152s, learning 0.105s)
             Mean action noise std: 2.52
          Mean value_function loss: 39.7615
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 60.8496
                       Mean reward: 23.85
               Mean episode length: 149.80
    Episode_Reward/reaching_object: 0.4858
     Episode_Reward/lifting_object: 4.8176
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.26s
                      Time elapsed: 00:23:52
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 43087 steps/s (collection: 2.163s, learning 0.118s)
             Mean action noise std: 2.52
          Mean value_function loss: 36.7441
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.8651
                       Mean reward: 32.87
               Mean episode length: 159.41
    Episode_Reward/reaching_object: 0.5104
     Episode_Reward/lifting_object: 5.2593
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.28s
                      Time elapsed: 00:23:54
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 40779 steps/s (collection: 2.312s, learning 0.099s)
             Mean action noise std: 2.52
          Mean value_function loss: 43.1218
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 60.8800
                       Mean reward: 25.95
               Mean episode length: 162.24
    Episode_Reward/reaching_object: 0.4977
     Episode_Reward/lifting_object: 4.9761
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.41s
                      Time elapsed: 00:23:56
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 42268 steps/s (collection: 2.221s, learning 0.105s)
             Mean action noise std: 2.53
          Mean value_function loss: 49.5746
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.9038
                       Mean reward: 29.73
               Mean episode length: 150.72
    Episode_Reward/reaching_object: 0.5048
     Episode_Reward/lifting_object: 5.4420
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.33s
                      Time elapsed: 00:23:59
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 38014 steps/s (collection: 2.396s, learning 0.190s)
             Mean action noise std: 2.53
          Mean value_function loss: 45.6484
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.9309
                       Mean reward: 34.51
               Mean episode length: 159.04
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: 5.2793
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.59s
                      Time elapsed: 00:24:01
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 43120 steps/s (collection: 2.149s, learning 0.131s)
             Mean action noise std: 2.53
          Mean value_function loss: 54.7810
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 60.9550
                       Mean reward: 23.80
               Mean episode length: 154.61
    Episode_Reward/reaching_object: 0.5113
     Episode_Reward/lifting_object: 5.4182
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.28s
                      Time elapsed: 00:24:04
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 43371 steps/s (collection: 2.131s, learning 0.135s)
             Mean action noise std: 2.53
          Mean value_function loss: 64.3805
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.9695
                       Mean reward: 32.25
               Mean episode length: 162.94
    Episode_Reward/reaching_object: 0.5316
     Episode_Reward/lifting_object: 4.8190
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.27s
                      Time elapsed: 00:24:06
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44049 steps/s (collection: 2.118s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 43.0579
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.9851
                       Mean reward: 31.01
               Mean episode length: 163.24
    Episode_Reward/reaching_object: 0.5258
     Episode_Reward/lifting_object: 5.3481
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.23s
                      Time elapsed: 00:24:08
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 41530 steps/s (collection: 2.238s, learning 0.129s)
             Mean action noise std: 2.53
          Mean value_function loss: 32.1789
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 60.9892
                       Mean reward: 34.86
               Mean episode length: 162.77
    Episode_Reward/reaching_object: 0.5206
     Episode_Reward/lifting_object: 5.6500
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.37s
                      Time elapsed: 00:24:10
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 42692 steps/s (collection: 2.201s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 46.3073
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 60.9934
                       Mean reward: 27.67
               Mean episode length: 168.17
    Episode_Reward/reaching_object: 0.5096
     Episode_Reward/lifting_object: 5.3115
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.30s
                      Time elapsed: 00:24:13
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 43679 steps/s (collection: 2.133s, learning 0.117s)
             Mean action noise std: 2.53
          Mean value_function loss: 47.0693
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.9979
                       Mean reward: 20.91
               Mean episode length: 156.52
    Episode_Reward/reaching_object: 0.5170
     Episode_Reward/lifting_object: 5.0419
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.25s
                      Time elapsed: 00:24:15
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 42124 steps/s (collection: 2.189s, learning 0.144s)
             Mean action noise std: 2.54
          Mean value_function loss: 60.8710
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.0133
                       Mean reward: 31.16
               Mean episode length: 166.21
    Episode_Reward/reaching_object: 0.5215
     Episode_Reward/lifting_object: 5.4669
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.33s
                      Time elapsed: 00:24:17
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 42386 steps/s (collection: 2.205s, learning 0.114s)
             Mean action noise std: 2.54
          Mean value_function loss: 32.3728
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 61.0360
                       Mean reward: 33.95
               Mean episode length: 163.78
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 5.3082
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.32s
                      Time elapsed: 00:24:20
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 43934 steps/s (collection: 2.138s, learning 0.100s)
             Mean action noise std: 2.54
          Mean value_function loss: 61.8795
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.0524
                       Mean reward: 33.80
               Mean episode length: 164.24
    Episode_Reward/reaching_object: 0.5241
     Episode_Reward/lifting_object: 6.0356
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.24s
                      Time elapsed: 00:24:22
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 42197 steps/s (collection: 2.182s, learning 0.148s)
             Mean action noise std: 2.54
          Mean value_function loss: 45.7842
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 61.0725
                       Mean reward: 30.79
               Mean episode length: 165.20
    Episode_Reward/reaching_object: 0.5273
     Episode_Reward/lifting_object: 5.4678
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.33s
                      Time elapsed: 00:24:24
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 42525 steps/s (collection: 2.205s, learning 0.107s)
             Mean action noise std: 2.54
          Mean value_function loss: 46.9527
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.0895
                       Mean reward: 28.73
               Mean episode length: 164.92
    Episode_Reward/reaching_object: 0.5388
     Episode_Reward/lifting_object: 5.4342
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.31s
                      Time elapsed: 00:24:27
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 42513 steps/s (collection: 2.202s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 30.1630
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 61.1016
                       Mean reward: 27.93
               Mean episode length: 152.82
    Episode_Reward/reaching_object: 0.5404
     Episode_Reward/lifting_object: 5.5899
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.31s
                      Time elapsed: 00:24:29
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 42936 steps/s (collection: 2.186s, learning 0.104s)
             Mean action noise std: 2.55
          Mean value_function loss: 31.0553
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.1170
                       Mean reward: 29.84
               Mean episode length: 163.30
    Episode_Reward/reaching_object: 0.5321
     Episode_Reward/lifting_object: 6.2850
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.29s
                      Time elapsed: 00:24:31
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 42929 steps/s (collection: 2.152s, learning 0.138s)
             Mean action noise std: 2.55
          Mean value_function loss: 38.4262
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 61.1268
                       Mean reward: 27.88
               Mean episode length: 162.57
    Episode_Reward/reaching_object: 0.5628
     Episode_Reward/lifting_object: 6.5933
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.29s
                      Time elapsed: 00:24:33
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 42260 steps/s (collection: 2.218s, learning 0.109s)
             Mean action noise std: 2.55
          Mean value_function loss: 32.4412
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.1392
                       Mean reward: 31.03
               Mean episode length: 144.25
    Episode_Reward/reaching_object: 0.5476
     Episode_Reward/lifting_object: 6.1310
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.33s
                      Time elapsed: 00:24:36
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 39160 steps/s (collection: 2.364s, learning 0.147s)
             Mean action noise std: 2.55
          Mean value_function loss: 54.5227
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 61.1454
                       Mean reward: 29.25
               Mean episode length: 161.11
    Episode_Reward/reaching_object: 0.5107
     Episode_Reward/lifting_object: 5.9256
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.51s
                      Time elapsed: 00:24:38
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 41084 steps/s (collection: 2.277s, learning 0.116s)
             Mean action noise std: 2.55
          Mean value_function loss: 45.9774
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.1532
                       Mean reward: 40.74
               Mean episode length: 156.85
    Episode_Reward/reaching_object: 0.5280
     Episode_Reward/lifting_object: 6.4500
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.39s
                      Time elapsed: 00:24:41
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 43372 steps/s (collection: 2.165s, learning 0.102s)
             Mean action noise std: 2.55
          Mean value_function loss: 37.6805
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 61.1777
                       Mean reward: 34.75
               Mean episode length: 161.93
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 6.0815
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.27s
                      Time elapsed: 00:24:43
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 43103 steps/s (collection: 2.138s, learning 0.142s)
             Mean action noise std: 2.56
          Mean value_function loss: 46.6009
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.2031
                       Mean reward: 35.50
               Mean episode length: 151.27
    Episode_Reward/reaching_object: 0.5107
     Episode_Reward/lifting_object: 5.7278
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.28s
                      Time elapsed: 00:24:45
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 38739 steps/s (collection: 2.403s, learning 0.135s)
             Mean action noise std: 2.56
          Mean value_function loss: 33.4135
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 61.2262
                       Mean reward: 39.69
               Mean episode length: 154.92
    Episode_Reward/reaching_object: 0.5248
     Episode_Reward/lifting_object: 6.0387
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.54s
                      Time elapsed: 00:24:48
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 38263 steps/s (collection: 2.406s, learning 0.163s)
             Mean action noise std: 2.56
          Mean value_function loss: 34.5071
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 61.2412
                       Mean reward: 33.10
               Mean episode length: 156.96
    Episode_Reward/reaching_object: 0.5221
     Episode_Reward/lifting_object: 6.2278
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.57s
                      Time elapsed: 00:24:50
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 41103 steps/s (collection: 2.285s, learning 0.107s)
             Mean action noise std: 2.56
          Mean value_function loss: 35.4535
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 61.2583
                       Mean reward: 36.32
               Mean episode length: 156.57
    Episode_Reward/reaching_object: 0.5131
     Episode_Reward/lifting_object: 6.6120
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.39s
                      Time elapsed: 00:24:53
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 43766 steps/s (collection: 2.114s, learning 0.132s)
             Mean action noise std: 2.56
          Mean value_function loss: 40.9013
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 61.2754
                       Mean reward: 32.11
               Mean episode length: 150.51
    Episode_Reward/reaching_object: 0.5240
     Episode_Reward/lifting_object: 6.7583
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.25s
                      Time elapsed: 00:24:55
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 44134 steps/s (collection: 2.115s, learning 0.113s)
             Mean action noise std: 2.56
          Mean value_function loss: 46.2081
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.2921
                       Mean reward: 39.50
               Mean episode length: 166.46
    Episode_Reward/reaching_object: 0.5613
     Episode_Reward/lifting_object: 6.6383
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.23s
                      Time elapsed: 00:24:57
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 42765 steps/s (collection: 2.135s, learning 0.164s)
             Mean action noise std: 2.57
          Mean value_function loss: 39.6622
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.3055
                       Mean reward: 33.41
               Mean episode length: 164.67
    Episode_Reward/reaching_object: 0.5361
     Episode_Reward/lifting_object: 6.7223
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.30s
                      Time elapsed: 00:25:00
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 41669 steps/s (collection: 2.190s, learning 0.169s)
             Mean action noise std: 2.57
          Mean value_function loss: 47.0935
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.3137
                       Mean reward: 34.59
               Mean episode length: 156.39
    Episode_Reward/reaching_object: 0.5209
     Episode_Reward/lifting_object: 6.4514
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.36s
                      Time elapsed: 00:25:02
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 44626 steps/s (collection: 2.099s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 84.6689
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.3277
                       Mean reward: 36.34
               Mean episode length: 150.24
    Episode_Reward/reaching_object: 0.5502
     Episode_Reward/lifting_object: 6.8583
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.20s
                      Time elapsed: 00:25:04
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 42425 steps/s (collection: 2.132s, learning 0.185s)
             Mean action noise std: 2.57
          Mean value_function loss: 103.6227
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 61.3555
                       Mean reward: 33.16
               Mean episode length: 156.67
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: 6.8438
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.32s
                      Time elapsed: 00:25:06
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 41537 steps/s (collection: 2.169s, learning 0.198s)
             Mean action noise std: 2.57
          Mean value_function loss: 45.5584
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.3855
                       Mean reward: 28.05
               Mean episode length: 141.92
    Episode_Reward/reaching_object: 0.5217
     Episode_Reward/lifting_object: 5.9109
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.37s
                      Time elapsed: 00:25:09
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 39276 steps/s (collection: 2.380s, learning 0.123s)
             Mean action noise std: 2.58
          Mean value_function loss: 57.5347
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 61.4021
                       Mean reward: 42.38
               Mean episode length: 168.68
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 6.8910
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.50s
                      Time elapsed: 00:25:11
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 41776 steps/s (collection: 2.249s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 67.6009
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 61.4163
                       Mean reward: 35.49
               Mean episode length: 145.46
    Episode_Reward/reaching_object: 0.5267
     Episode_Reward/lifting_object: 6.8809
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.35s
                      Time elapsed: 00:25:14
                               ETA: 00:54:09

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 41298 steps/s (collection: 2.245s, learning 0.135s)
             Mean action noise std: 2.58
          Mean value_function loss: 66.0338
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.4338
                       Mean reward: 38.21
               Mean episode length: 151.80
    Episode_Reward/reaching_object: 0.5543
     Episode_Reward/lifting_object: 7.0808
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.38s
                      Time elapsed: 00:25:16
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 43641 steps/s (collection: 2.137s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 52.0474
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 61.4516
                       Mean reward: 30.51
               Mean episode length: 156.39
    Episode_Reward/reaching_object: 0.5395
     Episode_Reward/lifting_object: 7.5462
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.25s
                      Time elapsed: 00:25:18
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 43394 steps/s (collection: 2.125s, learning 0.140s)
             Mean action noise std: 2.58
          Mean value_function loss: 59.9135
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 61.4685
                       Mean reward: 34.28
               Mean episode length: 153.36
    Episode_Reward/reaching_object: 0.5632
     Episode_Reward/lifting_object: 8.1392
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.27s
                      Time elapsed: 00:25:21
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 43728 steps/s (collection: 2.149s, learning 0.099s)
             Mean action noise std: 2.58
          Mean value_function loss: 59.5319
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 61.4855
                       Mean reward: 40.95
               Mean episode length: 159.12
    Episode_Reward/reaching_object: 0.5604
     Episode_Reward/lifting_object: 7.3721
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.25s
                      Time elapsed: 00:25:23
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 42886 steps/s (collection: 2.165s, learning 0.127s)
             Mean action noise std: 2.58
          Mean value_function loss: 59.3725
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 61.4904
                       Mean reward: 43.72
               Mean episode length: 158.47
    Episode_Reward/reaching_object: 0.5454
     Episode_Reward/lifting_object: 7.3197
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.29s
                      Time elapsed: 00:25:25
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 42629 steps/s (collection: 2.192s, learning 0.114s)
             Mean action noise std: 2.58
          Mean value_function loss: 68.8518
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 61.4922
                       Mean reward: 39.50
               Mean episode length: 140.85
    Episode_Reward/reaching_object: 0.5599
     Episode_Reward/lifting_object: 7.9849
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.31s
                      Time elapsed: 00:25:27
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 42883 steps/s (collection: 2.135s, learning 0.158s)
             Mean action noise std: 2.59
          Mean value_function loss: 52.6591
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.4962
                       Mean reward: 29.10
               Mean episode length: 141.25
    Episode_Reward/reaching_object: 0.5578
     Episode_Reward/lifting_object: 7.2795
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.29s
                      Time elapsed: 00:25:30
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 43071 steps/s (collection: 2.182s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 78.0525
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.5045
                       Mean reward: 52.16
               Mean episode length: 144.45
    Episode_Reward/reaching_object: 0.5519
     Episode_Reward/lifting_object: 8.3544
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.28s
                      Time elapsed: 00:25:32
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 44121 steps/s (collection: 2.114s, learning 0.114s)
             Mean action noise std: 2.59
          Mean value_function loss: 69.4005
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.5209
                       Mean reward: 37.30
               Mean episode length: 159.28
    Episode_Reward/reaching_object: 0.5711
     Episode_Reward/lifting_object: 8.4374
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.23s
                      Time elapsed: 00:25:34
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 42188 steps/s (collection: 2.188s, learning 0.143s)
             Mean action noise std: 2.59
          Mean value_function loss: 51.1691
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.5424
                       Mean reward: 45.02
               Mean episode length: 151.49
    Episode_Reward/reaching_object: 0.5392
     Episode_Reward/lifting_object: 7.8514
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.33s
                      Time elapsed: 00:25:36
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 42954 steps/s (collection: 2.187s, learning 0.102s)
             Mean action noise std: 2.59
          Mean value_function loss: 73.7725
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.5593
                       Mean reward: 44.78
               Mean episode length: 139.48
    Episode_Reward/reaching_object: 0.5515
     Episode_Reward/lifting_object: 8.5801
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.29s
                      Time elapsed: 00:25:39
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 42840 steps/s (collection: 2.181s, learning 0.114s)
             Mean action noise std: 2.59
          Mean value_function loss: 93.7506
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 61.5739
                       Mean reward: 39.69
               Mean episode length: 150.41
    Episode_Reward/reaching_object: 0.5413
     Episode_Reward/lifting_object: 8.2225
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.29s
                      Time elapsed: 00:25:41
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 44230 steps/s (collection: 2.119s, learning 0.103s)
             Mean action noise std: 2.59
          Mean value_function loss: 63.4090
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 61.5883
                       Mean reward: 43.88
               Mean episode length: 152.98
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 8.0227
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.22s
                      Time elapsed: 00:25:43
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 42821 steps/s (collection: 2.108s, learning 0.188s)
             Mean action noise std: 2.60
          Mean value_function loss: 85.2237
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.5942
                       Mean reward: 45.95
               Mean episode length: 145.14
    Episode_Reward/reaching_object: 0.5378
     Episode_Reward/lifting_object: 8.3302
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.30s
                      Time elapsed: 00:25:46
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 42343 steps/s (collection: 2.174s, learning 0.148s)
             Mean action noise std: 2.60
          Mean value_function loss: 88.2183
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.6038
                       Mean reward: 37.83
               Mean episode length: 155.18
    Episode_Reward/reaching_object: 0.5477
     Episode_Reward/lifting_object: 8.1052
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.32s
                      Time elapsed: 00:25:48
                               ETA: 00:53:30

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 39849 steps/s (collection: 2.357s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 75.1342
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.6092
                       Mean reward: 38.45
               Mean episode length: 141.84
    Episode_Reward/reaching_object: 0.5473
     Episode_Reward/lifting_object: 7.7440
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.47s
                      Time elapsed: 00:25:50
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 40794 steps/s (collection: 2.295s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 68.8839
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.6157
                       Mean reward: 49.34
               Mean episode length: 142.99
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 8.2648
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.41s
                      Time elapsed: 00:25:53
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 42492 steps/s (collection: 2.217s, learning 0.097s)
             Mean action noise std: 2.60
          Mean value_function loss: 73.4327
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 61.6244
                       Mean reward: 47.07
               Mean episode length: 144.49
    Episode_Reward/reaching_object: 0.5499
     Episode_Reward/lifting_object: 8.5861
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.31s
                      Time elapsed: 00:25:55
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 42642 steps/s (collection: 2.172s, learning 0.133s)
             Mean action noise std: 2.60
          Mean value_function loss: 82.9388
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 61.6403
                       Mean reward: 54.91
               Mean episode length: 139.46
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 9.3962
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.31s
                      Time elapsed: 00:25:57
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 41119 steps/s (collection: 2.266s, learning 0.125s)
             Mean action noise std: 2.60
          Mean value_function loss: 92.9948
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.6554
                       Mean reward: 46.95
               Mean episode length: 129.78
    Episode_Reward/reaching_object: 0.5471
     Episode_Reward/lifting_object: 8.4849
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.39s
                      Time elapsed: 00:26:00
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 41263 steps/s (collection: 2.268s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 114.4367
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.6735
                       Mean reward: 46.85
               Mean episode length: 147.04
    Episode_Reward/reaching_object: 0.5516
     Episode_Reward/lifting_object: 8.9275
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.38s
                      Time elapsed: 00:26:02
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 38468 steps/s (collection: 2.315s, learning 0.241s)
             Mean action noise std: 2.61
          Mean value_function loss: 91.4028
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.6959
                       Mean reward: 44.09
               Mean episode length: 149.95
    Episode_Reward/reaching_object: 0.5803
     Episode_Reward/lifting_object: 9.4340
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.56s
                      Time elapsed: 00:26:05
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 42773 steps/s (collection: 2.193s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 82.7208
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.7120
                       Mean reward: 50.74
               Mean episode length: 141.72
    Episode_Reward/reaching_object: 0.5745
     Episode_Reward/lifting_object: 9.3365
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.30s
                      Time elapsed: 00:26:07
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 39901 steps/s (collection: 2.357s, learning 0.107s)
             Mean action noise std: 2.61
          Mean value_function loss: 86.5275
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 61.7269
                       Mean reward: 47.58
               Mean episode length: 142.82
    Episode_Reward/reaching_object: 0.5405
     Episode_Reward/lifting_object: 8.5948
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.46s
                      Time elapsed: 00:26:09
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 41836 steps/s (collection: 2.191s, learning 0.159s)
             Mean action noise std: 2.61
          Mean value_function loss: 85.3000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.7390
                       Mean reward: 34.35
               Mean episode length: 140.49
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 8.7567
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.35s
                      Time elapsed: 00:26:12
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 41100 steps/s (collection: 2.243s, learning 0.149s)
             Mean action noise std: 2.61
          Mean value_function loss: 89.2291
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 61.7468
                       Mean reward: 49.50
               Mean episode length: 144.26
    Episode_Reward/reaching_object: 0.5838
     Episode_Reward/lifting_object: 9.5971
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.39s
                      Time elapsed: 00:26:14
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 41493 steps/s (collection: 2.270s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 81.9440
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.7521
                       Mean reward: 56.91
               Mean episode length: 139.98
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 9.7626
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.37s
                      Time elapsed: 00:26:17
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 42787 steps/s (collection: 2.188s, learning 0.109s)
             Mean action noise std: 2.61
          Mean value_function loss: 115.8038
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.7608
                       Mean reward: 43.48
               Mean episode length: 142.67
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 10.7437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.30s
                      Time elapsed: 00:26:19
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 42463 steps/s (collection: 2.205s, learning 0.110s)
             Mean action noise std: 2.61
          Mean value_function loss: 99.4410
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.7713
                       Mean reward: 55.52
               Mean episode length: 134.79
    Episode_Reward/reaching_object: 0.5909
     Episode_Reward/lifting_object: 10.3466
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.32s
                      Time elapsed: 00:26:21
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 42218 steps/s (collection: 2.222s, learning 0.106s)
             Mean action noise std: 2.62
          Mean value_function loss: 99.6336
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 61.7801
                       Mean reward: 55.88
               Mean episode length: 148.84
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 10.5074
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.33s
                      Time elapsed: 00:26:24
                               ETA: 00:52:55

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 24570 steps/s (collection: 3.881s, learning 0.120s)
             Mean action noise std: 2.62
          Mean value_function loss: 83.5038
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.7928
                       Mean reward: 59.71
               Mean episode length: 146.23
    Episode_Reward/reaching_object: 0.5939
     Episode_Reward/lifting_object: 10.7661
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 4.00s
                      Time elapsed: 00:26:28
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13542 steps/s (collection: 7.138s, learning 0.122s)
             Mean action noise std: 2.62
          Mean value_function loss: 90.8563
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.8020
                       Mean reward: 55.30
               Mean episode length: 136.19
    Episode_Reward/reaching_object: 0.6189
     Episode_Reward/lifting_object: 11.7105
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.26s
                      Time elapsed: 00:26:35
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 13457 steps/s (collection: 7.157s, learning 0.147s)
             Mean action noise std: 2.62
          Mean value_function loss: 133.2119
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.8123
                       Mean reward: 59.09
               Mean episode length: 138.75
    Episode_Reward/reaching_object: 0.6163
     Episode_Reward/lifting_object: 11.7759
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.30s
                      Time elapsed: 00:26:42
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13341 steps/s (collection: 7.249s, learning 0.120s)
             Mean action noise std: 2.62
          Mean value_function loss: 110.8796
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.8220
                       Mean reward: 56.53
               Mean episode length: 138.40
    Episode_Reward/reaching_object: 0.5982
     Episode_Reward/lifting_object: 10.4295
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.37s
                      Time elapsed: 00:26:49
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13801 steps/s (collection: 7.008s, learning 0.115s)
             Mean action noise std: 2.62
          Mean value_function loss: 93.0098
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 61.8276
                       Mean reward: 59.07
               Mean episode length: 139.86
    Episode_Reward/reaching_object: 0.6007
     Episode_Reward/lifting_object: 11.1784
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.12s
                      Time elapsed: 00:26:57
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 13643 steps/s (collection: 7.080s, learning 0.125s)
             Mean action noise std: 2.62
          Mean value_function loss: 85.5803
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 61.8339
                       Mean reward: 49.39
               Mean episode length: 140.46
    Episode_Reward/reaching_object: 0.6143
     Episode_Reward/lifting_object: 11.6264
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.21s
                      Time elapsed: 00:27:04
                               ETA: 00:53:32

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14154 steps/s (collection: 6.827s, learning 0.118s)
             Mean action noise std: 2.62
          Mean value_function loss: 93.2270
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 61.8447
                       Mean reward: 65.60
               Mean episode length: 150.62
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 12.1993
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.95s
                      Time elapsed: 00:27:11
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13688 steps/s (collection: 7.054s, learning 0.128s)
             Mean action noise std: 2.62
          Mean value_function loss: 111.9929
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 61.8515
                       Mean reward: 71.10
               Mean episode length: 153.34
    Episode_Reward/reaching_object: 0.6569
     Episode_Reward/lifting_object: 13.5618
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.18s
                      Time elapsed: 00:27:18
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13773 steps/s (collection: 7.024s, learning 0.113s)
             Mean action noise std: 2.62
          Mean value_function loss: 85.4772
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 61.8538
                       Mean reward: 63.86
               Mean episode length: 151.85
    Episode_Reward/reaching_object: 0.6205
     Episode_Reward/lifting_object: 11.7594
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.14s
                      Time elapsed: 00:27:25
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21543 steps/s (collection: 4.462s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 175.2089
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 61.8553
                       Mean reward: 73.13
               Mean episode length: 153.71
    Episode_Reward/reaching_object: 0.6597
     Episode_Reward/lifting_object: 13.2061
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.56s
                      Time elapsed: 00:27:30
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 43332 steps/s (collection: 2.149s, learning 0.120s)
             Mean action noise std: 2.62
          Mean value_function loss: 109.6878
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 61.8573
                       Mean reward: 64.66
               Mean episode length: 158.37
    Episode_Reward/reaching_object: 0.6284
     Episode_Reward/lifting_object: 11.8723
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.27s
                      Time elapsed: 00:27:32
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 42755 steps/s (collection: 2.179s, learning 0.120s)
             Mean action noise std: 2.62
          Mean value_function loss: 86.6023
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 61.8593
                       Mean reward: 68.65
               Mean episode length: 154.68
    Episode_Reward/reaching_object: 0.6471
     Episode_Reward/lifting_object: 12.7663
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.30s
                      Time elapsed: 00:27:34
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 42133 steps/s (collection: 2.206s, learning 0.127s)
             Mean action noise std: 2.62
          Mean value_function loss: 93.7508
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.8628
                       Mean reward: 69.96
               Mean episode length: 146.08
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 13.2705
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.33s
                      Time elapsed: 00:27:37
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 43943 steps/s (collection: 2.141s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 93.8132
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.8705
                       Mean reward: 61.20
               Mean episode length: 154.27
    Episode_Reward/reaching_object: 0.6367
     Episode_Reward/lifting_object: 12.3899
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.24s
                      Time elapsed: 00:27:39
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 43933 steps/s (collection: 2.133s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 87.9955
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.8799
                       Mean reward: 73.34
               Mean episode length: 144.02
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 14.2949
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.24s
                      Time elapsed: 00:27:41
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 41997 steps/s (collection: 2.196s, learning 0.145s)
             Mean action noise std: 2.63
          Mean value_function loss: 98.1555
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.8859
                       Mean reward: 53.57
               Mean episode length: 134.68
    Episode_Reward/reaching_object: 0.6613
     Episode_Reward/lifting_object: 13.5383
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.34s
                      Time elapsed: 00:27:43
                               ETA: 00:53:37

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 43295 steps/s (collection: 2.158s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 92.0721
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 61.8940
                       Mean reward: 77.09
               Mean episode length: 147.12
    Episode_Reward/reaching_object: 0.6605
     Episode_Reward/lifting_object: 14.4981
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.27s
                      Time elapsed: 00:27:46
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 43397 steps/s (collection: 2.154s, learning 0.111s)
             Mean action noise std: 2.63
          Mean value_function loss: 101.6293
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 61.8998
                       Mean reward: 72.10
               Mean episode length: 129.14
    Episode_Reward/reaching_object: 0.6574
     Episode_Reward/lifting_object: 14.4957
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.27s
                      Time elapsed: 00:27:48
                               ETA: 00:53:32

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 43286 steps/s (collection: 2.168s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 83.4591
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 61.9031
                       Mean reward: 77.42
               Mean episode length: 149.93
    Episode_Reward/reaching_object: 0.6767
     Episode_Reward/lifting_object: 14.6478
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.27s
                      Time elapsed: 00:27:50
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 41620 steps/s (collection: 2.206s, learning 0.156s)
             Mean action noise std: 2.63
          Mean value_function loss: 128.5518
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.9087
                       Mean reward: 80.15
               Mean episode length: 141.97
    Episode_Reward/reaching_object: 0.6865
     Episode_Reward/lifting_object: 15.1484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.36s
                      Time elapsed: 00:27:53
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 42820 steps/s (collection: 2.188s, learning 0.108s)
             Mean action noise std: 2.63
          Mean value_function loss: 146.9399
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 61.9167
                       Mean reward: 78.24
               Mean episode length: 146.10
    Episode_Reward/reaching_object: 0.6795
     Episode_Reward/lifting_object: 15.0903
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.30s
                      Time elapsed: 00:27:55
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 41568 steps/s (collection: 2.221s, learning 0.144s)
             Mean action noise std: 2.63
          Mean value_function loss: 129.2551
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 61.9253
                       Mean reward: 80.78
               Mean episode length: 147.31
    Episode_Reward/reaching_object: 0.6742
     Episode_Reward/lifting_object: 14.6932
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.36s
                      Time elapsed: 00:27:57
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 28117 steps/s (collection: 3.178s, learning 0.319s)
             Mean action noise std: 2.63
          Mean value_function loss: 152.8234
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.9335
                       Mean reward: 84.44
               Mean episode length: 151.57
    Episode_Reward/reaching_object: 0.6556
     Episode_Reward/lifting_object: 14.3245
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 3.50s
                      Time elapsed: 00:28:01
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 34705 steps/s (collection: 2.671s, learning 0.162s)
             Mean action noise std: 2.63
          Mean value_function loss: 139.1721
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.9418
                       Mean reward: 83.79
               Mean episode length: 148.07
    Episode_Reward/reaching_object: 0.6795
     Episode_Reward/lifting_object: 15.3428
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.83s
                      Time elapsed: 00:28:04
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 41608 steps/s (collection: 2.266s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 147.9711
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.9500
                       Mean reward: 78.34
               Mean episode length: 134.47
    Episode_Reward/reaching_object: 0.6648
     Episode_Reward/lifting_object: 14.7626
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.36s
                      Time elapsed: 00:28:06
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 41326 steps/s (collection: 2.226s, learning 0.153s)
             Mean action noise std: 2.63
          Mean value_function loss: 137.5973
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.9559
                       Mean reward: 83.11
               Mean episode length: 140.85
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 14.5203
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.38s
                      Time elapsed: 00:28:08
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 42354 steps/s (collection: 2.180s, learning 0.141s)
             Mean action noise std: 2.64
          Mean value_function loss: 137.5888
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 61.9635
                       Mean reward: 79.05
               Mean episode length: 150.39
    Episode_Reward/reaching_object: 0.6723
     Episode_Reward/lifting_object: 15.2537
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.32s
                      Time elapsed: 00:28:11
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 42880 steps/s (collection: 2.191s, learning 0.102s)
             Mean action noise std: 2.64
          Mean value_function loss: 153.1951
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.9760
                       Mean reward: 88.73
               Mean episode length: 149.21
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 16.8105
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.29s
                      Time elapsed: 00:28:13
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 42172 steps/s (collection: 2.219s, learning 0.112s)
             Mean action noise std: 2.64
          Mean value_function loss: 156.0973
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 61.9845
                       Mean reward: 77.96
               Mean episode length: 139.13
    Episode_Reward/reaching_object: 0.6936
     Episode_Reward/lifting_object: 16.1989
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.33s
                      Time elapsed: 00:28:15
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 42682 steps/s (collection: 2.193s, learning 0.110s)
             Mean action noise std: 2.64
          Mean value_function loss: 143.8561
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.9872
                       Mean reward: 91.19
               Mean episode length: 136.33
    Episode_Reward/reaching_object: 0.6607
     Episode_Reward/lifting_object: 16.0872
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.30s
                      Time elapsed: 00:28:17
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 42199 steps/s (collection: 2.208s, learning 0.122s)
             Mean action noise std: 2.64
          Mean value_function loss: 156.2543
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 61.9936
                       Mean reward: 92.77
               Mean episode length: 143.68
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 18.1853
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.33s
                      Time elapsed: 00:28:20
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 38820 steps/s (collection: 2.374s, learning 0.159s)
             Mean action noise std: 2.64
          Mean value_function loss: 110.6686
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 62.0010
                       Mean reward: 76.28
               Mean episode length: 146.76
    Episode_Reward/reaching_object: 0.6840
     Episode_Reward/lifting_object: 16.1070
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.53s
                      Time elapsed: 00:28:22
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 42128 steps/s (collection: 2.230s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 144.6221
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.0029
                       Mean reward: 92.61
               Mean episode length: 141.60
    Episode_Reward/reaching_object: 0.6958
     Episode_Reward/lifting_object: 18.1827
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.33s
                      Time elapsed: 00:28:25
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 39102 steps/s (collection: 2.390s, learning 0.124s)
             Mean action noise std: 2.64
          Mean value_function loss: 125.0233
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.0080
                       Mean reward: 87.74
               Mean episode length: 153.13
    Episode_Reward/reaching_object: 0.7151
     Episode_Reward/lifting_object: 17.7526
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.51s
                      Time elapsed: 00:28:27
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 40378 steps/s (collection: 2.253s, learning 0.181s)
             Mean action noise std: 2.64
          Mean value_function loss: 163.7915
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.0147
                       Mean reward: 87.44
               Mean episode length: 126.00
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 17.8393
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.43s
                      Time elapsed: 00:28:30
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 42251 steps/s (collection: 2.208s, learning 0.119s)
             Mean action noise std: 2.64
          Mean value_function loss: 161.1993
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 62.0234
                       Mean reward: 105.21
               Mean episode length: 143.51
    Episode_Reward/reaching_object: 0.6776
     Episode_Reward/lifting_object: 17.3856
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.33s
                      Time elapsed: 00:28:32
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 42126 steps/s (collection: 2.229s, learning 0.104s)
             Mean action noise std: 2.64
          Mean value_function loss: 145.1140
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.0330
                       Mean reward: 98.56
               Mean episode length: 142.48
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 18.5238
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.33s
                      Time elapsed: 00:28:34
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 41752 steps/s (collection: 2.193s, learning 0.162s)
             Mean action noise std: 2.64
          Mean value_function loss: 166.7513
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.0381
                       Mean reward: 103.54
               Mean episode length: 153.56
    Episode_Reward/reaching_object: 0.7108
     Episode_Reward/lifting_object: 18.5807
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.35s
                      Time elapsed: 00:28:37
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 42027 steps/s (collection: 2.230s, learning 0.109s)
             Mean action noise std: 2.64
          Mean value_function loss: 150.0614
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.0432
                       Mean reward: 95.75
               Mean episode length: 136.16
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 17.9745
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.34s
                      Time elapsed: 00:28:39
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 42348 steps/s (collection: 2.225s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 130.8006
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 62.0492
                       Mean reward: 97.01
               Mean episode length: 137.23
    Episode_Reward/reaching_object: 0.7041
     Episode_Reward/lifting_object: 19.1159
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.32s
                      Time elapsed: 00:28:41
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 42179 steps/s (collection: 2.185s, learning 0.145s)
             Mean action noise std: 2.65
          Mean value_function loss: 188.3277
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.0557
                       Mean reward: 80.63
               Mean episode length: 123.47
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 18.7063
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.33s
                      Time elapsed: 00:28:44
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 42455 steps/s (collection: 2.206s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 164.9466
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.0641
                       Mean reward: 95.87
               Mean episode length: 135.00
    Episode_Reward/reaching_object: 0.6621
     Episode_Reward/lifting_object: 17.0896
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.32s
                      Time elapsed: 00:28:46
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 42135 steps/s (collection: 2.212s, learning 0.121s)
             Mean action noise std: 2.65
          Mean value_function loss: 204.8926
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.0730
                       Mean reward: 93.55
               Mean episode length: 134.08
    Episode_Reward/reaching_object: 0.6774
     Episode_Reward/lifting_object: 19.0796
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.33s
                      Time elapsed: 00:28:48
                               ETA: 00:52:30

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 43068 steps/s (collection: 2.170s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 130.9878
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.0829
                       Mean reward: 112.79
               Mean episode length: 145.26
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 18.7957
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.28s
                      Time elapsed: 00:28:51
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 42920 steps/s (collection: 2.194s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 157.9955
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.0865
                       Mean reward: 103.10
               Mean episode length: 154.21
    Episode_Reward/reaching_object: 0.7071
     Episode_Reward/lifting_object: 20.1371
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.29s
                      Time elapsed: 00:28:53
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 42656 steps/s (collection: 2.197s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 174.9129
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 62.0897
                       Mean reward: 114.46
               Mean episode length: 146.61
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 19.4606
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.30s
                      Time elapsed: 00:28:55
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 42190 steps/s (collection: 2.194s, learning 0.136s)
             Mean action noise std: 2.65
          Mean value_function loss: 207.2785
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.0955
                       Mean reward: 108.92
               Mean episode length: 146.18
    Episode_Reward/reaching_object: 0.6795
     Episode_Reward/lifting_object: 19.1481
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.33s
                      Time elapsed: 00:28:57
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 41959 steps/s (collection: 2.201s, learning 0.142s)
             Mean action noise std: 2.65
          Mean value_function loss: 176.2967
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 62.1043
                       Mean reward: 109.43
               Mean episode length: 138.42
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 20.2149
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.34s
                      Time elapsed: 00:29:00
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 41150 steps/s (collection: 2.232s, learning 0.157s)
             Mean action noise std: 2.65
          Mean value_function loss: 177.7751
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.1103
                       Mean reward: 92.68
               Mean episode length: 146.00
    Episode_Reward/reaching_object: 0.7168
     Episode_Reward/lifting_object: 20.2095
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.39s
                      Time elapsed: 00:29:02
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 40441 steps/s (collection: 2.268s, learning 0.163s)
             Mean action noise std: 2.65
          Mean value_function loss: 192.9539
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 62.1148
                       Mean reward: 99.48
               Mean episode length: 132.75
    Episode_Reward/reaching_object: 0.6811
     Episode_Reward/lifting_object: 18.8694
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.43s
                      Time elapsed: 00:29:05
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 42429 steps/s (collection: 2.199s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 172.6967
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.1201
                       Mean reward: 90.76
               Mean episode length: 131.75
    Episode_Reward/reaching_object: 0.6883
     Episode_Reward/lifting_object: 19.7392
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.32s
                      Time elapsed: 00:29:07
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 41630 steps/s (collection: 2.243s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 160.7953
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.1236
                       Mean reward: 121.63
               Mean episode length: 136.27
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 21.1940
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.36s
                      Time elapsed: 00:29:09
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 41579 steps/s (collection: 2.209s, learning 0.156s)
             Mean action noise std: 2.65
          Mean value_function loss: 196.0579
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.1255
                       Mean reward: 91.71
               Mean episode length: 121.04
    Episode_Reward/reaching_object: 0.6801
     Episode_Reward/lifting_object: 19.6196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.36s
                      Time elapsed: 00:29:12
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 42228 steps/s (collection: 2.230s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 178.1546
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.1297
                       Mean reward: 105.60
               Mean episode length: 134.95
    Episode_Reward/reaching_object: 0.7267
     Episode_Reward/lifting_object: 22.3231
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.33s
                      Time elapsed: 00:29:14
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 41574 steps/s (collection: 2.245s, learning 0.120s)
             Mean action noise std: 2.65
          Mean value_function loss: 166.4107
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 62.1323
                       Mean reward: 107.77
               Mean episode length: 129.58
    Episode_Reward/reaching_object: 0.7010
     Episode_Reward/lifting_object: 21.5190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.36s
                      Time elapsed: 00:29:16
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 42009 steps/s (collection: 2.218s, learning 0.122s)
             Mean action noise std: 2.65
          Mean value_function loss: 160.8509
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 62.1337
                       Mean reward: 110.61
               Mean episode length: 133.87
    Episode_Reward/reaching_object: 0.7235
     Episode_Reward/lifting_object: 22.2904
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.34s
                      Time elapsed: 00:29:19
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 41951 steps/s (collection: 2.218s, learning 0.126s)
             Mean action noise std: 2.66
          Mean value_function loss: 219.7875
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 62.1370
                       Mean reward: 110.41
               Mean episode length: 132.44
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 22.7920
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0715
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.34s
                      Time elapsed: 00:29:21
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 38997 steps/s (collection: 2.380s, learning 0.141s)
             Mean action noise std: 2.66
          Mean value_function loss: 186.6065
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.1423
                       Mean reward: 116.84
               Mean episode length: 127.41
    Episode_Reward/reaching_object: 0.7153
     Episode_Reward/lifting_object: 22.2691
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.52s
                      Time elapsed: 00:29:24
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 41473 steps/s (collection: 2.259s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 226.8176
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 62.1446
                       Mean reward: 125.69
               Mean episode length: 130.74
    Episode_Reward/reaching_object: 0.7147
     Episode_Reward/lifting_object: 22.6332
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.37s
                      Time elapsed: 00:29:26
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 42050 steps/s (collection: 2.221s, learning 0.117s)
             Mean action noise std: 2.66
          Mean value_function loss: 200.8062
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.1466
                       Mean reward: 101.72
               Mean episode length: 123.49
    Episode_Reward/reaching_object: 0.7055
     Episode_Reward/lifting_object: 21.7118
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.34s
                      Time elapsed: 00:29:28
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 41415 steps/s (collection: 2.267s, learning 0.107s)
             Mean action noise std: 2.66
          Mean value_function loss: 184.3492
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.1496
                       Mean reward: 120.10
               Mean episode length: 131.31
    Episode_Reward/reaching_object: 0.7318
     Episode_Reward/lifting_object: 22.9334
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.37s
                      Time elapsed: 00:29:31
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 41474 steps/s (collection: 2.243s, learning 0.128s)
             Mean action noise std: 2.66
          Mean value_function loss: 214.3458
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.1555
                       Mean reward: 119.53
               Mean episode length: 138.98
    Episode_Reward/reaching_object: 0.7333
     Episode_Reward/lifting_object: 23.4965
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0705
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.37s
                      Time elapsed: 00:29:33
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 36962 steps/s (collection: 2.498s, learning 0.162s)
             Mean action noise std: 2.66
          Mean value_function loss: 236.5598
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.1659
                       Mean reward: 134.20
               Mean episode length: 139.63
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 22.9052
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.66s
                      Time elapsed: 00:29:36
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 37316 steps/s (collection: 2.504s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 203.2536
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 62.1730
                       Mean reward: 100.22
               Mean episode length: 128.41
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 24.1032
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.63s
                      Time elapsed: 00:29:38
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 42222 steps/s (collection: 2.216s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 222.9094
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 62.1746
                       Mean reward: 119.58
               Mean episode length: 128.92
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 25.4137
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.33s
                      Time elapsed: 00:29:41
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 40596 steps/s (collection: 2.228s, learning 0.193s)
             Mean action noise std: 2.66
          Mean value_function loss: 186.0003
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.1753
                       Mean reward: 126.13
               Mean episode length: 135.36
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 23.2768
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.42s
                      Time elapsed: 00:29:43
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 39616 steps/s (collection: 2.365s, learning 0.117s)
             Mean action noise std: 2.66
          Mean value_function loss: 206.0741
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 62.1755
                       Mean reward: 136.66
               Mean episode length: 142.55
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 24.4209
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.48s
                      Time elapsed: 00:29:46
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 38828 steps/s (collection: 2.436s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 204.9894
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 62.1757
                       Mean reward: 131.95
               Mean episode length: 140.78
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 25.4162
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.53s
                      Time elapsed: 00:29:48
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 41763 steps/s (collection: 2.247s, learning 0.107s)
             Mean action noise std: 2.66
          Mean value_function loss: 161.2379
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 62.1760
                       Mean reward: 136.77
               Mean episode length: 139.69
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 25.6756
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.35s
                      Time elapsed: 00:29:50
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 41813 steps/s (collection: 2.225s, learning 0.126s)
             Mean action noise std: 2.66
          Mean value_function loss: 182.4507
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 62.1762
                       Mean reward: 122.70
               Mean episode length: 132.79
    Episode_Reward/reaching_object: 0.7111
     Episode_Reward/lifting_object: 24.1069
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.35s
                      Time elapsed: 00:29:53
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 41246 steps/s (collection: 2.267s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 180.4008
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 62.1764
                       Mean reward: 138.96
               Mean episode length: 141.13
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 25.2091
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.38s
                      Time elapsed: 00:29:55
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 42640 steps/s (collection: 2.190s, learning 0.115s)
             Mean action noise std: 2.66
          Mean value_function loss: 223.8978
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.1775
                       Mean reward: 126.19
               Mean episode length: 128.13
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 25.6954
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.31s
                      Time elapsed: 00:29:58
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 40776 steps/s (collection: 2.252s, learning 0.159s)
             Mean action noise std: 2.66
          Mean value_function loss: 243.2946
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.1795
                       Mean reward: 131.70
               Mean episode length: 141.74
    Episode_Reward/reaching_object: 0.7263
     Episode_Reward/lifting_object: 24.8291
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.41s
                      Time elapsed: 00:30:00
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 42558 steps/s (collection: 2.213s, learning 0.097s)
             Mean action noise std: 2.66
          Mean value_function loss: 206.9276
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 62.1841
                       Mean reward: 128.80
               Mean episode length: 136.88
    Episode_Reward/reaching_object: 0.7286
     Episode_Reward/lifting_object: 25.2700
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.31s
                      Time elapsed: 00:30:02
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 42174 steps/s (collection: 2.221s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 248.0635
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.1914
                       Mean reward: 134.02
               Mean episode length: 135.76
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 24.6903
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.33s
                      Time elapsed: 00:30:05
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 39925 steps/s (collection: 2.309s, learning 0.153s)
             Mean action noise std: 2.66
          Mean value_function loss: 204.1825
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.1989
                       Mean reward: 131.40
               Mean episode length: 136.83
    Episode_Reward/reaching_object: 0.7265
     Episode_Reward/lifting_object: 24.5296
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.46s
                      Time elapsed: 00:30:07
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 37587 steps/s (collection: 2.480s, learning 0.135s)
             Mean action noise std: 2.66
          Mean value_function loss: 187.9520
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.2032
                       Mean reward: 122.11
               Mean episode length: 129.57
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 25.4050
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.62s
                      Time elapsed: 00:30:10
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 40412 steps/s (collection: 2.304s, learning 0.129s)
             Mean action noise std: 2.66
          Mean value_function loss: 219.0174
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.2058
                       Mean reward: 132.17
               Mean episode length: 141.93
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 26.0063
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.43s
                      Time elapsed: 00:30:12
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 42297 steps/s (collection: 2.229s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 175.6793
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 62.2076
                       Mean reward: 129.82
               Mean episode length: 134.46
    Episode_Reward/reaching_object: 0.7301
     Episode_Reward/lifting_object: 25.5674
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.32s
                      Time elapsed: 00:30:14
                               ETA: 00:50:59

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 41334 steps/s (collection: 2.244s, learning 0.135s)
             Mean action noise std: 2.66
          Mean value_function loss: 205.4399
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.2086
                       Mean reward: 137.71
               Mean episode length: 135.90
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 27.3867
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.38s
                      Time elapsed: 00:30:17
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 42296 steps/s (collection: 2.213s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 199.9350
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.2103
                       Mean reward: 151.50
               Mean episode length: 143.57
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 27.4230
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.32s
                      Time elapsed: 00:30:19
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 38408 steps/s (collection: 2.374s, learning 0.186s)
             Mean action noise std: 2.66
          Mean value_function loss: 227.4067
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.2120
                       Mean reward: 144.34
               Mean episode length: 143.29
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 27.3710
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.56s
                      Time elapsed: 00:30:22
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 37866 steps/s (collection: 2.466s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 203.4950
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 62.2143
                       Mean reward: 135.41
               Mean episode length: 124.54
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 26.3716
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.60s
                      Time elapsed: 00:30:24
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 39931 steps/s (collection: 2.357s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 229.6545
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.2148
                       Mean reward: 136.63
               Mean episode length: 141.13
    Episode_Reward/reaching_object: 0.7335
     Episode_Reward/lifting_object: 27.2485
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.46s
                      Time elapsed: 00:30:27
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 38521 steps/s (collection: 2.444s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 192.1227
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.2164
                       Mean reward: 141.52
               Mean episode length: 123.92
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 25.2441
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.55s
                      Time elapsed: 00:30:29
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 40290 steps/s (collection: 2.323s, learning 0.117s)
             Mean action noise std: 2.67
          Mean value_function loss: 192.0749
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.2187
                       Mean reward: 130.97
               Mean episode length: 122.56
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: 27.9493
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.44s
                      Time elapsed: 00:30:32
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 40037 steps/s (collection: 2.352s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 215.2971
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.2199
                       Mean reward: 126.20
               Mean episode length: 119.79
    Episode_Reward/reaching_object: 0.7136
     Episode_Reward/lifting_object: 27.3419
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.46s
                      Time elapsed: 00:30:34
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 40936 steps/s (collection: 2.271s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 212.7302
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.2208
                       Mean reward: 119.55
               Mean episode length: 121.37
    Episode_Reward/reaching_object: 0.7067
     Episode_Reward/lifting_object: 27.2178
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.40s
                      Time elapsed: 00:30:37
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 40412 steps/s (collection: 2.289s, learning 0.143s)
             Mean action noise std: 2.67
          Mean value_function loss: 187.2393
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.2223
                       Mean reward: 136.09
               Mean episode length: 114.10
    Episode_Reward/reaching_object: 0.7198
     Episode_Reward/lifting_object: 28.4117
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.43s
                      Time elapsed: 00:30:39
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 41754 steps/s (collection: 2.248s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 218.3159
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2232
                       Mean reward: 147.17
               Mean episode length: 124.89
    Episode_Reward/reaching_object: 0.7333
     Episode_Reward/lifting_object: 30.1169
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.35s
                      Time elapsed: 00:30:41
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 40946 steps/s (collection: 2.283s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 201.1999
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.2270
                       Mean reward: 155.40
               Mean episode length: 131.02
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: 29.1367
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.40s
                      Time elapsed: 00:30:44
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 40748 steps/s (collection: 2.309s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 215.3844
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.2312
                       Mean reward: 166.42
               Mean episode length: 123.47
    Episode_Reward/reaching_object: 0.6934
     Episode_Reward/lifting_object: 29.1768
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.41s
                      Time elapsed: 00:30:46
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 39459 steps/s (collection: 2.362s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 220.8770
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.2338
                       Mean reward: 143.96
               Mean episode length: 118.83
    Episode_Reward/reaching_object: 0.7029
     Episode_Reward/lifting_object: 29.5881
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.49s
                      Time elapsed: 00:30:49
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 41010 steps/s (collection: 2.250s, learning 0.147s)
             Mean action noise std: 2.67
          Mean value_function loss: 227.7099
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.2379
                       Mean reward: 171.09
               Mean episode length: 131.60
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 30.2758
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.40s
                      Time elapsed: 00:30:51
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 40507 steps/s (collection: 2.281s, learning 0.146s)
             Mean action noise std: 2.67
          Mean value_function loss: 213.1282
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.2425
                       Mean reward: 158.15
               Mean episode length: 123.23
    Episode_Reward/reaching_object: 0.6914
     Episode_Reward/lifting_object: 29.6667
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.43s
                      Time elapsed: 00:30:53
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 40742 steps/s (collection: 2.282s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 242.9972
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 62.2449
                       Mean reward: 168.19
               Mean episode length: 129.68
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 30.8923
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.41s
                      Time elapsed: 00:30:56
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 40432 steps/s (collection: 2.291s, learning 0.140s)
             Mean action noise std: 2.67
          Mean value_function loss: 230.3822
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.2453
                       Mean reward: 157.23
               Mean episode length: 123.13
    Episode_Reward/reaching_object: 0.7171
     Episode_Reward/lifting_object: 31.9659
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.43s
                      Time elapsed: 00:30:58
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 40777 steps/s (collection: 2.298s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 270.0381
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 62.2470
                       Mean reward: 132.16
               Mean episode length: 115.92
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 30.0537
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.41s
                      Time elapsed: 00:31:01
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 36510 steps/s (collection: 2.584s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 244.6244
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 62.2474
                       Mean reward: 174.89
               Mean episode length: 127.38
    Episode_Reward/reaching_object: 0.6970
     Episode_Reward/lifting_object: 30.5941
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.69s
                      Time elapsed: 00:31:03
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 40975 steps/s (collection: 2.285s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 243.8285
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.2476
                       Mean reward: 166.55
               Mean episode length: 121.76
    Episode_Reward/reaching_object: 0.6610
     Episode_Reward/lifting_object: 29.7292
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.40s
                      Time elapsed: 00:31:06
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 38534 steps/s (collection: 2.444s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 259.9149
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.2479
                       Mean reward: 147.58
               Mean episode length: 121.33
    Episode_Reward/reaching_object: 0.6893
     Episode_Reward/lifting_object: 30.6717
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.55s
                      Time elapsed: 00:31:08
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 41037 steps/s (collection: 2.285s, learning 0.110s)
             Mean action noise std: 2.67
          Mean value_function loss: 273.4093
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2488
                       Mean reward: 151.60
               Mean episode length: 117.50
    Episode_Reward/reaching_object: 0.6932
     Episode_Reward/lifting_object: 31.4598
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.40s
                      Time elapsed: 00:31:11
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 41018 steps/s (collection: 2.286s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 264.4347
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.2494
                       Mean reward: 134.60
               Mean episode length: 107.78
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 31.5573
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.40s
                      Time elapsed: 00:31:13
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 40783 steps/s (collection: 2.307s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 288.3383
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.2484
                       Mean reward: 155.33
               Mean episode length: 117.89
    Episode_Reward/reaching_object: 0.7072
     Episode_Reward/lifting_object: 32.5167
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.41s
                      Time elapsed: 00:31:16
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 40683 steps/s (collection: 2.304s, learning 0.113s)
             Mean action noise std: 2.67
          Mean value_function loss: 288.3797
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.2492
                       Mean reward: 172.45
               Mean episode length: 122.86
    Episode_Reward/reaching_object: 0.7044
     Episode_Reward/lifting_object: 32.0919
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.42s
                      Time elapsed: 00:31:18
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 41810 steps/s (collection: 2.253s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 305.1891
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 62.2523
                       Mean reward: 175.01
               Mean episode length: 131.37
    Episode_Reward/reaching_object: 0.7196
     Episode_Reward/lifting_object: 33.4293
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.35s
                      Time elapsed: 00:31:20
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 39638 steps/s (collection: 2.371s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 295.4837
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 62.2541
                       Mean reward: 196.43
               Mean episode length: 125.73
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 35.2642
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.48s
                      Time elapsed: 00:31:23
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 40721 steps/s (collection: 2.258s, learning 0.156s)
             Mean action noise std: 2.67
          Mean value_function loss: 296.6364
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.2540
                       Mean reward: 160.94
               Mean episode length: 117.76
    Episode_Reward/reaching_object: 0.6945
     Episode_Reward/lifting_object: 33.4086
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 30.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.41s
                      Time elapsed: 00:31:25
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 41394 steps/s (collection: 2.270s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 306.9431
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2548
                       Mean reward: 180.06
               Mean episode length: 119.31
    Episode_Reward/reaching_object: 0.7188
     Episode_Reward/lifting_object: 35.2520
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.37s
                      Time elapsed: 00:31:28
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 41829 steps/s (collection: 2.246s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 310.2049
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 62.2549
                       Mean reward: 187.05
               Mean episode length: 124.79
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 35.7692
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.35s
                      Time elapsed: 00:31:30
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 39805 steps/s (collection: 2.308s, learning 0.162s)
             Mean action noise std: 2.67
          Mean value_function loss: 311.4222
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 62.2533
                       Mean reward: 190.81
               Mean episode length: 125.63
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 37.5753
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.47s
                      Time elapsed: 00:31:32
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 40671 steps/s (collection: 2.277s, learning 0.140s)
             Mean action noise std: 2.67
          Mean value_function loss: 268.4169
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.2535
                       Mean reward: 198.95
               Mean episode length: 127.48
    Episode_Reward/reaching_object: 0.7170
     Episode_Reward/lifting_object: 36.0576
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.42s
                      Time elapsed: 00:31:35
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 39105 steps/s (collection: 2.413s, learning 0.101s)
             Mean action noise std: 2.67
          Mean value_function loss: 309.0339
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.2534
                       Mean reward: 191.74
               Mean episode length: 120.72
    Episode_Reward/reaching_object: 0.7242
     Episode_Reward/lifting_object: 36.8200
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.51s
                      Time elapsed: 00:31:37
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 40741 steps/s (collection: 2.310s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 321.7438
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.2546
                       Mean reward: 183.61
               Mean episode length: 113.65
    Episode_Reward/reaching_object: 0.7256
     Episode_Reward/lifting_object: 36.8815
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.41s
                      Time elapsed: 00:31:40
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 38295 steps/s (collection: 2.381s, learning 0.186s)
             Mean action noise std: 2.67
          Mean value_function loss: 321.4740
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.2574
                       Mean reward: 193.40
               Mean episode length: 120.72
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 37.4510
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.57s
                      Time elapsed: 00:31:42
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 39849 steps/s (collection: 2.361s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 364.0411
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.2585
                       Mean reward: 179.49
               Mean episode length: 116.89
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 38.8562
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.47s
                      Time elapsed: 00:31:45
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 39607 steps/s (collection: 2.337s, learning 0.145s)
             Mean action noise std: 2.67
          Mean value_function loss: 319.7257
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 62.2600
                       Mean reward: 206.97
               Mean episode length: 121.82
    Episode_Reward/reaching_object: 0.7113
     Episode_Reward/lifting_object: 38.5394
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.48s
                      Time elapsed: 00:31:47
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 41540 steps/s (collection: 2.259s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 342.7459
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 62.2604
                       Mean reward: 222.99
               Mean episode length: 134.42
    Episode_Reward/reaching_object: 0.7362
     Episode_Reward/lifting_object: 39.8586
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.37s
                      Time elapsed: 00:31:50
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 42168 steps/s (collection: 2.233s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 362.2449
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 62.2605
                       Mean reward: 210.23
               Mean episode length: 124.93
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 43.4775
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.33s
                      Time elapsed: 00:31:52
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 38285 steps/s (collection: 2.444s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 322.4567
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 62.2606
                       Mean reward: 210.77
               Mean episode length: 122.13
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 41.8672
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.57s
                      Time elapsed: 00:31:55
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 40491 steps/s (collection: 2.319s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 351.8011
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 62.2606
                       Mean reward: 184.93
               Mean episode length: 118.10
    Episode_Reward/reaching_object: 0.7394
     Episode_Reward/lifting_object: 40.9276
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.43s
                      Time elapsed: 00:31:57
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 39093 steps/s (collection: 2.385s, learning 0.129s)
             Mean action noise std: 2.67
          Mean value_function loss: 366.9134
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 62.2605
                       Mean reward: 231.10
               Mean episode length: 130.74
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 44.0836
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.51s
                      Time elapsed: 00:31:59
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 39595 steps/s (collection: 2.374s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 379.4473
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 62.2605
                       Mean reward: 236.62
               Mean episode length: 119.35
    Episode_Reward/reaching_object: 0.7320
     Episode_Reward/lifting_object: 42.7895
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.48s
                      Time elapsed: 00:32:02
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 41425 steps/s (collection: 2.252s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 347.4468
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 62.2606
                       Mean reward: 246.50
               Mean episode length: 123.67
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 43.6248
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.37s
                      Time elapsed: 00:32:04
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 40874 steps/s (collection: 2.278s, learning 0.127s)
             Mean action noise std: 2.67
          Mean value_function loss: 355.0924
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 62.2606
                       Mean reward: 289.00
               Mean episode length: 135.89
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 47.2219
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.41s
                      Time elapsed: 00:32:07
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 39866 steps/s (collection: 2.339s, learning 0.127s)
             Mean action noise std: 2.67
          Mean value_function loss: 369.5537
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 62.2608
                       Mean reward: 243.79
               Mean episode length: 125.96
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 46.4378
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.47s
                      Time elapsed: 00:32:09
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 41215 steps/s (collection: 2.266s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 356.0003
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.2612
                       Mean reward: 263.40
               Mean episode length: 126.55
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 46.6781
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.39s
                      Time elapsed: 00:32:12
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 41309 steps/s (collection: 2.263s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 348.6498
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2615
                       Mean reward: 215.33
               Mean episode length: 119.69
    Episode_Reward/reaching_object: 0.7279
     Episode_Reward/lifting_object: 44.3877
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.38s
                      Time elapsed: 00:32:14
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 40937 steps/s (collection: 2.298s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 364.5377
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 62.2608
                       Mean reward: 254.26
               Mean episode length: 124.81
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 47.8670
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.40s
                      Time elapsed: 00:32:16
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 42105 steps/s (collection: 2.235s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 367.3342
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.2614
                       Mean reward: 232.00
               Mean episode length: 113.31
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 46.7139
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.33s
                      Time elapsed: 00:32:19
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 39972 steps/s (collection: 2.362s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 375.8421
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.2610
                       Mean reward: 210.46
               Mean episode length: 113.29
    Episode_Reward/reaching_object: 0.7306
     Episode_Reward/lifting_object: 46.3695
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.46s
                      Time elapsed: 00:32:21
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 41720 steps/s (collection: 2.250s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 395.4501
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 62.2594
                       Mean reward: 247.01
               Mean episode length: 116.89
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 47.7353
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.36s
                      Time elapsed: 00:32:24
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 40521 steps/s (collection: 2.319s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 408.9220
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 62.2623
                       Mean reward: 238.09
               Mean episode length: 119.17
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 47.6593
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.43s
                      Time elapsed: 00:32:26
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 41122 steps/s (collection: 2.252s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 407.8893
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.2674
                       Mean reward: 295.92
               Mean episode length: 139.61
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 49.8747
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.39s
                      Time elapsed: 00:32:28
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 40725 steps/s (collection: 2.281s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 403.7100
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.2691
                       Mean reward: 264.73
               Mean episode length: 125.26
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 49.6379
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.41s
                      Time elapsed: 00:32:31
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 39662 steps/s (collection: 2.346s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 420.4769
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.2696
                       Mean reward: 289.78
               Mean episode length: 126.77
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 54.9803
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.48s
                      Time elapsed: 00:32:33
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 39760 steps/s (collection: 2.279s, learning 0.194s)
             Mean action noise std: 2.67
          Mean value_function loss: 474.9680
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.2702
                       Mean reward: 256.38
               Mean episode length: 118.71
    Episode_Reward/reaching_object: 0.7957
     Episode_Reward/lifting_object: 53.3916
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.47s
                      Time elapsed: 00:32:36
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 40797 steps/s (collection: 2.282s, learning 0.128s)
             Mean action noise std: 2.67
          Mean value_function loss: 417.1116
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 62.2716
                       Mean reward: 271.21
               Mean episode length: 121.84
    Episode_Reward/reaching_object: 0.8305
     Episode_Reward/lifting_object: 56.2407
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.41s
                      Time elapsed: 00:32:38
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 40775 steps/s (collection: 2.287s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 420.4438
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2718
                       Mean reward: 256.63
               Mean episode length: 121.93
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 55.3000
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.41s
                      Time elapsed: 00:32:41
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 41572 steps/s (collection: 2.267s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 416.3320
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 62.2705
                       Mean reward: 297.17
               Mean episode length: 130.61
    Episode_Reward/reaching_object: 0.8614
     Episode_Reward/lifting_object: 58.5403
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.36s
                      Time elapsed: 00:32:43
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 41337 steps/s (collection: 2.271s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 384.6004
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.2702
                       Mean reward: 314.39
               Mean episode length: 134.14
    Episode_Reward/reaching_object: 0.8663
     Episode_Reward/lifting_object: 60.0309
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.38s
                      Time elapsed: 00:32:45
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 41229 steps/s (collection: 2.254s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 386.8018
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 62.2673
                       Mean reward: 296.95
               Mean episode length: 126.58
    Episode_Reward/reaching_object: 0.8737
     Episode_Reward/lifting_object: 60.8427
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.38s
                      Time elapsed: 00:32:48
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 39579 steps/s (collection: 2.379s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 424.3651
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.2650
                       Mean reward: 349.99
               Mean episode length: 144.70
    Episode_Reward/reaching_object: 0.9318
     Episode_Reward/lifting_object: 66.8066
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.48s
                      Time elapsed: 00:32:50
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 39102 steps/s (collection: 2.393s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 422.8202
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 62.2637
                       Mean reward: 371.61
               Mean episode length: 154.52
    Episode_Reward/reaching_object: 0.9353
     Episode_Reward/lifting_object: 65.5791
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.51s
                      Time elapsed: 00:32:53
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 39595 steps/s (collection: 2.367s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 411.9408
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.2639
                       Mean reward: 352.85
               Mean episode length: 147.47
    Episode_Reward/reaching_object: 0.9814
     Episode_Reward/lifting_object: 70.0029
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.48s
                      Time elapsed: 00:32:55
                               ETA: 00:48:18

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 37963 steps/s (collection: 2.412s, learning 0.178s)
             Mean action noise std: 2.67
          Mean value_function loss: 455.8735
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 62.2638
                       Mean reward: 368.70
               Mean episode length: 151.53
    Episode_Reward/reaching_object: 1.0198
     Episode_Reward/lifting_object: 73.1089
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.59s
                      Time elapsed: 00:32:58
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 40025 steps/s (collection: 2.335s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 415.8540
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 62.2638
                       Mean reward: 378.47
               Mean episode length: 143.81
    Episode_Reward/reaching_object: 0.9700
     Episode_Reward/lifting_object: 69.9471
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.46s
                      Time elapsed: 00:33:00
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 38888 steps/s (collection: 2.390s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 411.5349
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 62.2638
                       Mean reward: 340.98
               Mean episode length: 135.91
    Episode_Reward/reaching_object: 0.9627
     Episode_Reward/lifting_object: 70.3765
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.53s
                      Time elapsed: 00:33:03
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 41107 steps/s (collection: 2.280s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 385.3208
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2638
                       Mean reward: 365.28
               Mean episode length: 134.47
    Episode_Reward/reaching_object: 0.9475
     Episode_Reward/lifting_object: 70.0873
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.39s
                      Time elapsed: 00:33:05
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 40312 steps/s (collection: 2.332s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 390.2596
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.2645
                       Mean reward: 320.75
               Mean episode length: 126.42
    Episode_Reward/reaching_object: 0.8882
     Episode_Reward/lifting_object: 66.7229
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.44s
                      Time elapsed: 00:33:08
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 39894 steps/s (collection: 2.351s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 436.2282
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.2653
                       Mean reward: 316.70
               Mean episode length: 127.97
    Episode_Reward/reaching_object: 0.8710
     Episode_Reward/lifting_object: 64.0616
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.46s
                      Time elapsed: 00:33:10
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 40064 steps/s (collection: 2.328s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 452.6004
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.2658
                       Mean reward: 344.66
               Mean episode length: 137.03
    Episode_Reward/reaching_object: 0.8342
     Episode_Reward/lifting_object: 60.8173
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.45s
                      Time elapsed: 00:33:12
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 39442 steps/s (collection: 2.319s, learning 0.173s)
             Mean action noise std: 2.68
          Mean value_function loss: 454.5219
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 62.2659
                       Mean reward: 327.67
               Mean episode length: 135.41
    Episode_Reward/reaching_object: 0.8378
     Episode_Reward/lifting_object: 59.5454
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.49s
                      Time elapsed: 00:33:15
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 41195 steps/s (collection: 2.276s, learning 0.111s)
             Mean action noise std: 2.68
          Mean value_function loss: 462.6054
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 62.2657
                       Mean reward: 332.29
               Mean episode length: 136.43
    Episode_Reward/reaching_object: 0.8810
     Episode_Reward/lifting_object: 63.0306
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.39s
                      Time elapsed: 00:33:17
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 41295 steps/s (collection: 2.279s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 532.1309
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 62.2658
                       Mean reward: 274.89
               Mean episode length: 114.09
    Episode_Reward/reaching_object: 0.8552
     Episode_Reward/lifting_object: 61.0137
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.38s
                      Time elapsed: 00:33:20
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 39583 steps/s (collection: 2.355s, learning 0.128s)
             Mean action noise std: 2.68
          Mean value_function loss: 533.4467
               Mean surrogate loss: 0.0139
                 Mean entropy loss: 62.2660
                       Mean reward: 398.67
               Mean episode length: 146.35
    Episode_Reward/reaching_object: 0.9822
     Episode_Reward/lifting_object: 71.4056
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.48s
                      Time elapsed: 00:33:22
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 39656 steps/s (collection: 2.382s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 486.8132
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 62.2661
                       Mean reward: 363.89
               Mean episode length: 141.91
    Episode_Reward/reaching_object: 0.9981
     Episode_Reward/lifting_object: 73.4255
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.48s
                      Time elapsed: 00:33:25
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 39755 steps/s (collection: 2.351s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 491.1841
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 62.2662
                       Mean reward: 388.79
               Mean episode length: 148.58
    Episode_Reward/reaching_object: 1.0810
     Episode_Reward/lifting_object: 80.2936
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.47s
                      Time elapsed: 00:33:27
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 38279 steps/s (collection: 2.470s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 472.6722
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2662
                       Mean reward: 407.66
               Mean episode length: 152.07
    Episode_Reward/reaching_object: 1.0938
     Episode_Reward/lifting_object: 81.8413
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.57s
                      Time elapsed: 00:33:30
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 39300 steps/s (collection: 2.359s, learning 0.142s)
             Mean action noise std: 2.68
          Mean value_function loss: 465.3083
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.2664
                       Mean reward: 395.12
               Mean episode length: 149.31
    Episode_Reward/reaching_object: 1.1046
     Episode_Reward/lifting_object: 83.5794
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.50s
                      Time elapsed: 00:33:32
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 41816 steps/s (collection: 2.242s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 502.1766
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.2679
                       Mean reward: 377.29
               Mean episode length: 138.52
    Episode_Reward/reaching_object: 1.0830
     Episode_Reward/lifting_object: 82.5336
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.35s
                      Time elapsed: 00:33:35
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 39606 steps/s (collection: 2.345s, learning 0.137s)
             Mean action noise std: 2.68
          Mean value_function loss: 449.3176
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.2692
                       Mean reward: 441.16
               Mean episode length: 155.17
    Episode_Reward/reaching_object: 1.1200
     Episode_Reward/lifting_object: 85.5640
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.48s
                      Time elapsed: 00:33:37
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 40226 steps/s (collection: 2.326s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 439.5396
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 62.2692
                       Mean reward: 402.86
               Mean episode length: 146.35
    Episode_Reward/reaching_object: 1.1040
     Episode_Reward/lifting_object: 84.5082
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.44s
                      Time elapsed: 00:33:40
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 39362 steps/s (collection: 2.320s, learning 0.178s)
             Mean action noise std: 2.68
          Mean value_function loss: 490.6014
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 62.2691
                       Mean reward: 380.59
               Mean episode length: 139.94
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 78.6010
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.50s
                      Time elapsed: 00:33:42
                               ETA: 00:47:33

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 36415 steps/s (collection: 2.584s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 498.5780
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.2694
                       Mean reward: 366.93
               Mean episode length: 134.53
    Episode_Reward/reaching_object: 1.0626
     Episode_Reward/lifting_object: 81.1987
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.70s
                      Time elapsed: 00:33:45
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 37269 steps/s (collection: 2.507s, learning 0.131s)
             Mean action noise std: 2.68
          Mean value_function loss: 509.4430
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.2712
                       Mean reward: 411.97
               Mean episode length: 144.98
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 79.9852
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.64s
                      Time elapsed: 00:33:47
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 39447 steps/s (collection: 2.387s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 519.4073
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.2729
                       Mean reward: 455.31
               Mean episode length: 151.94
    Episode_Reward/reaching_object: 1.1053
     Episode_Reward/lifting_object: 87.3652
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.49s
                      Time elapsed: 00:33:50
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 38588 steps/s (collection: 2.442s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 523.2382
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 62.2752
                       Mean reward: 423.50
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 1.0739
     Episode_Reward/lifting_object: 86.6398
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.55s
                      Time elapsed: 00:33:52
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 39543 steps/s (collection: 2.361s, learning 0.125s)
             Mean action noise std: 2.68
          Mean value_function loss: 516.0026
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 62.2772
                       Mean reward: 487.60
               Mean episode length: 160.62
    Episode_Reward/reaching_object: 1.0604
     Episode_Reward/lifting_object: 84.2947
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.49s
                      Time elapsed: 00:33:55
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 38097 steps/s (collection: 2.475s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 491.5745
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.2783
                       Mean reward: 443.92
               Mean episode length: 144.30
    Episode_Reward/reaching_object: 1.0746
     Episode_Reward/lifting_object: 86.1558
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.58s
                      Time elapsed: 00:33:57
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 38744 steps/s (collection: 2.433s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 477.5703
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 62.2786
                       Mean reward: 466.06
               Mean episode length: 147.33
    Episode_Reward/reaching_object: 1.0791
     Episode_Reward/lifting_object: 88.0300
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.54s
                      Time elapsed: 00:34:00
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 35287 steps/s (collection: 2.678s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 493.8144
               Mean surrogate loss: 0.0221
                 Mean entropy loss: 62.2789
                       Mean reward: 421.87
               Mean episode length: 141.58
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 85.4669
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.79s
                      Time elapsed: 00:34:03
                               ETA: 00:47:15

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 32417 steps/s (collection: 2.735s, learning 0.298s)
             Mean action noise std: 2.68
          Mean value_function loss: 490.3629
               Mean surrogate loss: 0.0189
                 Mean entropy loss: 62.2790
                       Mean reward: 473.62
               Mean episode length: 150.61
    Episode_Reward/reaching_object: 1.0836
     Episode_Reward/lifting_object: 88.0497
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 3.03s
                      Time elapsed: 00:34:06
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 30061 steps/s (collection: 3.156s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 492.4046
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 62.2790
                       Mean reward: 435.57
               Mean episode length: 143.22
    Episode_Reward/reaching_object: 1.0781
     Episode_Reward/lifting_object: 87.5997
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 3.27s
                      Time elapsed: 00:34:09
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 30831 steps/s (collection: 2.908s, learning 0.280s)
             Mean action noise std: 2.68
          Mean value_function loss: 480.7200
               Mean surrogate loss: 0.0169
                 Mean entropy loss: 62.2791
                       Mean reward: 507.77
               Mean episode length: 162.02
    Episode_Reward/reaching_object: 1.1434
     Episode_Reward/lifting_object: 92.8291
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 3.19s
                      Time elapsed: 00:34:12
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 30813 steps/s (collection: 3.002s, learning 0.188s)
             Mean action noise std: 2.68
          Mean value_function loss: 477.3229
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 62.2792
                       Mean reward: 469.82
               Mean episode length: 155.26
    Episode_Reward/reaching_object: 1.1448
     Episode_Reward/lifting_object: 93.2589
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 3.19s
                      Time elapsed: 00:34:15
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 29376 steps/s (collection: 3.084s, learning 0.262s)
             Mean action noise std: 2.68
          Mean value_function loss: 463.4810
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 62.2795
                       Mean reward: 520.56
               Mean episode length: 165.91
    Episode_Reward/reaching_object: 1.1963
     Episode_Reward/lifting_object: 98.0739
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 3.35s
                      Time elapsed: 00:34:19
                               ETA: 00:47:08

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 33108 steps/s (collection: 2.801s, learning 0.168s)
             Mean action noise std: 2.68
          Mean value_function loss: 474.9158
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 62.2798
                       Mean reward: 487.70
               Mean episode length: 152.71
    Episode_Reward/reaching_object: 1.1998
     Episode_Reward/lifting_object: 99.3580
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.97s
                      Time elapsed: 00:34:22
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 34955 steps/s (collection: 2.663s, learning 0.150s)
             Mean action noise std: 2.68
          Mean value_function loss: 467.1649
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.2799
                       Mean reward: 542.45
               Mean episode length: 167.71
    Episode_Reward/reaching_object: 1.2292
     Episode_Reward/lifting_object: 101.7891
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.81s
                      Time elapsed: 00:34:25
                               ETA: 00:47:05

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 34926 steps/s (collection: 2.642s, learning 0.173s)
             Mean action noise std: 2.68
          Mean value_function loss: 442.4124
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 62.2800
                       Mean reward: 462.10
               Mean episode length: 150.23
    Episode_Reward/reaching_object: 1.1246
     Episode_Reward/lifting_object: 92.8197
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.81s
                      Time elapsed: 00:34:27
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 34763 steps/s (collection: 2.602s, learning 0.226s)
             Mean action noise std: 2.68
          Mean value_function loss: 422.2886
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 62.2799
                       Mean reward: 446.63
               Mean episode length: 147.56
    Episode_Reward/reaching_object: 1.0735
     Episode_Reward/lifting_object: 88.2176
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.83s
                      Time elapsed: 00:34:30
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 36993 steps/s (collection: 2.549s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 438.7796
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 62.2798
                       Mean reward: 452.90
               Mean episode length: 152.13
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 85.7408
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.66s
                      Time elapsed: 00:34:33
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 40431 steps/s (collection: 2.324s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 437.5884
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 62.2794
                       Mean reward: 451.30
               Mean episode length: 156.32
    Episode_Reward/reaching_object: 1.1185
     Episode_Reward/lifting_object: 90.5244
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.43s
                      Time elapsed: 00:34:35
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 39245 steps/s (collection: 2.373s, learning 0.132s)
             Mean action noise std: 2.68
          Mean value_function loss: 435.0880
               Mean surrogate loss: 0.0218
                 Mean entropy loss: 62.2794
                       Mean reward: 475.74
               Mean episode length: 163.52
    Episode_Reward/reaching_object: 1.1670
     Episode_Reward/lifting_object: 92.7710
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.50s
                      Time elapsed: 00:34:38
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 38347 steps/s (collection: 2.431s, learning 0.132s)
             Mean action noise std: 2.68
          Mean value_function loss: 430.5424
               Mean surrogate loss: 0.0278
                 Mean entropy loss: 62.2794
                       Mean reward: 482.14
               Mean episode length: 158.67
    Episode_Reward/reaching_object: 1.2062
     Episode_Reward/lifting_object: 97.5373
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.56s
                      Time elapsed: 00:34:40
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 39371 steps/s (collection: 2.395s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 428.3727
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 62.2795
                       Mean reward: 542.08
               Mean episode length: 171.84
    Episode_Reward/reaching_object: 1.2661
     Episode_Reward/lifting_object: 102.2107
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.50s
                      Time elapsed: 00:34:43
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 38805 steps/s (collection: 2.412s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 446.6446
               Mean surrogate loss: 0.0220
                 Mean entropy loss: 62.2795
                       Mean reward: 520.63
               Mean episode length: 169.53
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 99.8169
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.53s
                      Time elapsed: 00:34:45
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 40063 steps/s (collection: 2.349s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 432.7281
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 62.2795
                       Mean reward: 559.34
               Mean episode length: 177.13
    Episode_Reward/reaching_object: 1.2708
     Episode_Reward/lifting_object: 102.5933
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.45s
                      Time elapsed: 00:34:48
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 41070 steps/s (collection: 2.286s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 431.6553
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 62.2796
                       Mean reward: 569.66
               Mean episode length: 180.80
    Episode_Reward/reaching_object: 1.3101
     Episode_Reward/lifting_object: 108.2327
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.39s
                      Time elapsed: 00:34:50
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 36817 steps/s (collection: 2.513s, learning 0.157s)
             Mean action noise std: 2.68
          Mean value_function loss: 445.4021
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 62.2795
                       Mean reward: 522.88
               Mean episode length: 169.57
    Episode_Reward/reaching_object: 1.3014
     Episode_Reward/lifting_object: 107.8467
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.67s
                      Time elapsed: 00:34:53
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 39630 steps/s (collection: 2.365s, learning 0.116s)
             Mean action noise std: 2.68
          Mean value_function loss: 452.5846
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 62.2789
                       Mean reward: 509.93
               Mean episode length: 168.61
    Episode_Reward/reaching_object: 1.2577
     Episode_Reward/lifting_object: 102.8454
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.48s
                      Time elapsed: 00:34:55
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 39627 steps/s (collection: 2.316s, learning 0.165s)
             Mean action noise std: 2.68
          Mean value_function loss: 435.5170
               Mean surrogate loss: 0.0168
                 Mean entropy loss: 62.2785
                       Mean reward: 559.58
               Mean episode length: 177.73
    Episode_Reward/reaching_object: 1.3397
     Episode_Reward/lifting_object: 110.4268
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.48s
                      Time elapsed: 00:34:58
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 38814 steps/s (collection: 2.418s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 460.0681
               Mean surrogate loss: 0.0163
                 Mean entropy loss: 62.2786
                       Mean reward: 466.03
               Mean episode length: 156.42
    Episode_Reward/reaching_object: 1.2270
     Episode_Reward/lifting_object: 99.0209
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.53s
                      Time elapsed: 00:35:00
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 37494 steps/s (collection: 2.490s, learning 0.132s)
             Mean action noise std: 2.68
          Mean value_function loss: 439.3758
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 62.2787
                       Mean reward: 544.77
               Mean episode length: 170.96
    Episode_Reward/reaching_object: 1.2391
     Episode_Reward/lifting_object: 99.8107
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.62s
                      Time elapsed: 00:35:03
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 37665 steps/s (collection: 2.491s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 430.8148
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 62.2787
                       Mean reward: 554.78
               Mean episode length: 173.79
    Episode_Reward/reaching_object: 1.2636
     Episode_Reward/lifting_object: 103.5526
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.61s
                      Time elapsed: 00:35:06
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 41436 steps/s (collection: 2.263s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 426.5549
               Mean surrogate loss: 0.0209
                 Mean entropy loss: 62.2788
                       Mean reward: 547.02
               Mean episode length: 170.66
    Episode_Reward/reaching_object: 1.3353
     Episode_Reward/lifting_object: 109.6487
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.37s
                      Time elapsed: 00:35:08
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 38617 steps/s (collection: 2.404s, learning 0.141s)
             Mean action noise std: 2.68
          Mean value_function loss: 428.1775
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 62.2789
                       Mean reward: 548.81
               Mean episode length: 169.66
    Episode_Reward/reaching_object: 1.2836
     Episode_Reward/lifting_object: 104.9582
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.55s
                      Time elapsed: 00:35:11
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 35632 steps/s (collection: 2.550s, learning 0.209s)
             Mean action noise std: 2.68
          Mean value_function loss: 423.1643
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 62.2789
                       Mean reward: 561.26
               Mean episode length: 169.75
    Episode_Reward/reaching_object: 1.3502
     Episode_Reward/lifting_object: 111.7303
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.76s
                      Time elapsed: 00:35:13
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 39485 steps/s (collection: 2.376s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 425.8938
               Mean surrogate loss: 0.0163
                 Mean entropy loss: 62.2789
                       Mean reward: 499.38
               Mean episode length: 163.70
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 111.5552
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.49s
                      Time elapsed: 00:35:16
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 38400 steps/s (collection: 2.427s, learning 0.133s)
             Mean action noise std: 2.68
          Mean value_function loss: 450.2439
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 62.2789
                       Mean reward: 501.22
               Mean episode length: 160.44
    Episode_Reward/reaching_object: 1.3246
     Episode_Reward/lifting_object: 109.2280
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.56s
                      Time elapsed: 00:35:18
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 38412 steps/s (collection: 2.382s, learning 0.177s)
             Mean action noise std: 2.68
          Mean value_function loss: 445.6007
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 62.2789
                       Mean reward: 503.12
               Mean episode length: 162.40
    Episode_Reward/reaching_object: 1.3257
     Episode_Reward/lifting_object: 109.2073
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.56s
                      Time elapsed: 00:35:21
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 38335 steps/s (collection: 2.460s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 426.6015
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 62.2790
                       Mean reward: 554.75
               Mean episode length: 171.29
    Episode_Reward/reaching_object: 1.3681
     Episode_Reward/lifting_object: 114.1538
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.56s
                      Time elapsed: 00:35:24
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 40357 steps/s (collection: 2.304s, learning 0.132s)
             Mean action noise std: 2.68
          Mean value_function loss: 444.6490
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 62.2791
                       Mean reward: 596.53
               Mean episode length: 181.06
    Episode_Reward/reaching_object: 1.3113
     Episode_Reward/lifting_object: 108.2977
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.44s
                      Time elapsed: 00:35:26
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 38488 steps/s (collection: 2.421s, learning 0.134s)
             Mean action noise std: 2.68
          Mean value_function loss: 464.3703
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 62.2793
                       Mean reward: 499.15
               Mean episode length: 159.23
    Episode_Reward/reaching_object: 1.2908
     Episode_Reward/lifting_object: 106.1561
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.55s
                      Time elapsed: 00:35:28
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 34141 steps/s (collection: 2.632s, learning 0.247s)
             Mean action noise std: 2.68
          Mean value_function loss: 431.0110
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 62.2795
                       Mean reward: 538.40
               Mean episode length: 168.03
    Episode_Reward/reaching_object: 1.3228
     Episode_Reward/lifting_object: 109.2703
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.88s
                      Time elapsed: 00:35:31
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 32816 steps/s (collection: 2.793s, learning 0.202s)
             Mean action noise std: 2.68
          Mean value_function loss: 473.6457
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.2797
                       Mean reward: 511.00
               Mean episode length: 164.05
    Episode_Reward/reaching_object: 1.3260
     Episode_Reward/lifting_object: 108.8442
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 3.00s
                      Time elapsed: 00:35:34
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 32331 steps/s (collection: 2.835s, learning 0.206s)
             Mean action noise std: 2.68
          Mean value_function loss: 493.7326
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 62.2798
                       Mean reward: 502.27
               Mean episode length: 163.19
    Episode_Reward/reaching_object: 1.2826
     Episode_Reward/lifting_object: 103.9550
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 3.04s
                      Time elapsed: 00:35:37
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 38375 steps/s (collection: 2.462s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 450.1813
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 62.2797
                       Mean reward: 498.35
               Mean episode length: 164.27
    Episode_Reward/reaching_object: 1.2750
     Episode_Reward/lifting_object: 102.0702
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.56s
                      Time elapsed: 00:35:40
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 39455 steps/s (collection: 2.340s, learning 0.151s)
             Mean action noise std: 2.68
          Mean value_function loss: 482.7786
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 62.2798
                       Mean reward: 526.60
               Mean episode length: 171.40
    Episode_Reward/reaching_object: 1.3267
     Episode_Reward/lifting_object: 107.7726
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.49s
                      Time elapsed: 00:35:42
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 41072 steps/s (collection: 2.295s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 455.4979
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 62.2799
                       Mean reward: 567.65
               Mean episode length: 178.43
    Episode_Reward/reaching_object: 1.2978
     Episode_Reward/lifting_object: 104.4188
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.39s
                      Time elapsed: 00:35:45
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 41080 steps/s (collection: 2.277s, learning 0.116s)
             Mean action noise std: 2.68
          Mean value_function loss: 447.9670
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.2801
                       Mean reward: 504.06
               Mean episode length: 164.41
    Episode_Reward/reaching_object: 1.2871
     Episode_Reward/lifting_object: 104.9889
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.39s
                      Time elapsed: 00:35:47
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 36708 steps/s (collection: 2.581s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 409.5094
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.2811
                       Mean reward: 560.48
               Mean episode length: 177.17
    Episode_Reward/reaching_object: 1.3062
     Episode_Reward/lifting_object: 106.4313
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.68s
                      Time elapsed: 00:35:50
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 36038 steps/s (collection: 2.598s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 481.1639
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2834
                       Mean reward: 564.50
               Mean episode length: 173.47
    Episode_Reward/reaching_object: 1.3160
     Episode_Reward/lifting_object: 109.1195
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.73s
                      Time elapsed: 00:35:53
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 34095 steps/s (collection: 2.728s, learning 0.156s)
             Mean action noise std: 2.68
          Mean value_function loss: 440.1320
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.2867
                       Mean reward: 555.92
               Mean episode length: 171.67
    Episode_Reward/reaching_object: 1.2321
     Episode_Reward/lifting_object: 100.8482
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.88s
                      Time elapsed: 00:35:56
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 38440 steps/s (collection: 2.385s, learning 0.173s)
             Mean action noise std: 2.68
          Mean value_function loss: 428.5797
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 62.2870
                       Mean reward: 524.73
               Mean episode length: 169.31
    Episode_Reward/reaching_object: 1.2929
     Episode_Reward/lifting_object: 107.0444
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.56s
                      Time elapsed: 00:35:58
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 34572 steps/s (collection: 2.593s, learning 0.250s)
             Mean action noise std: 2.68
          Mean value_function loss: 459.2400
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 62.2872
                       Mean reward: 527.35
               Mean episode length: 170.60
    Episode_Reward/reaching_object: 1.2546
     Episode_Reward/lifting_object: 103.3041
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.84s
                      Time elapsed: 00:36:01
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 34681 steps/s (collection: 2.631s, learning 0.203s)
             Mean action noise std: 2.68
          Mean value_function loss: 439.8669
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.2869
                       Mean reward: 538.33
               Mean episode length: 174.14
    Episode_Reward/reaching_object: 1.2756
     Episode_Reward/lifting_object: 105.4236
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.83s
                      Time elapsed: 00:36:04
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 33629 steps/s (collection: 2.774s, learning 0.149s)
             Mean action noise std: 2.68
          Mean value_function loss: 432.0134
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 62.2858
                       Mean reward: 504.64
               Mean episode length: 161.63
    Episode_Reward/reaching_object: 1.3098
     Episode_Reward/lifting_object: 109.3573
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.92s
                      Time elapsed: 00:36:07
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 31480 steps/s (collection: 2.810s, learning 0.313s)
             Mean action noise std: 2.68
          Mean value_function loss: 484.1402
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.2858
                       Mean reward: 601.12
               Mean episode length: 186.88
    Episode_Reward/reaching_object: 1.2951
     Episode_Reward/lifting_object: 107.6062
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 3.12s
                      Time elapsed: 00:36:10
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 31296 steps/s (collection: 2.965s, learning 0.176s)
             Mean action noise std: 2.68
          Mean value_function loss: 418.5578
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.2876
                       Mean reward: 527.14
               Mean episode length: 165.58
    Episode_Reward/reaching_object: 1.2589
     Episode_Reward/lifting_object: 105.8336
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 3.14s
                      Time elapsed: 00:36:13
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 33528 steps/s (collection: 2.718s, learning 0.214s)
             Mean action noise std: 2.68
          Mean value_function loss: 415.9623
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 62.2908
                       Mean reward: 563.94
               Mean episode length: 175.71
    Episode_Reward/reaching_object: 1.3010
     Episode_Reward/lifting_object: 108.1573
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.93s
                      Time elapsed: 00:36:16
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 33213 steps/s (collection: 2.779s, learning 0.181s)
             Mean action noise std: 2.68
          Mean value_function loss: 407.2175
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 62.2915
                       Mean reward: 535.87
               Mean episode length: 171.22
    Episode_Reward/reaching_object: 1.3557
     Episode_Reward/lifting_object: 112.7037
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.96s
                      Time elapsed: 00:36:19
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 35079 steps/s (collection: 2.641s, learning 0.162s)
             Mean action noise std: 2.68
          Mean value_function loss: 391.3775
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.2911
                       Mean reward: 604.90
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 1.3795
     Episode_Reward/lifting_object: 114.8508
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.80s
                      Time elapsed: 00:36:22
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 35545 steps/s (collection: 2.566s, learning 0.199s)
             Mean action noise std: 2.68
          Mean value_function loss: 404.2192
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 62.2908
                       Mean reward: 608.61
               Mean episode length: 188.56
    Episode_Reward/reaching_object: 1.3963
     Episode_Reward/lifting_object: 117.4906
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.77s
                      Time elapsed: 00:36:24
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 33422 steps/s (collection: 2.744s, learning 0.198s)
             Mean action noise std: 2.68
          Mean value_function loss: 401.0394
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.2974
                       Mean reward: 571.45
               Mean episode length: 177.13
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 116.8953
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.94s
                      Time elapsed: 00:36:27
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 38017 steps/s (collection: 2.436s, learning 0.150s)
             Mean action noise std: 2.68
          Mean value_function loss: 404.2682
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 62.3009
                       Mean reward: 662.73
               Mean episode length: 202.77
    Episode_Reward/reaching_object: 1.4490
     Episode_Reward/lifting_object: 121.3370
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.59s
                      Time elapsed: 00:36:30
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 34339 steps/s (collection: 2.701s, learning 0.162s)
             Mean action noise std: 2.68
          Mean value_function loss: 405.6188
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 62.3013
                       Mean reward: 641.20
               Mean episode length: 191.71
    Episode_Reward/reaching_object: 1.4069
     Episode_Reward/lifting_object: 118.1141
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.86s
                      Time elapsed: 00:36:33
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 35506 steps/s (collection: 2.554s, learning 0.214s)
             Mean action noise std: 2.68
          Mean value_function loss: 425.5160
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.3014
                       Mean reward: 641.13
               Mean episode length: 193.06
    Episode_Reward/reaching_object: 1.4576
     Episode_Reward/lifting_object: 122.4674
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.77s
                      Time elapsed: 00:36:36
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 34346 steps/s (collection: 2.684s, learning 0.179s)
             Mean action noise std: 2.68
          Mean value_function loss: 440.8172
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.3009
                       Mean reward: 588.52
               Mean episode length: 183.41
    Episode_Reward/reaching_object: 1.4182
     Episode_Reward/lifting_object: 117.8752
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.86s
                      Time elapsed: 00:36:38
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 38003 steps/s (collection: 2.433s, learning 0.154s)
             Mean action noise std: 2.68
          Mean value_function loss: 428.5013
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 62.2998
                       Mean reward: 574.64
               Mean episode length: 181.29
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 118.1266
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.59s
                      Time elapsed: 00:36:41
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 38691 steps/s (collection: 2.345s, learning 0.196s)
             Mean action noise std: 2.68
          Mean value_function loss: 449.0194
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 62.2998
                       Mean reward: 570.95
               Mean episode length: 173.12
    Episode_Reward/reaching_object: 1.3325
     Episode_Reward/lifting_object: 110.2096
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.54s
                      Time elapsed: 00:36:44
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 36234 steps/s (collection: 2.480s, learning 0.233s)
             Mean action noise std: 2.68
          Mean value_function loss: 441.5042
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 62.2998
                       Mean reward: 566.14
               Mean episode length: 176.85
    Episode_Reward/reaching_object: 1.3589
     Episode_Reward/lifting_object: 111.8245
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.71s
                      Time elapsed: 00:36:46
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 36199 steps/s (collection: 2.575s, learning 0.141s)
             Mean action noise std: 2.68
          Mean value_function loss: 422.8067
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 62.2998
                       Mean reward: 564.02
               Mean episode length: 176.68
    Episode_Reward/reaching_object: 1.4095
     Episode_Reward/lifting_object: 117.1054
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.72s
                      Time elapsed: 00:36:49
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 36706 steps/s (collection: 2.487s, learning 0.192s)
             Mean action noise std: 2.68
          Mean value_function loss: 405.5006
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 62.2999
                       Mean reward: 574.14
               Mean episode length: 177.47
    Episode_Reward/reaching_object: 1.3907
     Episode_Reward/lifting_object: 116.2233
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.68s
                      Time elapsed: 00:36:52
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 35600 steps/s (collection: 2.556s, learning 0.206s)
             Mean action noise std: 2.68
          Mean value_function loss: 445.5740
               Mean surrogate loss: 0.0212
                 Mean entropy loss: 62.3000
                       Mean reward: 664.23
               Mean episode length: 195.14
    Episode_Reward/reaching_object: 1.4173
     Episode_Reward/lifting_object: 118.6432
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.76s
                      Time elapsed: 00:36:54
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 31444 steps/s (collection: 3.020s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 431.8907
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 62.3002
                       Mean reward: 642.42
               Mean episode length: 194.30
    Episode_Reward/reaching_object: 1.4375
     Episode_Reward/lifting_object: 120.0201
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 3.13s
                      Time elapsed: 00:36:58
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 33139 steps/s (collection: 2.731s, learning 0.236s)
             Mean action noise std: 2.68
          Mean value_function loss: 454.0713
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 62.3005
                       Mean reward: 583.27
               Mean episode length: 182.12
    Episode_Reward/reaching_object: 1.4413
     Episode_Reward/lifting_object: 120.2078
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.97s
                      Time elapsed: 00:37:01
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 38136 steps/s (collection: 2.448s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 427.0372
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 62.3009
                       Mean reward: 534.12
               Mean episode length: 168.01
    Episode_Reward/reaching_object: 1.4005
     Episode_Reward/lifting_object: 115.7678
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.58s
                      Time elapsed: 00:37:03
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 35998 steps/s (collection: 2.559s, learning 0.172s)
             Mean action noise std: 2.68
          Mean value_function loss: 450.5344
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.3010
                       Mean reward: 578.91
               Mean episode length: 176.21
    Episode_Reward/reaching_object: 1.3814
     Episode_Reward/lifting_object: 115.1869
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.73s
                      Time elapsed: 00:37:06
                               ETA: 00:44:56

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 40768 steps/s (collection: 2.294s, learning 0.117s)
             Mean action noise std: 2.68
          Mean value_function loss: 447.6505
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.3004
                       Mean reward: 577.03
               Mean episode length: 180.86
    Episode_Reward/reaching_object: 1.4213
     Episode_Reward/lifting_object: 118.4331
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.41s
                      Time elapsed: 00:37:08
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 39438 steps/s (collection: 2.358s, learning 0.135s)
             Mean action noise std: 2.68
          Mean value_function loss: 458.3093
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 62.3003
                       Mean reward: 567.26
               Mean episode length: 177.23
    Episode_Reward/reaching_object: 1.3685
     Episode_Reward/lifting_object: 113.1793
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.49s
                      Time elapsed: 00:37:11
                               ETA: 00:44:51

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 41280 steps/s (collection: 2.282s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 476.7921
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.2998
                       Mean reward: 532.93
               Mean episode length: 170.16
    Episode_Reward/reaching_object: 1.3090
     Episode_Reward/lifting_object: 107.9412
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.38s
                      Time elapsed: 00:37:13
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 41046 steps/s (collection: 2.276s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 471.4691
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.2973
                       Mean reward: 524.37
               Mean episode length: 167.87
    Episode_Reward/reaching_object: 1.3244
     Episode_Reward/lifting_object: 108.8910
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.39s
                      Time elapsed: 00:37:16
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 41841 steps/s (collection: 2.245s, learning 0.104s)
             Mean action noise std: 2.68
          Mean value_function loss: 481.7244
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.2963
                       Mean reward: 550.01
               Mean episode length: 170.68
    Episode_Reward/reaching_object: 1.2926
     Episode_Reward/lifting_object: 106.0592
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.35s
                      Time elapsed: 00:37:18
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 40723 steps/s (collection: 2.313s, learning 0.101s)
             Mean action noise std: 2.68
          Mean value_function loss: 483.2767
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.2950
                       Mean reward: 514.45
               Mean episode length: 165.00
    Episode_Reward/reaching_object: 1.2699
     Episode_Reward/lifting_object: 102.6651
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.41s
                      Time elapsed: 00:37:20
                               ETA: 00:44:41

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 40906 steps/s (collection: 2.276s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 503.3239
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.2928
                       Mean reward: 521.63
               Mean episode length: 164.22
    Episode_Reward/reaching_object: 1.3061
     Episode_Reward/lifting_object: 107.4128
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.40s
                      Time elapsed: 00:37:23
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 40852 steps/s (collection: 2.294s, learning 0.112s)
             Mean action noise std: 2.68
          Mean value_function loss: 491.1693
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 62.2909
                       Mean reward: 539.86
               Mean episode length: 172.15
    Episode_Reward/reaching_object: 1.2784
     Episode_Reward/lifting_object: 103.6372
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.41s
                      Time elapsed: 00:37:25
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 41585 steps/s (collection: 2.236s, learning 0.128s)
             Mean action noise std: 2.68
          Mean value_function loss: 468.9120
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.2899
                       Mean reward: 583.47
               Mean episode length: 179.95
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 100.3469
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.36s
                      Time elapsed: 00:37:27
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 40970 steps/s (collection: 2.267s, learning 0.133s)
             Mean action noise std: 2.68
          Mean value_function loss: 461.9263
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.2902
                       Mean reward: 527.54
               Mean episode length: 165.85
    Episode_Reward/reaching_object: 1.2669
     Episode_Reward/lifting_object: 103.6862
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.40s
                      Time elapsed: 00:37:30
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 40882 steps/s (collection: 2.303s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 457.0264
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 62.2959
                       Mean reward: 550.73
               Mean episode length: 176.55
    Episode_Reward/reaching_object: 1.3645
     Episode_Reward/lifting_object: 112.5172
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.40s
                      Time elapsed: 00:37:32
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 39585 steps/s (collection: 2.329s, learning 0.155s)
             Mean action noise std: 2.68
          Mean value_function loss: 442.8963
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.2984
                       Mean reward: 528.29
               Mean episode length: 162.16
    Episode_Reward/reaching_object: 1.3514
     Episode_Reward/lifting_object: 111.8293
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.48s
                      Time elapsed: 00:37:35
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 41341 steps/s (collection: 2.269s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 438.8354
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.3011
                       Mean reward: 557.02
               Mean episode length: 171.96
    Episode_Reward/reaching_object: 1.3018
     Episode_Reward/lifting_object: 107.0957
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.38s
                      Time elapsed: 00:37:37
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 41139 steps/s (collection: 2.265s, learning 0.125s)
             Mean action noise std: 2.68
          Mean value_function loss: 438.0964
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.3007
                       Mean reward: 655.15
               Mean episode length: 192.79
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 114.5015
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.39s
                      Time elapsed: 00:37:40
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 39883 steps/s (collection: 2.333s, learning 0.131s)
             Mean action noise std: 2.68
          Mean value_function loss: 382458.3242
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.3025
                       Mean reward: 626.45
               Mean episode length: 184.67
    Episode_Reward/reaching_object: 1.4447
     Episode_Reward/lifting_object: 122.3512
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.1317
          Episode_Reward/joint_vel: -35.9630
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.46s
                      Time elapsed: 00:37:42
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 40427 steps/s (collection: 2.296s, learning 0.136s)
             Mean action noise std: 2.69
          Mean value_function loss: 400.3970
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.3053
                       Mean reward: 651.59
               Mean episode length: 192.05
    Episode_Reward/reaching_object: 1.5768
     Episode_Reward/lifting_object: 135.5796
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.43s
                      Time elapsed: 00:37:44
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 40226 steps/s (collection: 2.340s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 402.3060
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 62.3093
                       Mean reward: 710.63
               Mean episode length: 204.92
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 134.5547
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.44s
                      Time elapsed: 00:37:47
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 37761 steps/s (collection: 2.478s, learning 0.125s)
             Mean action noise std: 2.69
          Mean value_function loss: 400.6508
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.3122
                       Mean reward: 600.58
               Mean episode length: 181.50
    Episode_Reward/reaching_object: 1.4719
     Episode_Reward/lifting_object: 125.8894
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.60s
                      Time elapsed: 00:37:49
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 40177 steps/s (collection: 2.331s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 388.6849
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 62.3139
                       Mean reward: 602.81
               Mean episode length: 178.85
    Episode_Reward/reaching_object: 1.5313
     Episode_Reward/lifting_object: 132.1016
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.45s
                      Time elapsed: 00:37:52
                               ETA: 00:44:08

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 39765 steps/s (collection: 2.345s, learning 0.127s)
             Mean action noise std: 2.69
          Mean value_function loss: 392.4588
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 62.3141
                       Mean reward: 668.67
               Mean episode length: 195.28
    Episode_Reward/reaching_object: 1.5693
     Episode_Reward/lifting_object: 135.5059
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.47s
                      Time elapsed: 00:37:54
                               ETA: 00:44:06

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 38868 steps/s (collection: 2.353s, learning 0.176s)
             Mean action noise std: 2.69
          Mean value_function loss: 391.8599
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.3141
                       Mean reward: 607.54
               Mean episode length: 183.60
    Episode_Reward/reaching_object: 1.4784
     Episode_Reward/lifting_object: 125.1388
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.53s
                      Time elapsed: 00:37:57
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 40049 steps/s (collection: 2.350s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 407.8260
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.3145
                       Mean reward: 604.33
               Mean episode length: 182.21
    Episode_Reward/reaching_object: 1.4576
     Episode_Reward/lifting_object: 124.2093
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.45s
                      Time elapsed: 00:37:59
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 38716 steps/s (collection: 2.357s, learning 0.182s)
             Mean action noise std: 2.69
          Mean value_function loss: 396.1198
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.3159
                       Mean reward: 592.05
               Mean episode length: 178.48
    Episode_Reward/reaching_object: 1.4125
     Episode_Reward/lifting_object: 119.6704
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.54s
                      Time elapsed: 00:38:02
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 40435 steps/s (collection: 2.327s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 404.7059
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.3202
                       Mean reward: 674.61
               Mean episode length: 197.78
    Episode_Reward/reaching_object: 1.4957
     Episode_Reward/lifting_object: 127.8946
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.43s
                      Time elapsed: 00:38:04
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 39397 steps/s (collection: 2.311s, learning 0.184s)
             Mean action noise std: 2.69
          Mean value_function loss: 373.8646
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 62.3322
                       Mean reward: 626.35
               Mean episode length: 189.58
    Episode_Reward/reaching_object: 1.4838
     Episode_Reward/lifting_object: 126.0358
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.50s
                      Time elapsed: 00:38:07
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 38949 steps/s (collection: 2.415s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 353.7397
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.3379
                       Mean reward: 683.66
               Mean episode length: 196.89
    Episode_Reward/reaching_object: 1.5209
     Episode_Reward/lifting_object: 130.1814
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.52s
                      Time elapsed: 00:38:09
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 42069 steps/s (collection: 2.237s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 353.0116
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3421
                       Mean reward: 690.79
               Mean episode length: 199.11
    Episode_Reward/reaching_object: 1.5614
     Episode_Reward/lifting_object: 133.0590
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.34s
                      Time elapsed: 00:38:12
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 41342 steps/s (collection: 2.224s, learning 0.154s)
             Mean action noise std: 2.69
          Mean value_function loss: 362.4478
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 62.3418
                       Mean reward: 696.77
               Mean episode length: 200.09
    Episode_Reward/reaching_object: 1.5487
     Episode_Reward/lifting_object: 133.2546
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.38s
                      Time elapsed: 00:38:14
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 42131 steps/s (collection: 2.237s, learning 0.096s)
             Mean action noise std: 2.69
          Mean value_function loss: 353.6479
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 62.3408
                       Mean reward: 691.92
               Mean episode length: 201.57
    Episode_Reward/reaching_object: 1.5802
     Episode_Reward/lifting_object: 136.2508
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.33s
                      Time elapsed: 00:38:16
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 40626 steps/s (collection: 2.280s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 371.8002
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 62.3408
                       Mean reward: 677.72
               Mean episode length: 199.41
    Episode_Reward/reaching_object: 1.5772
     Episode_Reward/lifting_object: 135.0450
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.42s
                      Time elapsed: 00:38:19
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 39166 steps/s (collection: 2.370s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 355.1813
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 62.3409
                       Mean reward: 706.09
               Mean episode length: 204.49
    Episode_Reward/reaching_object: 1.6053
     Episode_Reward/lifting_object: 138.1488
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.51s
                      Time elapsed: 00:38:21
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 41411 steps/s (collection: 2.264s, learning 0.110s)
             Mean action noise std: 2.69
          Mean value_function loss: 376.3679
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 62.3407
                       Mean reward: 625.03
               Mean episode length: 186.86
    Episode_Reward/reaching_object: 1.5413
     Episode_Reward/lifting_object: 131.2770
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.37s
                      Time elapsed: 00:38:24
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 38107 steps/s (collection: 2.442s, learning 0.138s)
             Mean action noise std: 2.69
          Mean value_function loss: 389.1976
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 62.3401
                       Mean reward: 668.44
               Mean episode length: 191.40
    Episode_Reward/reaching_object: 1.5434
     Episode_Reward/lifting_object: 132.6283
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.58s
                      Time elapsed: 00:38:26
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 35205 steps/s (collection: 2.651s, learning 0.141s)
             Mean action noise std: 2.69
          Mean value_function loss: 400.1671
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 62.3403
                       Mean reward: 659.60
               Mean episode length: 194.07
    Episode_Reward/reaching_object: 1.4725
     Episode_Reward/lifting_object: 125.4223
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.79s
                      Time elapsed: 00:38:29
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 38605 steps/s (collection: 2.422s, learning 0.125s)
             Mean action noise std: 2.69
          Mean value_function loss: 371.2736
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 62.3406
                       Mean reward: 557.38
               Mean episode length: 169.82
    Episode_Reward/reaching_object: 1.4342
     Episode_Reward/lifting_object: 121.4987
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.55s
                      Time elapsed: 00:38:32
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 34845 steps/s (collection: 2.712s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 339.3792
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.3407
                       Mean reward: 614.52
               Mean episode length: 182.56
    Episode_Reward/reaching_object: 1.4688
     Episode_Reward/lifting_object: 124.8231
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.82s
                      Time elapsed: 00:38:34
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 37025 steps/s (collection: 2.537s, learning 0.118s)
             Mean action noise std: 2.69
          Mean value_function loss: 319.0200
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 62.3419
                       Mean reward: 663.10
               Mean episode length: 194.26
    Episode_Reward/reaching_object: 1.5043
     Episode_Reward/lifting_object: 128.9758
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.66s
                      Time elapsed: 00:38:37
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 40358 steps/s (collection: 2.332s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 328.8652
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.3443
                       Mean reward: 656.91
               Mean episode length: 196.99
    Episode_Reward/reaching_object: 1.5723
     Episode_Reward/lifting_object: 135.2336
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.44s
                      Time elapsed: 00:38:40
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 39474 steps/s (collection: 2.352s, learning 0.139s)
             Mean action noise std: 2.69
          Mean value_function loss: 363.7561
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.3502
                       Mean reward: 635.74
               Mean episode length: 183.90
    Episode_Reward/reaching_object: 1.5136
     Episode_Reward/lifting_object: 130.7953
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.49s
                      Time elapsed: 00:38:42
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 37733 steps/s (collection: 2.477s, learning 0.129s)
             Mean action noise std: 2.69
          Mean value_function loss: 352.6572
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.3610
                       Mean reward: 665.20
               Mean episode length: 194.96
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 131.2727
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.61s
                      Time elapsed: 00:38:45
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 38743 steps/s (collection: 2.411s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 343.6214
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.3659
                       Mean reward: 689.31
               Mean episode length: 197.50
    Episode_Reward/reaching_object: 1.5764
     Episode_Reward/lifting_object: 137.0620
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.54s
                      Time elapsed: 00:38:47
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 38121 steps/s (collection: 2.470s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 349.9652
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.3674
                       Mean reward: 644.40
               Mean episode length: 189.20
    Episode_Reward/reaching_object: 1.4975
     Episode_Reward/lifting_object: 129.8689
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.58s
                      Time elapsed: 00:38:50
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 38770 steps/s (collection: 2.412s, learning 0.124s)
             Mean action noise std: 2.70
          Mean value_function loss: 329.5745
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.3708
                       Mean reward: 731.06
               Mean episode length: 207.05
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: 137.0248
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.54s
                      Time elapsed: 00:38:52
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 38667 steps/s (collection: 2.402s, learning 0.141s)
             Mean action noise std: 2.70
          Mean value_function loss: 321.1829
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 62.3737
                       Mean reward: 681.07
               Mean episode length: 194.77
    Episode_Reward/reaching_object: 1.5908
     Episode_Reward/lifting_object: 138.7925
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.54s
                      Time elapsed: 00:38:55
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 37454 steps/s (collection: 2.512s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 305.0321
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.3743
                       Mean reward: 720.27
               Mean episode length: 202.63
    Episode_Reward/reaching_object: 1.5930
     Episode_Reward/lifting_object: 138.3397
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.62s
                      Time elapsed: 00:38:57
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 39214 steps/s (collection: 2.390s, learning 0.117s)
             Mean action noise std: 2.70
          Mean value_function loss: 306.1873
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.3770
                       Mean reward: 702.23
               Mean episode length: 200.38
    Episode_Reward/reaching_object: 1.6464
     Episode_Reward/lifting_object: 142.7730
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.51s
                      Time elapsed: 00:39:00
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 37880 steps/s (collection: 2.466s, learning 0.130s)
             Mean action noise std: 2.70
          Mean value_function loss: 284.4638
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.3853
                       Mean reward: 756.00
               Mean episode length: 211.24
    Episode_Reward/reaching_object: 1.6492
     Episode_Reward/lifting_object: 143.4538
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.60s
                      Time elapsed: 00:39:03
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 37210 steps/s (collection: 2.521s, learning 0.121s)
             Mean action noise std: 2.70
          Mean value_function loss: 287.4529
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 62.3911
                       Mean reward: 726.15
               Mean episode length: 208.37
    Episode_Reward/reaching_object: 1.6053
     Episode_Reward/lifting_object: 138.7096
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.64s
                      Time elapsed: 00:39:05
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 38002 steps/s (collection: 2.458s, learning 0.129s)
             Mean action noise std: 2.70
          Mean value_function loss: 294.2813
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 62.3922
                       Mean reward: 743.14
               Mean episode length: 208.33
    Episode_Reward/reaching_object: 1.7199
     Episode_Reward/lifting_object: 149.0300
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.59s
                      Time elapsed: 00:39:08
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 38811 steps/s (collection: 2.400s, learning 0.133s)
             Mean action noise std: 2.70
          Mean value_function loss: 337.5163
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.3926
                       Mean reward: 703.40
               Mean episode length: 202.88
    Episode_Reward/reaching_object: 1.7006
     Episode_Reward/lifting_object: 147.3913
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.53s
                      Time elapsed: 00:39:10
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 40199 steps/s (collection: 2.316s, learning 0.129s)
             Mean action noise std: 2.70
          Mean value_function loss: 428.4944
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.3926
                       Mean reward: 721.61
               Mean episode length: 208.12
    Episode_Reward/reaching_object: 1.6632
     Episode_Reward/lifting_object: 142.7358
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.45s
                      Time elapsed: 00:39:13
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 39270 steps/s (collection: 2.361s, learning 0.143s)
             Mean action noise std: 2.70
          Mean value_function loss: 420.2100
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.3925
                       Mean reward: 694.58
               Mean episode length: 199.31
    Episode_Reward/reaching_object: 1.5676
     Episode_Reward/lifting_object: 133.5661
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.50s
                      Time elapsed: 00:39:15
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 38298 steps/s (collection: 2.452s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 392.3624
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.3942
                       Mean reward: 724.03
               Mean episode length: 208.21
    Episode_Reward/reaching_object: 1.6160
     Episode_Reward/lifting_object: 137.0225
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.57s
                      Time elapsed: 00:39:18
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 39380 steps/s (collection: 2.369s, learning 0.127s)
             Mean action noise std: 2.70
          Mean value_function loss: 407.5419
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 62.3984
                       Mean reward: 713.82
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 1.6161
     Episode_Reward/lifting_object: 137.5395
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.50s
                      Time elapsed: 00:39:20
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 41876 steps/s (collection: 2.242s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 356.4534
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.4013
                       Mean reward: 636.13
               Mean episode length: 189.68
    Episode_Reward/reaching_object: 1.5293
     Episode_Reward/lifting_object: 130.2417
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.35s
                      Time elapsed: 00:39:23
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 41679 steps/s (collection: 2.254s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 399.1696
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 62.4009
                       Mean reward: 651.03
               Mean episode length: 189.98
    Episode_Reward/reaching_object: 1.5015
     Episode_Reward/lifting_object: 127.5096
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.36s
                      Time elapsed: 00:39:25
                               ETA: 00:42:39

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 40899 steps/s (collection: 2.296s, learning 0.108s)
             Mean action noise std: 2.70
          Mean value_function loss: 374.2471
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.4044
                       Mean reward: 657.44
               Mean episode length: 192.87
    Episode_Reward/reaching_object: 1.4708
     Episode_Reward/lifting_object: 124.4512
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.40s
                      Time elapsed: 00:39:27
                               ETA: 00:42:37

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 41690 steps/s (collection: 2.246s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 337.1280
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 62.4085
                       Mean reward: 661.43
               Mean episode length: 193.79
    Episode_Reward/reaching_object: 1.5110
     Episode_Reward/lifting_object: 128.2342
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.36s
                      Time elapsed: 00:39:30
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 41906 steps/s (collection: 2.234s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 306.9670
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.4095
                       Mean reward: 670.85
               Mean episode length: 194.78
    Episode_Reward/reaching_object: 1.5348
     Episode_Reward/lifting_object: 131.0312
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.35s
                      Time elapsed: 00:39:32
                               ETA: 00:42:32

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 40924 steps/s (collection: 2.304s, learning 0.098s)
             Mean action noise std: 2.70
          Mean value_function loss: 283.2748
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4114
                       Mean reward: 759.50
               Mean episode length: 216.11
    Episode_Reward/reaching_object: 1.6968
     Episode_Reward/lifting_object: 145.5514
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.40s
                      Time elapsed: 00:39:35
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 42126 steps/s (collection: 2.222s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 320.2834
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.4154
                       Mean reward: 754.47
               Mean episode length: 213.79
    Episode_Reward/reaching_object: 1.7288
     Episode_Reward/lifting_object: 149.7637
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.33s
                      Time elapsed: 00:39:37
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 42180 steps/s (collection: 2.220s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 324.8733
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.4163
                       Mean reward: 733.18
               Mean episode length: 207.03
    Episode_Reward/reaching_object: 1.7289
     Episode_Reward/lifting_object: 149.7919
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.33s
                      Time elapsed: 00:39:39
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 42164 steps/s (collection: 2.227s, learning 0.105s)
             Mean action noise std: 2.70
          Mean value_function loss: 313.7233
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.4174
                       Mean reward: 693.28
               Mean episode length: 195.76
    Episode_Reward/reaching_object: 1.6684
     Episode_Reward/lifting_object: 144.4090
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.33s
                      Time elapsed: 00:39:42
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 41633 steps/s (collection: 2.240s, learning 0.121s)
             Mean action noise std: 2.70
          Mean value_function loss: 273.7659
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.4222
                       Mean reward: 805.65
               Mean episode length: 224.07
    Episode_Reward/reaching_object: 1.7336
     Episode_Reward/lifting_object: 151.1637
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.36s
                      Time elapsed: 00:39:44
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 39881 steps/s (collection: 2.360s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 285.5427
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.4319
                       Mean reward: 784.73
               Mean episode length: 218.87
    Episode_Reward/reaching_object: 1.7133
     Episode_Reward/lifting_object: 149.5389
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.46s
                      Time elapsed: 00:39:46
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 40183 steps/s (collection: 2.326s, learning 0.121s)
             Mean action noise std: 2.71
          Mean value_function loss: 290.9635
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.4384
                       Mean reward: 760.33
               Mean episode length: 212.29
    Episode_Reward/reaching_object: 1.6960
     Episode_Reward/lifting_object: 148.2343
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.45s
                      Time elapsed: 00:39:49
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 41502 steps/s (collection: 2.250s, learning 0.119s)
             Mean action noise std: 2.71
          Mean value_function loss: 269.9777
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4471
                       Mean reward: 757.63
               Mean episode length: 209.11
    Episode_Reward/reaching_object: 1.7130
     Episode_Reward/lifting_object: 149.5972
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.37s
                      Time elapsed: 00:39:51
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 40848 steps/s (collection: 2.295s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 258.5253
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.4543
                       Mean reward: 780.67
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 1.7176
     Episode_Reward/lifting_object: 149.5504
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.41s
                      Time elapsed: 00:39:54
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 41195 steps/s (collection: 2.277s, learning 0.109s)
             Mean action noise std: 2.71
          Mean value_function loss: 272.1841
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.4602
                       Mean reward: 742.71
               Mean episode length: 207.27
    Episode_Reward/reaching_object: 1.6594
     Episode_Reward/lifting_object: 144.6003
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.39s
                      Time elapsed: 00:39:56
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 40149 steps/s (collection: 2.326s, learning 0.122s)
             Mean action noise std: 2.71
          Mean value_function loss: 261.6518
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.4668
                       Mean reward: 789.66
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 1.6966
     Episode_Reward/lifting_object: 147.4787
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.45s
                      Time elapsed: 00:39:58
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 41318 steps/s (collection: 2.268s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 263.0701
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.4771
                       Mean reward: 837.95
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 1.7759
     Episode_Reward/lifting_object: 155.7947
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.38s
                      Time elapsed: 00:40:01
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 41796 steps/s (collection: 2.240s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 249.8445
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 62.4878
                       Mean reward: 777.34
               Mean episode length: 214.48
    Episode_Reward/reaching_object: 1.7857
     Episode_Reward/lifting_object: 156.4487
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.35s
                      Time elapsed: 00:40:03
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 40253 steps/s (collection: 2.325s, learning 0.117s)
             Mean action noise std: 2.71
          Mean value_function loss: 260.3054
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 62.4899
                       Mean reward: 767.82
               Mean episode length: 213.60
    Episode_Reward/reaching_object: 1.7969
     Episode_Reward/lifting_object: 157.2938
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.44s
                      Time elapsed: 00:40:06
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 41209 steps/s (collection: 2.274s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 299.2152
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.4904
                       Mean reward: 792.46
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 1.7575
     Episode_Reward/lifting_object: 153.6359
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.39s
                      Time elapsed: 00:40:08
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 40159 steps/s (collection: 2.346s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 302.8369
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 62.4934
                       Mean reward: 756.99
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 1.7165
     Episode_Reward/lifting_object: 149.4453
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.45s
                      Time elapsed: 00:40:10
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 40935 steps/s (collection: 2.300s, learning 0.102s)
             Mean action noise std: 2.71
          Mean value_function loss: 252.9632
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.5002
                       Mean reward: 796.90
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.7637
     Episode_Reward/lifting_object: 154.3578
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.40s
                      Time elapsed: 00:40:13
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 40000 steps/s (collection: 2.346s, learning 0.112s)
             Mean action noise std: 2.72
          Mean value_function loss: 247.1730
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.5027
                       Mean reward: 792.57
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.8049
     Episode_Reward/lifting_object: 158.0336
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.46s
                      Time elapsed: 00:40:15
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 40476 steps/s (collection: 2.320s, learning 0.109s)
             Mean action noise std: 2.72
          Mean value_function loss: 230.5433
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.5057
                       Mean reward: 814.99
               Mean episode length: 221.76
    Episode_Reward/reaching_object: 1.8022
     Episode_Reward/lifting_object: 158.6187
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.43s
                      Time elapsed: 00:40:18
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 40241 steps/s (collection: 2.308s, learning 0.135s)
             Mean action noise std: 2.72
          Mean value_function loss: 261.1750
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.5080
                       Mean reward: 795.42
               Mean episode length: 223.31
    Episode_Reward/reaching_object: 1.7147
     Episode_Reward/lifting_object: 149.0129
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.44s
                      Time elapsed: 00:40:20
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 40003 steps/s (collection: 2.338s, learning 0.119s)
             Mean action noise std: 2.72
          Mean value_function loss: 248.9657
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.5109
                       Mean reward: 775.87
               Mean episode length: 217.14
    Episode_Reward/reaching_object: 1.7226
     Episode_Reward/lifting_object: 149.7392
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.46s
                      Time elapsed: 00:40:23
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 41079 steps/s (collection: 2.272s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 259.0520
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 62.5162
                       Mean reward: 758.63
               Mean episode length: 213.35
    Episode_Reward/reaching_object: 1.7044
     Episode_Reward/lifting_object: 147.9102
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.39s
                      Time elapsed: 00:40:25
                               ETA: 00:41:36

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 40583 steps/s (collection: 2.309s, learning 0.114s)
             Mean action noise std: 2.72
          Mean value_function loss: 278.8125
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 62.5169
                       Mean reward: 772.95
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 1.7948
     Episode_Reward/lifting_object: 155.4804
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.42s
                      Time elapsed: 00:40:27
                               ETA: 00:41:34

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 41227 steps/s (collection: 2.279s, learning 0.106s)
             Mean action noise std: 2.72
          Mean value_function loss: 306.5151
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 62.5172
                       Mean reward: 795.53
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 1.7323
     Episode_Reward/lifting_object: 149.8864
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.38s
                      Time elapsed: 00:40:30
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 40878 steps/s (collection: 2.292s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 337.9604
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.5177
                       Mean reward: 766.54
               Mean episode length: 214.18
    Episode_Reward/reaching_object: 1.7176
     Episode_Reward/lifting_object: 148.2512
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.40s
                      Time elapsed: 00:40:32
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 39232 steps/s (collection: 2.370s, learning 0.136s)
             Mean action noise std: 2.72
          Mean value_function loss: 284.6757
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.5240
                       Mean reward: 808.03
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.7962
     Episode_Reward/lifting_object: 155.7213
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.51s
                      Time elapsed: 00:40:35
                               ETA: 00:41:26

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 40357 steps/s (collection: 2.330s, learning 0.106s)
             Mean action noise std: 2.72
          Mean value_function loss: 252.4247
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5359
                       Mean reward: 798.59
               Mean episode length: 223.59
    Episode_Reward/reaching_object: 1.7589
     Episode_Reward/lifting_object: 152.7080
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.44s
                      Time elapsed: 00:40:37
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 41840 steps/s (collection: 2.251s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 261.4693
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.5423
                       Mean reward: 731.48
               Mean episode length: 207.37
    Episode_Reward/reaching_object: 1.7054
     Episode_Reward/lifting_object: 147.3387
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.35s
                      Time elapsed: 00:40:39
                               ETA: 00:41:21

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 42286 steps/s (collection: 2.227s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 240.0350
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 62.5562
                       Mean reward: 798.42
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.7601
     Episode_Reward/lifting_object: 152.5512
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.32s
                      Time elapsed: 00:40:42
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 41349 steps/s (collection: 2.263s, learning 0.114s)
             Mean action noise std: 2.72
          Mean value_function loss: 242.7792
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5604
                       Mean reward: 786.51
               Mean episode length: 216.96
    Episode_Reward/reaching_object: 1.7249
     Episode_Reward/lifting_object: 150.5482
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.38s
                      Time elapsed: 00:40:44
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 39805 steps/s (collection: 2.347s, learning 0.123s)
             Mean action noise std: 2.72
          Mean value_function loss: 222.1153
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 62.5626
                       Mean reward: 818.49
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.7949
     Episode_Reward/lifting_object: 155.8205
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.47s
                      Time elapsed: 00:40:47
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 39579 steps/s (collection: 2.370s, learning 0.114s)
             Mean action noise std: 2.72
          Mean value_function loss: 209.7446
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.5642
                       Mean reward: 854.45
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.8377
     Episode_Reward/lifting_object: 160.6204
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.48s
                      Time elapsed: 00:40:49
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 39996 steps/s (collection: 2.330s, learning 0.128s)
             Mean action noise std: 2.72
          Mean value_function loss: 230.4215
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.5709
                       Mean reward: 824.69
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.8012
     Episode_Reward/lifting_object: 157.7184
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.46s
                      Time elapsed: 00:40:52
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 40766 steps/s (collection: 2.301s, learning 0.111s)
             Mean action noise std: 2.73
          Mean value_function loss: 227.9583
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.5774
                       Mean reward: 781.75
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 1.8096
     Episode_Reward/lifting_object: 158.5271
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.41s
                      Time elapsed: 00:40:54
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 42147 steps/s (collection: 2.220s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 231.7086
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.5841
                       Mean reward: 811.62
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 1.8534
     Episode_Reward/lifting_object: 162.9074
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.33s
                      Time elapsed: 00:40:56
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 41494 steps/s (collection: 2.270s, learning 0.099s)
             Mean action noise std: 2.73
          Mean value_function loss: 209.8467
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.5962
                       Mean reward: 795.40
               Mean episode length: 219.23
    Episode_Reward/reaching_object: 1.8464
     Episode_Reward/lifting_object: 162.4538
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.37s
                      Time elapsed: 00:40:59
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13650 steps/s (collection: 7.075s, learning 0.126s)
             Mean action noise std: 2.73
          Mean value_function loss: 218.6982
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.6133
                       Mean reward: 809.83
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 1.8451
     Episode_Reward/lifting_object: 163.3623
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.20s
                      Time elapsed: 00:41:06
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13589 steps/s (collection: 7.113s, learning 0.121s)
             Mean action noise std: 2.73
          Mean value_function loss: 225.0544
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 62.6199
                       Mean reward: 804.34
               Mean episode length: 220.64
    Episode_Reward/reaching_object: 1.7736
     Episode_Reward/lifting_object: 155.7804
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.23s
                      Time elapsed: 00:41:13
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 13885 steps/s (collection: 6.942s, learning 0.138s)
             Mean action noise std: 2.73
          Mean value_function loss: 223.5570
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.6214
                       Mean reward: 815.83
               Mean episode length: 224.03
    Episode_Reward/reaching_object: 1.8327
     Episode_Reward/lifting_object: 161.6672
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.08s
                      Time elapsed: 00:41:20
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13727 steps/s (collection: 7.041s, learning 0.121s)
             Mean action noise std: 2.73
          Mean value_function loss: 218.0862
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.6272
                       Mean reward: 787.80
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.7896
     Episode_Reward/lifting_object: 157.5191
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.16s
                      Time elapsed: 00:41:27
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 13436 steps/s (collection: 7.192s, learning 0.124s)
             Mean action noise std: 2.73
          Mean value_function loss: 213.7216
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.6422
                       Mean reward: 761.42
               Mean episode length: 210.71
    Episode_Reward/reaching_object: 1.8020
     Episode_Reward/lifting_object: 158.8694
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.32s
                      Time elapsed: 00:41:35
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 13243 steps/s (collection: 7.299s, learning 0.124s)
             Mean action noise std: 2.74
          Mean value_function loss: 193.4196
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.6654
                       Mean reward: 766.93
               Mean episode length: 208.93
    Episode_Reward/reaching_object: 1.7970
     Episode_Reward/lifting_object: 158.4534
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.42s
                      Time elapsed: 00:41:42
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13263 steps/s (collection: 7.291s, learning 0.121s)
             Mean action noise std: 2.74
          Mean value_function loss: 207.3686
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 62.6842
                       Mean reward: 826.81
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 1.8937
     Episode_Reward/lifting_object: 167.7364
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.41s
                      Time elapsed: 00:41:50
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 13720 steps/s (collection: 7.042s, learning 0.123s)
             Mean action noise std: 2.74
          Mean value_function loss: 226.1401
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.6909
                       Mean reward: 793.48
               Mean episode length: 216.41
    Episode_Reward/reaching_object: 1.7843
     Episode_Reward/lifting_object: 157.3150
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.16s
                      Time elapsed: 00:41:57
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 16047 steps/s (collection: 5.993s, learning 0.133s)
             Mean action noise std: 2.74
          Mean value_function loss: 222.7517
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 62.6915
                       Mean reward: 822.85
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.8154
     Episode_Reward/lifting_object: 160.3779
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.13s
                      Time elapsed: 00:42:03
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 42683 steps/s (collection: 2.191s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 219.0814
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.6927
                       Mean reward: 768.84
               Mean episode length: 212.15
    Episode_Reward/reaching_object: 1.7468
     Episode_Reward/lifting_object: 153.9301
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.30s
                      Time elapsed: 00:42:05
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 42789 steps/s (collection: 2.189s, learning 0.108s)
             Mean action noise std: 2.74
          Mean value_function loss: 233.4333
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.7007
                       Mean reward: 779.72
               Mean episode length: 213.49
    Episode_Reward/reaching_object: 1.8539
     Episode_Reward/lifting_object: 163.8531
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.30s
                      Time elapsed: 00:42:07
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.226s, learning 0.128s)
             Mean action noise std: 2.74
          Mean value_function loss: 212.6487
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.7110
                       Mean reward: 801.91
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 1.7977
     Episode_Reward/lifting_object: 158.5457
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.35s
                      Time elapsed: 00:42:10
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 42474 steps/s (collection: 2.206s, learning 0.108s)
             Mean action noise std: 2.74
          Mean value_function loss: 194.4289
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 62.7222
                       Mean reward: 790.24
               Mean episode length: 218.07
    Episode_Reward/reaching_object: 1.8054
     Episode_Reward/lifting_object: 158.9520
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.31s
                      Time elapsed: 00:42:12
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 41112 steps/s (collection: 2.265s, learning 0.126s)
             Mean action noise std: 2.74
          Mean value_function loss: 200.6791
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 62.7231
                       Mean reward: 812.24
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.7997
     Episode_Reward/lifting_object: 158.1719
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.39s
                      Time elapsed: 00:42:14
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 41520 steps/s (collection: 2.259s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 193.4522
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.7243
                       Mean reward: 780.07
               Mean episode length: 214.85
    Episode_Reward/reaching_object: 1.7684
     Episode_Reward/lifting_object: 155.7863
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.37s
                      Time elapsed: 00:42:17
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 42108 steps/s (collection: 2.228s, learning 0.107s)
             Mean action noise std: 2.74
          Mean value_function loss: 207.6357
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 62.7286
                       Mean reward: 821.19
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.7993
     Episode_Reward/lifting_object: 158.0504
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.33s
                      Time elapsed: 00:42:19
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 43116 steps/s (collection: 2.183s, learning 0.097s)
             Mean action noise std: 2.74
          Mean value_function loss: 225.6332
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.7304
                       Mean reward: 801.47
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 1.8131
     Episode_Reward/lifting_object: 159.1280
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.28s
                      Time elapsed: 00:42:21
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 42762 steps/s (collection: 2.181s, learning 0.118s)
             Mean action noise std: 2.74
          Mean value_function loss: 224.0147
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 62.7322
                       Mean reward: 812.23
               Mean episode length: 223.86
    Episode_Reward/reaching_object: 1.8599
     Episode_Reward/lifting_object: 162.6936
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.30s
                      Time elapsed: 00:42:24
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 41501 steps/s (collection: 2.248s, learning 0.120s)
             Mean action noise std: 2.74
          Mean value_function loss: 235.8563
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.7329
                       Mean reward: 750.61
               Mean episode length: 209.49
    Episode_Reward/reaching_object: 1.8460
     Episode_Reward/lifting_object: 162.0730
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.37s
                      Time elapsed: 00:42:26
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 41435 steps/s (collection: 2.255s, learning 0.118s)
             Mean action noise std: 2.75
          Mean value_function loss: 292.6208
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.7347
                       Mean reward: 781.87
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.7976
     Episode_Reward/lifting_object: 156.4346
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.37s
                      Time elapsed: 00:42:29
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 40718 steps/s (collection: 2.301s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 301.0292
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 62.7402
                       Mean reward: 759.83
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 1.7273
     Episode_Reward/lifting_object: 149.9306
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.41s
                      Time elapsed: 00:42:31
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 42122 steps/s (collection: 2.230s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 306.3487
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.7497
                       Mean reward: 778.73
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.7223
     Episode_Reward/lifting_object: 148.8677
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.33s
                      Time elapsed: 00:42:33
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 42782 steps/s (collection: 2.185s, learning 0.113s)
             Mean action noise std: 2.75
          Mean value_function loss: 308.1429
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.7590
                       Mean reward: 732.79
               Mean episode length: 208.27
    Episode_Reward/reaching_object: 1.6618
     Episode_Reward/lifting_object: 142.6213
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.30s
                      Time elapsed: 00:42:36
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 42852 steps/s (collection: 2.179s, learning 0.115s)
             Mean action noise std: 2.75
          Mean value_function loss: 218.1146
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 62.7612
                       Mean reward: 781.51
               Mean episode length: 216.59
    Episode_Reward/reaching_object: 1.8135
     Episode_Reward/lifting_object: 157.8025
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.29s
                      Time elapsed: 00:42:38
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 42760 steps/s (collection: 2.186s, learning 0.113s)
             Mean action noise std: 2.75
          Mean value_function loss: 249.8263
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.7623
                       Mean reward: 731.71
               Mean episode length: 208.10
    Episode_Reward/reaching_object: 1.7604
     Episode_Reward/lifting_object: 152.1826
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.30s
                      Time elapsed: 00:42:40
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 42182 steps/s (collection: 2.210s, learning 0.120s)
             Mean action noise std: 2.75
          Mean value_function loss: 228.8873
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 62.7628
                       Mean reward: 799.36
               Mean episode length: 222.13
    Episode_Reward/reaching_object: 1.7591
     Episode_Reward/lifting_object: 152.4133
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.33s
                      Time elapsed: 00:42:42
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 43355 steps/s (collection: 2.165s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 241.7253
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.7643
                       Mean reward: 747.94
               Mean episode length: 209.26
    Episode_Reward/reaching_object: 1.7251
     Episode_Reward/lifting_object: 148.8727
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.27s
                      Time elapsed: 00:42:45
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 43654 steps/s (collection: 2.153s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 263.3642
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.7719
                       Mean reward: 757.96
               Mean episode length: 213.94
    Episode_Reward/reaching_object: 1.7303
     Episode_Reward/lifting_object: 149.3198
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.25s
                      Time elapsed: 00:42:47
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 42708 steps/s (collection: 2.194s, learning 0.108s)
             Mean action noise std: 2.75
          Mean value_function loss: 278.9419
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.7835
                       Mean reward: 781.86
               Mean episode length: 219.43
    Episode_Reward/reaching_object: 1.7437
     Episode_Reward/lifting_object: 150.6148
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.30s
                      Time elapsed: 00:42:49
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 41401 steps/s (collection: 2.264s, learning 0.110s)
             Mean action noise std: 2.75
          Mean value_function loss: 233.7255
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.7939
                       Mean reward: 773.01
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.7459
     Episode_Reward/lifting_object: 151.1057
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.37s
                      Time elapsed: 00:42:52
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 41513 steps/s (collection: 2.243s, learning 0.125s)
             Mean action noise std: 2.76
          Mean value_function loss: 205.0107
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 62.7977
                       Mean reward: 772.86
               Mean episode length: 214.33
    Episode_Reward/reaching_object: 1.8520
     Episode_Reward/lifting_object: 162.3852
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.37s
                      Time elapsed: 00:42:54
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 42583 steps/s (collection: 2.206s, learning 0.103s)
             Mean action noise std: 2.76
          Mean value_function loss: 232.6963
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.7988
                       Mean reward: 817.84
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.8281
     Episode_Reward/lifting_object: 159.8674
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.31s
                      Time elapsed: 00:42:56
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 43104 steps/s (collection: 2.180s, learning 0.101s)
             Mean action noise std: 2.76
          Mean value_function loss: 219.3976
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.7993
                       Mean reward: 817.92
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.8337
     Episode_Reward/lifting_object: 160.6400
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.28s
                      Time elapsed: 00:42:59
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 41355 steps/s (collection: 2.273s, learning 0.105s)
             Mean action noise std: 2.76
          Mean value_function loss: 215.4706
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.7997
                       Mean reward: 796.68
               Mean episode length: 220.35
    Episode_Reward/reaching_object: 1.7531
     Episode_Reward/lifting_object: 153.3454
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.38s
                      Time elapsed: 00:43:01
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 41950 steps/s (collection: 2.209s, learning 0.134s)
             Mean action noise std: 2.76
          Mean value_function loss: 207.0492
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.8014
                       Mean reward: 803.69
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.7883
     Episode_Reward/lifting_object: 157.1463
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.34s
                      Time elapsed: 00:43:03
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 41207 steps/s (collection: 2.257s, learning 0.129s)
             Mean action noise std: 2.76
          Mean value_function loss: 200.4001
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.8084
                       Mean reward: 839.17
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 1.8192
     Episode_Reward/lifting_object: 159.6166
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.39s
                      Time elapsed: 00:43:06
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 42454 steps/s (collection: 2.201s, learning 0.115s)
             Mean action noise std: 2.76
          Mean value_function loss: 198.7467
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 62.8277
                       Mean reward: 796.53
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 1.7986
     Episode_Reward/lifting_object: 157.1449
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.32s
                      Time elapsed: 00:43:08
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 42226 steps/s (collection: 2.210s, learning 0.118s)
             Mean action noise std: 2.76
          Mean value_function loss: 180.9298
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.8509
                       Mean reward: 866.15
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 1.8406
     Episode_Reward/lifting_object: 161.7533
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.33s
                      Time elapsed: 00:43:10
                               ETA: 00:40:03

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 41681 steps/s (collection: 2.246s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 200.8406
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.8619
                       Mean reward: 782.62
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.8044
     Episode_Reward/lifting_object: 158.6866
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.36s
                      Time elapsed: 00:43:13
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 41995 steps/s (collection: 2.235s, learning 0.106s)
             Mean action noise std: 2.76
          Mean value_function loss: 173.0696
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.8702
                       Mean reward: 800.33
               Mean episode length: 221.42
    Episode_Reward/reaching_object: 1.8478
     Episode_Reward/lifting_object: 163.0125
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.34s
                      Time elapsed: 00:43:15
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 41618 steps/s (collection: 2.254s, learning 0.108s)
             Mean action noise std: 2.76
          Mean value_function loss: 182.2159
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.8739
                       Mean reward: 818.72
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 1.8725
     Episode_Reward/lifting_object: 165.6212
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.36s
                      Time elapsed: 00:43:17
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 41952 steps/s (collection: 2.245s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 194.8806
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.8803
                       Mean reward: 862.28
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.8681
     Episode_Reward/lifting_object: 165.4749
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.34s
                      Time elapsed: 00:43:20
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 42961 steps/s (collection: 2.173s, learning 0.115s)
             Mean action noise std: 2.77
          Mean value_function loss: 197.0112
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.8941
                       Mean reward: 835.12
               Mean episode length: 228.55
    Episode_Reward/reaching_object: 1.8798
     Episode_Reward/lifting_object: 165.6890
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.29s
                      Time elapsed: 00:43:22
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 42873 steps/s (collection: 2.190s, learning 0.103s)
             Mean action noise std: 2.77
          Mean value_function loss: 192.8780
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.9025
                       Mean reward: 806.21
               Mean episode length: 222.47
    Episode_Reward/reaching_object: 1.8740
     Episode_Reward/lifting_object: 164.7953
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.29s
                      Time elapsed: 00:43:24
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 42397 steps/s (collection: 2.203s, learning 0.116s)
             Mean action noise std: 2.77
          Mean value_function loss: 187.0525
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.9135
                       Mean reward: 843.70
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 1.8090
     Episode_Reward/lifting_object: 158.9602
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.32s
                      Time elapsed: 00:43:27
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 42918 steps/s (collection: 2.177s, learning 0.113s)
             Mean action noise std: 2.77
          Mean value_function loss: 176.1341
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.9227
                       Mean reward: 775.78
               Mean episode length: 212.06
    Episode_Reward/reaching_object: 1.8175
     Episode_Reward/lifting_object: 159.5214
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.29s
                      Time elapsed: 00:43:29
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 41306 steps/s (collection: 2.271s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 162.3926
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.9323
                       Mean reward: 838.16
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.8458
     Episode_Reward/lifting_object: 163.1641
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.38s
                      Time elapsed: 00:43:31
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 41450 steps/s (collection: 2.250s, learning 0.121s)
             Mean action noise std: 2.77
          Mean value_function loss: 156.9171
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.9387
                       Mean reward: 753.78
               Mean episode length: 208.59
    Episode_Reward/reaching_object: 1.8426
     Episode_Reward/lifting_object: 162.7215
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.37s
                      Time elapsed: 00:43:34
                               ETA: 00:39:37

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 41757 steps/s (collection: 2.245s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 159.4277
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.9422
                       Mean reward: 817.67
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.8472
     Episode_Reward/lifting_object: 162.7084
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.35s
                      Time elapsed: 00:43:36
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 42063 steps/s (collection: 2.216s, learning 0.121s)
             Mean action noise std: 2.77
          Mean value_function loss: 179.4163
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.9474
                       Mean reward: 871.32
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 1.8300
     Episode_Reward/lifting_object: 161.4149
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.34s
                      Time elapsed: 00:43:38
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 41304 steps/s (collection: 2.266s, learning 0.114s)
             Mean action noise std: 2.78
          Mean value_function loss: 193.7968
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.9546
                       Mean reward: 814.70
               Mean episode length: 221.51
    Episode_Reward/reaching_object: 1.8474
     Episode_Reward/lifting_object: 162.6832
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.38s
                      Time elapsed: 00:43:41
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 41844 steps/s (collection: 2.230s, learning 0.119s)
             Mean action noise std: 2.78
          Mean value_function loss: 157.3938
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.9623
                       Mean reward: 860.30
               Mean episode length: 235.82
    Episode_Reward/reaching_object: 1.9050
     Episode_Reward/lifting_object: 167.9138
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.35s
                      Time elapsed: 00:43:43
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 42678 steps/s (collection: 2.199s, learning 0.104s)
             Mean action noise std: 2.78
          Mean value_function loss: 178.6005
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9739
                       Mean reward: 878.96
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.8927
     Episode_Reward/lifting_object: 167.7655
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.30s
                      Time elapsed: 00:43:45
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 42515 steps/s (collection: 2.210s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 174.1906
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.9883
                       Mean reward: 832.19
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.9239
     Episode_Reward/lifting_object: 170.8488
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.31s
                      Time elapsed: 00:43:48
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 41897 steps/s (collection: 2.234s, learning 0.112s)
             Mean action noise std: 2.78
          Mean value_function loss: 206.1249
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.9982
                       Mean reward: 796.98
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 1.8717
     Episode_Reward/lifting_object: 165.1793
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.35s
                      Time elapsed: 00:43:50
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 41990 steps/s (collection: 2.231s, learning 0.110s)
             Mean action noise std: 2.78
          Mean value_function loss: 199.3210
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.0070
                       Mean reward: 833.09
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.8723
     Episode_Reward/lifting_object: 165.4061
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.34s
                      Time elapsed: 00:43:52
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 42663 steps/s (collection: 2.204s, learning 0.100s)
             Mean action noise std: 2.79
          Mean value_function loss: 193.9508
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.0264
                       Mean reward: 768.24
               Mean episode length: 210.77
    Episode_Reward/reaching_object: 1.7869
     Episode_Reward/lifting_object: 157.6942
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.30s
                      Time elapsed: 00:43:55
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 42873 steps/s (collection: 2.191s, learning 0.102s)
             Mean action noise std: 2.79
          Mean value_function loss: 154.5136
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.0367
                       Mean reward: 829.31
               Mean episode length: 223.72
    Episode_Reward/reaching_object: 1.8749
     Episode_Reward/lifting_object: 166.5089
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.29s
                      Time elapsed: 00:43:57
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 41641 steps/s (collection: 2.252s, learning 0.109s)
             Mean action noise std: 2.79
          Mean value_function loss: 190.0842
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.0580
                       Mean reward: 864.02
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.8267
     Episode_Reward/lifting_object: 161.7138
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.36s
                      Time elapsed: 00:43:59
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 43238 steps/s (collection: 2.172s, learning 0.101s)
             Mean action noise std: 2.79
          Mean value_function loss: 168.5628
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 63.0868
                       Mean reward: 865.73
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.9131
     Episode_Reward/lifting_object: 169.4440
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.27s
                      Time elapsed: 00:44:02
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 42712 steps/s (collection: 2.165s, learning 0.137s)
             Mean action noise std: 2.80
          Mean value_function loss: 171.7398
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.1137
                       Mean reward: 849.11
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 1.8687
     Episode_Reward/lifting_object: 166.0918
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.30s
                      Time elapsed: 00:44:04
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 41680 steps/s (collection: 2.230s, learning 0.128s)
             Mean action noise std: 2.80
          Mean value_function loss: 177.7555
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.1351
                       Mean reward: 854.83
               Mean episode length: 231.43
    Episode_Reward/reaching_object: 1.8817
     Episode_Reward/lifting_object: 166.8204
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.36s
                      Time elapsed: 00:44:06
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 42796 steps/s (collection: 2.167s, learning 0.130s)
             Mean action noise std: 2.80
          Mean value_function loss: 166.0570
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 63.1524
                       Mean reward: 823.18
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.8871
     Episode_Reward/lifting_object: 166.8380
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.30s
                      Time elapsed: 00:44:09
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 43001 steps/s (collection: 2.169s, learning 0.118s)
             Mean action noise std: 2.80
          Mean value_function loss: 196.4576
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.1725
                       Mean reward: 822.07
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.8244
     Episode_Reward/lifting_object: 160.9354
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.29s
                      Time elapsed: 00:44:11
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 42932 steps/s (collection: 2.180s, learning 0.110s)
             Mean action noise std: 2.80
          Mean value_function loss: 200.3928
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 63.1908
                       Mean reward: 820.20
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.8747
     Episode_Reward/lifting_object: 166.6469
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.29s
                      Time elapsed: 00:44:13
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 43187 steps/s (collection: 2.170s, learning 0.107s)
             Mean action noise std: 2.80
          Mean value_function loss: 193.2784
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 63.1946
                       Mean reward: 831.39
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 1.8403
     Episode_Reward/lifting_object: 162.9504
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.28s
                      Time elapsed: 00:44:16
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 43423 steps/s (collection: 2.156s, learning 0.108s)
             Mean action noise std: 2.81
          Mean value_function loss: 192.3227
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.1977
                       Mean reward: 853.89
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 1.8310
     Episode_Reward/lifting_object: 162.2979
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.26s
                      Time elapsed: 00:44:18
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 42446 steps/s (collection: 2.188s, learning 0.128s)
             Mean action noise std: 2.81
          Mean value_function loss: 198.0769
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.2102
                       Mean reward: 823.78
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 1.8222
     Episode_Reward/lifting_object: 161.1549
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.32s
                      Time elapsed: 00:44:20
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 41402 steps/s (collection: 2.247s, learning 0.127s)
             Mean action noise std: 2.81
          Mean value_function loss: 174.0205
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.2258
                       Mean reward: 856.22
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 1.8212
     Episode_Reward/lifting_object: 160.9415
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.37s
                      Time elapsed: 00:44:22
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 41893 steps/s (collection: 2.241s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 178.7784
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.2332
                       Mean reward: 793.29
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 1.8430
     Episode_Reward/lifting_object: 164.1804
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.35s
                      Time elapsed: 00:44:25
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 40145 steps/s (collection: 2.330s, learning 0.119s)
             Mean action noise std: 2.81
          Mean value_function loss: 226.7466
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.2393
                       Mean reward: 819.96
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.8197
     Episode_Reward/lifting_object: 161.1086
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.45s
                      Time elapsed: 00:44:27
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 42099 steps/s (collection: 2.220s, learning 0.115s)
             Mean action noise std: 2.81
          Mean value_function loss: 206.9996
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.2522
                       Mean reward: 821.43
               Mean episode length: 223.34
    Episode_Reward/reaching_object: 1.8383
     Episode_Reward/lifting_object: 162.5610
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.34s
                      Time elapsed: 00:44:30
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 40637 steps/s (collection: 2.302s, learning 0.118s)
             Mean action noise std: 2.82
          Mean value_function loss: 193.3659
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.2748
                       Mean reward: 839.96
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 1.8436
     Episode_Reward/lifting_object: 162.8588
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.42s
                      Time elapsed: 00:44:32
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 39368 steps/s (collection: 2.360s, learning 0.137s)
             Mean action noise std: 2.82
          Mean value_function loss: 169.5153
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.3018
                       Mean reward: 812.31
               Mean episode length: 221.87
    Episode_Reward/reaching_object: 1.9035
     Episode_Reward/lifting_object: 168.3448
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.50s
                      Time elapsed: 00:44:35
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 42642 steps/s (collection: 2.198s, learning 0.107s)
             Mean action noise std: 2.82
          Mean value_function loss: 203.0590
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.3211
                       Mean reward: 817.61
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 1.8763
     Episode_Reward/lifting_object: 166.5831
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.31s
                      Time elapsed: 00:44:37
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.189s, learning 0.106s)
             Mean action noise std: 2.82
          Mean value_function loss: 194.7969
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 63.3342
                       Mean reward: 820.97
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.9029
     Episode_Reward/lifting_object: 167.7975
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.29s
                      Time elapsed: 00:44:39
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 40306 steps/s (collection: 2.319s, learning 0.120s)
             Mean action noise std: 2.82
          Mean value_function loss: 187.2144
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.3479
                       Mean reward: 811.01
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 1.8678
     Episode_Reward/lifting_object: 164.9864
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.44s
                      Time elapsed: 00:44:42
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 40937 steps/s (collection: 2.278s, learning 0.124s)
             Mean action noise std: 2.83
          Mean value_function loss: 148.8871
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.3590
                       Mean reward: 853.46
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 1.8721
     Episode_Reward/lifting_object: 165.8964
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.40s
                      Time elapsed: 00:44:44
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 41819 steps/s (collection: 2.243s, learning 0.108s)
             Mean action noise std: 2.83
          Mean value_function loss: 174.3429
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 63.3666
                       Mean reward: 846.53
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.8496
     Episode_Reward/lifting_object: 163.9899
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.35s
                      Time elapsed: 00:44:46
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 41087 steps/s (collection: 2.266s, learning 0.126s)
             Mean action noise std: 2.83
          Mean value_function loss: 172.1932
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.3740
                       Mean reward: 857.66
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.8469
     Episode_Reward/lifting_object: 163.7103
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.39s
                      Time elapsed: 00:44:49
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 41256 steps/s (collection: 2.268s, learning 0.115s)
             Mean action noise std: 2.83
          Mean value_function loss: 165.2378
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.3858
                       Mean reward: 832.88
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 1.8795
     Episode_Reward/lifting_object: 166.5863
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.38s
                      Time elapsed: 00:44:51
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 42745 steps/s (collection: 2.196s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 160.8182
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.3965
                       Mean reward: 822.98
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.8721
     Episode_Reward/lifting_object: 166.5766
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.30s
                      Time elapsed: 00:44:53
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 42825 steps/s (collection: 2.192s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 162.1573
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.4099
                       Mean reward: 854.01
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 1.9114
     Episode_Reward/lifting_object: 170.3395
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.30s
                      Time elapsed: 00:44:56
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 41740 steps/s (collection: 2.249s, learning 0.106s)
             Mean action noise std: 2.83
          Mean value_function loss: 161.2647
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.4194
                       Mean reward: 876.49
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.9184
     Episode_Reward/lifting_object: 171.1448
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.36s
                      Time elapsed: 00:44:58
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 42424 steps/s (collection: 2.218s, learning 0.100s)
             Mean action noise std: 2.83
          Mean value_function loss: 196.7353
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.4285
                       Mean reward: 827.35
               Mean episode length: 223.59
    Episode_Reward/reaching_object: 1.8547
     Episode_Reward/lifting_object: 165.1381
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.32s
                      Time elapsed: 00:45:00
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 43122 steps/s (collection: 2.179s, learning 0.101s)
             Mean action noise std: 2.84
          Mean value_function loss: 173.5387
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.4390
                       Mean reward: 861.15
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.8809
     Episode_Reward/lifting_object: 167.9991
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.28s
                      Time elapsed: 00:45:03
                               ETA: 00:37:57

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 42082 steps/s (collection: 2.223s, learning 0.113s)
             Mean action noise std: 2.84
          Mean value_function loss: 175.1577
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.4537
                       Mean reward: 751.80
               Mean episode length: 207.38
    Episode_Reward/reaching_object: 1.8272
     Episode_Reward/lifting_object: 162.9410
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.34s
                      Time elapsed: 00:45:05
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 40950 steps/s (collection: 2.280s, learning 0.121s)
             Mean action noise std: 2.84
          Mean value_function loss: 164.5908
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 63.4672
                       Mean reward: 836.90
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.8688
     Episode_Reward/lifting_object: 166.2245
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.40s
                      Time elapsed: 00:45:07
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 43110 steps/s (collection: 2.171s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 140.6297
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 63.4974
                       Mean reward: 843.94
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.8993
     Episode_Reward/lifting_object: 168.9581
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.28s
                      Time elapsed: 00:45:10
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 41176 steps/s (collection: 2.258s, learning 0.129s)
             Mean action noise std: 2.85
          Mean value_function loss: 144.9139
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.5245
                       Mean reward: 851.84
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.9035
     Episode_Reward/lifting_object: 169.8364
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.39s
                      Time elapsed: 00:45:12
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 40800 steps/s (collection: 2.286s, learning 0.123s)
             Mean action noise std: 2.85
          Mean value_function loss: 160.3158
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.5402
                       Mean reward: 838.41
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 1.8703
     Episode_Reward/lifting_object: 166.3441
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.41s
                      Time elapsed: 00:45:14
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 41329 steps/s (collection: 2.266s, learning 0.112s)
             Mean action noise std: 2.85
          Mean value_function loss: 156.7916
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.5627
                       Mean reward: 850.53
               Mean episode length: 229.21
    Episode_Reward/reaching_object: 1.8773
     Episode_Reward/lifting_object: 166.4515
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.38s
                      Time elapsed: 00:45:17
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 41555 steps/s (collection: 2.249s, learning 0.117s)
             Mean action noise std: 2.85
          Mean value_function loss: 180.9036
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 63.5844
                       Mean reward: 795.66
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 1.8644
     Episode_Reward/lifting_object: 165.7879
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.37s
                      Time elapsed: 00:45:19
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 41992 steps/s (collection: 2.231s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 173.8380
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.6131
                       Mean reward: 859.73
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.9075
     Episode_Reward/lifting_object: 170.0169
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.34s
                      Time elapsed: 00:45:22
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 43434 steps/s (collection: 2.158s, learning 0.106s)
             Mean action noise std: 2.86
          Mean value_function loss: 180.1399
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.6364
                       Mean reward: 872.46
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.8929
     Episode_Reward/lifting_object: 168.2748
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.26s
                      Time elapsed: 00:45:24
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 42575 steps/s (collection: 2.210s, learning 0.099s)
             Mean action noise std: 2.86
          Mean value_function loss: 170.6633
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 63.6481
                       Mean reward: 783.61
               Mean episode length: 214.29
    Episode_Reward/reaching_object: 1.8843
     Episode_Reward/lifting_object: 167.3795
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.31s
                      Time elapsed: 00:45:26
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 42703 steps/s (collection: 2.199s, learning 0.103s)
             Mean action noise std: 2.86
          Mean value_function loss: 162.6965
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.6628
                       Mean reward: 882.22
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.8798
     Episode_Reward/lifting_object: 167.2202
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.30s
                      Time elapsed: 00:45:28
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 42795 steps/s (collection: 2.190s, learning 0.107s)
             Mean action noise std: 2.86
          Mean value_function loss: 163.7935
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.6758
                       Mean reward: 876.84
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.8869
     Episode_Reward/lifting_object: 167.6808
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.30s
                      Time elapsed: 00:45:31
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 42193 steps/s (collection: 2.214s, learning 0.116s)
             Mean action noise std: 2.87
          Mean value_function loss: 149.6384
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.6828
                       Mean reward: 845.59
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 1.9068
     Episode_Reward/lifting_object: 169.2572
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.33s
                      Time elapsed: 00:45:33
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 42629 steps/s (collection: 2.189s, learning 0.117s)
             Mean action noise std: 2.87
          Mean value_function loss: 185.0097
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.6885
                       Mean reward: 781.45
               Mean episode length: 213.19
    Episode_Reward/reaching_object: 1.7768
     Episode_Reward/lifting_object: 157.6227
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.31s
                      Time elapsed: 00:45:35
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 41278 steps/s (collection: 2.252s, learning 0.129s)
             Mean action noise std: 2.87
          Mean value_function loss: 188.4795
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 63.6999
                       Mean reward: 837.17
               Mean episode length: 225.47
    Episode_Reward/reaching_object: 1.8421
     Episode_Reward/lifting_object: 164.1080
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.38s
                      Time elapsed: 00:45:38
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 40358 steps/s (collection: 2.312s, learning 0.124s)
             Mean action noise std: 2.87
          Mean value_function loss: 181.7425
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.7161
                       Mean reward: 835.68
               Mean episode length: 223.39
    Episode_Reward/reaching_object: 1.8207
     Episode_Reward/lifting_object: 162.1759
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.44s
                      Time elapsed: 00:45:40
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 41544 steps/s (collection: 2.253s, learning 0.113s)
             Mean action noise std: 2.87
          Mean value_function loss: 200.4213
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.7295
                       Mean reward: 818.89
               Mean episode length: 221.61
    Episode_Reward/reaching_object: 1.8515
     Episode_Reward/lifting_object: 164.4926
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.37s
                      Time elapsed: 00:45:43
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 41321 steps/s (collection: 2.266s, learning 0.113s)
             Mean action noise std: 2.87
          Mean value_function loss: 206.3383
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 63.7461
                       Mean reward: 833.79
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.8537
     Episode_Reward/lifting_object: 165.0305
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.38s
                      Time elapsed: 00:45:45
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 41958 steps/s (collection: 2.231s, learning 0.112s)
             Mean action noise std: 2.88
          Mean value_function loss: 187.8891
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.7627
                       Mean reward: 867.93
               Mean episode length: 231.17
    Episode_Reward/reaching_object: 1.8999
     Episode_Reward/lifting_object: 169.6226
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.34s
                      Time elapsed: 00:45:47
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 42666 steps/s (collection: 2.194s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 197.8124
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.7738
                       Mean reward: 794.28
               Mean episode length: 215.54
    Episode_Reward/reaching_object: 1.8347
     Episode_Reward/lifting_object: 163.3695
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.30s
                      Time elapsed: 00:45:50
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 42396 steps/s (collection: 2.210s, learning 0.109s)
             Mean action noise std: 2.88
          Mean value_function loss: 173.8203
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.7887
                       Mean reward: 855.55
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.8729
     Episode_Reward/lifting_object: 166.7375
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.32s
                      Time elapsed: 00:45:52
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 40229 steps/s (collection: 2.337s, learning 0.107s)
             Mean action noise std: 2.88
          Mean value_function loss: 171.1106
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.8008
                       Mean reward: 791.77
               Mean episode length: 213.31
    Episode_Reward/reaching_object: 1.8528
     Episode_Reward/lifting_object: 164.7196
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.44s
                      Time elapsed: 00:45:54
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 42164 steps/s (collection: 2.232s, learning 0.100s)
             Mean action noise std: 2.88
          Mean value_function loss: 179.9153
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.8197
                       Mean reward: 816.28
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.8471
     Episode_Reward/lifting_object: 164.3261
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.33s
                      Time elapsed: 00:45:57
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 42462 steps/s (collection: 2.208s, learning 0.107s)
             Mean action noise std: 2.89
          Mean value_function loss: 146.1856
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.8359
                       Mean reward: 832.44
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 1.8908
     Episode_Reward/lifting_object: 167.3928
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.32s
                      Time elapsed: 00:45:59
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 42065 steps/s (collection: 2.231s, learning 0.106s)
             Mean action noise std: 2.89
          Mean value_function loss: 132.0934
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 63.8476
                       Mean reward: 865.15
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.9229
     Episode_Reward/lifting_object: 171.0717
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.34s
                      Time elapsed: 00:46:01
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 42451 steps/s (collection: 2.213s, learning 0.103s)
             Mean action noise std: 2.89
          Mean value_function loss: 163.5761
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.8671
                       Mean reward: 809.44
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 1.8791
     Episode_Reward/lifting_object: 166.3589
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.32s
                      Time elapsed: 00:46:04
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 42108 steps/s (collection: 2.230s, learning 0.105s)
             Mean action noise std: 2.89
          Mean value_function loss: 136.5013
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 63.8865
                       Mean reward: 862.44
               Mean episode length: 230.73
    Episode_Reward/reaching_object: 1.8720
     Episode_Reward/lifting_object: 165.7790
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.33s
                      Time elapsed: 00:46:06
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 42525 steps/s (collection: 2.210s, learning 0.102s)
             Mean action noise std: 2.89
          Mean value_function loss: 166.8664
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.9018
                       Mean reward: 853.49
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 1.9053
     Episode_Reward/lifting_object: 168.4504
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.31s
                      Time elapsed: 00:46:08
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 41599 steps/s (collection: 2.240s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 160.1180
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 63.9153
                       Mean reward: 821.42
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.9067
     Episode_Reward/lifting_object: 168.4568
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.36s
                      Time elapsed: 00:46:11
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 38967 steps/s (collection: 2.388s, learning 0.135s)
             Mean action noise std: 2.90
          Mean value_function loss: 151.0851
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.9433
                       Mean reward: 866.77
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.8994
     Episode_Reward/lifting_object: 167.9040
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.52s
                      Time elapsed: 00:46:13
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 41758 steps/s (collection: 2.238s, learning 0.116s)
             Mean action noise std: 2.90
          Mean value_function loss: 138.8928
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.9630
                       Mean reward: 867.27
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.9120
     Episode_Reward/lifting_object: 168.4903
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.35s
                      Time elapsed: 00:46:15
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 42538 steps/s (collection: 2.207s, learning 0.104s)
             Mean action noise std: 2.90
          Mean value_function loss: 120.5466
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9840
                       Mean reward: 871.43
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.9691
     Episode_Reward/lifting_object: 174.1839
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.31s
                      Time elapsed: 00:46:18
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 41968 steps/s (collection: 2.226s, learning 0.116s)
             Mean action noise std: 2.91
          Mean value_function loss: 134.5842
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.0015
                       Mean reward: 863.30
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.9430
     Episode_Reward/lifting_object: 170.8776
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.34s
                      Time elapsed: 00:46:20
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 42572 steps/s (collection: 2.212s, learning 0.097s)
             Mean action noise std: 2.91
          Mean value_function loss: 119.8716
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.0154
                       Mean reward: 846.15
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.9368
     Episode_Reward/lifting_object: 170.5978
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.31s
                      Time elapsed: 00:46:22
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 42714 steps/s (collection: 2.197s, learning 0.104s)
             Mean action noise std: 2.91
          Mean value_function loss: 140.2432
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 64.0319
                       Mean reward: 850.49
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.9244
     Episode_Reward/lifting_object: 169.6768
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.30s
                      Time elapsed: 00:46:25
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 42586 steps/s (collection: 2.194s, learning 0.115s)
             Mean action noise std: 2.91
          Mean value_function loss: 130.1613
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 64.0523
                       Mean reward: 837.21
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 170.8716
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.31s
                      Time elapsed: 00:46:27
                               ETA: 00:36:23

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 42207 steps/s (collection: 2.224s, learning 0.105s)
             Mean action noise std: 2.92
          Mean value_function loss: 143.1395
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 64.0713
                       Mean reward: 841.97
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.9134
     Episode_Reward/lifting_object: 168.7205
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.33s
                      Time elapsed: 00:46:29
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 41101 steps/s (collection: 2.285s, learning 0.107s)
             Mean action noise std: 2.92
          Mean value_function loss: 140.3538
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.0932
                       Mean reward: 858.11
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 1.9459
     Episode_Reward/lifting_object: 171.6197
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.39s
                      Time elapsed: 00:46:32
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 40122 steps/s (collection: 2.330s, learning 0.121s)
             Mean action noise std: 2.92
          Mean value_function loss: 132.2069
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.1137
                       Mean reward: 846.90
               Mean episode length: 230.80
    Episode_Reward/reaching_object: 1.9347
     Episode_Reward/lifting_object: 169.9636
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.45s
                      Time elapsed: 00:46:34
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 40604 steps/s (collection: 2.306s, learning 0.115s)
             Mean action noise std: 2.92
          Mean value_function loss: 173.1559
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.1263
                       Mean reward: 871.02
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 1.9378
     Episode_Reward/lifting_object: 170.8532
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.42s
                      Time elapsed: 00:46:37
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 41609 steps/s (collection: 2.227s, learning 0.136s)
             Mean action noise std: 2.92
          Mean value_function loss: 142.2639
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.1451
                       Mean reward: 846.82
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 1.9551
     Episode_Reward/lifting_object: 172.9280
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.36s
                      Time elapsed: 00:46:39
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 41760 steps/s (collection: 2.237s, learning 0.117s)
             Mean action noise std: 2.93
          Mean value_function loss: 161.5246
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.1653
                       Mean reward: 823.06
               Mean episode length: 221.88
    Episode_Reward/reaching_object: 1.9221
     Episode_Reward/lifting_object: 169.7560
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.35s
                      Time elapsed: 00:46:41
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 42132 steps/s (collection: 2.221s, learning 0.112s)
             Mean action noise std: 2.93
          Mean value_function loss: 143.4024
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 64.1961
                       Mean reward: 858.97
               Mean episode length: 231.01
    Episode_Reward/reaching_object: 1.8977
     Episode_Reward/lifting_object: 167.8497
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.33s
                      Time elapsed: 00:46:44
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 40934 steps/s (collection: 2.284s, learning 0.117s)
             Mean action noise std: 2.93
          Mean value_function loss: 143.4924
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 64.2116
                       Mean reward: 852.69
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 1.8772
     Episode_Reward/lifting_object: 165.7899
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.40s
                      Time elapsed: 00:46:46
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 41078 steps/s (collection: 2.275s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 160.3941
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 64.2384
                       Mean reward: 885.82
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.9201
     Episode_Reward/lifting_object: 170.1641
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.39s
                      Time elapsed: 00:46:48
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 41782 steps/s (collection: 2.217s, learning 0.136s)
             Mean action noise std: 2.94
          Mean value_function loss: 143.1450
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.2756
                       Mean reward: 866.47
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.8544
     Episode_Reward/lifting_object: 164.6446
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.35s
                      Time elapsed: 00:46:51
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 41212 steps/s (collection: 2.274s, learning 0.111s)
             Mean action noise std: 2.94
          Mean value_function loss: 138.1398
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.2960
                       Mean reward: 850.69
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.8885
     Episode_Reward/lifting_object: 167.1234
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.39s
                      Time elapsed: 00:46:53
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 41695 steps/s (collection: 2.252s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 97.7543
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.3134
                       Mean reward: 894.72
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 175.4395
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.36s
                      Time elapsed: 00:46:56
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 41285 steps/s (collection: 2.270s, learning 0.111s)
             Mean action noise std: 2.95
          Mean value_function loss: 144.7361
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.3236
                       Mean reward: 881.82
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.9080
     Episode_Reward/lifting_object: 169.2375
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.38s
                      Time elapsed: 00:46:58
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 42640 steps/s (collection: 2.207s, learning 0.098s)
             Mean action noise std: 2.95
          Mean value_function loss: 161.0491
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.3429
                       Mean reward: 872.54
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 1.9102
     Episode_Reward/lifting_object: 169.1740
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.31s
                      Time elapsed: 00:47:00
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 43202 steps/s (collection: 2.176s, learning 0.099s)
             Mean action noise std: 2.95
          Mean value_function loss: 131.4173
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.3660
                       Mean reward: 871.34
               Mean episode length: 236.43
    Episode_Reward/reaching_object: 1.9629
     Episode_Reward/lifting_object: 173.2551
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.28s
                      Time elapsed: 00:47:03
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 41403 steps/s (collection: 2.255s, learning 0.119s)
             Mean action noise std: 2.95
          Mean value_function loss: 131.6441
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 64.3786
                       Mean reward: 835.57
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.9442
     Episode_Reward/lifting_object: 171.8749
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.37s
                      Time elapsed: 00:47:05
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 40924 steps/s (collection: 2.257s, learning 0.146s)
             Mean action noise std: 2.95
          Mean value_function loss: 116.4418
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 64.3967
                       Mean reward: 897.73
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.9483
     Episode_Reward/lifting_object: 172.2124
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.40s
                      Time elapsed: 00:47:07
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 41000 steps/s (collection: 2.262s, learning 0.136s)
             Mean action noise std: 2.96
          Mean value_function loss: 129.8658
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.4133
                       Mean reward: 862.38
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.9362
     Episode_Reward/lifting_object: 171.4097
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.40s
                      Time elapsed: 00:47:10
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 41948 steps/s (collection: 2.208s, learning 0.136s)
             Mean action noise std: 2.96
          Mean value_function loss: 137.4419
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.4247
                       Mean reward: 876.10
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.9297
     Episode_Reward/lifting_object: 170.5772
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.34s
                      Time elapsed: 00:47:12
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 41367 steps/s (collection: 2.261s, learning 0.115s)
             Mean action noise std: 2.96
          Mean value_function loss: 121.4580
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 64.4357
                       Mean reward: 873.53
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.8891
     Episode_Reward/lifting_object: 166.5349
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.38s
                      Time elapsed: 00:47:14
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 41112 steps/s (collection: 2.259s, learning 0.133s)
             Mean action noise std: 2.96
          Mean value_function loss: 129.1368
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.4515
                       Mean reward: 848.41
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.9307
     Episode_Reward/lifting_object: 170.8748
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.39s
                      Time elapsed: 00:47:17
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 42691 steps/s (collection: 2.193s, learning 0.110s)
             Mean action noise std: 2.96
          Mean value_function loss: 167.3935
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 64.4669
                       Mean reward: 861.38
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 1.9304
     Episode_Reward/lifting_object: 170.5284
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.30s
                      Time elapsed: 00:47:19
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 42886 steps/s (collection: 2.195s, learning 0.098s)
             Mean action noise std: 2.97
          Mean value_function loss: 144.4882
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 64.4874
                       Mean reward: 892.70
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 1.9454
     Episode_Reward/lifting_object: 171.4448
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.29s
                      Time elapsed: 00:47:21
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 42424 steps/s (collection: 2.213s, learning 0.105s)
             Mean action noise std: 2.97
          Mean value_function loss: 141.9161
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 64.5054
                       Mean reward: 826.76
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.9090
     Episode_Reward/lifting_object: 169.0112
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.32s
                      Time elapsed: 00:47:24
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 41855 steps/s (collection: 2.213s, learning 0.136s)
             Mean action noise std: 2.97
          Mean value_function loss: 127.5178
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 64.5200
                       Mean reward: 879.16
               Mean episode length: 235.47
    Episode_Reward/reaching_object: 1.9655
     Episode_Reward/lifting_object: 174.2710
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.35s
                      Time elapsed: 00:47:26
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 41407 steps/s (collection: 2.242s, learning 0.132s)
             Mean action noise std: 2.97
          Mean value_function loss: 131.6024
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.5381
                       Mean reward: 885.02
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.9207
     Episode_Reward/lifting_object: 169.9956
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.37s
                      Time elapsed: 00:47:28
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 42798 steps/s (collection: 2.190s, learning 0.107s)
             Mean action noise std: 2.97
          Mean value_function loss: 134.8576
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.5453
                       Mean reward: 866.21
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.9358
     Episode_Reward/lifting_object: 171.1296
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.30s
                      Time elapsed: 00:47:31
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 41133 steps/s (collection: 2.271s, learning 0.119s)
             Mean action noise std: 2.98
          Mean value_function loss: 111.3992
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 64.5595
                       Mean reward: 891.89
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.9410
     Episode_Reward/lifting_object: 172.1413
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.39s
                      Time elapsed: 00:47:33
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 41604 steps/s (collection: 2.237s, learning 0.126s)
             Mean action noise std: 2.98
          Mean value_function loss: 133.5599
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.5874
                       Mean reward: 831.81
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.9135
     Episode_Reward/lifting_object: 169.4206
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.36s
                      Time elapsed: 00:47:36
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 40317 steps/s (collection: 2.313s, learning 0.126s)
             Mean action noise std: 2.98
          Mean value_function loss: 109.4015
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.6052
                       Mean reward: 882.06
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.9736
     Episode_Reward/lifting_object: 174.8869
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.44s
                      Time elapsed: 00:47:38
                               ETA: 00:35:06

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 40444 steps/s (collection: 2.307s, learning 0.124s)
             Mean action noise std: 2.98
          Mean value_function loss: 126.3607
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 64.6267
                       Mean reward: 849.47
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.9528
     Episode_Reward/lifting_object: 172.8109
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.43s
                      Time elapsed: 00:47:40
                               ETA: 00:35:04

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 40695 steps/s (collection: 2.314s, learning 0.102s)
             Mean action noise std: 2.99
          Mean value_function loss: 126.5354
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.6513
                       Mean reward: 821.21
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 1.9085
     Episode_Reward/lifting_object: 168.2162
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.42s
                      Time elapsed: 00:47:43
                               ETA: 00:35:01

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 42350 steps/s (collection: 2.214s, learning 0.108s)
             Mean action noise std: 2.99
          Mean value_function loss: 108.3144
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.6618
                       Mean reward: 915.19
               Mean episode length: 242.29
    Episode_Reward/reaching_object: 1.9999
     Episode_Reward/lifting_object: 177.5411
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.32s
                      Time elapsed: 00:47:45
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 41839 steps/s (collection: 2.220s, learning 0.129s)
             Mean action noise std: 2.99
          Mean value_function loss: 111.8674
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.6724
                       Mean reward: 869.21
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.9732
     Episode_Reward/lifting_object: 174.8956
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.35s
                      Time elapsed: 00:47:47
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 41813 steps/s (collection: 2.245s, learning 0.106s)
             Mean action noise std: 2.99
          Mean value_function loss: 132.0123
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.6819
                       Mean reward: 880.95
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 1.9442
     Episode_Reward/lifting_object: 172.5935
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.35s
                      Time elapsed: 00:47:50
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 41612 steps/s (collection: 2.248s, learning 0.115s)
             Mean action noise std: 2.99
          Mean value_function loss: 135.4741
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.6964
                       Mean reward: 802.34
               Mean episode length: 217.22
    Episode_Reward/reaching_object: 1.8980
     Episode_Reward/lifting_object: 168.1457
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.36s
                      Time elapsed: 00:47:52
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 40872 steps/s (collection: 2.289s, learning 0.116s)
             Mean action noise std: 2.99
          Mean value_function loss: 116.5580
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.7101
                       Mean reward: 867.32
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 1.9453
     Episode_Reward/lifting_object: 172.3206
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.41s
                      Time elapsed: 00:47:55
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 41680 steps/s (collection: 2.251s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 122.6825
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 64.7198
                       Mean reward: 900.10
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.9516
     Episode_Reward/lifting_object: 173.7923
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.36s
                      Time elapsed: 00:47:57
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 43063 steps/s (collection: 2.181s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 109.1490
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.7388
                       Mean reward: 894.58
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 1.9731
     Episode_Reward/lifting_object: 175.3478
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.28s
                      Time elapsed: 00:47:59
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 42740 steps/s (collection: 2.192s, learning 0.108s)
             Mean action noise std: 3.00
          Mean value_function loss: 125.6692
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.7554
                       Mean reward: 886.76
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 1.9546
     Episode_Reward/lifting_object: 173.6611
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.30s
                      Time elapsed: 00:48:02
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 40950 steps/s (collection: 2.285s, learning 0.115s)
             Mean action noise std: 3.00
          Mean value_function loss: 129.2189
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.7704
                       Mean reward: 879.68
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.8881
     Episode_Reward/lifting_object: 167.4631
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.40s
                      Time elapsed: 00:48:04
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 41036 steps/s (collection: 2.295s, learning 0.101s)
             Mean action noise std: 3.00
          Mean value_function loss: 107.4902
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.7935
                       Mean reward: 892.13
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 1.9674
     Episode_Reward/lifting_object: 174.7975
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.40s
                      Time elapsed: 00:48:06
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 41221 steps/s (collection: 2.256s, learning 0.129s)
             Mean action noise std: 3.01
          Mean value_function loss: 109.8857
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.8157
                       Mean reward: 891.42
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.9499
     Episode_Reward/lifting_object: 173.5003
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.38s
                      Time elapsed: 00:48:09
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 41287 steps/s (collection: 2.245s, learning 0.136s)
             Mean action noise std: 3.01
          Mean value_function loss: 125.1902
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.8336
                       Mean reward: 854.83
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.9852
     Episode_Reward/lifting_object: 176.5335
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.38s
                      Time elapsed: 00:48:11
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 42141 steps/s (collection: 2.222s, learning 0.111s)
             Mean action noise std: 3.01
          Mean value_function loss: 134.4500
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 64.8541
                       Mean reward: 838.91
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.9250
     Episode_Reward/lifting_object: 170.7101
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.33s
                      Time elapsed: 00:48:13
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 41021 steps/s (collection: 2.294s, learning 0.102s)
             Mean action noise std: 3.01
          Mean value_function loss: 135.0808
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 64.8767
                       Mean reward: 854.80
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 1.9128
     Episode_Reward/lifting_object: 169.7779
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.40s
                      Time elapsed: 00:48:16
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 41653 steps/s (collection: 2.235s, learning 0.125s)
             Mean action noise std: 3.02
          Mean value_function loss: 129.7172
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.8934
                       Mean reward: 808.00
               Mean episode length: 219.13
    Episode_Reward/reaching_object: 1.9146
     Episode_Reward/lifting_object: 168.9620
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.36s
                      Time elapsed: 00:48:18
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 41820 steps/s (collection: 2.237s, learning 0.114s)
             Mean action noise std: 3.02
          Mean value_function loss: 128.3449
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.9068
                       Mean reward: 907.68
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 1.9985
     Episode_Reward/lifting_object: 177.3064
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.35s
                      Time elapsed: 00:48:21
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 41841 steps/s (collection: 2.241s, learning 0.108s)
             Mean action noise std: 3.02
          Mean value_function loss: 148.3481
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 64.9148
                       Mean reward: 862.93
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.9339
     Episode_Reward/lifting_object: 170.8926
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.35s
                      Time elapsed: 00:48:23
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 41504 steps/s (collection: 2.250s, learning 0.119s)
             Mean action noise std: 3.02
          Mean value_function loss: 110.2979
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.9286
                       Mean reward: 884.92
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.9909
     Episode_Reward/lifting_object: 176.2803
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.37s
                      Time elapsed: 00:48:25
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 39403 steps/s (collection: 2.368s, learning 0.127s)
             Mean action noise std: 3.02
          Mean value_function loss: 120.4281
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.9373
                       Mean reward: 879.16
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 1.9441
     Episode_Reward/lifting_object: 172.2277
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.49s
                      Time elapsed: 00:48:28
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 41865 steps/s (collection: 2.244s, learning 0.105s)
             Mean action noise std: 3.02
          Mean value_function loss: 132.6321
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.9525
                       Mean reward: 882.50
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 173.3731
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.35s
                      Time elapsed: 00:48:30
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 42123 steps/s (collection: 2.225s, learning 0.109s)
             Mean action noise std: 3.03
          Mean value_function loss: 128.2193
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 64.9781
                       Mean reward: 891.13
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.9579
     Episode_Reward/lifting_object: 172.9765
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.33s
                      Time elapsed: 00:48:32
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 41464 steps/s (collection: 2.260s, learning 0.111s)
             Mean action noise std: 3.03
          Mean value_function loss: 128.4691
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.9958
                       Mean reward: 873.65
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 173.0097
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.37s
                      Time elapsed: 00:48:35
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 41831 steps/s (collection: 2.252s, learning 0.098s)
             Mean action noise std: 3.03
          Mean value_function loss: 135.0396
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.0118
                       Mean reward: 833.44
               Mean episode length: 224.14
    Episode_Reward/reaching_object: 1.9199
     Episode_Reward/lifting_object: 169.5930
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.35s
                      Time elapsed: 00:48:37
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 42488 steps/s (collection: 2.192s, learning 0.122s)
             Mean action noise std: 3.03
          Mean value_function loss: 128.9381
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 65.0321
                       Mean reward: 873.97
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.9537
     Episode_Reward/lifting_object: 173.1013
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.31s
                      Time elapsed: 00:48:39
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 43236 steps/s (collection: 2.173s, learning 0.101s)
             Mean action noise std: 3.03
          Mean value_function loss: 161.8778
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.0532
                       Mean reward: 839.76
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.9230
     Episode_Reward/lifting_object: 169.9352
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.27s
                      Time elapsed: 00:48:42
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 41904 steps/s (collection: 2.221s, learning 0.125s)
             Mean action noise std: 3.04
          Mean value_function loss: 107.5626
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 65.0746
                       Mean reward: 889.31
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 2.0015
     Episode_Reward/lifting_object: 177.3393
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.35s
                      Time elapsed: 00:48:44
                               ETA: 00:33:54

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 42102 steps/s (collection: 2.223s, learning 0.112s)
             Mean action noise std: 3.04
          Mean value_function loss: 138.7496
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.0866
                       Mean reward: 879.93
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 1.9704
     Episode_Reward/lifting_object: 174.9049
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.33s
                      Time elapsed: 00:48:46
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 42002 steps/s (collection: 2.235s, learning 0.106s)
             Mean action noise std: 3.04
          Mean value_function loss: 114.6022
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.0984
                       Mean reward: 870.46
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 1.9543
     Episode_Reward/lifting_object: 172.8950
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.34s
                      Time elapsed: 00:48:49
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 43088 steps/s (collection: 2.182s, learning 0.099s)
             Mean action noise std: 3.04
          Mean value_function loss: 111.7349
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 65.1232
                       Mean reward: 871.64
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 1.9821
     Episode_Reward/lifting_object: 175.7692
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.28s
                      Time elapsed: 00:48:51
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 41539 steps/s (collection: 2.262s, learning 0.104s)
             Mean action noise std: 3.05
          Mean value_function loss: 104.3471
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.1461
                       Mean reward: 878.94
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.9416
     Episode_Reward/lifting_object: 171.8890
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.37s
                      Time elapsed: 00:48:53
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 41645 steps/s (collection: 2.259s, learning 0.102s)
             Mean action noise std: 3.05
          Mean value_function loss: 138.1152
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.1558
                       Mean reward: 854.40
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9400
     Episode_Reward/lifting_object: 172.0720
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.36s
                      Time elapsed: 00:48:56
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 41047 steps/s (collection: 2.279s, learning 0.116s)
             Mean action noise std: 3.05
          Mean value_function loss: 106.1092
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.1626
                       Mean reward: 873.88
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.9757
     Episode_Reward/lifting_object: 175.2726
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.39s
                      Time elapsed: 00:48:58
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 41720 steps/s (collection: 2.235s, learning 0.122s)
             Mean action noise std: 3.05
          Mean value_function loss: 100.9326
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.1739
                       Mean reward: 866.94
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 1.9682
     Episode_Reward/lifting_object: 174.3447
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.36s
                      Time elapsed: 00:49:01
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 41110 steps/s (collection: 2.289s, learning 0.102s)
             Mean action noise std: 3.05
          Mean value_function loss: 110.8776
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.1886
                       Mean reward: 853.78
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.9598
     Episode_Reward/lifting_object: 174.4374
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.39s
                      Time elapsed: 00:49:03
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 42252 steps/s (collection: 2.223s, learning 0.104s)
             Mean action noise std: 3.05
          Mean value_function loss: 128.2197
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 65.2036
                       Mean reward: 906.92
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 1.9409
     Episode_Reward/lifting_object: 172.5265
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.33s
                      Time elapsed: 00:49:05
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 41183 steps/s (collection: 2.278s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 122.3936
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 65.2200
                       Mean reward: 827.40
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.9319
     Episode_Reward/lifting_object: 171.5764
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.39s
                      Time elapsed: 00:49:08
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 41635 steps/s (collection: 2.226s, learning 0.135s)
             Mean action noise std: 3.06
          Mean value_function loss: 121.2944
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.2365
                       Mean reward: 876.01
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.9420
     Episode_Reward/lifting_object: 173.0257
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.36s
                      Time elapsed: 00:49:10
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 41895 steps/s (collection: 2.207s, learning 0.139s)
             Mean action noise std: 3.06
          Mean value_function loss: 120.1947
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 65.2538
                       Mean reward: 841.50
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 1.9371
     Episode_Reward/lifting_object: 171.7819
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.35s
                      Time elapsed: 00:49:12
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 42354 steps/s (collection: 2.222s, learning 0.099s)
             Mean action noise std: 3.06
          Mean value_function loss: 100.4452
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.2746
                       Mean reward: 879.89
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.9837
     Episode_Reward/lifting_object: 176.7870
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.32s
                      Time elapsed: 00:49:15
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 41040 steps/s (collection: 2.264s, learning 0.132s)
             Mean action noise std: 3.06
          Mean value_function loss: 111.7101
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 65.2964
                       Mean reward: 866.76
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.9394
     Episode_Reward/lifting_object: 172.7746
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.40s
                      Time elapsed: 00:49:17
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 40658 steps/s (collection: 2.309s, learning 0.109s)
             Mean action noise std: 3.07
          Mean value_function loss: 126.6625
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 65.3116
                       Mean reward: 859.13
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 1.9134
     Episode_Reward/lifting_object: 169.7301
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.42s
                      Time elapsed: 00:49:19
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 41991 steps/s (collection: 2.243s, learning 0.098s)
             Mean action noise std: 3.07
          Mean value_function loss: 144.3044
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.3303
                       Mean reward: 861.62
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.9378
     Episode_Reward/lifting_object: 172.2438
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.34s
                      Time elapsed: 00:49:22
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 43083 steps/s (collection: 2.184s, learning 0.098s)
             Mean action noise std: 3.07
          Mean value_function loss: 141.1026
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.3431
                       Mean reward: 892.01
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.9216
     Episode_Reward/lifting_object: 170.7070
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.28s
                      Time elapsed: 00:49:24
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 43137 steps/s (collection: 2.175s, learning 0.104s)
             Mean action noise std: 3.07
          Mean value_function loss: 96.8997
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.3525
                       Mean reward: 897.99
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.9638
     Episode_Reward/lifting_object: 174.0898
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.28s
                      Time elapsed: 00:49:26
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 41763 steps/s (collection: 2.251s, learning 0.103s)
             Mean action noise std: 3.07
          Mean value_function loss: 113.9351
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.3642
                       Mean reward: 884.26
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.9840
     Episode_Reward/lifting_object: 176.4539
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.35s
                      Time elapsed: 00:49:29
                               ETA: 00:33:06

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 41880 steps/s (collection: 2.244s, learning 0.103s)
             Mean action noise std: 3.07
          Mean value_function loss: 102.0651
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 65.3774
                       Mean reward: 919.85
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 1.9549
     Episode_Reward/lifting_object: 173.7825
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.35s
                      Time elapsed: 00:49:31
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 41141 steps/s (collection: 2.279s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 85.9392
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.3919
                       Mean reward: 917.02
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 2.0004
     Episode_Reward/lifting_object: 177.9890
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.39s
                      Time elapsed: 00:49:33
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 41496 steps/s (collection: 2.259s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 102.5366
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.4051
                       Mean reward: 872.66
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.9747
     Episode_Reward/lifting_object: 175.4214
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.37s
                      Time elapsed: 00:49:36
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 42393 steps/s (collection: 2.205s, learning 0.114s)
             Mean action noise std: 3.08
          Mean value_function loss: 114.0741
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.4191
                       Mean reward: 867.50
               Mean episode length: 231.45
    Episode_Reward/reaching_object: 1.9184
     Episode_Reward/lifting_object: 170.0995
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.32s
                      Time elapsed: 00:49:38
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 42112 steps/s (collection: 2.225s, learning 0.109s)
             Mean action noise std: 3.08
          Mean value_function loss: 90.7707
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 65.4338
                       Mean reward: 910.35
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 1.9534
     Episode_Reward/lifting_object: 172.8238
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.33s
                      Time elapsed: 00:49:40
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 41102 steps/s (collection: 2.263s, learning 0.128s)
             Mean action noise std: 3.08
          Mean value_function loss: 108.7086
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.4481
                       Mean reward: 872.07
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.9573
     Episode_Reward/lifting_object: 173.4572
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.39s
                      Time elapsed: 00:49:43
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 41376 steps/s (collection: 2.267s, learning 0.109s)
             Mean action noise std: 3.08
          Mean value_function loss: 85.5389
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.4623
                       Mean reward: 917.61
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 2.0160
     Episode_Reward/lifting_object: 178.8139
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.38s
                      Time elapsed: 00:49:45
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 41880 steps/s (collection: 2.217s, learning 0.130s)
             Mean action noise std: 3.09
          Mean value_function loss: 128.4151
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.4769
                       Mean reward: 866.42
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.9442
     Episode_Reward/lifting_object: 172.2200
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.35s
                      Time elapsed: 00:49:48
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 41568 steps/s (collection: 2.255s, learning 0.110s)
             Mean action noise std: 3.09
          Mean value_function loss: 104.6697
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 65.4908
                       Mean reward: 900.50
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 175.9453
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.36s
                      Time elapsed: 00:49:50
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 42336 steps/s (collection: 2.221s, learning 0.101s)
             Mean action noise std: 3.09
          Mean value_function loss: 127.0623
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 65.5066
                       Mean reward: 845.39
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 1.9367
     Episode_Reward/lifting_object: 171.0847
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.32s
                      Time elapsed: 00:49:52
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 40710 steps/s (collection: 2.288s, learning 0.127s)
             Mean action noise std: 3.09
          Mean value_function loss: 117.1775
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.5196
                       Mean reward: 846.53
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.9268
     Episode_Reward/lifting_object: 170.8420
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.41s
                      Time elapsed: 00:49:55
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 42680 steps/s (collection: 2.203s, learning 0.101s)
             Mean action noise std: 3.09
          Mean value_function loss: 130.3833
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.5282
                       Mean reward: 905.86
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 1.9722
     Episode_Reward/lifting_object: 174.6827
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.30s
                      Time elapsed: 00:49:57
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 42114 steps/s (collection: 2.235s, learning 0.100s)
             Mean action noise std: 3.09
          Mean value_function loss: 116.4749
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.5380
                       Mean reward: 889.45
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.9525
     Episode_Reward/lifting_object: 172.5277
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.33s
                      Time elapsed: 00:49:59
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 43012 steps/s (collection: 2.181s, learning 0.104s)
             Mean action noise std: 3.10
          Mean value_function loss: 118.5244
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.5521
                       Mean reward: 857.76
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 1.9094
     Episode_Reward/lifting_object: 168.8825
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.29s
                      Time elapsed: 00:50:02
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 42917 steps/s (collection: 2.184s, learning 0.106s)
             Mean action noise std: 3.10
          Mean value_function loss: 147.9221
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.5598
                       Mean reward: 847.14
               Mean episode length: 226.61
    Episode_Reward/reaching_object: 1.9310
     Episode_Reward/lifting_object: 170.9600
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.29s
                      Time elapsed: 00:50:04
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 42861 steps/s (collection: 2.193s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 127.1923
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.5720
                       Mean reward: 836.29
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.9103
     Episode_Reward/lifting_object: 168.9746
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.29s
                      Time elapsed: 00:50:06
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 41802 steps/s (collection: 2.259s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 118.2291
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 65.5895
                       Mean reward: 900.96
               Mean episode length: 239.59
    Episode_Reward/reaching_object: 1.9654
     Episode_Reward/lifting_object: 174.5654
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.35s
                      Time elapsed: 00:50:09
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 42739 steps/s (collection: 2.172s, learning 0.128s)
             Mean action noise std: 3.10
          Mean value_function loss: 125.1321
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.6071
                       Mean reward: 850.73
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.9283
     Episode_Reward/lifting_object: 170.4185
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.30s
                      Time elapsed: 00:50:11
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 42353 steps/s (collection: 2.203s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 129.6149
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.6192
                       Mean reward: 882.96
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.9252
     Episode_Reward/lifting_object: 170.1804
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.32s
                      Time elapsed: 00:50:13
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 41125 steps/s (collection: 2.246s, learning 0.144s)
             Mean action noise std: 3.11
          Mean value_function loss: 116.7401
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.6287
                       Mean reward: 863.70
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.9980
     Episode_Reward/lifting_object: 176.9835
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.39s
                      Time elapsed: 00:50:16
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 41003 steps/s (collection: 2.276s, learning 0.121s)
             Mean action noise std: 3.11
          Mean value_function loss: 107.0212
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 65.6408
                       Mean reward: 886.23
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.9751
     Episode_Reward/lifting_object: 174.6363
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.40s
                      Time elapsed: 00:50:18
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 41831 steps/s (collection: 2.245s, learning 0.105s)
             Mean action noise std: 3.11
          Mean value_function loss: 127.9842
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 65.6572
                       Mean reward: 880.03
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.9387
     Episode_Reward/lifting_object: 171.1157
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.35s
                      Time elapsed: 00:50:20
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 42593 steps/s (collection: 2.200s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 105.2332
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.6751
                       Mean reward: 861.72
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.9077
     Episode_Reward/lifting_object: 168.0370
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.31s
                      Time elapsed: 00:50:23
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 42808 steps/s (collection: 2.194s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 94.7035
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.6881
                       Mean reward: 852.09
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.9796
     Episode_Reward/lifting_object: 174.9688
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.30s
                      Time elapsed: 00:50:25
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 42890 steps/s (collection: 2.191s, learning 0.101s)
             Mean action noise std: 3.12
          Mean value_function loss: 107.5499
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.7085
                       Mean reward: 864.83
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.9383
     Episode_Reward/lifting_object: 171.0927
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.29s
                      Time elapsed: 00:50:27
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 42493 steps/s (collection: 2.207s, learning 0.106s)
             Mean action noise std: 3.12
          Mean value_function loss: 92.4388
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.7306
                       Mean reward: 894.91
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 1.9463
     Episode_Reward/lifting_object: 171.7168
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.31s
                      Time elapsed: 00:50:30
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 41339 steps/s (collection: 2.266s, learning 0.112s)
             Mean action noise std: 3.12
          Mean value_function loss: 92.4768
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 65.7460
                       Mean reward: 872.69
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.9566
     Episode_Reward/lifting_object: 172.7251
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.38s
                      Time elapsed: 00:50:32
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 42852 steps/s (collection: 2.197s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 89.3734
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 65.7628
                       Mean reward: 926.56
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 1.9999
     Episode_Reward/lifting_object: 176.9540
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.29s
                      Time elapsed: 00:50:34
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 43354 steps/s (collection: 2.169s, learning 0.098s)
             Mean action noise std: 3.12
          Mean value_function loss: 119.6156
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.7770
                       Mean reward: 877.95
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 1.9320
     Episode_Reward/lifting_object: 170.5959
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.27s
                      Time elapsed: 00:50:36
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 43408 steps/s (collection: 2.163s, learning 0.102s)
             Mean action noise std: 3.12
          Mean value_function loss: 94.6643
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.7905
                       Mean reward: 893.87
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 2.0078
     Episode_Reward/lifting_object: 177.4856
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.26s
                      Time elapsed: 00:50:39
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 43061 steps/s (collection: 2.181s, learning 0.102s)
             Mean action noise std: 3.13
          Mean value_function loss: 112.6048
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 65.7983
                       Mean reward: 866.05
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 1.9271
     Episode_Reward/lifting_object: 170.0321
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.28s
                      Time elapsed: 00:50:41
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 43071 steps/s (collection: 2.182s, learning 0.100s)
             Mean action noise std: 3.13
          Mean value_function loss: 105.1388
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.8078
                       Mean reward: 908.59
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 1.9595
     Episode_Reward/lifting_object: 173.4657
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.28s
                      Time elapsed: 00:50:43
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 42692 steps/s (collection: 2.191s, learning 0.111s)
             Mean action noise std: 3.13
          Mean value_function loss: 90.6385
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 65.8177
                       Mean reward: 878.67
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.9722
     Episode_Reward/lifting_object: 174.7365
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.30s
                      Time elapsed: 00:50:46
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 41514 steps/s (collection: 2.250s, learning 0.118s)
             Mean action noise std: 3.13
          Mean value_function loss: 154.4402
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.8286
                       Mean reward: 826.78
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.9080
     Episode_Reward/lifting_object: 168.4015
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.37s
                      Time elapsed: 00:50:48
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 42328 steps/s (collection: 2.209s, learning 0.113s)
             Mean action noise std: 3.13
          Mean value_function loss: 95.1189
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 65.8421
                       Mean reward: 879.14
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 2.0014
     Episode_Reward/lifting_object: 177.0517
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.32s
                      Time elapsed: 00:50:50
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 42277 steps/s (collection: 2.221s, learning 0.104s)
             Mean action noise std: 3.13
          Mean value_function loss: 83.1163
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 65.8622
                       Mean reward: 886.22
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.9797
     Episode_Reward/lifting_object: 175.6592
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.33s
                      Time elapsed: 00:50:53
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 43135 steps/s (collection: 2.174s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 100.1163
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 65.8820
                       Mean reward: 881.77
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9383
     Episode_Reward/lifting_object: 171.3850
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.28s
                      Time elapsed: 00:50:55
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 42573 steps/s (collection: 2.201s, learning 0.108s)
             Mean action noise std: 3.14
          Mean value_function loss: 100.5931
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.8958
                       Mean reward: 877.40
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.9548
     Episode_Reward/lifting_object: 173.0861
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.31s
                      Time elapsed: 00:50:57
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 42009 steps/s (collection: 2.238s, learning 0.102s)
             Mean action noise std: 3.14
          Mean value_function loss: 95.3838
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.9064
                       Mean reward: 914.81
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.9869
     Episode_Reward/lifting_object: 176.0060
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.34s
                      Time elapsed: 00:51:00
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 40544 steps/s (collection: 2.325s, learning 0.100s)
             Mean action noise std: 3.14
          Mean value_function loss: 101.0197
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.9131
                       Mean reward: 854.89
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 1.9625
     Episode_Reward/lifting_object: 174.0021
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.42s
                      Time elapsed: 00:51:02
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 42476 steps/s (collection: 2.211s, learning 0.104s)
             Mean action noise std: 3.14
          Mean value_function loss: 119.0333
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 65.9277
                       Mean reward: 895.94
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.9844
     Episode_Reward/lifting_object: 175.3588
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.31s
                      Time elapsed: 00:51:04
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 42559 steps/s (collection: 2.211s, learning 0.099s)
             Mean action noise std: 3.15
          Mean value_function loss: 87.4694
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.9508
                       Mean reward: 915.41
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 1.9918
     Episode_Reward/lifting_object: 176.5719
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.31s
                      Time elapsed: 00:51:07
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 42490 steps/s (collection: 2.213s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 99.7347
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 65.9681
                       Mean reward: 891.98
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.9593
     Episode_Reward/lifting_object: 173.1772
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.31s
                      Time elapsed: 00:51:09
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 43279 steps/s (collection: 2.161s, learning 0.111s)
             Mean action noise std: 3.15
          Mean value_function loss: 123.2979
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.9879
                       Mean reward: 843.84
               Mean episode length: 227.03
    Episode_Reward/reaching_object: 1.9365
     Episode_Reward/lifting_object: 170.9207
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.27s
                      Time elapsed: 00:51:11
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 42645 steps/s (collection: 2.204s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 89.4073
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.0069
                       Mean reward: 901.23
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.9830
     Episode_Reward/lifting_object: 175.8073
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.31s
                      Time elapsed: 00:51:13
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 42264 steps/s (collection: 2.223s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 89.6895
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.0198
                       Mean reward: 887.90
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.9542
     Episode_Reward/lifting_object: 172.9646
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.33s
                      Time elapsed: 00:51:16
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 42787 steps/s (collection: 2.174s, learning 0.123s)
             Mean action noise std: 3.16
          Mean value_function loss: 115.9456
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.0355
                       Mean reward: 828.35
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.9130
     Episode_Reward/lifting_object: 169.0054
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.30s
                      Time elapsed: 00:51:18
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 43182 steps/s (collection: 2.174s, learning 0.102s)
             Mean action noise std: 3.16
          Mean value_function loss: 130.4191
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.0503
                       Mean reward: 900.41
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.9720
     Episode_Reward/lifting_object: 174.1512
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.28s
                      Time elapsed: 00:51:20
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 42746 steps/s (collection: 2.195s, learning 0.105s)
             Mean action noise std: 3.16
          Mean value_function loss: 150.1438
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.0663
                       Mean reward: 878.71
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.9309
     Episode_Reward/lifting_object: 170.3926
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.30s
                      Time elapsed: 00:51:23
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 42573 steps/s (collection: 2.206s, learning 0.103s)
             Mean action noise std: 3.16
          Mean value_function loss: 105.9247
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 66.0871
                       Mean reward: 891.96
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.9600
     Episode_Reward/lifting_object: 173.5430
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.31s
                      Time elapsed: 00:51:25
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 42981 steps/s (collection: 2.184s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 88.9751
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.1038
                       Mean reward: 891.75
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 1.9774
     Episode_Reward/lifting_object: 174.9445
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.29s
                      Time elapsed: 00:51:27
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 43325 steps/s (collection: 2.162s, learning 0.107s)
             Mean action noise std: 3.17
          Mean value_function loss: 101.1434
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.1102
                       Mean reward: 895.64
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.9974
     Episode_Reward/lifting_object: 176.6414
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.27s
                      Time elapsed: 00:51:30
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 42302 steps/s (collection: 2.225s, learning 0.099s)
             Mean action noise std: 3.17
          Mean value_function loss: 140.2022
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.1229
                       Mean reward: 864.17
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.9750
     Episode_Reward/lifting_object: 174.6722
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.32s
                      Time elapsed: 00:51:32
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 42983 steps/s (collection: 2.186s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 121.1213
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1369
                       Mean reward: 836.46
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 1.9517
     Episode_Reward/lifting_object: 172.1214
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.29s
                      Time elapsed: 00:51:34
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 42314 steps/s (collection: 2.215s, learning 0.109s)
             Mean action noise std: 3.17
          Mean value_function loss: 108.5235
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.1478
                       Mean reward: 857.09
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 1.9370
     Episode_Reward/lifting_object: 171.3992
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.32s
                      Time elapsed: 00:51:36
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 42643 steps/s (collection: 2.197s, learning 0.109s)
             Mean action noise std: 3.17
          Mean value_function loss: 111.6799
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.1627
                       Mean reward: 896.20
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 1.9223
     Episode_Reward/lifting_object: 169.8434
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.31s
                      Time elapsed: 00:51:39
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 42399 steps/s (collection: 2.192s, learning 0.126s)
             Mean action noise std: 3.17
          Mean value_function loss: 92.2621
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1761
                       Mean reward: 890.43
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 1.9377
     Episode_Reward/lifting_object: 171.2449
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.32s
                      Time elapsed: 00:51:41
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 43053 steps/s (collection: 2.166s, learning 0.117s)
             Mean action noise std: 3.17
          Mean value_function loss: 107.8681
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.1872
                       Mean reward: 833.74
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 1.9662
     Episode_Reward/lifting_object: 174.1549
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.28s
                      Time elapsed: 00:51:43
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 42826 steps/s (collection: 2.190s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 110.8430
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.1995
                       Mean reward: 879.85
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 1.9340
     Episode_Reward/lifting_object: 170.5325
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.30s
                      Time elapsed: 00:51:46
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 41675 steps/s (collection: 2.215s, learning 0.144s)
             Mean action noise std: 3.18
          Mean value_function loss: 77.7254
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.2107
                       Mean reward: 893.00
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.9993
     Episode_Reward/lifting_object: 177.4821
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.36s
                      Time elapsed: 00:51:48
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 41902 steps/s (collection: 2.227s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 127.5690
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.2214
                       Mean reward: 879.09
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.9320
     Episode_Reward/lifting_object: 171.2462
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.35s
                      Time elapsed: 00:51:50
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 42457 steps/s (collection: 2.214s, learning 0.102s)
             Mean action noise std: 3.18
          Mean value_function loss: 102.2990
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.2283
                       Mean reward: 903.84
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 1.9780
     Episode_Reward/lifting_object: 175.5563
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.32s
                      Time elapsed: 00:51:53
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 43116 steps/s (collection: 2.177s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 96.8131
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.2396
                       Mean reward: 843.93
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 1.9690
     Episode_Reward/lifting_object: 174.7025
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.28s
                      Time elapsed: 00:51:55
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 42988 steps/s (collection: 2.178s, learning 0.109s)
             Mean action noise std: 3.18
          Mean value_function loss: 102.6443
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.2493
                       Mean reward: 865.11
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.9413
     Episode_Reward/lifting_object: 172.2153
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.29s
                      Time elapsed: 00:51:57
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 42490 steps/s (collection: 2.210s, learning 0.104s)
             Mean action noise std: 3.18
          Mean value_function loss: 82.7058
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.2645
                       Mean reward: 875.03
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.9925
     Episode_Reward/lifting_object: 176.8240
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.31s
                      Time elapsed: 00:52:00
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 41946 steps/s (collection: 2.237s, learning 0.106s)
             Mean action noise std: 3.19
          Mean value_function loss: 114.2649
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.2758
                       Mean reward: 851.61
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 1.9496
     Episode_Reward/lifting_object: 172.8767
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.34s
                      Time elapsed: 00:52:02
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 42524 steps/s (collection: 2.184s, learning 0.128s)
             Mean action noise std: 3.19
          Mean value_function loss: 94.3591
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.2928
                       Mean reward: 923.85
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 1.9711
     Episode_Reward/lifting_object: 175.0599
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.31s
                      Time elapsed: 00:52:04
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 43115 steps/s (collection: 2.177s, learning 0.103s)
             Mean action noise std: 3.19
          Mean value_function loss: 102.6779
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 66.3167
                       Mean reward: 881.15
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.9514
     Episode_Reward/lifting_object: 173.3850
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.28s
                      Time elapsed: 00:52:07
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 42281 steps/s (collection: 2.224s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 98.7856
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.3371
                       Mean reward: 898.92
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.9579
     Episode_Reward/lifting_object: 173.8778
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.32s
                      Time elapsed: 00:52:09
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 42061 steps/s (collection: 2.220s, learning 0.118s)
             Mean action noise std: 3.19
          Mean value_function loss: 88.4692
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.3471
                       Mean reward: 904.24
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.9907
     Episode_Reward/lifting_object: 177.3248
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.34s
                      Time elapsed: 00:52:11
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 40413 steps/s (collection: 2.321s, learning 0.112s)
             Mean action noise std: 3.20
          Mean value_function loss: 112.1158
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.3550
                       Mean reward: 858.62
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.9508
     Episode_Reward/lifting_object: 172.8599
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.43s
                      Time elapsed: 00:52:14
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 42437 steps/s (collection: 2.215s, learning 0.101s)
             Mean action noise std: 3.20
          Mean value_function loss: 106.1087
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.3613
                       Mean reward: 874.66
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.9308
     Episode_Reward/lifting_object: 171.3159
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.32s
                      Time elapsed: 00:52:16
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 42029 steps/s (collection: 2.225s, learning 0.114s)
             Mean action noise std: 3.20
          Mean value_function loss: 115.8797
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.3708
                       Mean reward: 865.74
               Mean episode length: 229.98
    Episode_Reward/reaching_object: 1.9291
     Episode_Reward/lifting_object: 170.6139
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.34s
                      Time elapsed: 00:52:18
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.227s, learning 0.123s)
             Mean action noise std: 3.20
          Mean value_function loss: 121.3933
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.3841
                       Mean reward: 887.57
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.9406
     Episode_Reward/lifting_object: 171.8438
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.35s
                      Time elapsed: 00:52:21
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 41747 steps/s (collection: 2.246s, learning 0.109s)
             Mean action noise std: 3.20
          Mean value_function loss: 108.7828
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.3929
                       Mean reward: 887.72
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 1.9632
     Episode_Reward/lifting_object: 173.3634
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.35s
                      Time elapsed: 00:52:23
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 41417 steps/s (collection: 2.259s, learning 0.115s)
             Mean action noise std: 3.20
          Mean value_function loss: 119.3972
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.4040
                       Mean reward: 878.25
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.9018
     Episode_Reward/lifting_object: 167.5645
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.37s
                      Time elapsed: 00:52:25
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 41831 steps/s (collection: 2.255s, learning 0.095s)
             Mean action noise std: 3.20
          Mean value_function loss: 99.1913
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.4172
                       Mean reward: 889.03
               Mean episode length: 235.91
    Episode_Reward/reaching_object: 1.9597
     Episode_Reward/lifting_object: 173.5114
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.35s
                      Time elapsed: 00:52:28
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 41667 steps/s (collection: 2.248s, learning 0.112s)
             Mean action noise std: 3.20
          Mean value_function loss: 104.3670
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.4236
                       Mean reward: 855.17
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.9340
     Episode_Reward/lifting_object: 170.9142
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.36s
                      Time elapsed: 00:52:30
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 41941 steps/s (collection: 2.239s, learning 0.105s)
             Mean action noise std: 3.21
          Mean value_function loss: 102.9731
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.4339
                       Mean reward: 870.11
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.9929
     Episode_Reward/lifting_object: 176.3581
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.34s
                      Time elapsed: 00:52:32
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 42268 steps/s (collection: 2.220s, learning 0.106s)
             Mean action noise std: 3.21
          Mean value_function loss: 122.0141
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.4527
                       Mean reward: 879.93
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.9743
     Episode_Reward/lifting_object: 175.1735
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.33s
                      Time elapsed: 00:52:35
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 42420 steps/s (collection: 2.218s, learning 0.100s)
             Mean action noise std: 3.21
          Mean value_function loss: 97.7098
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 66.4707
                       Mean reward: 861.74
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 1.9558
     Episode_Reward/lifting_object: 173.0586
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.32s
                      Time elapsed: 00:52:37
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 41995 steps/s (collection: 2.235s, learning 0.106s)
             Mean action noise std: 3.21
          Mean value_function loss: 121.2124
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.4875
                       Mean reward: 882.71
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.9663
     Episode_Reward/lifting_object: 174.1530
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.34s
                      Time elapsed: 00:52:39
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 42236 steps/s (collection: 2.221s, learning 0.106s)
             Mean action noise std: 3.21
          Mean value_function loss: 107.4203
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.4975
                       Mean reward: 888.12
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 1.9392
     Episode_Reward/lifting_object: 171.6382
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.33s
                      Time elapsed: 00:52:42
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 42224 steps/s (collection: 2.215s, learning 0.114s)
             Mean action noise std: 3.21
          Mean value_function loss: 121.2754
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.5011
                       Mean reward: 840.99
               Mean episode length: 223.28
    Episode_Reward/reaching_object: 1.9094
     Episode_Reward/lifting_object: 169.3409
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.33s
                      Time elapsed: 00:52:44
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 42448 steps/s (collection: 2.214s, learning 0.102s)
             Mean action noise std: 3.22
          Mean value_function loss: 100.4731
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.5042
                       Mean reward: 866.84
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 1.9216
     Episode_Reward/lifting_object: 169.8929
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.32s
                      Time elapsed: 00:52:46
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 41431 steps/s (collection: 2.242s, learning 0.131s)
             Mean action noise std: 3.22
          Mean value_function loss: 101.5490
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.5147
                       Mean reward: 875.43
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 1.9433
     Episode_Reward/lifting_object: 171.8437
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.37s
                      Time elapsed: 00:52:49
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 41809 steps/s (collection: 2.234s, learning 0.117s)
             Mean action noise std: 3.22
          Mean value_function loss: 111.2739
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.5278
                       Mean reward: 856.36
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 175.1702
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.35s
                      Time elapsed: 00:52:51
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 41672 steps/s (collection: 2.265s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 80.2703
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.5409
                       Mean reward: 909.07
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 2.0031
     Episode_Reward/lifting_object: 176.9916
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.36s
                      Time elapsed: 00:52:53
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 42206 steps/s (collection: 2.220s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 118.0453
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.5508
                       Mean reward: 863.91
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 1.9571
     Episode_Reward/lifting_object: 172.5175
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.33s
                      Time elapsed: 00:52:56
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 42645 steps/s (collection: 2.199s, learning 0.107s)
             Mean action noise std: 3.22
          Mean value_function loss: 98.7965
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.5726
                       Mean reward: 892.61
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.9531
     Episode_Reward/lifting_object: 172.4894
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.31s
                      Time elapsed: 00:52:58
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 42602 steps/s (collection: 2.208s, learning 0.100s)
             Mean action noise std: 3.23
          Mean value_function loss: 90.6191
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.5937
                       Mean reward: 908.40
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 1.9926
     Episode_Reward/lifting_object: 176.2082
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.31s
                      Time elapsed: 00:53:00
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 40435 steps/s (collection: 2.322s, learning 0.109s)
             Mean action noise std: 3.23
          Mean value_function loss: 95.3931
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 66.6123
                       Mean reward: 872.18
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 1.9669
     Episode_Reward/lifting_object: 173.5558
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.43s
                      Time elapsed: 00:53:03
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 43018 steps/s (collection: 2.184s, learning 0.101s)
             Mean action noise std: 3.23
          Mean value_function loss: 94.4390
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.6291
                       Mean reward: 875.92
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 1.9882
     Episode_Reward/lifting_object: 175.3609
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.29s
                      Time elapsed: 00:53:05
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 42595 steps/s (collection: 2.199s, learning 0.109s)
             Mean action noise std: 3.23
          Mean value_function loss: 120.6741
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6357
                       Mean reward: 869.23
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 1.9738
     Episode_Reward/lifting_object: 174.1435
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.31s
                      Time elapsed: 00:53:07
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 41117 steps/s (collection: 2.273s, learning 0.118s)
             Mean action noise std: 3.23
          Mean value_function loss: 74.4601
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.6419
                       Mean reward: 909.37
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 2.0271
     Episode_Reward/lifting_object: 179.0170
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.39s
                      Time elapsed: 00:53:10
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 40783 steps/s (collection: 2.294s, learning 0.116s)
             Mean action noise std: 3.23
          Mean value_function loss: 140.8716
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.6492
                       Mean reward: 889.22
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.9651
     Episode_Reward/lifting_object: 173.3904
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.41s
                      Time elapsed: 00:53:12
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 37230 steps/s (collection: 2.422s, learning 0.219s)
             Mean action noise std: 3.23
          Mean value_function loss: 123.1664
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.6573
                       Mean reward: 862.08
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 1.9225
     Episode_Reward/lifting_object: 169.4064
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.64s
                      Time elapsed: 00:53:15
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 39188 steps/s (collection: 2.372s, learning 0.137s)
             Mean action noise std: 3.24
          Mean value_function loss: 110.5303
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.6704
                       Mean reward: 881.76
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.9628
     Episode_Reward/lifting_object: 172.9834
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.51s
                      Time elapsed: 00:53:17
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 38032 steps/s (collection: 2.423s, learning 0.161s)
             Mean action noise std: 3.24
          Mean value_function loss: 130.1329
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.6749
                       Mean reward: 854.18
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.9380
     Episode_Reward/lifting_object: 170.8442
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.58s
                      Time elapsed: 00:53:20
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 41908 steps/s (collection: 2.225s, learning 0.121s)
             Mean action noise std: 3.24
          Mean value_function loss: 110.7930
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.6814
                       Mean reward: 908.48
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.9746
     Episode_Reward/lifting_object: 173.7811
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.35s
                      Time elapsed: 00:53:22
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 41795 steps/s (collection: 2.209s, learning 0.143s)
             Mean action noise std: 3.24
          Mean value_function loss: 110.5543
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.6940
                       Mean reward: 856.31
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 1.9833
     Episode_Reward/lifting_object: 175.1759
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.35s
                      Time elapsed: 00:53:25
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 38831 steps/s (collection: 2.396s, learning 0.136s)
             Mean action noise std: 3.24
          Mean value_function loss: 105.7650
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 66.7055
                       Mean reward: 888.99
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.9484
     Episode_Reward/lifting_object: 171.8250
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.53s
                      Time elapsed: 00:53:27
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 40904 steps/s (collection: 2.265s, learning 0.138s)
             Mean action noise std: 3.24
          Mean value_function loss: 131.8874
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.7254
                       Mean reward: 852.67
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.9797
     Episode_Reward/lifting_object: 175.1964
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.40s
                      Time elapsed: 00:53:30
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 41607 steps/s (collection: 2.224s, learning 0.139s)
             Mean action noise std: 3.25
          Mean value_function loss: 141.7286
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.7408
                       Mean reward: 861.19
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 1.9547
     Episode_Reward/lifting_object: 172.1599
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.36s
                      Time elapsed: 00:53:32
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 41652 steps/s (collection: 2.258s, learning 0.103s)
             Mean action noise std: 3.25
          Mean value_function loss: 100.8713
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.7597
                       Mean reward: 862.68
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 173.0746
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.36s
                      Time elapsed: 00:53:34
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 41676 steps/s (collection: 2.249s, learning 0.110s)
             Mean action noise std: 3.25
          Mean value_function loss: 101.3154
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.7801
                       Mean reward: 853.04
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 1.9805
     Episode_Reward/lifting_object: 174.3793
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.36s
                      Time elapsed: 00:53:37
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 42229 steps/s (collection: 2.214s, learning 0.114s)
             Mean action noise std: 3.25
          Mean value_function loss: 108.4058
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.7951
                       Mean reward: 851.97
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 2.0039
     Episode_Reward/lifting_object: 177.3335
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.33s
                      Time elapsed: 00:53:39
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 41634 steps/s (collection: 2.261s, learning 0.101s)
             Mean action noise std: 3.25
          Mean value_function loss: 128.7506
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.8086
                       Mean reward: 886.92
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.9659
     Episode_Reward/lifting_object: 173.6113
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.36s
                      Time elapsed: 00:53:41
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 41689 steps/s (collection: 2.243s, learning 0.115s)
             Mean action noise std: 3.26
          Mean value_function loss: 128.5808
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.8214
                       Mean reward: 810.00
               Mean episode length: 217.62
    Episode_Reward/reaching_object: 1.9071
     Episode_Reward/lifting_object: 167.9952
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.36s
                      Time elapsed: 00:53:44
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 42634 steps/s (collection: 2.210s, learning 0.096s)
             Mean action noise std: 3.26
          Mean value_function loss: 82.7047
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.8373
                       Mean reward: 914.25
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 2.0002
     Episode_Reward/lifting_object: 176.5581
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.31s
                      Time elapsed: 00:53:46
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 42262 steps/s (collection: 2.224s, learning 0.102s)
             Mean action noise std: 3.26
          Mean value_function loss: 104.0457
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.8487
                       Mean reward: 871.70
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.9656
     Episode_Reward/lifting_object: 173.3853
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.33s
                      Time elapsed: 00:53:48
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 40412 steps/s (collection: 2.280s, learning 0.153s)
             Mean action noise std: 3.26
          Mean value_function loss: 142.9690
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.8560
                       Mean reward: 858.90
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 1.9496
     Episode_Reward/lifting_object: 171.8176
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.43s
                      Time elapsed: 00:53:51
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 39516 steps/s (collection: 2.373s, learning 0.115s)
             Mean action noise std: 3.26
          Mean value_function loss: 101.5650
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.8736
                       Mean reward: 859.24
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 1.9587
     Episode_Reward/lifting_object: 172.6348
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.49s
                      Time elapsed: 00:53:53
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 40136 steps/s (collection: 2.291s, learning 0.159s)
             Mean action noise std: 3.26
          Mean value_function loss: 91.4647
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 66.8901
                       Mean reward: 876.41
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.9855
     Episode_Reward/lifting_object: 175.4751
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.45s
                      Time elapsed: 00:53:56
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 40863 steps/s (collection: 2.281s, learning 0.125s)
             Mean action noise std: 3.27
          Mean value_function loss: 126.1700
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.9019
                       Mean reward: 871.58
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 1.9663
     Episode_Reward/lifting_object: 173.8224
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.41s
                      Time elapsed: 00:53:58
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 41018 steps/s (collection: 2.259s, learning 0.137s)
             Mean action noise std: 3.27
          Mean value_function loss: 122.2592
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.9138
                       Mean reward: 888.21
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.9335
     Episode_Reward/lifting_object: 170.7551
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.40s
                      Time elapsed: 00:54:00
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 40018 steps/s (collection: 2.339s, learning 0.117s)
             Mean action noise std: 3.27
          Mean value_function loss: 114.4135
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.9279
                       Mean reward: 867.63
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 1.9517
     Episode_Reward/lifting_object: 172.1724
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.46s
                      Time elapsed: 00:54:03
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 40677 steps/s (collection: 2.286s, learning 0.130s)
             Mean action noise std: 3.27
          Mean value_function loss: 101.4675
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.9412
                       Mean reward: 913.57
               Mean episode length: 241.55
    Episode_Reward/reaching_object: 1.9885
     Episode_Reward/lifting_object: 175.7888
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.42s
                      Time elapsed: 00:54:05
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 41438 steps/s (collection: 2.265s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 131.3025
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.9579
                       Mean reward: 851.50
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.9305
     Episode_Reward/lifting_object: 169.7953
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.37s
                      Time elapsed: 00:54:08
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 39524 steps/s (collection: 2.343s, learning 0.145s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.3571
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.9721
                       Mean reward: 867.99
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.9498
     Episode_Reward/lifting_object: 172.3099
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.49s
                      Time elapsed: 00:54:10
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 41912 steps/s (collection: 2.244s, learning 0.101s)
             Mean action noise std: 3.28
          Mean value_function loss: 84.2437
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.9830
                       Mean reward: 930.72
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 2.0055
     Episode_Reward/lifting_object: 177.3987
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.35s
                      Time elapsed: 00:54:13
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 41381 steps/s (collection: 2.252s, learning 0.123s)
             Mean action noise std: 3.28
          Mean value_function loss: 104.9353
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.9927
                       Mean reward: 872.34
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.9819
     Episode_Reward/lifting_object: 175.0834
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.38s
                      Time elapsed: 00:54:15
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 41967 steps/s (collection: 2.243s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 98.6918
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 67.0043
                       Mean reward: 887.61
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.9492
     Episode_Reward/lifting_object: 171.7499
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.34s
                      Time elapsed: 00:54:17
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41072 steps/s (collection: 2.258s, learning 0.136s)
             Mean action noise std: 3.28
          Mean value_function loss: 137.4072
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.0187
                       Mean reward: 828.82
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.9132
     Episode_Reward/lifting_object: 168.2272
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.39s
                      Time elapsed: 00:54:20
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 40565 steps/s (collection: 2.312s, learning 0.111s)
             Mean action noise std: 3.28
          Mean value_function loss: 124.2892
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 67.0360
                       Mean reward: 844.17
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 1.9605
     Episode_Reward/lifting_object: 172.6371
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.42s
                      Time elapsed: 00:54:22
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 41357 steps/s (collection: 2.232s, learning 0.145s)
             Mean action noise std: 3.29
          Mean value_function loss: 102.5584
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 67.0535
                       Mean reward: 890.10
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.9572
     Episode_Reward/lifting_object: 172.3397
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.38s
                      Time elapsed: 00:54:24
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 38045 steps/s (collection: 2.402s, learning 0.182s)
             Mean action noise std: 3.29
          Mean value_function loss: 144.4352
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.0717
                       Mean reward: 897.72
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 1.9505
     Episode_Reward/lifting_object: 171.6645
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.58s
                      Time elapsed: 00:54:27
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 38119 steps/s (collection: 2.480s, learning 0.099s)
             Mean action noise std: 3.29
          Mean value_function loss: 135.5375
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.0859
                       Mean reward: 859.25
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.9510
     Episode_Reward/lifting_object: 171.4420
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.58s
                      Time elapsed: 00:54:30
                               ETA: 00:27:40

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 40042 steps/s (collection: 2.351s, learning 0.104s)
             Mean action noise std: 3.29
          Mean value_function loss: 100.5861
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.0989
                       Mean reward: 857.59
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.9812
     Episode_Reward/lifting_object: 174.7727
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.46s
                      Time elapsed: 00:54:32
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 39284 steps/s (collection: 2.365s, learning 0.138s)
             Mean action noise std: 3.29
          Mean value_function loss: 108.8019
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.1098
                       Mean reward: 885.66
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.9777
     Episode_Reward/lifting_object: 174.2338
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.50s
                      Time elapsed: 00:54:35
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 40305 steps/s (collection: 2.341s, learning 0.098s)
             Mean action noise std: 3.29
          Mean value_function loss: 110.2374
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.1208
                       Mean reward: 865.52
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 1.9243
     Episode_Reward/lifting_object: 168.8715
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.44s
                      Time elapsed: 00:54:37
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 41537 steps/s (collection: 2.255s, learning 0.112s)
             Mean action noise std: 3.30
          Mean value_function loss: 102.6901
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 67.1339
                       Mean reward: 871.82
               Mean episode length: 232.27
    Episode_Reward/reaching_object: 1.9647
     Episode_Reward/lifting_object: 172.9822
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.37s
                      Time elapsed: 00:54:39
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 39940 steps/s (collection: 2.310s, learning 0.151s)
             Mean action noise std: 3.30
          Mean value_function loss: 116.5251
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.1465
                       Mean reward: 875.87
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.9632
     Episode_Reward/lifting_object: 173.2492
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.46s
                      Time elapsed: 00:54:42
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 40230 steps/s (collection: 2.339s, learning 0.105s)
             Mean action noise std: 3.30
          Mean value_function loss: 110.7594
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.1601
                       Mean reward: 874.16
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 1.9197
     Episode_Reward/lifting_object: 169.0008
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.44s
                      Time elapsed: 00:54:44
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 17246 steps/s (collection: 5.574s, learning 0.126s)
             Mean action noise std: 3.30
          Mean value_function loss: 108.6188
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.1723
                       Mean reward: 858.01
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 1.9625
     Episode_Reward/lifting_object: 173.0345
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.70s
                      Time elapsed: 00:54:50
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 13008 steps/s (collection: 7.394s, learning 0.164s)
             Mean action noise std: 3.30
          Mean value_function loss: 114.5690
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.1828
                       Mean reward: 878.15
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.9646
     Episode_Reward/lifting_object: 173.1441
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.56s
                      Time elapsed: 00:54:58
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13495 steps/s (collection: 7.161s, learning 0.123s)
             Mean action noise std: 3.30
          Mean value_function loss: 102.1792
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.1995
                       Mean reward: 879.09
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.9828
     Episode_Reward/lifting_object: 175.2888
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.28s
                      Time elapsed: 00:55:05
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13254 steps/s (collection: 7.306s, learning 0.111s)
             Mean action noise std: 3.31
          Mean value_function loss: 120.7594
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.2130
                       Mean reward: 853.11
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 1.9631
     Episode_Reward/lifting_object: 173.2063
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.42s
                      Time elapsed: 00:55:12
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13825 steps/s (collection: 6.979s, learning 0.131s)
             Mean action noise std: 3.31
          Mean value_function loss: 141.5586
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.2273
                       Mean reward: 883.99
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.9704
     Episode_Reward/lifting_object: 173.5293
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.11s
                      Time elapsed: 00:55:19
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13611 steps/s (collection: 7.090s, learning 0.133s)
             Mean action noise std: 3.31
          Mean value_function loss: 84.8266
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.2354
                       Mean reward: 897.73
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.9525
     Episode_Reward/lifting_object: 171.9302
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.22s
                      Time elapsed: 00:55:27
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13479 steps/s (collection: 7.168s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 130.2765
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.2451
                       Mean reward: 862.31
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 1.9658
     Episode_Reward/lifting_object: 173.0418
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.29s
                      Time elapsed: 00:55:34
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 13735 steps/s (collection: 7.030s, learning 0.127s)
             Mean action noise std: 3.31
          Mean value_function loss: 99.0765
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.2505
                       Mean reward: 881.70
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 1.9744
     Episode_Reward/lifting_object: 173.7616
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.16s
                      Time elapsed: 00:55:41
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12333 steps/s (collection: 7.872s, learning 0.098s)
             Mean action noise std: 3.31
          Mean value_function loss: 131.8833
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.2591
                       Mean reward: 856.48
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.8989
     Episode_Reward/lifting_object: 166.1272
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.97s
                      Time elapsed: 00:55:49
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 42107 steps/s (collection: 2.201s, learning 0.134s)
             Mean action noise std: 3.31
          Mean value_function loss: 108.1187
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.2769
                       Mean reward: 877.49
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.9499
     Episode_Reward/lifting_object: 171.1942
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.33s
                      Time elapsed: 00:55:51
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 42120 steps/s (collection: 2.221s, learning 0.113s)
             Mean action noise std: 3.31
          Mean value_function loss: 90.6683
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 67.2895
                       Mean reward: 893.72
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.9538
     Episode_Reward/lifting_object: 171.4927
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.33s
                      Time elapsed: 00:55:54
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 41688 steps/s (collection: 2.203s, learning 0.155s)
             Mean action noise std: 3.32
          Mean value_function loss: 110.5792
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.2931
                       Mean reward: 889.56
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 2.0200
     Episode_Reward/lifting_object: 177.7128
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.36s
                      Time elapsed: 00:55:56
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 42120 steps/s (collection: 2.235s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 90.1237
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.3033
                       Mean reward: 887.07
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 2.0018
     Episode_Reward/lifting_object: 176.1170
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.33s
                      Time elapsed: 00:55:58
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 41031 steps/s (collection: 2.283s, learning 0.113s)
             Mean action noise std: 3.32
          Mean value_function loss: 114.0845
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 67.3160
                       Mean reward: 895.08
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.9508
     Episode_Reward/lifting_object: 171.2178
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.40s
                      Time elapsed: 00:56:01
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 40351 steps/s (collection: 2.311s, learning 0.126s)
             Mean action noise std: 3.32
          Mean value_function loss: 126.3038
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.3388
                       Mean reward: 854.70
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 1.9604
     Episode_Reward/lifting_object: 172.2430
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.44s
                      Time elapsed: 00:56:03
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 42966 steps/s (collection: 2.174s, learning 0.114s)
             Mean action noise std: 3.32
          Mean value_function loss: 130.0450
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.3541
                       Mean reward: 829.81
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 1.9204
     Episode_Reward/lifting_object: 168.4754
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.29s
                      Time elapsed: 00:56:06
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 41535 steps/s (collection: 2.256s, learning 0.111s)
             Mean action noise std: 3.33
          Mean value_function loss: 92.4807
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.3716
                       Mean reward: 867.63
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.9657
     Episode_Reward/lifting_object: 172.9163
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.37s
                      Time elapsed: 00:56:08
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 40660 steps/s (collection: 2.278s, learning 0.140s)
             Mean action noise std: 3.33
          Mean value_function loss: 111.1981
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.3880
                       Mean reward: 898.22
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.9721
     Episode_Reward/lifting_object: 173.1795
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.42s
                      Time elapsed: 00:56:10
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 38345 steps/s (collection: 2.450s, learning 0.114s)
             Mean action noise std: 3.33
          Mean value_function loss: 95.7421
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.4024
                       Mean reward: 901.51
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 2.0021
     Episode_Reward/lifting_object: 176.3564
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.56s
                      Time elapsed: 00:56:13
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 41441 steps/s (collection: 2.232s, learning 0.140s)
             Mean action noise std: 3.33
          Mean value_function loss: 117.8174
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.4174
                       Mean reward: 864.95
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 2.0106
     Episode_Reward/lifting_object: 177.3389
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.37s
                      Time elapsed: 00:56:15
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 41372 steps/s (collection: 2.259s, learning 0.117s)
             Mean action noise std: 3.34
          Mean value_function loss: 93.5743
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.4381
                       Mean reward: 878.19
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 1.9728
     Episode_Reward/lifting_object: 173.7239
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.38s
                      Time elapsed: 00:56:18
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 42210 steps/s (collection: 2.222s, learning 0.107s)
             Mean action noise std: 3.34
          Mean value_function loss: 109.6011
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.4629
                       Mean reward: 862.65
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 1.9720
     Episode_Reward/lifting_object: 173.4693
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.33s
                      Time elapsed: 00:56:20
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 41174 steps/s (collection: 2.219s, learning 0.168s)
             Mean action noise std: 3.34
          Mean value_function loss: 114.8557
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.4775
                       Mean reward: 875.72
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.9318
     Episode_Reward/lifting_object: 169.4447
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.39s
                      Time elapsed: 00:56:22
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 42927 steps/s (collection: 2.162s, learning 0.128s)
             Mean action noise std: 3.34
          Mean value_function loss: 108.9863
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4853
                       Mean reward: 896.25
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 1.9800
     Episode_Reward/lifting_object: 174.4912
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.29s
                      Time elapsed: 00:56:25
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 42648 steps/s (collection: 2.190s, learning 0.115s)
             Mean action noise std: 3.34
          Mean value_function loss: 98.4835
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.5011
                       Mean reward: 862.02
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.9935
     Episode_Reward/lifting_object: 175.1107
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.30s
                      Time elapsed: 00:56:27
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 42033 steps/s (collection: 2.192s, learning 0.147s)
             Mean action noise std: 3.35
          Mean value_function loss: 102.5198
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.5170
                       Mean reward: 854.95
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.9760
     Episode_Reward/lifting_object: 174.1484
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.34s
                      Time elapsed: 00:56:29
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 41745 steps/s (collection: 2.243s, learning 0.112s)
             Mean action noise std: 3.35
          Mean value_function loss: 138.1890
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.5371
                       Mean reward: 871.78
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.9221
     Episode_Reward/lifting_object: 168.9037
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.35s
                      Time elapsed: 00:56:32
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 40021 steps/s (collection: 2.340s, learning 0.117s)
             Mean action noise std: 3.35
          Mean value_function loss: 95.4834
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.5572
                       Mean reward: 856.97
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.9665
     Episode_Reward/lifting_object: 172.3777
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.46s
                      Time elapsed: 00:56:34
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 40647 steps/s (collection: 2.313s, learning 0.105s)
             Mean action noise std: 3.35
          Mean value_function loss: 109.4951
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.5702
                       Mean reward: 852.46
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 1.9364
     Episode_Reward/lifting_object: 170.4653
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.42s
                      Time elapsed: 00:56:36
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 41015 steps/s (collection: 2.291s, learning 0.106s)
             Mean action noise std: 3.35
          Mean value_function loss: 102.3918
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.5795
                       Mean reward: 903.24
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.9834
     Episode_Reward/lifting_object: 174.1552
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.40s
                      Time elapsed: 00:56:39
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 40578 steps/s (collection: 2.315s, learning 0.108s)
             Mean action noise std: 3.36
          Mean value_function loss: 115.1778
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.5983
                       Mean reward: 825.59
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 1.9573
     Episode_Reward/lifting_object: 171.8563
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.42s
                      Time elapsed: 00:56:41
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 42315 steps/s (collection: 2.199s, learning 0.124s)
             Mean action noise std: 3.36
          Mean value_function loss: 92.8336
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.6187
                       Mean reward: 896.71
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 174.0481
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.32s
                      Time elapsed: 00:56:44
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 41269 steps/s (collection: 2.265s, learning 0.117s)
             Mean action noise std: 3.36
          Mean value_function loss: 85.6688
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 67.6320
                       Mean reward: 895.42
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 2.0190
     Episode_Reward/lifting_object: 177.7215
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.38s
                      Time elapsed: 00:56:46
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 39542 steps/s (collection: 2.359s, learning 0.127s)
             Mean action noise std: 3.36
          Mean value_function loss: 135.7706
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 67.6506
                       Mean reward: 819.01
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.9444
     Episode_Reward/lifting_object: 170.5323
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.49s
                      Time elapsed: 00:56:48
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 39725 steps/s (collection: 2.361s, learning 0.113s)
             Mean action noise std: 3.36
          Mean value_function loss: 122.7795
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.6688
                       Mean reward: 862.40
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 1.9571
     Episode_Reward/lifting_object: 171.6517
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.47s
                      Time elapsed: 00:56:51
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 40971 steps/s (collection: 2.213s, learning 0.186s)
             Mean action noise std: 3.37
          Mean value_function loss: 123.2236
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.6786
                       Mean reward: 848.41
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.9654
     Episode_Reward/lifting_object: 172.7165
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.40s
                      Time elapsed: 00:56:53
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 41843 steps/s (collection: 2.223s, learning 0.127s)
             Mean action noise std: 3.37
          Mean value_function loss: 113.8853
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.6878
                       Mean reward: 877.99
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.9491
     Episode_Reward/lifting_object: 170.8255
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.35s
                      Time elapsed: 00:56:56
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 41057 steps/s (collection: 2.292s, learning 0.103s)
             Mean action noise std: 3.37
          Mean value_function loss: 81.0312
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.6972
                       Mean reward: 922.71
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 2.0126
     Episode_Reward/lifting_object: 177.0899
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.39s
                      Time elapsed: 00:56:58
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 40541 steps/s (collection: 2.297s, learning 0.128s)
             Mean action noise std: 3.37
          Mean value_function loss: 102.4078
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.7056
                       Mean reward: 864.96
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.9566
     Episode_Reward/lifting_object: 171.5960
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.42s
                      Time elapsed: 00:57:01
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 42215 steps/s (collection: 2.214s, learning 0.115s)
             Mean action noise std: 3.37
          Mean value_function loss: 117.5571
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.7203
                       Mean reward: 840.64
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.9372
     Episode_Reward/lifting_object: 170.2020
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.33s
                      Time elapsed: 00:57:03
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 41698 steps/s (collection: 2.230s, learning 0.128s)
             Mean action noise std: 3.37
          Mean value_function loss: 101.5649
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 67.7321
                       Mean reward: 866.57
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.9849
     Episode_Reward/lifting_object: 173.9717
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.36s
                      Time elapsed: 00:57:05
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 42092 steps/s (collection: 2.207s, learning 0.128s)
             Mean action noise std: 3.37
          Mean value_function loss: 115.1478
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.7434
                       Mean reward: 886.26
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 1.9859
     Episode_Reward/lifting_object: 174.2049
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.34s
                      Time elapsed: 00:57:08
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 40812 steps/s (collection: 2.304s, learning 0.105s)
             Mean action noise std: 3.38
          Mean value_function loss: 121.6401
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 67.7590
                       Mean reward: 882.20
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 1.9572
     Episode_Reward/lifting_object: 171.5774
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.41s
                      Time elapsed: 00:57:10
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 40634 steps/s (collection: 2.318s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 125.8699
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 67.7747
                       Mean reward: 846.95
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 1.9400
     Episode_Reward/lifting_object: 169.5814
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.42s
                      Time elapsed: 00:57:12
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 41881 steps/s (collection: 2.227s, learning 0.121s)
             Mean action noise std: 3.38
          Mean value_function loss: 87.4451
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.7849
                       Mean reward: 901.57
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.9765
     Episode_Reward/lifting_object: 173.6462
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.35s
                      Time elapsed: 00:57:15
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 40511 steps/s (collection: 2.310s, learning 0.116s)
             Mean action noise std: 3.38
          Mean value_function loss: 109.4507
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.7976
                       Mean reward: 914.83
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 1.9536
     Episode_Reward/lifting_object: 171.5873
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.43s
                      Time elapsed: 00:57:17
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 40613 steps/s (collection: 2.299s, learning 0.122s)
             Mean action noise std: 3.38
          Mean value_function loss: 104.2083
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 67.8090
                       Mean reward: 882.42
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.9608
     Episode_Reward/lifting_object: 172.0051
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.42s
                      Time elapsed: 00:57:20
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 40348 steps/s (collection: 2.317s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 141.8008
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.8242
                       Mean reward: 871.38
               Mean episode length: 232.02
    Episode_Reward/reaching_object: 1.9290
     Episode_Reward/lifting_object: 169.8903
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.44s
                      Time elapsed: 00:57:22
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 40382 steps/s (collection: 2.324s, learning 0.111s)
             Mean action noise std: 3.39
          Mean value_function loss: 106.2836
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.8378
                       Mean reward: 863.17
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.9585
     Episode_Reward/lifting_object: 172.1898
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.43s
                      Time elapsed: 00:57:24
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 42137 steps/s (collection: 2.218s, learning 0.115s)
             Mean action noise std: 3.39
          Mean value_function loss: 110.7110
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.8498
                       Mean reward: 865.14
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 1.9429
     Episode_Reward/lifting_object: 171.3172
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.33s
                      Time elapsed: 00:57:27
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 38168 steps/s (collection: 2.408s, learning 0.168s)
             Mean action noise std: 3.39
          Mean value_function loss: 85.5773
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.8599
                       Mean reward: 895.61
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 2.0133
     Episode_Reward/lifting_object: 177.9387
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.58s
                      Time elapsed: 00:57:29
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 40688 steps/s (collection: 2.271s, learning 0.145s)
             Mean action noise std: 3.39
          Mean value_function loss: 117.2651
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.8709
                       Mean reward: 885.20
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 173.0991
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.42s
                      Time elapsed: 00:57:32
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 39970 steps/s (collection: 2.335s, learning 0.124s)
             Mean action noise std: 3.39
          Mean value_function loss: 98.3832
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.8867
                       Mean reward: 892.75
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 2.0120
     Episode_Reward/lifting_object: 177.3353
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.46s
                      Time elapsed: 00:57:34
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 41484 steps/s (collection: 2.274s, learning 0.096s)
             Mean action noise std: 3.39
          Mean value_function loss: 90.0641
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.8999
                       Mean reward: 873.12
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.9923
     Episode_Reward/lifting_object: 175.7888
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.37s
                      Time elapsed: 00:57:37
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 42117 steps/s (collection: 2.236s, learning 0.098s)
             Mean action noise std: 3.40
          Mean value_function loss: 91.8529
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.9147
                       Mean reward: 882.07
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.9798
     Episode_Reward/lifting_object: 174.6680
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.33s
                      Time elapsed: 00:57:39
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 39069 steps/s (collection: 2.370s, learning 0.146s)
             Mean action noise std: 3.40
          Mean value_function loss: 113.8401
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.9307
                       Mean reward: 834.06
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.9460
     Episode_Reward/lifting_object: 171.4237
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.52s
                      Time elapsed: 00:57:41
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 41319 steps/s (collection: 2.245s, learning 0.134s)
             Mean action noise std: 3.40
          Mean value_function loss: 125.6155
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.9463
                       Mean reward: 877.27
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.9444
     Episode_Reward/lifting_object: 170.9921
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.38s
                      Time elapsed: 00:57:44
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 41690 steps/s (collection: 2.210s, learning 0.148s)
             Mean action noise std: 3.40
          Mean value_function loss: 129.1971
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.9633
                       Mean reward: 851.53
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.9411
     Episode_Reward/lifting_object: 170.8328
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.36s
                      Time elapsed: 00:57:46
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 41460 steps/s (collection: 2.215s, learning 0.156s)
             Mean action noise std: 3.40
          Mean value_function loss: 122.2009
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.9729
                       Mean reward: 837.73
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 1.9525
     Episode_Reward/lifting_object: 171.3742
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.37s
                      Time elapsed: 00:57:49
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 38957 steps/s (collection: 2.355s, learning 0.169s)
             Mean action noise std: 3.40
          Mean value_function loss: 125.1848
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.9788
                       Mean reward: 846.69
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.9153
     Episode_Reward/lifting_object: 167.9776
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.52s
                      Time elapsed: 00:57:51
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 40479 steps/s (collection: 2.326s, learning 0.102s)
             Mean action noise std: 3.41
          Mean value_function loss: 111.3612
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.9864
                       Mean reward: 878.28
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.9722
     Episode_Reward/lifting_object: 173.4643
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.43s
                      Time elapsed: 00:57:54
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 41225 steps/s (collection: 2.246s, learning 0.138s)
             Mean action noise std: 3.41
          Mean value_function loss: 98.9639
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.9952
                       Mean reward: 908.37
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 1.9735
     Episode_Reward/lifting_object: 173.4454
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.38s
                      Time elapsed: 00:57:56
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 41543 steps/s (collection: 2.252s, learning 0.114s)
             Mean action noise std: 3.41
          Mean value_function loss: 95.3020
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.0096
                       Mean reward: 917.67
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 1.9994
     Episode_Reward/lifting_object: 175.3966
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.37s
                      Time elapsed: 00:57:58
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 34392 steps/s (collection: 2.691s, learning 0.167s)
             Mean action noise std: 3.41
          Mean value_function loss: 140.8922
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.0262
                       Mean reward: 853.37
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 1.9230
     Episode_Reward/lifting_object: 168.4313
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.86s
                      Time elapsed: 00:58:01
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 38815 steps/s (collection: 2.354s, learning 0.179s)
             Mean action noise std: 3.41
          Mean value_function loss: 110.4577
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.0324
                       Mean reward: 922.09
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 1.9675
     Episode_Reward/lifting_object: 172.7199
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.53s
                      Time elapsed: 00:58:04
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 38657 steps/s (collection: 2.404s, learning 0.139s)
             Mean action noise std: 3.41
          Mean value_function loss: 118.9273
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.0409
                       Mean reward: 880.83
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 176.9846
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.54s
                      Time elapsed: 00:58:06
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 37800 steps/s (collection: 2.406s, learning 0.195s)
             Mean action noise std: 3.42
          Mean value_function loss: 123.8286
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 68.0581
                       Mean reward: 864.12
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.9098
     Episode_Reward/lifting_object: 167.3745
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.60s
                      Time elapsed: 00:58:09
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 38789 steps/s (collection: 2.414s, learning 0.120s)
             Mean action noise std: 3.42
          Mean value_function loss: 105.5812
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.0706
                       Mean reward: 885.81
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.9662
     Episode_Reward/lifting_object: 172.7198
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.53s
                      Time elapsed: 00:58:11
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 39272 steps/s (collection: 2.376s, learning 0.128s)
             Mean action noise std: 3.42
          Mean value_function loss: 127.4105
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.0803
                       Mean reward: 831.20
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.9289
     Episode_Reward/lifting_object: 169.6090
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.50s
                      Time elapsed: 00:58:14
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 39813 steps/s (collection: 2.325s, learning 0.144s)
             Mean action noise std: 3.42
          Mean value_function loss: 124.5029
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.0902
                       Mean reward: 897.47
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 1.9395
     Episode_Reward/lifting_object: 170.4468
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.47s
                      Time elapsed: 00:58:16
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 37880 steps/s (collection: 2.484s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 106.6975
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.1038
                       Mean reward: 871.48
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 1.9656
     Episode_Reward/lifting_object: 172.9127
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.60s
                      Time elapsed: 00:58:19
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 39516 steps/s (collection: 2.375s, learning 0.113s)
             Mean action noise std: 3.42
          Mean value_function loss: 121.2872
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 68.1195
                       Mean reward: 846.73
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.9653
     Episode_Reward/lifting_object: 172.8995
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.49s
                      Time elapsed: 00:58:21
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 41491 steps/s (collection: 2.253s, learning 0.117s)
             Mean action noise std: 3.43
          Mean value_function loss: 117.8898
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1360
                       Mean reward: 856.66
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.9377
     Episode_Reward/lifting_object: 170.5671
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.37s
                      Time elapsed: 00:58:24
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 40237 steps/s (collection: 2.325s, learning 0.118s)
             Mean action noise std: 3.43
          Mean value_function loss: 102.0893
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.1488
                       Mean reward: 887.76
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.9522
     Episode_Reward/lifting_object: 171.6592
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.44s
                      Time elapsed: 00:58:26
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 42925 steps/s (collection: 2.176s, learning 0.115s)
             Mean action noise std: 3.43
          Mean value_function loss: 122.1931
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.1603
                       Mean reward: 900.39
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.9743
     Episode_Reward/lifting_object: 173.5459
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.29s
                      Time elapsed: 00:58:28
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 42357 steps/s (collection: 2.224s, learning 0.097s)
             Mean action noise std: 3.43
          Mean value_function loss: 132.5784
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.1735
                       Mean reward: 867.72
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.9737
     Episode_Reward/lifting_object: 173.5584
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.32s
                      Time elapsed: 00:58:31
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 41988 steps/s (collection: 2.235s, learning 0.106s)
             Mean action noise std: 3.43
          Mean value_function loss: 114.0945
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.1968
                       Mean reward: 908.22
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 1.9731
     Episode_Reward/lifting_object: 173.6255
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.34s
                      Time elapsed: 00:58:33
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 43094 steps/s (collection: 2.183s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 152.3486
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.2219
                       Mean reward: 845.63
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 1.9697
     Episode_Reward/lifting_object: 173.5155
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.28s
                      Time elapsed: 00:58:35
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 41223 steps/s (collection: 2.252s, learning 0.133s)
             Mean action noise std: 3.44
          Mean value_function loss: 108.1403
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 68.2394
                       Mean reward: 897.88
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 1.9737
     Episode_Reward/lifting_object: 172.9213
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.38s
                      Time elapsed: 00:58:38
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 42995 steps/s (collection: 2.176s, learning 0.110s)
             Mean action noise std: 3.44
          Mean value_function loss: 120.4662
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.2542
                       Mean reward: 907.11
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 2.0061
     Episode_Reward/lifting_object: 176.4842
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.29s
                      Time elapsed: 00:58:40
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 42362 steps/s (collection: 2.188s, learning 0.133s)
             Mean action noise std: 3.44
          Mean value_function loss: 117.4676
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.2634
                       Mean reward: 870.28
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 1.9538
     Episode_Reward/lifting_object: 171.7518
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.32s
                      Time elapsed: 00:58:42
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 40972 steps/s (collection: 2.262s, learning 0.138s)
             Mean action noise std: 3.44
          Mean value_function loss: 119.3009
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.2697
                       Mean reward: 886.06
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 1.9320
     Episode_Reward/lifting_object: 169.6992
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.40s
                      Time elapsed: 00:58:45
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 38698 steps/s (collection: 2.395s, learning 0.145s)
             Mean action noise std: 3.44
          Mean value_function loss: 108.9731
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.2800
                       Mean reward: 868.91
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.9737
     Episode_Reward/lifting_object: 173.6522
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0699
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.54s
                      Time elapsed: 00:58:47
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 37300 steps/s (collection: 2.509s, learning 0.127s)
             Mean action noise std: 3.45
          Mean value_function loss: 114.9300
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.2877
                       Mean reward: 903.87
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.9750
     Episode_Reward/lifting_object: 173.9185
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.64s
                      Time elapsed: 00:58:50
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 41843 steps/s (collection: 2.240s, learning 0.109s)
             Mean action noise std: 3.45
          Mean value_function loss: 114.0466
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.2958
                       Mean reward: 864.16
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.9805
     Episode_Reward/lifting_object: 174.4270
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.35s
                      Time elapsed: 00:58:52
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 41716 steps/s (collection: 2.244s, learning 0.113s)
             Mean action noise std: 3.45
          Mean value_function loss: 132.9773
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.3080
                       Mean reward: 824.51
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.9219
     Episode_Reward/lifting_object: 168.8896
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.36s
                      Time elapsed: 00:58:55
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 42109 steps/s (collection: 2.227s, learning 0.108s)
             Mean action noise std: 3.45
          Mean value_function loss: 132.4222
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.3268
                       Mean reward: 874.37
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.9104
     Episode_Reward/lifting_object: 168.0205
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.33s
                      Time elapsed: 00:58:57
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 42305 steps/s (collection: 2.205s, learning 0.119s)
             Mean action noise std: 3.45
          Mean value_function loss: 144.8130
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.3461
                       Mean reward: 863.22
               Mean episode length: 231.42
    Episode_Reward/reaching_object: 1.9318
     Episode_Reward/lifting_object: 170.2320
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.32s
                      Time elapsed: 00:58:59
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 40714 steps/s (collection: 2.292s, learning 0.123s)
             Mean action noise std: 3.46
          Mean value_function loss: 101.7739
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.3640
                       Mean reward: 872.88
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.9890
     Episode_Reward/lifting_object: 174.6172
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.41s
                      Time elapsed: 00:59:02
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 41983 steps/s (collection: 2.237s, learning 0.105s)
             Mean action noise std: 3.46
          Mean value_function loss: 108.4015
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.3866
                       Mean reward: 897.04
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.9860
     Episode_Reward/lifting_object: 175.3002
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.34s
                      Time elapsed: 00:59:04
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 43040 steps/s (collection: 2.179s, learning 0.105s)
             Mean action noise std: 3.46
          Mean value_function loss: 117.5244
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.4003
                       Mean reward: 854.16
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.9747
     Episode_Reward/lifting_object: 174.3894
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.28s
                      Time elapsed: 00:59:06
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 40524 steps/s (collection: 2.260s, learning 0.166s)
             Mean action noise std: 3.46
          Mean value_function loss: 115.1531
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.4107
                       Mean reward: 859.91
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.9702
     Episode_Reward/lifting_object: 173.6279
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.43s
                      Time elapsed: 00:59:09
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 42345 steps/s (collection: 2.190s, learning 0.132s)
             Mean action noise std: 3.46
          Mean value_function loss: 124.1543
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.4196
                       Mean reward: 860.94
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.9435
     Episode_Reward/lifting_object: 171.2163
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.32s
                      Time elapsed: 00:59:11
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 41779 steps/s (collection: 2.221s, learning 0.132s)
             Mean action noise std: 3.46
          Mean value_function loss: 129.4718
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.4314
                       Mean reward: 856.04
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 1.9610
     Episode_Reward/lifting_object: 172.6980
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.35s
                      Time elapsed: 00:59:14
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 41631 steps/s (collection: 2.225s, learning 0.137s)
             Mean action noise std: 3.47
          Mean value_function loss: 96.6691
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.4426
                       Mean reward: 870.20
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.9686
     Episode_Reward/lifting_object: 173.6613
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.36s
                      Time elapsed: 00:59:16
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 37656 steps/s (collection: 2.443s, learning 0.168s)
             Mean action noise std: 3.47
          Mean value_function loss: 108.3374
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.4558
                       Mean reward: 897.96
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 1.9622
     Episode_Reward/lifting_object: 173.1824
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.61s
                      Time elapsed: 00:59:18
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 41001 steps/s (collection: 2.258s, learning 0.140s)
             Mean action noise std: 3.47
          Mean value_function loss: 101.1426
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.4617
                       Mean reward: 847.19
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 1.9681
     Episode_Reward/lifting_object: 173.2895
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0695
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.40s
                      Time elapsed: 00:59:21
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 41321 steps/s (collection: 2.249s, learning 0.130s)
             Mean action noise std: 3.47
          Mean value_function loss: 77.1006
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.4719
                       Mean reward: 881.98
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 2.0196
     Episode_Reward/lifting_object: 177.8999
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.38s
                      Time elapsed: 00:59:23
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 43017 steps/s (collection: 2.163s, learning 0.122s)
             Mean action noise std: 3.47
          Mean value_function loss: 88.6309
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.4859
                       Mean reward: 874.65
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 1.9762
     Episode_Reward/lifting_object: 174.2929
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.29s
                      Time elapsed: 00:59:26
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 39012 steps/s (collection: 2.372s, learning 0.148s)
             Mean action noise std: 3.47
          Mean value_function loss: 99.2764
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.4967
                       Mean reward: 882.49
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.9707
     Episode_Reward/lifting_object: 173.8395
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.52s
                      Time elapsed: 00:59:28
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 36375 steps/s (collection: 2.469s, learning 0.233s)
             Mean action noise std: 3.48
          Mean value_function loss: 110.3435
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.5105
                       Mean reward: 834.17
               Mean episode length: 222.32
    Episode_Reward/reaching_object: 1.9418
     Episode_Reward/lifting_object: 171.2894
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.70s
                      Time elapsed: 00:59:31
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 36635 steps/s (collection: 2.547s, learning 0.136s)
             Mean action noise std: 3.48
          Mean value_function loss: 87.3005
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.5295
                       Mean reward: 881.10
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 2.0004
     Episode_Reward/lifting_object: 176.6976
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.68s
                      Time elapsed: 00:59:33
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 40016 steps/s (collection: 2.257s, learning 0.199s)
             Mean action noise std: 3.48
          Mean value_function loss: 86.7287
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.5461
                       Mean reward: 886.83
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.9821
     Episode_Reward/lifting_object: 174.7095
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.46s
                      Time elapsed: 00:59:36
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 40430 steps/s (collection: 2.309s, learning 0.122s)
             Mean action noise std: 3.48
          Mean value_function loss: 91.8473
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.5569
                       Mean reward: 871.13
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 2.0093
     Episode_Reward/lifting_object: 177.4228
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.43s
                      Time elapsed: 00:59:38
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 41220 steps/s (collection: 2.278s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 95.1521
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.5731
                       Mean reward: 905.77
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.9850
     Episode_Reward/lifting_object: 175.4805
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.38s
                      Time elapsed: 00:59:41
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 39452 steps/s (collection: 2.370s, learning 0.122s)
             Mean action noise std: 3.49
          Mean value_function loss: 87.9028
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.5878
                       Mean reward: 909.91
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 2.0002
     Episode_Reward/lifting_object: 176.9465
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.49s
                      Time elapsed: 00:59:43
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 42008 steps/s (collection: 2.229s, learning 0.111s)
             Mean action noise std: 3.49
          Mean value_function loss: 82.1883
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.6064
                       Mean reward: 904.89
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 1.9658
     Episode_Reward/lifting_object: 173.5063
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.34s
                      Time elapsed: 00:59:46
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 40060 steps/s (collection: 2.313s, learning 0.141s)
             Mean action noise std: 3.49
          Mean value_function loss: 99.6091
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 68.6178
                       Mean reward: 888.58
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 1.9578
     Episode_Reward/lifting_object: 172.9601
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.45s
                      Time elapsed: 00:59:48
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 38198 steps/s (collection: 2.408s, learning 0.166s)
             Mean action noise std: 3.49
          Mean value_function loss: 124.3581
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.6288
                       Mean reward: 827.74
               Mean episode length: 221.01
    Episode_Reward/reaching_object: 1.8994
     Episode_Reward/lifting_object: 167.6762
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.57s
                      Time elapsed: 00:59:51
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 38807 steps/s (collection: 2.425s, learning 0.109s)
             Mean action noise std: 3.49
          Mean value_function loss: 103.8531
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.6422
                       Mean reward: 868.93
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.9518
     Episode_Reward/lifting_object: 172.0477
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.53s
                      Time elapsed: 00:59:53
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 41262 steps/s (collection: 2.279s, learning 0.103s)
             Mean action noise std: 3.50
          Mean value_function loss: 108.8677
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.6511
                       Mean reward: 861.45
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.9610
     Episode_Reward/lifting_object: 172.8135
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.38s
                      Time elapsed: 00:59:55
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 39400 steps/s (collection: 2.340s, learning 0.155s)
             Mean action noise std: 3.50
          Mean value_function loss: 123.6051
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.6694
                       Mean reward: 882.39
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.9526
     Episode_Reward/lifting_object: 172.0513
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0685
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.49s
                      Time elapsed: 00:59:58
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 37064 steps/s (collection: 2.454s, learning 0.198s)
             Mean action noise std: 3.50
          Mean value_function loss: 122.2082
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.6895
                       Mean reward: 846.14
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.9386
     Episode_Reward/lifting_object: 171.1664
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.65s
                      Time elapsed: 01:00:01
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 40690 steps/s (collection: 2.285s, learning 0.131s)
             Mean action noise std: 3.50
          Mean value_function loss: 104.5023
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.7039
                       Mean reward: 899.29
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 1.9956
     Episode_Reward/lifting_object: 176.0502
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.42s
                      Time elapsed: 01:00:03
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 41655 steps/s (collection: 2.242s, learning 0.118s)
             Mean action noise std: 3.50
          Mean value_function loss: 133.8003
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.7178
                       Mean reward: 870.36
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 1.9299
     Episode_Reward/lifting_object: 170.0517
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.36s
                      Time elapsed: 01:00:05
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 41702 steps/s (collection: 2.248s, learning 0.109s)
             Mean action noise std: 3.50
          Mean value_function loss: 81.7657
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.7263
                       Mean reward: 904.02
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 2.0109
     Episode_Reward/lifting_object: 177.6761
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.36s
                      Time elapsed: 01:00:08
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 41459 steps/s (collection: 2.261s, learning 0.111s)
             Mean action noise std: 3.51
          Mean value_function loss: 95.1630
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.7340
                       Mean reward: 866.97
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.9737
     Episode_Reward/lifting_object: 173.9074
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.37s
                      Time elapsed: 01:00:10
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 41574 steps/s (collection: 2.251s, learning 0.114s)
             Mean action noise std: 3.51
          Mean value_function loss: 117.0440
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.7458
                       Mean reward: 849.41
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.9493
     Episode_Reward/lifting_object: 171.9248
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0694
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.36s
                      Time elapsed: 01:00:13
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 39456 steps/s (collection: 2.360s, learning 0.132s)
             Mean action noise std: 3.51
          Mean value_function loss: 97.3353
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.7576
                       Mean reward: 906.04
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 2.0056
     Episode_Reward/lifting_object: 177.2460
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.49s
                      Time elapsed: 01:00:15
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 38227 steps/s (collection: 2.437s, learning 0.134s)
             Mean action noise std: 3.51
          Mean value_function loss: 94.8635
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.7714
                       Mean reward: 873.66
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.9934
     Episode_Reward/lifting_object: 176.0176
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.57s
                      Time elapsed: 01:00:18
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 40735 steps/s (collection: 2.293s, learning 0.120s)
             Mean action noise std: 3.51
          Mean value_function loss: 104.2476
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.7869
                       Mean reward: 892.75
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.9914
     Episode_Reward/lifting_object: 175.7057
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.41s
                      Time elapsed: 01:00:20
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 40830 steps/s (collection: 2.284s, learning 0.124s)
             Mean action noise std: 3.51
          Mean value_function loss: 112.6713
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.8000
                       Mean reward: 864.90
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.9615
     Episode_Reward/lifting_object: 172.8869
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0704
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.41s
                      Time elapsed: 01:00:22
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 41656 steps/s (collection: 2.237s, learning 0.123s)
             Mean action noise std: 3.52
          Mean value_function loss: 96.5656
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 68.8098
                       Mean reward: 842.51
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 1.9399
     Episode_Reward/lifting_object: 170.6275
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.36s
                      Time elapsed: 01:00:25
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 40332 steps/s (collection: 2.317s, learning 0.120s)
             Mean action noise std: 3.52
          Mean value_function loss: 88.6074
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.8210
                       Mean reward: 902.59
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 2.0156
     Episode_Reward/lifting_object: 177.7271
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.44s
                      Time elapsed: 01:00:27
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 40162 steps/s (collection: 2.340s, learning 0.108s)
             Mean action noise std: 3.52
          Mean value_function loss: 98.5325
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.8286
                       Mean reward: 877.79
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 1.9808
     Episode_Reward/lifting_object: 174.8123
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.45s
                      Time elapsed: 01:00:30
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 39448 steps/s (collection: 2.382s, learning 0.110s)
             Mean action noise std: 3.52
          Mean value_function loss: 122.5525
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.8433
                       Mean reward: 859.82
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 174.3514
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.49s
                      Time elapsed: 01:00:32
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 40730 steps/s (collection: 2.301s, learning 0.112s)
             Mean action noise std: 3.52
          Mean value_function loss: 95.4705
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.8581
                       Mean reward: 882.34
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.9968
     Episode_Reward/lifting_object: 175.8865
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.41s
                      Time elapsed: 01:00:35
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 38302 steps/s (collection: 2.386s, learning 0.180s)
             Mean action noise std: 3.52
          Mean value_function loss: 120.5884
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.8705
                       Mean reward: 854.32
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.9668
     Episode_Reward/lifting_object: 173.4709
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.57s
                      Time elapsed: 01:00:37
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 40626 steps/s (collection: 2.295s, learning 0.125s)
             Mean action noise std: 3.53
          Mean value_function loss: 115.4645
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.8878
                       Mean reward: 883.51
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 1.9604
     Episode_Reward/lifting_object: 172.9293
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0710
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.42s
                      Time elapsed: 01:00:40
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 40387 steps/s (collection: 2.284s, learning 0.150s)
             Mean action noise std: 3.53
          Mean value_function loss: 94.1244
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.9013
                       Mean reward: 891.21
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 1.9619
     Episode_Reward/lifting_object: 172.9971
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.43s
                      Time elapsed: 01:00:42
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 42150 steps/s (collection: 2.235s, learning 0.097s)
             Mean action noise std: 3.53
          Mean value_function loss: 109.9061
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.9133
                       Mean reward: 886.92
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 170.9048
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.33s
                      Time elapsed: 01:00:44
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 41642 steps/s (collection: 2.260s, learning 0.101s)
             Mean action noise std: 3.53
          Mean value_function loss: 72.0854
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.9278
                       Mean reward: 909.68
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 2.0202
     Episode_Reward/lifting_object: 178.5112
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.36s
                      Time elapsed: 01:00:47
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 40178 steps/s (collection: 2.306s, learning 0.141s)
             Mean action noise std: 3.53
          Mean value_function loss: 88.1385
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.9344
                       Mean reward: 872.17
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.9861
     Episode_Reward/lifting_object: 175.3476
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0722
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.45s
                      Time elapsed: 01:00:49
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 40522 steps/s (collection: 2.297s, learning 0.129s)
             Mean action noise std: 3.54
          Mean value_function loss: 111.7101
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.9499
                       Mean reward: 859.38
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 1.9854
     Episode_Reward/lifting_object: 175.0664
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.43s
                      Time elapsed: 01:00:52
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 38349 steps/s (collection: 2.407s, learning 0.157s)
             Mean action noise std: 3.54
          Mean value_function loss: 89.7394
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 68.9649
                       Mean reward: 887.88
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 2.0190
     Episode_Reward/lifting_object: 177.8068
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.56s
                      Time elapsed: 01:00:54
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 41027 steps/s (collection: 2.295s, learning 0.101s)
             Mean action noise std: 3.54
          Mean value_function loss: 124.2311
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.9683
                       Mean reward: 881.33
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 1.9679
     Episode_Reward/lifting_object: 173.1118
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.40s
                      Time elapsed: 01:00:56
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 41955 steps/s (collection: 2.239s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 127.3261
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.9765
                       Mean reward: 844.40
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 1.9043
     Episode_Reward/lifting_object: 167.1197
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.34s
                      Time elapsed: 01:00:59
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 37928 steps/s (collection: 2.469s, learning 0.123s)
             Mean action noise std: 3.54
          Mean value_function loss: 82.2975
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 68.9895
                       Mean reward: 890.53
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.9708
     Episode_Reward/lifting_object: 173.6107
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.59s
                      Time elapsed: 01:01:01
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 39299 steps/s (collection: 2.375s, learning 0.127s)
             Mean action noise std: 3.54
          Mean value_function loss: 93.7671
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.0055
                       Mean reward: 866.34
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 1.9955
     Episode_Reward/lifting_object: 175.8034
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.50s
                      Time elapsed: 01:01:04
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 41655 steps/s (collection: 2.256s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 105.8705
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.0188
                       Mean reward: 834.91
               Mean episode length: 222.09
    Episode_Reward/reaching_object: 1.9693
     Episode_Reward/lifting_object: 173.6490
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.36s
                      Time elapsed: 01:01:06
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 41406 steps/s (collection: 2.266s, learning 0.108s)
             Mean action noise std: 3.55
          Mean value_function loss: 86.7474
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.0282
                       Mean reward: 874.44
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.9887
     Episode_Reward/lifting_object: 175.4760
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.37s
                      Time elapsed: 01:01:09
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 39487 steps/s (collection: 2.312s, learning 0.178s)
             Mean action noise std: 3.55
          Mean value_function loss: 103.0059
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.0376
                       Mean reward: 826.53
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.9640
     Episode_Reward/lifting_object: 172.7950
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0713
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.49s
                      Time elapsed: 01:01:11
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 39611 steps/s (collection: 2.385s, learning 0.097s)
             Mean action noise std: 3.55
          Mean value_function loss: 88.2894
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.0476
                       Mean reward: 876.31
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 1.9781
     Episode_Reward/lifting_object: 174.1816
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.48s
                      Time elapsed: 01:01:14
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 36823 steps/s (collection: 2.368s, learning 0.301s)
             Mean action noise std: 3.55
          Mean value_function loss: 114.5839
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.0592
                       Mean reward: 879.37
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.9498
     Episode_Reward/lifting_object: 171.8042
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.67s
                      Time elapsed: 01:01:16
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 34592 steps/s (collection: 2.640s, learning 0.202s)
             Mean action noise std: 3.55
          Mean value_function loss: 102.2590
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.0686
                       Mean reward: 864.28
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.9598
     Episode_Reward/lifting_object: 172.0709
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.84s
                      Time elapsed: 01:01:19
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 40996 steps/s (collection: 2.283s, learning 0.115s)
             Mean action noise std: 3.55
          Mean value_function loss: 94.0502
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.0825
                       Mean reward: 874.49
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 1.9620
     Episode_Reward/lifting_object: 172.5049
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.40s
                      Time elapsed: 01:01:22
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 41592 steps/s (collection: 2.262s, learning 0.102s)
             Mean action noise std: 3.56
          Mean value_function loss: 97.5621
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 69.0965
                       Mean reward: 875.46
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 1.9701
     Episode_Reward/lifting_object: 173.8469
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.36s
                      Time elapsed: 01:01:24
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 41262 steps/s (collection: 2.278s, learning 0.105s)
             Mean action noise std: 3.56
          Mean value_function loss: 105.9183
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.1099
                       Mean reward: 875.51
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 170.7975
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.38s
                      Time elapsed: 01:01:26
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 39943 steps/s (collection: 2.358s, learning 0.104s)
             Mean action noise std: 3.56
          Mean value_function loss: 96.3153
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.1171
                       Mean reward: 855.91
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 1.9793
     Episode_Reward/lifting_object: 174.5115
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.46s
                      Time elapsed: 01:01:29
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 41468 steps/s (collection: 2.248s, learning 0.123s)
             Mean action noise std: 3.56
          Mean value_function loss: 115.7923
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.1320
                       Mean reward: 879.13
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 1.9375
     Episode_Reward/lifting_object: 170.9885
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.37s
                      Time elapsed: 01:01:31
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 41419 steps/s (collection: 2.264s, learning 0.109s)
             Mean action noise std: 3.56
          Mean value_function loss: 112.9855
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.1410
                       Mean reward: 876.82
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.9348
     Episode_Reward/lifting_object: 170.3956
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0711
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.37s
                      Time elapsed: 01:01:33
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 41837 steps/s (collection: 2.251s, learning 0.099s)
             Mean action noise std: 3.56
          Mean value_function loss: 102.6678
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.1501
                       Mean reward: 874.44
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.9699
     Episode_Reward/lifting_object: 173.1588
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.35s
                      Time elapsed: 01:01:36
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 39426 steps/s (collection: 2.334s, learning 0.160s)
             Mean action noise std: 3.57
          Mean value_function loss: 112.0435
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 69.1622
                       Mean reward: 886.58
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.9562
     Episode_Reward/lifting_object: 172.2266
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0712
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.49s
                      Time elapsed: 01:01:38
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 40412 steps/s (collection: 2.333s, learning 0.100s)
             Mean action noise std: 3.57
          Mean value_function loss: 99.4894
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.1794
                       Mean reward: 866.20
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 1.9875
     Episode_Reward/lifting_object: 175.5157
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.43s
                      Time elapsed: 01:01:41
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 39931 steps/s (collection: 2.280s, learning 0.182s)
             Mean action noise std: 3.57
          Mean value_function loss: 102.5989
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.1917
                       Mean reward: 884.29
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.9721
     Episode_Reward/lifting_object: 173.8199
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0720
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.46s
                      Time elapsed: 01:01:43
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 40057 steps/s (collection: 2.319s, learning 0.136s)
             Mean action noise std: 3.57
          Mean value_function loss: 100.2012
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.2063
                       Mean reward: 905.26
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.9766
     Episode_Reward/lifting_object: 174.0085
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.45s
                      Time elapsed: 01:01:46
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 40821 steps/s (collection: 2.308s, learning 0.101s)
             Mean action noise std: 3.57
          Mean value_function loss: 75.6634
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.2153
                       Mean reward: 865.67
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 2.0099
     Episode_Reward/lifting_object: 177.2989
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.41s
                      Time elapsed: 01:01:48
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 40675 steps/s (collection: 2.288s, learning 0.129s)
             Mean action noise std: 3.57
          Mean value_function loss: 69.4673
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.2243
                       Mean reward: 884.37
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9913
     Episode_Reward/lifting_object: 175.5874
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.42s
                      Time elapsed: 01:01:51
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 40904 steps/s (collection: 2.285s, learning 0.119s)
             Mean action noise std: 3.57
          Mean value_function loss: 112.9703
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.2345
                       Mean reward: 855.63
               Mean episode length: 227.83
    Episode_Reward/reaching_object: 1.9475
     Episode_Reward/lifting_object: 171.5886
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0725
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.40s
                      Time elapsed: 01:01:53
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 39761 steps/s (collection: 2.308s, learning 0.164s)
             Mean action noise std: 3.58
          Mean value_function loss: 100.3710
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.2501
                       Mean reward: 866.34
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.9517
     Episode_Reward/lifting_object: 171.7104
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.47s
                      Time elapsed: 01:01:55
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 40316 steps/s (collection: 2.326s, learning 0.112s)
             Mean action noise std: 3.58
          Mean value_function loss: 102.6933
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.2640
                       Mean reward: 884.86
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.9785
     Episode_Reward/lifting_object: 174.5730
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.44s
                      Time elapsed: 01:01:58
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 38680 steps/s (collection: 2.389s, learning 0.153s)
             Mean action noise std: 3.58
          Mean value_function loss: 124.8912
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.2804
                       Mean reward: 858.43
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.9646
     Episode_Reward/lifting_object: 173.0236
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.54s
                      Time elapsed: 01:02:00
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 38553 steps/s (collection: 2.439s, learning 0.111s)
             Mean action noise std: 3.58
          Mean value_function loss: 112.3513
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.2950
                       Mean reward: 886.71
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.9620
     Episode_Reward/lifting_object: 172.6241
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.55s
                      Time elapsed: 01:02:03
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 38305 steps/s (collection: 2.389s, learning 0.177s)
             Mean action noise std: 3.58
          Mean value_function loss: 123.2612
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.3082
                       Mean reward: 854.41
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.9506
     Episode_Reward/lifting_object: 171.8563
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0724
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.57s
                      Time elapsed: 01:02:05
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 40745 steps/s (collection: 2.306s, learning 0.106s)
             Mean action noise std: 3.59
          Mean value_function loss: 103.6721
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.3204
                       Mean reward: 854.66
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.9508
     Episode_Reward/lifting_object: 171.6765
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.41s
                      Time elapsed: 01:02:08
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 40228 steps/s (collection: 2.288s, learning 0.156s)
             Mean action noise std: 3.59
          Mean value_function loss: 129.3156
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.3323
                       Mean reward: 873.45
               Mean episode length: 231.59
    Episode_Reward/reaching_object: 1.9740
     Episode_Reward/lifting_object: 173.7402
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.44s
                      Time elapsed: 01:02:10
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 41276 steps/s (collection: 2.258s, learning 0.124s)
             Mean action noise std: 3.59
          Mean value_function loss: 115.1183
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.3491
                       Mean reward: 909.38
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 1.9409
     Episode_Reward/lifting_object: 170.4318
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.38s
                      Time elapsed: 01:02:13
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 41420 steps/s (collection: 2.260s, learning 0.114s)
             Mean action noise std: 3.59
          Mean value_function loss: 104.3930
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.3644
                       Mean reward: 892.65
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.9540
     Episode_Reward/lifting_object: 171.9004
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.37s
                      Time elapsed: 01:02:15
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 39405 steps/s (collection: 2.382s, learning 0.113s)
             Mean action noise std: 3.60
          Mean value_function loss: 99.5550
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.3816
                       Mean reward: 895.92
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.9639
     Episode_Reward/lifting_object: 172.9268
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.49s
                      Time elapsed: 01:02:18
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 41291 steps/s (collection: 2.254s, learning 0.127s)
             Mean action noise std: 3.60
          Mean value_function loss: 84.9945
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.3955
                       Mean reward: 896.32
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.9594
     Episode_Reward/lifting_object: 172.9188
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.38s
                      Time elapsed: 01:02:20
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 40812 steps/s (collection: 2.262s, learning 0.147s)
             Mean action noise std: 3.60
          Mean value_function loss: 110.8166
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.4110
                       Mean reward: 914.16
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 1.9961
     Episode_Reward/lifting_object: 175.8771
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.41s
                      Time elapsed: 01:02:22
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 40381 steps/s (collection: 2.286s, learning 0.149s)
             Mean action noise std: 3.60
          Mean value_function loss: 124.9408
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.4193
                       Mean reward: 891.18
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.9365
     Episode_Reward/lifting_object: 170.7701
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.43s
                      Time elapsed: 01:02:25
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 40068 steps/s (collection: 2.313s, learning 0.141s)
             Mean action noise std: 3.60
          Mean value_function loss: 87.8895
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.4280
                       Mean reward: 920.89
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 1.9705
     Episode_Reward/lifting_object: 174.0913
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.45s
                      Time elapsed: 01:02:27
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 40821 steps/s (collection: 2.305s, learning 0.103s)
             Mean action noise std: 3.60
          Mean value_function loss: 95.4529
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.4366
                       Mean reward: 899.71
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.9686
     Episode_Reward/lifting_object: 174.1707
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.41s
                      Time elapsed: 01:02:30
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 41800 steps/s (collection: 2.245s, learning 0.107s)
             Mean action noise std: 3.60
          Mean value_function loss: 99.6980
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.4462
                       Mean reward: 849.12
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.9816
     Episode_Reward/lifting_object: 175.2041
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.35s
                      Time elapsed: 01:02:32
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 40842 steps/s (collection: 2.308s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 94.2669
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.4577
                       Mean reward: 891.25
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.9786
     Episode_Reward/lifting_object: 175.0352
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.41s
                      Time elapsed: 01:02:34
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 37354 steps/s (collection: 2.367s, learning 0.264s)
             Mean action noise std: 3.61
          Mean value_function loss: 117.8294
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.4685
                       Mean reward: 868.89
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.9573
     Episode_Reward/lifting_object: 173.0134
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.63s
                      Time elapsed: 01:02:37
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 36909 steps/s (collection: 2.527s, learning 0.136s)
             Mean action noise std: 3.61
          Mean value_function loss: 98.0844
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.4809
                       Mean reward: 875.92
               Mean episode length: 231.09
    Episode_Reward/reaching_object: 1.9776
     Episode_Reward/lifting_object: 175.3268
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.66s
                      Time elapsed: 01:02:40
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 37250 steps/s (collection: 2.446s, learning 0.193s)
             Mean action noise std: 3.61
          Mean value_function loss: 131.5018
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.4912
                       Mean reward: 853.88
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.9338
     Episode_Reward/lifting_object: 170.3771
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.64s
                      Time elapsed: 01:02:42
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 40203 steps/s (collection: 2.336s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 109.8714
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 69.5010
                       Mean reward: 863.55
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 1.9240
     Episode_Reward/lifting_object: 169.8095
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.45s
                      Time elapsed: 01:02:45
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 41834 steps/s (collection: 2.251s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 94.7245
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 69.5056
                       Mean reward: 853.86
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.9216
     Episode_Reward/lifting_object: 169.7194
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.35s
                      Time elapsed: 01:02:47
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 40642 steps/s (collection: 2.298s, learning 0.121s)
             Mean action noise std: 3.61
          Mean value_function loss: 112.3795
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.5104
                       Mean reward: 883.43
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.9816
     Episode_Reward/lifting_object: 175.0051
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.42s
                      Time elapsed: 01:02:50
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 38690 steps/s (collection: 2.441s, learning 0.100s)
             Mean action noise std: 3.61
          Mean value_function loss: 119.0294
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.5211
                       Mean reward: 867.57
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.9441
     Episode_Reward/lifting_object: 171.6961
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.54s
                      Time elapsed: 01:02:52
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 37615 steps/s (collection: 2.363s, learning 0.251s)
             Mean action noise std: 3.62
          Mean value_function loss: 104.0550
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.5352
                       Mean reward: 895.35
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.9776
     Episode_Reward/lifting_object: 175.0318
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.61s
                      Time elapsed: 01:02:55
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 37606 steps/s (collection: 2.444s, learning 0.170s)
             Mean action noise std: 3.62
          Mean value_function loss: 122.6603
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.5513
                       Mean reward: 851.45
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.8913
     Episode_Reward/lifting_object: 166.8161
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.61s
                      Time elapsed: 01:02:57
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 36389 steps/s (collection: 2.548s, learning 0.153s)
             Mean action noise std: 3.62
          Mean value_function loss: 16923.5009
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 69.5599
                       Mean reward: 851.45
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 1.9284
     Episode_Reward/lifting_object: 170.2692
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -10.0135
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.70s
                      Time elapsed: 01:03:00
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 37113 steps/s (collection: 2.438s, learning 0.211s)
             Mean action noise std: 3.62
          Mean value_function loss: 115.3496
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.5642
                       Mean reward: 874.51
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 1.9675
     Episode_Reward/lifting_object: 173.1468
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.65s
                      Time elapsed: 01:03:03
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 29707 steps/s (collection: 3.099s, learning 0.210s)
             Mean action noise std: 3.62
          Mean value_function loss: 126.4979
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.5744
                       Mean reward: 904.42
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 1.9785
     Episode_Reward/lifting_object: 174.4790
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 3.31s
                      Time elapsed: 01:03:06
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 32271 steps/s (collection: 2.918s, learning 0.129s)
             Mean action noise std: 3.62
          Mean value_function loss: 118.6707
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.5825
                       Mean reward: 832.15
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.9315
     Episode_Reward/lifting_object: 170.0618
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 3.05s
                      Time elapsed: 01:03:09
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 35924 steps/s (collection: 2.555s, learning 0.181s)
             Mean action noise std: 3.63
          Mean value_function loss: 108.1614
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.5952
                       Mean reward: 886.44
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.9616
     Episode_Reward/lifting_object: 173.1008
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.74s
                      Time elapsed: 01:03:12
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 34261 steps/s (collection: 2.658s, learning 0.211s)
             Mean action noise std: 3.63
          Mean value_function loss: 126.0646
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.6126
                       Mean reward: 887.74
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 1.9253
     Episode_Reward/lifting_object: 169.0666
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.87s
                      Time elapsed: 01:03:15
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 33399 steps/s (collection: 2.671s, learning 0.273s)
             Mean action noise std: 3.63
          Mean value_function loss: 124.1714
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.6205
                       Mean reward: 865.47
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.9614
     Episode_Reward/lifting_object: 172.6506
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.94s
                      Time elapsed: 01:03:18
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 32524 steps/s (collection: 2.793s, learning 0.230s)
             Mean action noise std: 3.63
          Mean value_function loss: 105.2130
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 69.6318
                       Mean reward: 897.22
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.9682
     Episode_Reward/lifting_object: 172.8173
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0754
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 3.02s
                      Time elapsed: 01:03:21
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 36083 steps/s (collection: 2.613s, learning 0.112s)
             Mean action noise std: 3.63
          Mean value_function loss: 86.8570
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.6421
                       Mean reward: 874.80
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 2.0074
     Episode_Reward/lifting_object: 176.7187
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.72s
                      Time elapsed: 01:03:23
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 33258 steps/s (collection: 2.724s, learning 0.232s)
             Mean action noise std: 3.63
          Mean value_function loss: 94.4829
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.6561
                       Mean reward: 878.93
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 2.0006
     Episode_Reward/lifting_object: 175.8398
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.96s
                      Time elapsed: 01:03:26
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 33315 steps/s (collection: 2.778s, learning 0.173s)
             Mean action noise std: 3.63
          Mean value_function loss: 103.2682
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.6666
                       Mean reward: 876.50
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.9729
     Episode_Reward/lifting_object: 173.5076
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0757
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.95s
                      Time elapsed: 01:03:29
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 36947 steps/s (collection: 2.527s, learning 0.134s)
             Mean action noise std: 3.64
          Mean value_function loss: 116.3076
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.6776
                       Mean reward: 890.16
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.9693
     Episode_Reward/lifting_object: 173.1635
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.66s
                      Time elapsed: 01:03:32
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 36515 steps/s (collection: 2.404s, learning 0.288s)
             Mean action noise std: 3.64
          Mean value_function loss: 117.2119
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.6913
                       Mean reward: 887.92
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 1.9386
     Episode_Reward/lifting_object: 169.9192
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.69s
                      Time elapsed: 01:03:35
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 37812 steps/s (collection: 2.465s, learning 0.135s)
             Mean action noise std: 3.64
          Mean value_function loss: 79.2154
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.7003
                       Mean reward: 895.49
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.9977
     Episode_Reward/lifting_object: 176.1305
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.60s
                      Time elapsed: 01:03:37
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 37790 steps/s (collection: 2.380s, learning 0.222s)
             Mean action noise std: 3.64
          Mean value_function loss: 120.6745
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.7096
                       Mean reward: 896.96
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.9755
     Episode_Reward/lifting_object: 173.9753
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0752
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.60s
                      Time elapsed: 01:03:40
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 36039 steps/s (collection: 2.528s, learning 0.200s)
             Mean action noise std: 3.64
          Mean value_function loss: 98.3267
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 69.7202
                       Mean reward: 870.31
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.9652
     Episode_Reward/lifting_object: 173.4228
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.73s
                      Time elapsed: 01:03:43
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 35311 steps/s (collection: 2.551s, learning 0.233s)
             Mean action noise std: 3.64
          Mean value_function loss: 102.3169
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.7288
                       Mean reward: 859.62
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 1.9436
     Episode_Reward/lifting_object: 171.2715
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.78s
                      Time elapsed: 01:03:45
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 34474 steps/s (collection: 2.699s, learning 0.153s)
             Mean action noise std: 3.65
          Mean value_function loss: 110.8070
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 69.7447
                       Mean reward: 906.51
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.9684
     Episode_Reward/lifting_object: 173.7521
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.85s
                      Time elapsed: 01:03:48
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 36401 steps/s (collection: 2.545s, learning 0.155s)
             Mean action noise std: 3.65
          Mean value_function loss: 143.1128
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.7639
                       Mean reward: 868.42
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.9091
     Episode_Reward/lifting_object: 167.9588
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0743
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.70s
                      Time elapsed: 01:03:51
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 35789 steps/s (collection: 2.507s, learning 0.240s)
             Mean action noise std: 3.65
          Mean value_function loss: 107.4008
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 69.7789
                       Mean reward: 880.12
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.9281
     Episode_Reward/lifting_object: 170.2525
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.75s
                      Time elapsed: 01:03:54
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 34796 steps/s (collection: 2.601s, learning 0.224s)
             Mean action noise std: 3.65
          Mean value_function loss: 113.1334
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.7873
                       Mean reward: 823.08
               Mean episode length: 221.14
    Episode_Reward/reaching_object: 1.9261
     Episode_Reward/lifting_object: 169.7743
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.0742
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.83s
                      Time elapsed: 01:03:56
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 35923 steps/s (collection: 2.550s, learning 0.186s)
             Mean action noise std: 3.65
          Mean value_function loss: 87.7443
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 69.7971
                       Mean reward: 913.55
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 2.0115
     Episode_Reward/lifting_object: 177.6664
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.74s
                      Time elapsed: 01:03:59
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 37108 steps/s (collection: 2.477s, learning 0.173s)
             Mean action noise std: 3.65
          Mean value_function loss: 90.7750
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.8070
                       Mean reward: 874.63
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.9677
     Episode_Reward/lifting_object: 173.8092
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.65s
                      Time elapsed: 01:04:02
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 34152 steps/s (collection: 2.645s, learning 0.233s)
             Mean action noise std: 3.66
          Mean value_function loss: 134.9904
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.8182
                       Mean reward: 895.53
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 1.9616
     Episode_Reward/lifting_object: 172.9387
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.88s
                      Time elapsed: 01:04:05
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 33767 steps/s (collection: 2.661s, learning 0.250s)
             Mean action noise std: 3.66
          Mean value_function loss: 118.0278
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.8334
                       Mean reward: 878.98
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.9389
     Episode_Reward/lifting_object: 170.4165
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.91s
                      Time elapsed: 01:04:08
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 33314 steps/s (collection: 2.811s, learning 0.140s)
             Mean action noise std: 3.66
          Mean value_function loss: 109.5183
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 69.8453
                       Mean reward: 862.41
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.9712
     Episode_Reward/lifting_object: 173.8723
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.95s
                      Time elapsed: 01:04:11
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 35765 steps/s (collection: 2.595s, learning 0.154s)
             Mean action noise std: 3.66
          Mean value_function loss: 136.8527
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.8495
                       Mean reward: 844.58
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 1.8996
     Episode_Reward/lifting_object: 167.2082
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.75s
                      Time elapsed: 01:04:13
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 35403 steps/s (collection: 2.579s, learning 0.198s)
             Mean action noise std: 3.66
          Mean value_function loss: 105.9002
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.8602
                       Mean reward: 853.17
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 1.9362
     Episode_Reward/lifting_object: 170.5451
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.78s
                      Time elapsed: 01:04:16
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 37111 steps/s (collection: 2.421s, learning 0.228s)
             Mean action noise std: 3.66
          Mean value_function loss: 122.7156
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.8748
                       Mean reward: 900.73
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.9771
     Episode_Reward/lifting_object: 174.7271
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.65s
                      Time elapsed: 01:04:19
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 36758 steps/s (collection: 2.530s, learning 0.144s)
             Mean action noise std: 3.67
          Mean value_function loss: 112.8918
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.8875
                       Mean reward: 867.29
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.9362
     Episode_Reward/lifting_object: 170.8659
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0756
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.67s
                      Time elapsed: 01:04:21
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 36678 steps/s (collection: 2.480s, learning 0.200s)
             Mean action noise std: 3.67
          Mean value_function loss: 114.5953
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.9030
                       Mean reward: 905.43
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 1.8767
     Episode_Reward/lifting_object: 165.8077
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.68s
                      Time elapsed: 01:04:24
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 36418 steps/s (collection: 2.560s, learning 0.140s)
             Mean action noise std: 3.67
          Mean value_function loss: 98.5055
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 69.9177
                       Mean reward: 857.21
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 1.9667
     Episode_Reward/lifting_object: 173.9525
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.70s
                      Time elapsed: 01:04:27
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 40595 steps/s (collection: 2.293s, learning 0.129s)
             Mean action noise std: 3.67
          Mean value_function loss: 99.9475
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.9343
                       Mean reward: 912.47
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 1.9739
     Episode_Reward/lifting_object: 174.9937
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.42s
                      Time elapsed: 01:04:29
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 38208 steps/s (collection: 2.410s, learning 0.163s)
             Mean action noise std: 3.67
          Mean value_function loss: 107.2324
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.9436
                       Mean reward: 841.43
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.9802
     Episode_Reward/lifting_object: 175.9731
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0769
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.57s
                      Time elapsed: 01:04:32
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 40971 steps/s (collection: 2.290s, learning 0.109s)
             Mean action noise std: 3.68
          Mean value_function loss: 108.4957
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.9512
                       Mean reward: 797.76
               Mean episode length: 215.67
    Episode_Reward/reaching_object: 1.9476
     Episode_Reward/lifting_object: 172.3447
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0759
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.40s
                      Time elapsed: 01:04:34
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 37380 steps/s (collection: 2.456s, learning 0.174s)
             Mean action noise std: 3.68
          Mean value_function loss: 110.4916
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.9629
                       Mean reward: 888.15
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 1.9469
     Episode_Reward/lifting_object: 172.8244
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0764
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.63s
                      Time elapsed: 01:04:37
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 38970 steps/s (collection: 2.420s, learning 0.102s)
             Mean action noise std: 3.68
          Mean value_function loss: 111.0973
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.9734
                       Mean reward: 875.93
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.9247
     Episode_Reward/lifting_object: 170.3840
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.52s
                      Time elapsed: 01:04:39
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 40767 steps/s (collection: 2.293s, learning 0.119s)
             Mean action noise std: 3.68
          Mean value_function loss: 84.8335
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 69.9869
                       Mean reward: 929.68
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 1.9711
     Episode_Reward/lifting_object: 174.5812
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.41s
                      Time elapsed: 01:04:42
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 40265 steps/s (collection: 2.331s, learning 0.110s)
             Mean action noise std: 3.68
          Mean value_function loss: 121.0927
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.9998
                       Mean reward: 890.15
               Mean episode length: 235.21
    Episode_Reward/reaching_object: 1.9945
     Episode_Reward/lifting_object: 176.4336
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.44s
                      Time elapsed: 01:04:44
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 39861 steps/s (collection: 2.294s, learning 0.173s)
             Mean action noise std: 3.68
          Mean value_function loss: 137.5730
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.0124
                       Mean reward: 874.93
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 1.9408
     Episode_Reward/lifting_object: 171.3484
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.47s
                      Time elapsed: 01:04:47
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 36337 steps/s (collection: 2.562s, learning 0.143s)
             Mean action noise std: 3.69
          Mean value_function loss: 122.0464
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.0270
                       Mean reward: 871.07
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 171.2813
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.71s
                      Time elapsed: 01:04:49
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 32839 steps/s (collection: 2.859s, learning 0.135s)
             Mean action noise std: 3.69
          Mean value_function loss: 109.9109
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 70.0373
                       Mean reward: 884.63
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.9712
     Episode_Reward/lifting_object: 173.4738
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.99s
                      Time elapsed: 01:04:52
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.245s, learning 0.102s)
             Mean action noise std: 3.69
          Mean value_function loss: 109.7473
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.0444
                       Mean reward: 884.32
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.9658
     Episode_Reward/lifting_object: 173.5013
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.35s
                      Time elapsed: 01:04:55
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 42449 steps/s (collection: 2.215s, learning 0.101s)
             Mean action noise std: 3.69
          Mean value_function loss: 79.6568
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.0572
                       Mean reward: 888.99
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 1.9447
     Episode_Reward/lifting_object: 171.6178
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.32s
                      Time elapsed: 01:04:57
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 40670 steps/s (collection: 2.307s, learning 0.111s)
             Mean action noise std: 3.69
          Mean value_function loss: 85.3010
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.0691
                       Mean reward: 912.97
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 1.9831
     Episode_Reward/lifting_object: 174.9173
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.42s
                      Time elapsed: 01:04:59
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 40832 steps/s (collection: 2.292s, learning 0.115s)
             Mean action noise std: 3.69
          Mean value_function loss: 100.1107
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 70.0805
                       Mean reward: 879.68
               Mean episode length: 234.27
    Episode_Reward/reaching_object: 1.9808
     Episode_Reward/lifting_object: 174.2892
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.41s
                      Time elapsed: 01:05:02
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 41462 steps/s (collection: 2.239s, learning 0.132s)
             Mean action noise std: 3.70
          Mean value_function loss: 162.8531
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.0952
                       Mean reward: 873.70
               Mean episode length: 232.68
    Episode_Reward/reaching_object: 1.9628
     Episode_Reward/lifting_object: 172.4527
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.37s
                      Time elapsed: 01:05:04
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 40990 steps/s (collection: 2.277s, learning 0.122s)
             Mean action noise std: 3.70
          Mean value_function loss: 110.6798
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 70.1089
                       Mean reward: 851.39
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.9486
     Episode_Reward/lifting_object: 171.3857
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.40s
                      Time elapsed: 01:05:07
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 40967 steps/s (collection: 2.294s, learning 0.106s)
             Mean action noise std: 3.70
          Mean value_function loss: 140.6379
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 70.1306
                       Mean reward: 857.46
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 174.0760
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.40s
                      Time elapsed: 01:05:09
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 40938 steps/s (collection: 2.267s, learning 0.135s)
             Mean action noise std: 3.70
          Mean value_function loss: 115.7974
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.1467
                       Mean reward: 865.78
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 1.9709
     Episode_Reward/lifting_object: 172.4979
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.40s
                      Time elapsed: 01:05:11
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 36500 steps/s (collection: 2.565s, learning 0.128s)
             Mean action noise std: 3.70
          Mean value_function loss: 126.3371
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.1532
                       Mean reward: 875.86
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 1.9467
     Episode_Reward/lifting_object: 170.8672
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0776
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.69s
                      Time elapsed: 01:05:14
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 37938 steps/s (collection: 2.450s, learning 0.141s)
             Mean action noise std: 3.70
          Mean value_function loss: 130.3891
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.1614
                       Mean reward: 863.22
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 1.9890
     Episode_Reward/lifting_object: 174.7790
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.59s
                      Time elapsed: 01:05:17
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 39812 steps/s (collection: 2.344s, learning 0.125s)
             Mean action noise std: 3.71
          Mean value_function loss: 106.1046
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 70.1716
                       Mean reward: 879.00
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.9758
     Episode_Reward/lifting_object: 173.6860
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.47s
                      Time elapsed: 01:05:19
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 37301 steps/s (collection: 2.451s, learning 0.185s)
             Mean action noise std: 3.71
          Mean value_function loss: 117.2694
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.1861
                       Mean reward: 878.83
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.9591
     Episode_Reward/lifting_object: 172.4857
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.64s
                      Time elapsed: 01:05:22
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 40074 steps/s (collection: 2.331s, learning 0.122s)
             Mean action noise std: 3.71
          Mean value_function loss: 123.9692
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.1978
                       Mean reward: 889.98
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.9649
     Episode_Reward/lifting_object: 172.5168
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.45s
                      Time elapsed: 01:05:24
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 40256 steps/s (collection: 2.313s, learning 0.129s)
             Mean action noise std: 3.71
          Mean value_function loss: 145.5305
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.2119
                       Mean reward: 891.37
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 1.9556
     Episode_Reward/lifting_object: 171.8274
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.44s
                      Time elapsed: 01:05:27
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 39847 steps/s (collection: 2.307s, learning 0.160s)
             Mean action noise std: 3.72
          Mean value_function loss: 135.0101
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.2316
                       Mean reward: 853.15
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 1.9718
     Episode_Reward/lifting_object: 173.1420
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.47s
                      Time elapsed: 01:05:29
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 40000 steps/s (collection: 2.348s, learning 0.109s)
             Mean action noise std: 3.72
          Mean value_function loss: 144.5172
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.2540
                       Mean reward: 844.03
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 1.9210
     Episode_Reward/lifting_object: 168.7164
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.46s
                      Time elapsed: 01:05:32
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 42069 steps/s (collection: 2.239s, learning 0.098s)
             Mean action noise std: 3.72
          Mean value_function loss: 138.1893
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.2682
                       Mean reward: 877.94
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.9405
     Episode_Reward/lifting_object: 170.5479
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.34s
                      Time elapsed: 01:05:34
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 40913 steps/s (collection: 2.292s, learning 0.111s)
             Mean action noise std: 3.72
          Mean value_function loss: 117.4804
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.2774
                       Mean reward: 885.41
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.9796
     Episode_Reward/lifting_object: 174.4714
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.40s
                      Time elapsed: 01:05:36
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 41428 steps/s (collection: 2.261s, learning 0.112s)
             Mean action noise std: 3.72
          Mean value_function loss: 131.5046
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.2910
                       Mean reward: 871.38
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.9724
     Episode_Reward/lifting_object: 173.6190
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.37s
                      Time elapsed: 01:05:39
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 41944 steps/s (collection: 2.241s, learning 0.103s)
             Mean action noise std: 3.72
          Mean value_function loss: 145.2134
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.3017
                       Mean reward: 841.33
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.9255
     Episode_Reward/lifting_object: 169.2264
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.34s
                      Time elapsed: 01:05:41
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 41126 steps/s (collection: 2.279s, learning 0.111s)
             Mean action noise std: 3.73
          Mean value_function loss: 120.5507
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.3105
                       Mean reward: 892.74
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 1.9469
     Episode_Reward/lifting_object: 171.5953
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.39s
                      Time elapsed: 01:05:43
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 40719 steps/s (collection: 2.274s, learning 0.140s)
             Mean action noise std: 3.73
          Mean value_function loss: 159.0618
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.3241
                       Mean reward: 809.34
               Mean episode length: 218.05
    Episode_Reward/reaching_object: 1.9179
     Episode_Reward/lifting_object: 168.5943
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.41s
                      Time elapsed: 01:05:46
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 41050 steps/s (collection: 2.289s, learning 0.106s)
             Mean action noise std: 3.73
          Mean value_function loss: 123.2259
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.3348
                       Mean reward: 890.39
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.9718
     Episode_Reward/lifting_object: 173.5975
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.39s
                      Time elapsed: 01:05:48
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 41083 steps/s (collection: 2.272s, learning 0.121s)
             Mean action noise std: 3.73
          Mean value_function loss: 115.1435
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.3464
                       Mean reward: 837.78
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.9685
     Episode_Reward/lifting_object: 173.2350
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.39s
                      Time elapsed: 01:05:51
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 38867 steps/s (collection: 2.366s, learning 0.164s)
             Mean action noise std: 3.73
          Mean value_function loss: 121.0999
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 70.3564
                       Mean reward: 873.69
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.9461
     Episode_Reward/lifting_object: 171.4305
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.53s
                      Time elapsed: 01:05:53
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 38843 steps/s (collection: 2.400s, learning 0.131s)
             Mean action noise std: 3.73
          Mean value_function loss: 110.6656
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.3590
                       Mean reward: 825.37
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 1.9428
     Episode_Reward/lifting_object: 171.4048
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.53s
                      Time elapsed: 01:05:56
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 37134 steps/s (collection: 2.480s, learning 0.167s)
             Mean action noise std: 3.73
          Mean value_function loss: 113.3706
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.3651
                       Mean reward: 889.19
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.9784
     Episode_Reward/lifting_object: 175.0330
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.65s
                      Time elapsed: 01:05:58
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 35485 steps/s (collection: 2.566s, learning 0.205s)
             Mean action noise std: 3.74
          Mean value_function loss: 102.3988
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.3749
                       Mean reward: 850.13
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 1.9418
     Episode_Reward/lifting_object: 171.2323
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.77s
                      Time elapsed: 01:06:01
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 37296 steps/s (collection: 2.459s, learning 0.176s)
             Mean action noise std: 3.74
          Mean value_function loss: 96.3476
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.3893
                       Mean reward: 882.32
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 2.0087
     Episode_Reward/lifting_object: 177.3288
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.64s
                      Time elapsed: 01:06:04
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 37175 steps/s (collection: 2.447s, learning 0.197s)
             Mean action noise std: 3.74
          Mean value_function loss: 135.0168
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.3986
                       Mean reward: 839.92
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 1.9407
     Episode_Reward/lifting_object: 170.8094
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.64s
                      Time elapsed: 01:06:06
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 37982 steps/s (collection: 2.409s, learning 0.179s)
             Mean action noise std: 3.74
          Mean value_function loss: 103.0816
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.4060
                       Mean reward: 879.71
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 1.9861
     Episode_Reward/lifting_object: 175.3604
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.59s
                      Time elapsed: 01:06:09
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 37894 steps/s (collection: 2.408s, learning 0.187s)
             Mean action noise std: 3.74
          Mean value_function loss: 103.9317
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.4145
                       Mean reward: 885.25
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 1.9849
     Episode_Reward/lifting_object: 175.4078
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.59s
                      Time elapsed: 01:06:12
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 36909 steps/s (collection: 2.533s, learning 0.130s)
             Mean action noise std: 3.74
          Mean value_function loss: 107.4346
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.4224
                       Mean reward: 881.16
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 1.9655
     Episode_Reward/lifting_object: 173.7081
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.66s
                      Time elapsed: 01:06:14
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 37053 steps/s (collection: 2.463s, learning 0.190s)
             Mean action noise std: 3.75
          Mean value_function loss: 102.0499
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.4364
                       Mean reward: 868.94
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.9985
     Episode_Reward/lifting_object: 176.3756
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.65s
                      Time elapsed: 01:06:17
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 38426 steps/s (collection: 2.372s, learning 0.186s)
             Mean action noise std: 3.75
          Mean value_function loss: 127.2874
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 70.4485
                       Mean reward: 875.37
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.9581
     Episode_Reward/lifting_object: 172.4141
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.56s
                      Time elapsed: 01:06:19
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 37650 steps/s (collection: 2.449s, learning 0.162s)
             Mean action noise std: 3.75
          Mean value_function loss: 113.3902
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.4559
                       Mean reward: 886.89
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.9628
     Episode_Reward/lifting_object: 173.1637
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.61s
                      Time elapsed: 01:06:22
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 37727 steps/s (collection: 2.452s, learning 0.154s)
             Mean action noise std: 3.75
          Mean value_function loss: 119.6507
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.4694
                       Mean reward: 850.23
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 1.9540
     Episode_Reward/lifting_object: 171.8197
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.61s
                      Time elapsed: 01:06:25
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 36976 steps/s (collection: 2.519s, learning 0.140s)
             Mean action noise std: 3.75
          Mean value_function loss: 122.3816
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.4761
                       Mean reward: 840.58
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 1.9538
     Episode_Reward/lifting_object: 172.1255
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.66s
                      Time elapsed: 01:06:27
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 37897 steps/s (collection: 2.409s, learning 0.185s)
             Mean action noise std: 3.75
          Mean value_function loss: 135.0127
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.4862
                       Mean reward: 900.79
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 1.9461
     Episode_Reward/lifting_object: 171.3208
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.59s
                      Time elapsed: 01:06:30
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 37255 steps/s (collection: 2.423s, learning 0.216s)
             Mean action noise std: 3.75
          Mean value_function loss: 101.2890
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 70.5014
                       Mean reward: 875.11
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.9684
     Episode_Reward/lifting_object: 172.9383
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.64s
                      Time elapsed: 01:06:33
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 39523 steps/s (collection: 2.307s, learning 0.181s)
             Mean action noise std: 3.76
          Mean value_function loss: 118.4992
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 70.5110
                       Mean reward: 878.61
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 1.9723
     Episode_Reward/lifting_object: 173.4992
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.49s
                      Time elapsed: 01:06:35
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 38908 steps/s (collection: 2.410s, learning 0.116s)
             Mean action noise std: 3.76
          Mean value_function loss: 157.5388
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.5135
                       Mean reward: 866.66
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.9823
     Episode_Reward/lifting_object: 174.4955
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.53s
                      Time elapsed: 01:06:38
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 37530 steps/s (collection: 2.478s, learning 0.142s)
             Mean action noise std: 3.76
          Mean value_function loss: 127.6679
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.5199
                       Mean reward: 859.31
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.9511
     Episode_Reward/lifting_object: 171.4412
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.62s
                      Time elapsed: 01:06:40
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 37313 steps/s (collection: 2.427s, learning 0.207s)
             Mean action noise std: 3.76
          Mean value_function loss: 124.1926
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.5364
                       Mean reward: 853.69
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.9245
     Episode_Reward/lifting_object: 168.8518
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.63s
                      Time elapsed: 01:06:43
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 36679 steps/s (collection: 2.521s, learning 0.159s)
             Mean action noise std: 3.76
          Mean value_function loss: 138.9421
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.5509
                       Mean reward: 845.14
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 1.9614
     Episode_Reward/lifting_object: 172.2607
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.68s
                      Time elapsed: 01:06:46
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 38818 steps/s (collection: 2.373s, learning 0.160s)
             Mean action noise std: 3.76
          Mean value_function loss: 113.2250
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.5660
                       Mean reward: 852.56
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 1.9920
     Episode_Reward/lifting_object: 174.9297
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.53s
                      Time elapsed: 01:06:48
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 38001 steps/s (collection: 2.381s, learning 0.206s)
             Mean action noise std: 3.77
          Mean value_function loss: 115.8321
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.5821
                       Mean reward: 869.88
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 2.0011
     Episode_Reward/lifting_object: 175.8972
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.59s
                      Time elapsed: 01:06:51
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 38874 steps/s (collection: 2.360s, learning 0.169s)
             Mean action noise std: 3.77
          Mean value_function loss: 130.4180
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.5946
                       Mean reward: 844.67
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.9131
     Episode_Reward/lifting_object: 168.0583
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.53s
                      Time elapsed: 01:06:53
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 38445 steps/s (collection: 2.415s, learning 0.142s)
             Mean action noise std: 3.77
          Mean value_function loss: 103.5375
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.6055
                       Mean reward: 865.97
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.9699
     Episode_Reward/lifting_object: 173.3367
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.56s
                      Time elapsed: 01:06:56
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 36099 steps/s (collection: 2.539s, learning 0.184s)
             Mean action noise std: 3.77
          Mean value_function loss: 118.8964
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.6162
                       Mean reward: 877.95
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 1.9734
     Episode_Reward/lifting_object: 173.4515
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0782
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.72s
                      Time elapsed: 01:06:58
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 36974 steps/s (collection: 2.474s, learning 0.185s)
             Mean action noise std: 3.77
          Mean value_function loss: 115.3435
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 70.6259
                       Mean reward: 879.26
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.9890
     Episode_Reward/lifting_object: 173.8749
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.66s
                      Time elapsed: 01:07:01
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 37886 steps/s (collection: 2.390s, learning 0.205s)
             Mean action noise std: 3.77
          Mean value_function loss: 110.4550
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.6371
                       Mean reward: 874.52
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.9806
     Episode_Reward/lifting_object: 174.2968
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.59s
                      Time elapsed: 01:07:04
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 39674 steps/s (collection: 2.358s, learning 0.120s)
             Mean action noise std: 3.78
          Mean value_function loss: 127.4663
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.6425
                       Mean reward: 861.98
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: 172.7454
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.48s
                      Time elapsed: 01:07:06
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 38023 steps/s (collection: 2.442s, learning 0.143s)
             Mean action noise std: 3.78
          Mean value_function loss: 104.5304
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.6535
                       Mean reward: 873.33
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 1.9351
     Episode_Reward/lifting_object: 169.8769
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.59s
                      Time elapsed: 01:07:09
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 39755 steps/s (collection: 2.363s, learning 0.110s)
             Mean action noise std: 3.78
          Mean value_function loss: 123.5927
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.6645
                       Mean reward: 913.84
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 1.9626
     Episode_Reward/lifting_object: 173.3040
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.47s
                      Time elapsed: 01:07:11
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 39440 steps/s (collection: 2.342s, learning 0.150s)
             Mean action noise std: 3.78
          Mean value_function loss: 109.9769
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.6760
                       Mean reward: 889.70
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.9629
     Episode_Reward/lifting_object: 173.0707
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.49s
                      Time elapsed: 01:07:14
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 35321 steps/s (collection: 2.585s, learning 0.198s)
             Mean action noise std: 3.78
          Mean value_function loss: 124.3854
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.6919
                       Mean reward: 868.98
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 1.9543
     Episode_Reward/lifting_object: 172.7295
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.78s
                      Time elapsed: 01:07:17
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 37267 steps/s (collection: 2.466s, learning 0.172s)
             Mean action noise std: 3.78
          Mean value_function loss: 123.1710
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 70.7027
                       Mean reward: 846.10
               Mean episode length: 227.52
    Episode_Reward/reaching_object: 1.9447
     Episode_Reward/lifting_object: 171.3974
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.64s
                      Time elapsed: 01:07:19
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 38443 steps/s (collection: 2.402s, learning 0.155s)
             Mean action noise std: 3.78
          Mean value_function loss: 155.2657
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 70.7051
                       Mean reward: 852.29
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9571
     Episode_Reward/lifting_object: 173.2847
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.56s
                      Time elapsed: 01:07:22
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 36795 steps/s (collection: 2.509s, learning 0.163s)
             Mean action noise std: 3.79
          Mean value_function loss: 170.1177
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.7100
                       Mean reward: 866.97
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.9041
     Episode_Reward/lifting_object: 167.8036
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.67s
                      Time elapsed: 01:07:24
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 32951 steps/s (collection: 2.731s, learning 0.252s)
             Mean action noise std: 3.79
          Mean value_function loss: 105.0667
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.7203
                       Mean reward: 883.32
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 1.9679
     Episode_Reward/lifting_object: 173.4313
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.98s
                      Time elapsed: 01:07:27
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 35041 steps/s (collection: 2.596s, learning 0.210s)
             Mean action noise std: 3.79
          Mean value_function loss: 127.5597
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.7317
                       Mean reward: 877.00
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.9286
     Episode_Reward/lifting_object: 170.5971
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.81s
                      Time elapsed: 01:07:30
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 41330 steps/s (collection: 2.267s, learning 0.112s)
             Mean action noise std: 3.79
          Mean value_function loss: 107.9693
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.7407
                       Mean reward: 881.39
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.9650
     Episode_Reward/lifting_object: 173.9053
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.38s
                      Time elapsed: 01:07:33
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 42268 steps/s (collection: 2.219s, learning 0.107s)
             Mean action noise std: 3.79
          Mean value_function loss: 120.3962
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.7527
                       Mean reward: 848.73
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 1.9508
     Episode_Reward/lifting_object: 172.9534
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.33s
                      Time elapsed: 01:07:35
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 40413 steps/s (collection: 2.325s, learning 0.107s)
             Mean action noise std: 3.79
          Mean value_function loss: 111.3474
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.7668
                       Mean reward: 843.43
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 1.9298
     Episode_Reward/lifting_object: 170.8374
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.43s
                      Time elapsed: 01:07:37
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 42208 steps/s (collection: 2.229s, learning 0.100s)
             Mean action noise std: 3.80
          Mean value_function loss: 107.6796
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.7781
                       Mean reward: 905.44
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 1.9685
     Episode_Reward/lifting_object: 174.5089
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.33s
                      Time elapsed: 01:07:40
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 41863 steps/s (collection: 2.248s, learning 0.101s)
             Mean action noise std: 3.80
          Mean value_function loss: 101.2342
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.7888
                       Mean reward: 869.21
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 172.9868
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.35s
                      Time elapsed: 01:07:42
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 40726 steps/s (collection: 2.282s, learning 0.132s)
             Mean action noise std: 3.80
          Mean value_function loss: 102.5021
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.8026
                       Mean reward: 887.19
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.9456
     Episode_Reward/lifting_object: 172.0325
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.41s
                      Time elapsed: 01:07:44
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 36639 steps/s (collection: 2.553s, learning 0.130s)
             Mean action noise std: 3.80
          Mean value_function loss: 111.3904
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.8150
                       Mean reward: 853.03
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 1.9259
     Episode_Reward/lifting_object: 170.0925
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.68s
                      Time elapsed: 01:07:47
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 40805 steps/s (collection: 2.260s, learning 0.149s)
             Mean action noise std: 3.80
          Mean value_function loss: 107.9692
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.8256
                       Mean reward: 881.75
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 1.9784
     Episode_Reward/lifting_object: 174.7383
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.41s
                      Time elapsed: 01:07:50
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 38454 steps/s (collection: 2.438s, learning 0.119s)
             Mean action noise std: 3.81
          Mean value_function loss: 105.7100
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.8388
                       Mean reward: 869.22
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 1.9625
     Episode_Reward/lifting_object: 173.6474
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.56s
                      Time elapsed: 01:07:52
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 40860 steps/s (collection: 2.299s, learning 0.107s)
             Mean action noise std: 3.81
          Mean value_function loss: 124.0424
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.8480
                       Mean reward: 879.27
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 1.9356
     Episode_Reward/lifting_object: 170.9300
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.41s
                      Time elapsed: 01:07:54
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 41316 steps/s (collection: 2.261s, learning 0.118s)
             Mean action noise std: 3.81
          Mean value_function loss: 104.6023
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.8611
                       Mean reward: 881.66
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 1.9380
     Episode_Reward/lifting_object: 171.5279
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.38s
                      Time elapsed: 01:07:57
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 40478 steps/s (collection: 2.323s, learning 0.105s)
             Mean action noise std: 3.81
          Mean value_function loss: 95.9041
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.8745
                       Mean reward: 893.42
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.9799
     Episode_Reward/lifting_object: 175.0462
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.43s
                      Time elapsed: 01:07:59
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 39872 steps/s (collection: 2.353s, learning 0.112s)
             Mean action noise std: 3.81
          Mean value_function loss: 109.5684
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 70.8804
                       Mean reward: 876.18
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.9411
     Episode_Reward/lifting_object: 171.6793
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.47s
                      Time elapsed: 01:08:02
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 41302 steps/s (collection: 2.274s, learning 0.106s)
             Mean action noise std: 3.81
          Mean value_function loss: 116.2813
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.8889
                       Mean reward: 880.93
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.9863
     Episode_Reward/lifting_object: 175.7408
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.38s
                      Time elapsed: 01:08:04
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 41453 steps/s (collection: 2.266s, learning 0.106s)
             Mean action noise std: 3.81
          Mean value_function loss: 87.3601
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.8941
                       Mean reward: 874.01
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 1.9774
     Episode_Reward/lifting_object: 175.2140
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.37s
                      Time elapsed: 01:08:06
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 41277 steps/s (collection: 2.280s, learning 0.102s)
             Mean action noise std: 3.82
          Mean value_function loss: 106.9203
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.9019
                       Mean reward: 896.45
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 1.9691
     Episode_Reward/lifting_object: 174.3114
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.38s
                      Time elapsed: 01:08:09
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 41871 steps/s (collection: 2.240s, learning 0.108s)
             Mean action noise std: 3.82
          Mean value_function loss: 117.3080
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.9149
                       Mean reward: 888.34
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.9736
     Episode_Reward/lifting_object: 174.4516
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.35s
                      Time elapsed: 01:08:11
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 41558 steps/s (collection: 2.233s, learning 0.133s)
             Mean action noise std: 3.82
          Mean value_function loss: 78.5916
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.9302
                       Mean reward: 904.67
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 2.0010
     Episode_Reward/lifting_object: 176.6625
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.37s
                      Time elapsed: 01:08:14
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 40182 steps/s (collection: 2.316s, learning 0.131s)
             Mean action noise std: 3.82
          Mean value_function loss: 103.6284
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.9419
                       Mean reward: 856.98
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 1.9650
     Episode_Reward/lifting_object: 173.9363
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.45s
                      Time elapsed: 01:08:16
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 40843 steps/s (collection: 2.304s, learning 0.103s)
             Mean action noise std: 3.82
          Mean value_function loss: 124.2703
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.9503
                       Mean reward: 894.81
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.9410
     Episode_Reward/lifting_object: 171.6042
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.41s
                      Time elapsed: 01:08:18
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 40791 steps/s (collection: 2.298s, learning 0.112s)
             Mean action noise std: 3.82
          Mean value_function loss: 104.9608
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.9567
                       Mean reward: 883.50
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.9962
     Episode_Reward/lifting_object: 176.6471
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.41s
                      Time elapsed: 01:08:21
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 41361 steps/s (collection: 2.266s, learning 0.111s)
             Mean action noise std: 3.83
          Mean value_function loss: 137.7075
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 70.9658
                       Mean reward: 871.04
               Mean episode length: 230.88
    Episode_Reward/reaching_object: 1.9204
     Episode_Reward/lifting_object: 169.6098
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.38s
                      Time elapsed: 01:08:23
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 40901 steps/s (collection: 2.275s, learning 0.128s)
             Mean action noise std: 3.83
          Mean value_function loss: 112.6555
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.9735
                       Mean reward: 892.57
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.9451
     Episode_Reward/lifting_object: 172.1269
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.40s
                      Time elapsed: 01:08:26
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 41839 steps/s (collection: 2.244s, learning 0.106s)
             Mean action noise std: 3.83
          Mean value_function loss: 130.6420
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.9853
                       Mean reward: 882.49
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 1.9551
     Episode_Reward/lifting_object: 172.6957
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.35s
                      Time elapsed: 01:08:28
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 41382 steps/s (collection: 2.265s, learning 0.111s)
             Mean action noise std: 3.83
          Mean value_function loss: 99.6160
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.9983
                       Mean reward: 872.34
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.9571
     Episode_Reward/lifting_object: 173.1687
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.38s
                      Time elapsed: 01:08:30
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 39816 steps/s (collection: 2.361s, learning 0.108s)
             Mean action noise std: 3.83
          Mean value_function loss: 102.3661
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.0072
                       Mean reward: 892.91
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.9701
     Episode_Reward/lifting_object: 174.7336
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.47s
                      Time elapsed: 01:08:33
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 41221 steps/s (collection: 2.277s, learning 0.108s)
             Mean action noise std: 3.83
          Mean value_function loss: 148.9741
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.0131
                       Mean reward: 842.66
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 1.8806
     Episode_Reward/lifting_object: 165.9017
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.38s
                      Time elapsed: 01:08:35
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 40436 steps/s (collection: 2.295s, learning 0.136s)
             Mean action noise std: 3.83
          Mean value_function loss: 90.0559
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.0232
                       Mean reward: 911.39
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 1.9857
     Episode_Reward/lifting_object: 175.9320
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.43s
                      Time elapsed: 01:08:38
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 40429 steps/s (collection: 2.328s, learning 0.104s)
             Mean action noise std: 3.84
          Mean value_function loss: 97.9256
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.0381
                       Mean reward: 886.09
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.9795
     Episode_Reward/lifting_object: 175.3478
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.43s
                      Time elapsed: 01:08:40
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 39472 steps/s (collection: 2.371s, learning 0.119s)
             Mean action noise std: 3.84
          Mean value_function loss: 123.2251
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.0536
                       Mean reward: 842.73
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.9616
     Episode_Reward/lifting_object: 173.6538
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.49s
                      Time elapsed: 01:08:43
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 41116 steps/s (collection: 2.281s, learning 0.110s)
             Mean action noise std: 3.84
          Mean value_function loss: 107.7896
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.0622
                       Mean reward: 891.34
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.9595
     Episode_Reward/lifting_object: 173.4843
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.39s
                      Time elapsed: 01:08:45
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 41533 steps/s (collection: 2.265s, learning 0.102s)
             Mean action noise std: 3.84
          Mean value_function loss: 101.2343
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.0683
                       Mean reward: 886.88
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.9734
     Episode_Reward/lifting_object: 174.8591
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.37s
                      Time elapsed: 01:08:47
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 39691 steps/s (collection: 2.330s, learning 0.147s)
             Mean action noise std: 3.84
          Mean value_function loss: 114.5736
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.0791
                       Mean reward: 882.08
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.9792
     Episode_Reward/lifting_object: 175.0608
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.48s
                      Time elapsed: 01:08:50
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 39776 steps/s (collection: 2.350s, learning 0.121s)
             Mean action noise std: 3.84
          Mean value_function loss: 134.3878
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.0855
                       Mean reward: 854.82
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 1.9682
     Episode_Reward/lifting_object: 174.3439
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.47s
                      Time elapsed: 01:08:52
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 40761 steps/s (collection: 2.295s, learning 0.117s)
             Mean action noise std: 3.85
          Mean value_function loss: 101.2903
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.0979
                       Mean reward: 886.36
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 1.9452
     Episode_Reward/lifting_object: 171.6445
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.41s
                      Time elapsed: 01:08:55
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 40985 steps/s (collection: 2.283s, learning 0.116s)
             Mean action noise std: 3.85
          Mean value_function loss: 133.0348
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.1129
                       Mean reward: 823.11
               Mean episode length: 221.67
    Episode_Reward/reaching_object: 1.9308
     Episode_Reward/lifting_object: 170.2528
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.40s
                      Time elapsed: 01:08:57
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 40471 steps/s (collection: 2.322s, learning 0.107s)
             Mean action noise std: 3.85
          Mean value_function loss: 137.7403
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.1256
                       Mean reward: 872.14
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.9480
     Episode_Reward/lifting_object: 171.7452
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.43s
                      Time elapsed: 01:09:00
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 41482 steps/s (collection: 2.254s, learning 0.116s)
             Mean action noise std: 3.85
          Mean value_function loss: 107.7417
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.1317
                       Mean reward: 815.60
               Mean episode length: 218.97
    Episode_Reward/reaching_object: 1.9680
     Episode_Reward/lifting_object: 173.5869
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.37s
                      Time elapsed: 01:09:02
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 41407 steps/s (collection: 2.267s, learning 0.107s)
             Mean action noise std: 3.85
          Mean value_function loss: 134.1552
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.1379
                       Mean reward: 888.60
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 1.9361
     Episode_Reward/lifting_object: 170.3631
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.37s
                      Time elapsed: 01:09:04
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 40158 steps/s (collection: 2.334s, learning 0.114s)
             Mean action noise std: 3.85
          Mean value_function loss: 91.1461
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.1456
                       Mean reward: 869.97
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.9702
     Episode_Reward/lifting_object: 173.7206
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.45s
                      Time elapsed: 01:09:07
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 40370 steps/s (collection: 2.312s, learning 0.123s)
             Mean action noise std: 3.85
          Mean value_function loss: 104.1370
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.1560
                       Mean reward: 850.73
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.9513
     Episode_Reward/lifting_object: 172.1582
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.44s
                      Time elapsed: 01:09:09
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 38617 steps/s (collection: 2.430s, learning 0.116s)
             Mean action noise std: 3.86
          Mean value_function loss: 93.4359
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 71.1680
                       Mean reward: 898.07
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.9975
     Episode_Reward/lifting_object: 176.3653
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.55s
                      Time elapsed: 01:09:12
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 39173 steps/s (collection: 2.399s, learning 0.111s)
             Mean action noise std: 3.86
          Mean value_function loss: 109.3605
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.1802
                       Mean reward: 918.93
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 2.0028
     Episode_Reward/lifting_object: 176.9452
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.51s
                      Time elapsed: 01:09:14
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 40463 steps/s (collection: 2.302s, learning 0.128s)
             Mean action noise std: 3.86
          Mean value_function loss: 137.7408
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.1870
                       Mean reward: 840.93
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.9696
     Episode_Reward/lifting_object: 173.7764
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.43s
                      Time elapsed: 01:09:17
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 24925 steps/s (collection: 3.829s, learning 0.115s)
             Mean action noise std: 3.86
          Mean value_function loss: 95.0966
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.1962
                       Mean reward: 849.30
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 2.0006
     Episode_Reward/lifting_object: 176.4059
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.94s
                      Time elapsed: 01:09:21
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13203 steps/s (collection: 7.319s, learning 0.126s)
             Mean action noise std: 3.86
          Mean value_function loss: 101.0469
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.2168
                       Mean reward: 878.84
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 172.4657
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.45s
                      Time elapsed: 01:09:28
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13322 steps/s (collection: 7.265s, learning 0.114s)
             Mean action noise std: 3.87
          Mean value_function loss: 87.2537
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.2315
                       Mean reward: 890.59
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 1.9558
     Episode_Reward/lifting_object: 171.7480
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.38s
                      Time elapsed: 01:09:35
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13546 steps/s (collection: 7.134s, learning 0.123s)
             Mean action noise std: 3.87
          Mean value_function loss: 94.5334
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.2396
                       Mean reward: 874.13
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.9921
     Episode_Reward/lifting_object: 176.2197
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.26s
                      Time elapsed: 01:09:43
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 13576 steps/s (collection: 7.123s, learning 0.118s)
             Mean action noise std: 3.87
          Mean value_function loss: 89.0410
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.2457
                       Mean reward: 889.77
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 2.0081
     Episode_Reward/lifting_object: 177.1900
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0787
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.24s
                      Time elapsed: 01:09:50
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13675 steps/s (collection: 7.052s, learning 0.137s)
             Mean action noise std: 3.87
          Mean value_function loss: 104.1502
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.2523
                       Mean reward: 890.38
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.9694
     Episode_Reward/lifting_object: 173.7441
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.19s
                      Time elapsed: 01:09:57
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13698 steps/s (collection: 7.061s, learning 0.115s)
             Mean action noise std: 3.87
          Mean value_function loss: 98.9642
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.2613
                       Mean reward: 904.37
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 1.9786
     Episode_Reward/lifting_object: 174.6945
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.18s
                      Time elapsed: 01:10:04
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13884 steps/s (collection: 6.965s, learning 0.115s)
             Mean action noise std: 3.87
          Mean value_function loss: 111.6664
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.2735
                       Mean reward: 885.58
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 1.9405
     Episode_Reward/lifting_object: 171.5122
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0766
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.08s
                      Time elapsed: 01:10:11
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13710 steps/s (collection: 7.048s, learning 0.122s)
             Mean action noise std: 3.87
          Mean value_function loss: 91.0271
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.2843
                       Mean reward: 897.85
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.9864
     Episode_Reward/lifting_object: 175.2313
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0794
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.17s
                      Time elapsed: 01:10:18
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 21504 steps/s (collection: 4.485s, learning 0.087s)
             Mean action noise std: 3.87
          Mean value_function loss: 110.7095
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.2881
                       Mean reward: 895.10
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 1.9735
     Episode_Reward/lifting_object: 174.6640
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.57s
                      Time elapsed: 01:10:23
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 42646 steps/s (collection: 2.202s, learning 0.103s)
             Mean action noise std: 3.87
          Mean value_function loss: 127.7654
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.2928
                       Mean reward: 895.54
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.9258
     Episode_Reward/lifting_object: 170.3115
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.31s
                      Time elapsed: 01:10:25
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 42202 steps/s (collection: 2.226s, learning 0.103s)
             Mean action noise std: 3.88
          Mean value_function loss: 115.9078
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.3015
                       Mean reward: 887.05
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 1.9648
     Episode_Reward/lifting_object: 173.8783
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.33s
                      Time elapsed: 01:10:28
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 41207 steps/s (collection: 2.276s, learning 0.109s)
             Mean action noise std: 3.88
          Mean value_function loss: 110.9055
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.3134
                       Mean reward: 891.40
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.9377
     Episode_Reward/lifting_object: 171.4781
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.39s
                      Time elapsed: 01:10:30
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 42042 steps/s (collection: 2.215s, learning 0.123s)
             Mean action noise std: 3.88
          Mean value_function loss: 110.0530
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.3287
                       Mean reward: 884.73
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 1.9060
     Episode_Reward/lifting_object: 168.3454
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.34s
                      Time elapsed: 01:10:32
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 42546 steps/s (collection: 2.207s, learning 0.104s)
             Mean action noise std: 3.88
          Mean value_function loss: 95.8490
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.3420
                       Mean reward: 874.82
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.9405
     Episode_Reward/lifting_object: 171.4938
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.31s
                      Time elapsed: 01:10:35
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 42520 steps/s (collection: 2.202s, learning 0.110s)
             Mean action noise std: 3.88
          Mean value_function loss: 92.4177
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.3548
                       Mean reward: 889.53
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.9515
     Episode_Reward/lifting_object: 172.4089
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.31s
                      Time elapsed: 01:10:37
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.242s, learning 0.108s)
             Mean action noise std: 3.89
          Mean value_function loss: 136.9205
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.3688
                       Mean reward: 841.11
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 1.9366
     Episode_Reward/lifting_object: 170.9413
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.35s
                      Time elapsed: 01:10:39
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 42171 steps/s (collection: 2.214s, learning 0.118s)
             Mean action noise std: 3.89
          Mean value_function loss: 123.3660
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.3787
                       Mean reward: 866.22
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 1.9452
     Episode_Reward/lifting_object: 172.0203
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.33s
                      Time elapsed: 01:10:42
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 42145 steps/s (collection: 2.228s, learning 0.105s)
             Mean action noise std: 3.89
          Mean value_function loss: 136.3225
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.3892
                       Mean reward: 776.75
               Mean episode length: 210.15
    Episode_Reward/reaching_object: 1.9107
     Episode_Reward/lifting_object: 168.5298
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.33s
                      Time elapsed: 01:10:44
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 42305 steps/s (collection: 2.218s, learning 0.106s)
             Mean action noise std: 3.89
          Mean value_function loss: 104.2784
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.4012
                       Mean reward: 878.25
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 1.9868
     Episode_Reward/lifting_object: 176.0489
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.32s
                      Time elapsed: 01:10:46
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 41141 steps/s (collection: 2.266s, learning 0.124s)
             Mean action noise std: 3.89
          Mean value_function loss: 114.0083
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.4113
                       Mean reward: 867.20
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 1.9673
     Episode_Reward/lifting_object: 173.8810
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.39s
                      Time elapsed: 01:10:49
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 41225 steps/s (collection: 2.273s, learning 0.111s)
             Mean action noise std: 3.89
          Mean value_function loss: 95.7088
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.4183
                       Mean reward: 897.43
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.9549
     Episode_Reward/lifting_object: 173.0318
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.38s
                      Time elapsed: 01:10:51
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 40647 steps/s (collection: 2.309s, learning 0.109s)
             Mean action noise std: 3.89
          Mean value_function loss: 114.5179
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.4252
                       Mean reward: 860.15
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 1.9614
     Episode_Reward/lifting_object: 173.7184
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.42s
                      Time elapsed: 01:10:54
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 40806 steps/s (collection: 2.288s, learning 0.122s)
             Mean action noise std: 3.90
          Mean value_function loss: 93.1939
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.4337
                       Mean reward: 914.30
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: 173.2929
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.41s
                      Time elapsed: 01:10:56
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 40607 steps/s (collection: 2.317s, learning 0.104s)
             Mean action noise std: 3.90
          Mean value_function loss: 104.4494
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.4476
                       Mean reward: 861.91
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 1.9839
     Episode_Reward/lifting_object: 175.7302
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.42s
                      Time elapsed: 01:10:58
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 41563 steps/s (collection: 2.247s, learning 0.119s)
             Mean action noise std: 3.90
          Mean value_function loss: 118.5251
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.4616
                       Mean reward: 879.90
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.9504
     Episode_Reward/lifting_object: 172.2386
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0798
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.37s
                      Time elapsed: 01:11:01
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 40190 steps/s (collection: 2.309s, learning 0.137s)
             Mean action noise std: 3.90
          Mean value_function loss: 97.2174
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.4734
                       Mean reward: 896.82
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.9669
     Episode_Reward/lifting_object: 174.0813
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0800
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.45s
                      Time elapsed: 01:11:03
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 41676 steps/s (collection: 2.239s, learning 0.120s)
             Mean action noise std: 3.90
          Mean value_function loss: 131.0468
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.4819
                       Mean reward: 882.63
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 1.9516
     Episode_Reward/lifting_object: 172.4402
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0796
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.36s
                      Time elapsed: 01:11:06
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 40291 steps/s (collection: 2.309s, learning 0.131s)
             Mean action noise std: 3.91
          Mean value_function loss: 125.3437
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.4936
                       Mean reward: 856.97
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.9122
     Episode_Reward/lifting_object: 168.8785
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.44s
                      Time elapsed: 01:11:08
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 41578 steps/s (collection: 2.249s, learning 0.116s)
             Mean action noise std: 3.91
          Mean value_function loss: 117.7801
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 71.5075
                       Mean reward: 868.96
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 174.7202
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.36s
                      Time elapsed: 01:11:10
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 40228 steps/s (collection: 2.342s, learning 0.102s)
             Mean action noise std: 3.91
          Mean value_function loss: 121.2116
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 71.5280
                       Mean reward: 859.69
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.9517
     Episode_Reward/lifting_object: 172.1078
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.44s
                      Time elapsed: 01:11:13
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 42522 steps/s (collection: 2.210s, learning 0.102s)
             Mean action noise std: 3.91
          Mean value_function loss: 108.1818
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.5450
                       Mean reward: 903.98
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 1.9804
     Episode_Reward/lifting_object: 174.9483
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0803
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.31s
                      Time elapsed: 01:11:15
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 40939 steps/s (collection: 2.282s, learning 0.120s)
             Mean action noise std: 3.91
          Mean value_function loss: 88.8037
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.5589
                       Mean reward: 887.11
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 1.9681
     Episode_Reward/lifting_object: 173.2691
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.40s
                      Time elapsed: 01:11:18
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 40884 steps/s (collection: 2.277s, learning 0.127s)
             Mean action noise std: 3.92
          Mean value_function loss: 109.1270
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.5707
                       Mean reward: 855.92
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 1.9375
     Episode_Reward/lifting_object: 170.7141
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.40s
                      Time elapsed: 01:11:20
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 41979 steps/s (collection: 2.234s, learning 0.108s)
             Mean action noise std: 3.92
          Mean value_function loss: 109.2802
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.5883
                       Mean reward: 887.71
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.9897
     Episode_Reward/lifting_object: 175.6403
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.34s
                      Time elapsed: 01:11:22
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 41410 steps/s (collection: 2.242s, learning 0.132s)
             Mean action noise std: 3.92
          Mean value_function loss: 128.9347
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 71.5969
                       Mean reward: 843.35
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.8947
     Episode_Reward/lifting_object: 166.8402
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.37s
                      Time elapsed: 01:11:25
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 41833 steps/s (collection: 2.225s, learning 0.125s)
             Mean action noise std: 3.92
          Mean value_function loss: 96.2532
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.6018
                       Mean reward: 859.51
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 1.9789
     Episode_Reward/lifting_object: 174.4450
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.35s
                      Time elapsed: 01:11:27
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 41392 steps/s (collection: 2.271s, learning 0.104s)
             Mean action noise std: 3.92
          Mean value_function loss: 144.9972
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.6093
                       Mean reward: 863.52
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.9739
     Episode_Reward/lifting_object: 173.8840
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.37s
                      Time elapsed: 01:11:29
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 41830 steps/s (collection: 2.245s, learning 0.105s)
             Mean action noise std: 3.92
          Mean value_function loss: 109.8186
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.6192
                       Mean reward: 906.04
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 1.9765
     Episode_Reward/lifting_object: 173.9089
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.35s
                      Time elapsed: 01:11:32
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 41744 steps/s (collection: 2.234s, learning 0.121s)
             Mean action noise std: 3.93
          Mean value_function loss: 127.3306
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.6315
                       Mean reward: 857.30
               Mean episode length: 231.12
    Episode_Reward/reaching_object: 1.9704
     Episode_Reward/lifting_object: 172.5048
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.35s
                      Time elapsed: 01:11:34
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 40878 steps/s (collection: 2.279s, learning 0.126s)
             Mean action noise std: 3.93
          Mean value_function loss: 121.1922
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.6396
                       Mean reward: 869.70
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 1.9264
     Episode_Reward/lifting_object: 169.8434
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.40s
                      Time elapsed: 01:11:36
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 40822 steps/s (collection: 2.282s, learning 0.126s)
             Mean action noise std: 3.93
          Mean value_function loss: 122.8550
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.6444
                       Mean reward: 832.58
               Mean episode length: 223.48
    Episode_Reward/reaching_object: 1.9659
     Episode_Reward/lifting_object: 172.6493
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.41s
                      Time elapsed: 01:11:39
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 39963 steps/s (collection: 2.336s, learning 0.124s)
             Mean action noise std: 3.93
          Mean value_function loss: 138.6919
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.6484
                       Mean reward: 873.22
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.9632
     Episode_Reward/lifting_object: 172.7370
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.46s
                      Time elapsed: 01:11:41
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 40912 steps/s (collection: 2.295s, learning 0.108s)
             Mean action noise std: 3.93
          Mean value_function loss: 118.1273
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.6601
                       Mean reward: 882.59
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.9434
     Episode_Reward/lifting_object: 170.4524
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.40s
                      Time elapsed: 01:11:44
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 42055 steps/s (collection: 2.233s, learning 0.104s)
             Mean action noise std: 3.93
          Mean value_function loss: 110.7898
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.6765
                       Mean reward: 844.84
               Mean episode length: 226.98
    Episode_Reward/reaching_object: 1.9369
     Episode_Reward/lifting_object: 170.0771
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.34s
                      Time elapsed: 01:11:46
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 41971 steps/s (collection: 2.237s, learning 0.106s)
             Mean action noise std: 3.93
          Mean value_function loss: 111.7208
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 71.6854
                       Mean reward: 872.25
               Mean episode length: 231.97
    Episode_Reward/reaching_object: 1.9798
     Episode_Reward/lifting_object: 174.8805
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0808
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.34s
                      Time elapsed: 01:11:48
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 41925 steps/s (collection: 2.238s, learning 0.107s)
             Mean action noise std: 3.94
          Mean value_function loss: 128.1723
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.6999
                       Mean reward: 864.43
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.9281
     Episode_Reward/lifting_object: 169.9145
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0793
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.34s
                      Time elapsed: 01:11:51
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 41848 steps/s (collection: 2.238s, learning 0.111s)
             Mean action noise std: 3.94
          Mean value_function loss: 136.4501
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.7112
                       Mean reward: 844.91
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.9414
     Episode_Reward/lifting_object: 171.2018
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.35s
                      Time elapsed: 01:11:53
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.238s, learning 0.104s)
             Mean action noise std: 3.94
          Mean value_function loss: 152.6769
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.7284
                       Mean reward: 853.90
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 1.9179
     Episode_Reward/lifting_object: 168.3455
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.34s
                      Time elapsed: 01:11:55
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 41584 steps/s (collection: 2.261s, learning 0.103s)
             Mean action noise std: 3.94
          Mean value_function loss: 165.5295
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.7434
                       Mean reward: 851.23
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.8994
     Episode_Reward/lifting_object: 167.2106
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.36s
                      Time elapsed: 01:11:58
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 41590 steps/s (collection: 2.246s, learning 0.118s)
             Mean action noise std: 3.94
          Mean value_function loss: 130.0369
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.7521
                       Mean reward: 903.60
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 1.9445
     Episode_Reward/lifting_object: 171.7480
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.36s
                      Time elapsed: 01:12:00
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 41238 steps/s (collection: 2.265s, learning 0.119s)
             Mean action noise std: 3.95
          Mean value_function loss: 147.2200
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.7582
                       Mean reward: 820.28
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 1.8975
     Episode_Reward/lifting_object: 166.4200
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.38s
                      Time elapsed: 01:12:03
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 41627 steps/s (collection: 2.256s, learning 0.106s)
             Mean action noise std: 3.95
          Mean value_function loss: 143.7149
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.7714
                       Mean reward: 868.13
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.9348
     Episode_Reward/lifting_object: 170.7682
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.36s
                      Time elapsed: 01:12:05
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 40286 steps/s (collection: 2.300s, learning 0.140s)
             Mean action noise std: 3.95
          Mean value_function loss: 148.6927
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.7901
                       Mean reward: 852.89
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 1.8623
     Episode_Reward/lifting_object: 163.8636
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.44s
                      Time elapsed: 01:12:07
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 42149 steps/s (collection: 2.219s, learning 0.114s)
             Mean action noise std: 3.95
          Mean value_function loss: 102.3942
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.8089
                       Mean reward: 860.45
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 1.9924
     Episode_Reward/lifting_object: 175.9221
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.33s
                      Time elapsed: 01:12:10
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 40005 steps/s (collection: 2.342s, learning 0.116s)
             Mean action noise std: 3.96
          Mean value_function loss: 114.4105
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.8229
                       Mean reward: 880.99
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 171.4220
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.46s
                      Time elapsed: 01:12:12
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 39900 steps/s (collection: 2.355s, learning 0.109s)
             Mean action noise std: 3.96
          Mean value_function loss: 138.8166
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.8374
                       Mean reward: 873.36
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.9189
     Episode_Reward/lifting_object: 169.1639
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.46s
                      Time elapsed: 01:12:15
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 40232 steps/s (collection: 2.335s, learning 0.108s)
             Mean action noise std: 3.96
          Mean value_function loss: 126.6156
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.8473
                       Mean reward: 845.12
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 1.9500
     Episode_Reward/lifting_object: 171.8224
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.44s
                      Time elapsed: 01:12:17
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 41322 steps/s (collection: 2.275s, learning 0.104s)
             Mean action noise std: 3.96
          Mean value_function loss: 126.9338
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.8633
                       Mean reward: 880.81
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 1.9617
     Episode_Reward/lifting_object: 172.9999
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.38s
                      Time elapsed: 01:12:19
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 41908 steps/s (collection: 2.238s, learning 0.107s)
             Mean action noise std: 3.97
          Mean value_function loss: 132.9025
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.8815
                       Mean reward: 834.79
               Mean episode length: 224.12
    Episode_Reward/reaching_object: 1.9419
     Episode_Reward/lifting_object: 171.0852
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.35s
                      Time elapsed: 01:12:22
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 41251 steps/s (collection: 2.269s, learning 0.114s)
             Mean action noise std: 3.97
          Mean value_function loss: 157.9245
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.8887
                       Mean reward: 850.60
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.9211
     Episode_Reward/lifting_object: 168.8558
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.38s
                      Time elapsed: 01:12:24
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 40647 steps/s (collection: 2.306s, learning 0.112s)
             Mean action noise std: 3.97
          Mean value_function loss: 131.5977
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.8970
                       Mean reward: 880.55
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 1.9298
     Episode_Reward/lifting_object: 169.7166
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.42s
                      Time elapsed: 01:12:27
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 41352 steps/s (collection: 2.276s, learning 0.102s)
             Mean action noise std: 3.97
          Mean value_function loss: 148.2224
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.9084
                       Mean reward: 905.09
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.9700
     Episode_Reward/lifting_object: 173.8464
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.38s
                      Time elapsed: 01:12:29
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 39913 steps/s (collection: 2.339s, learning 0.124s)
             Mean action noise std: 3.97
          Mean value_function loss: 154.2489
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.9199
                       Mean reward: 818.31
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 1.8931
     Episode_Reward/lifting_object: 166.5557
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.46s
                      Time elapsed: 01:12:31
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 40907 steps/s (collection: 2.280s, learning 0.123s)
             Mean action noise std: 3.97
          Mean value_function loss: 145.8601
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.9312
                       Mean reward: 845.52
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 1.9335
     Episode_Reward/lifting_object: 169.5233
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.40s
                      Time elapsed: 01:12:34
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 41450 steps/s (collection: 2.243s, learning 0.129s)
             Mean action noise std: 3.98
          Mean value_function loss: 167.4279
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.9416
                       Mean reward: 846.54
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 1.9155
     Episode_Reward/lifting_object: 168.4281
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.37s
                      Time elapsed: 01:12:36
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 42108 steps/s (collection: 2.229s, learning 0.106s)
             Mean action noise std: 3.98
          Mean value_function loss: 143.0477
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.9537
                       Mean reward: 819.59
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 1.8920
     Episode_Reward/lifting_object: 165.6033
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.33s
                      Time elapsed: 01:12:39
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 41593 steps/s (collection: 2.244s, learning 0.119s)
             Mean action noise std: 3.98
          Mean value_function loss: 145.4078
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.9643
                       Mean reward: 875.36
               Mean episode length: 231.77
    Episode_Reward/reaching_object: 1.9812
     Episode_Reward/lifting_object: 174.5103
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.36s
                      Time elapsed: 01:12:41
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 42178 steps/s (collection: 2.225s, learning 0.106s)
             Mean action noise std: 3.98
          Mean value_function loss: 137.0121
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.9744
                       Mean reward: 858.63
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.9690
     Episode_Reward/lifting_object: 172.7453
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.33s
                      Time elapsed: 01:12:43
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 40588 steps/s (collection: 2.314s, learning 0.108s)
             Mean action noise std: 3.98
          Mean value_function loss: 154.9949
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.9823
                       Mean reward: 862.16
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.9568
     Episode_Reward/lifting_object: 172.2872
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.42s
                      Time elapsed: 01:12:46
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 40393 steps/s (collection: 2.329s, learning 0.105s)
             Mean action noise std: 3.98
          Mean value_function loss: 168.9939
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.9939
                       Mean reward: 865.46
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 1.9319
     Episode_Reward/lifting_object: 169.5758
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.43s
                      Time elapsed: 01:12:48
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 41660 steps/s (collection: 2.248s, learning 0.112s)
             Mean action noise std: 3.99
          Mean value_function loss: 178.9474
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.0044
                       Mean reward: 799.82
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.8907
     Episode_Reward/lifting_object: 165.8762
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.36s
                      Time elapsed: 01:12:50
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 41530 steps/s (collection: 2.258s, learning 0.109s)
             Mean action noise std: 3.99
          Mean value_function loss: 199.0226
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 72.0150
                       Mean reward: 852.89
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.8930
     Episode_Reward/lifting_object: 166.0977
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.37s
                      Time elapsed: 01:12:53
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 41342 steps/s (collection: 2.266s, learning 0.112s)
             Mean action noise std: 3.99
          Mean value_function loss: 144.0868
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.0292
                       Mean reward: 820.77
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 1.9270
     Episode_Reward/lifting_object: 169.5398
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.38s
                      Time elapsed: 01:12:55
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 41292 steps/s (collection: 2.259s, learning 0.122s)
             Mean action noise std: 3.99
          Mean value_function loss: 141.1923
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.0445
                       Mean reward: 865.67
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 1.9147
     Episode_Reward/lifting_object: 168.1949
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.38s
                      Time elapsed: 01:12:58
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 42067 steps/s (collection: 2.220s, learning 0.117s)
             Mean action noise std: 4.00
          Mean value_function loss: 121.5479
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.0579
                       Mean reward: 884.74
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.9654
     Episode_Reward/lifting_object: 173.4481
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.34s
                      Time elapsed: 01:13:00
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 41524 steps/s (collection: 2.252s, learning 0.115s)
             Mean action noise std: 4.00
          Mean value_function loss: 158.1824
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.0755
                       Mean reward: 884.11
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.9224
     Episode_Reward/lifting_object: 169.2083
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.37s
                      Time elapsed: 01:13:02
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 41575 steps/s (collection: 2.240s, learning 0.125s)
             Mean action noise std: 4.00
          Mean value_function loss: 200.0312
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.0861
                       Mean reward: 828.69
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.9084
     Episode_Reward/lifting_object: 167.3721
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.36s
                      Time elapsed: 01:13:05
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 41424 steps/s (collection: 2.255s, learning 0.118s)
             Mean action noise std: 4.00
          Mean value_function loss: 155.6594
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.0922
                       Mean reward: 791.46
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.8807
     Episode_Reward/lifting_object: 165.1281
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.37s
                      Time elapsed: 01:13:07
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 41747 steps/s (collection: 2.247s, learning 0.108s)
             Mean action noise std: 4.00
          Mean value_function loss: 134.0388
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.1024
                       Mean reward: 889.08
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 1.9688
     Episode_Reward/lifting_object: 173.0690
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.35s
                      Time elapsed: 01:13:09
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 41940 steps/s (collection: 2.227s, learning 0.117s)
             Mean action noise std: 4.00
          Mean value_function loss: 147.7641
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 72.1091
                       Mean reward: 887.53
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 2.0134
     Episode_Reward/lifting_object: 177.0951
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.34s
                      Time elapsed: 01:13:12
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 41921 steps/s (collection: 2.233s, learning 0.112s)
             Mean action noise std: 4.00
          Mean value_function loss: 136.5808
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.1146
                       Mean reward: 854.58
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.9086
     Episode_Reward/lifting_object: 167.4064
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.34s
                      Time elapsed: 01:13:14
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 40042 steps/s (collection: 2.325s, learning 0.130s)
             Mean action noise std: 4.01
          Mean value_function loss: 134.7995
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.1262
                       Mean reward: 812.60
               Mean episode length: 218.36
    Episode_Reward/reaching_object: 1.9329
     Episode_Reward/lifting_object: 169.6609
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.45s
                      Time elapsed: 01:13:17
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 41170 steps/s (collection: 2.282s, learning 0.106s)
             Mean action noise std: 4.01
          Mean value_function loss: 159.3042
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.1370
                       Mean reward: 876.92
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: 172.8060
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.39s
                      Time elapsed: 01:13:19
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 42224 steps/s (collection: 2.226s, learning 0.102s)
             Mean action noise std: 4.01
          Mean value_function loss: 132.7737
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.1464
                       Mean reward: 891.53
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.9550
     Episode_Reward/lifting_object: 171.9398
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.33s
                      Time elapsed: 01:13:21
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 41602 steps/s (collection: 2.259s, learning 0.104s)
             Mean action noise std: 4.01
          Mean value_function loss: 123.9928
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.1550
                       Mean reward: 853.42
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.9806
     Episode_Reward/lifting_object: 174.4349
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.36s
                      Time elapsed: 01:13:24
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 41198 steps/s (collection: 2.268s, learning 0.118s)
             Mean action noise std: 4.01
          Mean value_function loss: 148.2470
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 72.1641
                       Mean reward: 834.49
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 1.8853
     Episode_Reward/lifting_object: 165.4801
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.39s
                      Time elapsed: 01:13:26
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 41668 steps/s (collection: 2.253s, learning 0.107s)
             Mean action noise std: 4.01
          Mean value_function loss: 122.3053
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.1686
                       Mean reward: 874.35
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 1.9423
     Episode_Reward/lifting_object: 170.2686
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.36s
                      Time elapsed: 01:13:28
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 41059 steps/s (collection: 2.296s, learning 0.098s)
             Mean action noise std: 4.01
          Mean value_function loss: 124.5662
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.1740
                       Mean reward: 868.64
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.9370
     Episode_Reward/lifting_object: 170.3179
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.39s
                      Time elapsed: 01:13:31
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 41050 steps/s (collection: 2.265s, learning 0.130s)
             Mean action noise std: 4.01
          Mean value_function loss: 100.6505
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.1801
                       Mean reward: 906.07
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 1.9626
     Episode_Reward/lifting_object: 172.4121
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.39s
                      Time elapsed: 01:13:33
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 42023 steps/s (collection: 2.236s, learning 0.103s)
             Mean action noise std: 4.02
          Mean value_function loss: 108.8988
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.1887
                       Mean reward: 913.19
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 1.9962
     Episode_Reward/lifting_object: 175.2489
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.34s
                      Time elapsed: 01:13:35
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 41263 steps/s (collection: 2.261s, learning 0.121s)
             Mean action noise std: 4.02
          Mean value_function loss: 147.0064
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 72.1959
                       Mean reward: 864.18
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 1.9435
     Episode_Reward/lifting_object: 170.5817
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0809
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.38s
                      Time elapsed: 01:13:38
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 42027 steps/s (collection: 2.235s, learning 0.104s)
             Mean action noise std: 4.02
          Mean value_function loss: 146.7875
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.2065
                       Mean reward: 814.78
               Mean episode length: 218.94
    Episode_Reward/reaching_object: 1.9325
     Episode_Reward/lifting_object: 169.6406
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.34s
                      Time elapsed: 01:13:40
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 40912 steps/s (collection: 2.263s, learning 0.140s)
             Mean action noise std: 4.02
          Mean value_function loss: 96.9096
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.2172
                       Mean reward: 912.99
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 1.9901
     Episode_Reward/lifting_object: 173.8958
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.40s
                      Time elapsed: 01:13:43
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 40326 steps/s (collection: 2.336s, learning 0.102s)
             Mean action noise std: 4.02
          Mean value_function loss: 132.0779
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.2266
                       Mean reward: 842.48
               Mean episode length: 224.06
    Episode_Reward/reaching_object: 1.9321
     Episode_Reward/lifting_object: 169.2666
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.44s
                      Time elapsed: 01:13:45
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 42037 steps/s (collection: 2.233s, learning 0.105s)
             Mean action noise std: 4.02
          Mean value_function loss: 91.6855
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.2344
                       Mean reward: 879.39
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 1.9767
     Episode_Reward/lifting_object: 173.4203
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.34s
                      Time elapsed: 01:13:47
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 41090 steps/s (collection: 2.283s, learning 0.109s)
             Mean action noise std: 4.02
          Mean value_function loss: 125.8899
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.2415
                       Mean reward: 862.85
               Mean episode length: 229.22
    Episode_Reward/reaching_object: 1.9449
     Episode_Reward/lifting_object: 170.7787
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.39s
                      Time elapsed: 01:13:50
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 41706 steps/s (collection: 2.250s, learning 0.107s)
             Mean action noise std: 4.03
          Mean value_function loss: 109.6494
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.2534
                       Mean reward: 864.02
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 1.9872
     Episode_Reward/lifting_object: 174.6224
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.36s
                      Time elapsed: 01:13:52
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 41186 steps/s (collection: 2.285s, learning 0.102s)
             Mean action noise std: 4.03
          Mean value_function loss: 123.6530
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.2636
                       Mean reward: 867.30
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.9573
     Episode_Reward/lifting_object: 171.8966
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.39s
                      Time elapsed: 01:13:55
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 40815 steps/s (collection: 2.265s, learning 0.143s)
             Mean action noise std: 4.03
          Mean value_function loss: 118.2415
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.2733
                       Mean reward: 877.03
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 1.9672
     Episode_Reward/lifting_object: 172.5941
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.41s
                      Time elapsed: 01:13:57
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 41826 steps/s (collection: 2.245s, learning 0.106s)
             Mean action noise std: 4.03
          Mean value_function loss: 118.7805
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.2797
                       Mean reward: 866.98
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.9636
     Episode_Reward/lifting_object: 172.6907
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.35s
                      Time elapsed: 01:13:59
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 42004 steps/s (collection: 2.241s, learning 0.099s)
             Mean action noise std: 4.03
          Mean value_function loss: 118.9741
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.2912
                       Mean reward: 868.52
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.9920
     Episode_Reward/lifting_object: 175.2808
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.34s
                      Time elapsed: 01:14:02
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 41079 steps/s (collection: 2.255s, learning 0.138s)
             Mean action noise std: 4.03
          Mean value_function loss: 103.8041
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3018
                       Mean reward: 864.09
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 1.9942
     Episode_Reward/lifting_object: 175.2527
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.39s
                      Time elapsed: 01:14:04
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 41704 steps/s (collection: 2.249s, learning 0.108s)
             Mean action noise std: 4.04
          Mean value_function loss: 107.0033
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.3150
                       Mean reward: 862.14
               Mean episode length: 229.81
    Episode_Reward/reaching_object: 1.9295
     Episode_Reward/lifting_object: 169.3132
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.36s
                      Time elapsed: 01:14:06
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 41029 steps/s (collection: 2.259s, learning 0.137s)
             Mean action noise std: 4.04
          Mean value_function loss: 105.0582
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.3323
                       Mean reward: 930.42
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 2.0058
     Episode_Reward/lifting_object: 176.5862
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.40s
                      Time elapsed: 01:14:09
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 41811 steps/s (collection: 2.243s, learning 0.109s)
             Mean action noise std: 4.04
          Mean value_function loss: 137.8943
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.3427
                       Mean reward: 860.65
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 1.9458
     Episode_Reward/lifting_object: 170.7372
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.35s
                      Time elapsed: 01:14:11
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 41245 steps/s (collection: 2.261s, learning 0.123s)
             Mean action noise std: 4.04
          Mean value_function loss: 140.4917
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.3539
                       Mean reward: 837.79
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 1.8966
     Episode_Reward/lifting_object: 166.5420
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0822
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.38s
                      Time elapsed: 01:14:14
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 40955 steps/s (collection: 2.300s, learning 0.101s)
             Mean action noise std: 4.04
          Mean value_function loss: 125.9192
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3643
                       Mean reward: 910.27
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 1.9664
     Episode_Reward/lifting_object: 172.4978
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.40s
                      Time elapsed: 01:14:16
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 39397 steps/s (collection: 2.386s, learning 0.109s)
             Mean action noise std: 4.04
          Mean value_function loss: 119.8448
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.3710
                       Mean reward: 824.80
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 1.9777
     Episode_Reward/lifting_object: 173.3835
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.50s
                      Time elapsed: 01:14:18
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 41828 steps/s (collection: 2.239s, learning 0.112s)
             Mean action noise std: 4.05
          Mean value_function loss: 111.1676
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3795
                       Mean reward: 897.02
               Mean episode length: 237.78
    Episode_Reward/reaching_object: 1.9770
     Episode_Reward/lifting_object: 173.1897
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.35s
                      Time elapsed: 01:14:21
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 40070 steps/s (collection: 2.358s, learning 0.096s)
             Mean action noise std: 4.05
          Mean value_function loss: 136.3799
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.3937
                       Mean reward: 850.11
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.9526
     Episode_Reward/lifting_object: 171.3875
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.45s
                      Time elapsed: 01:14:23
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 41290 steps/s (collection: 2.288s, learning 0.093s)
             Mean action noise std: 4.05
          Mean value_function loss: 136.1487
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.4066
                       Mean reward: 870.84
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.9484
     Episode_Reward/lifting_object: 170.9926
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.38s
                      Time elapsed: 01:14:26
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 40580 steps/s (collection: 2.315s, learning 0.108s)
             Mean action noise std: 4.05
          Mean value_function loss: 124.5695
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.4112
                       Mean reward: 872.33
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.9696
     Episode_Reward/lifting_object: 173.0583
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.42s
                      Time elapsed: 01:14:28
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 41389 steps/s (collection: 2.260s, learning 0.115s)
             Mean action noise std: 4.05
          Mean value_function loss: 141.2599
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.4179
                       Mean reward: 885.79
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.9388
     Episode_Reward/lifting_object: 169.8849
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.38s
                      Time elapsed: 01:14:30
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 39482 steps/s (collection: 2.373s, learning 0.117s)
             Mean action noise std: 4.05
          Mean value_function loss: 133.9639
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.4235
                       Mean reward: 864.26
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.9531
     Episode_Reward/lifting_object: 171.3872
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.49s
                      Time elapsed: 01:14:33
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 38781 steps/s (collection: 2.403s, learning 0.132s)
             Mean action noise std: 4.05
          Mean value_function loss: 117.8044
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.4298
                       Mean reward: 859.58
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 1.9419
     Episode_Reward/lifting_object: 169.5319
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.53s
                      Time elapsed: 01:14:35
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 34826 steps/s (collection: 2.588s, learning 0.235s)
             Mean action noise std: 4.06
          Mean value_function loss: 115.2305
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.4393
                       Mean reward: 870.77
               Mean episode length: 231.05
    Episode_Reward/reaching_object: 1.9487
     Episode_Reward/lifting_object: 171.2827
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.82s
                      Time elapsed: 01:14:38
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 35405 steps/s (collection: 2.605s, learning 0.171s)
             Mean action noise std: 4.06
          Mean value_function loss: 121.4496
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.4466
                       Mean reward: 892.06
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 2.0026
     Episode_Reward/lifting_object: 175.6526
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.78s
                      Time elapsed: 01:14:41
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 36952 steps/s (collection: 2.556s, learning 0.104s)
             Mean action noise std: 4.06
          Mean value_function loss: 132.2929
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.4591
                       Mean reward: 884.60
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.9950
     Episode_Reward/lifting_object: 174.7916
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.66s
                      Time elapsed: 01:14:44
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 38223 steps/s (collection: 2.430s, learning 0.142s)
             Mean action noise std: 4.06
          Mean value_function loss: 132.5633
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.4697
                       Mean reward: 882.75
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 1.9609
     Episode_Reward/lifting_object: 171.6601
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.57s
                      Time elapsed: 01:14:46
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 36528 steps/s (collection: 2.487s, learning 0.204s)
             Mean action noise std: 4.06
          Mean value_function loss: 132.2115
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.4758
                       Mean reward: 863.25
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 1.9333
     Episode_Reward/lifting_object: 169.3889
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0821
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.69s
                      Time elapsed: 01:14:49
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 37423 steps/s (collection: 2.481s, learning 0.146s)
             Mean action noise std: 4.06
          Mean value_function loss: 160.5826
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.4869
                       Mean reward: 856.79
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.9293
     Episode_Reward/lifting_object: 169.2062
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.63s
                      Time elapsed: 01:14:52
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 36497 steps/s (collection: 2.518s, learning 0.176s)
             Mean action noise std: 4.06
          Mean value_function loss: 167.0895
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.4945
                       Mean reward: 844.73
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.9559
     Episode_Reward/lifting_object: 171.1586
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.69s
                      Time elapsed: 01:14:54
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 36132 steps/s (collection: 2.509s, learning 0.212s)
             Mean action noise std: 4.07
          Mean value_function loss: 135.7779
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.5049
                       Mean reward: 863.05
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 1.9640
     Episode_Reward/lifting_object: 171.4672
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.72s
                      Time elapsed: 01:14:57
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 37314 steps/s (collection: 2.508s, learning 0.126s)
             Mean action noise std: 4.07
          Mean value_function loss: 128.5472
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.5235
                       Mean reward: 873.35
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.9807
     Episode_Reward/lifting_object: 173.7809
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.63s
                      Time elapsed: 01:15:00
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 36473 steps/s (collection: 2.512s, learning 0.184s)
             Mean action noise std: 4.07
          Mean value_function loss: 145.4195
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.5337
                       Mean reward: 886.44
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.9382
     Episode_Reward/lifting_object: 170.0409
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.70s
                      Time elapsed: 01:15:02
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 38122 steps/s (collection: 2.417s, learning 0.162s)
             Mean action noise std: 4.07
          Mean value_function loss: 142.6415
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.5408
                       Mean reward: 845.34
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.9232
     Episode_Reward/lifting_object: 168.4023
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0804
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.58s
                      Time elapsed: 01:15:05
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 37774 steps/s (collection: 2.466s, learning 0.137s)
             Mean action noise std: 4.07
          Mean value_function loss: 116.4633
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.5470
                       Mean reward: 866.54
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.9358
     Episode_Reward/lifting_object: 170.2448
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.60s
                      Time elapsed: 01:15:07
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 37797 steps/s (collection: 2.455s, learning 0.146s)
             Mean action noise std: 4.07
          Mean value_function loss: 141.4344
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.5527
                       Mean reward: 825.85
               Mean episode length: 221.73
    Episode_Reward/reaching_object: 1.9465
     Episode_Reward/lifting_object: 170.7196
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.60s
                      Time elapsed: 01:15:10
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 39260 steps/s (collection: 2.382s, learning 0.122s)
             Mean action noise std: 4.08
          Mean value_function loss: 108.2237
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.5643
                       Mean reward: 876.85
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 1.9758
     Episode_Reward/lifting_object: 173.3859
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.50s
                      Time elapsed: 01:15:13
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 36520 steps/s (collection: 2.522s, learning 0.170s)
             Mean action noise std: 4.08
          Mean value_function loss: 102.1927
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.5774
                       Mean reward: 872.19
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.9739
     Episode_Reward/lifting_object: 173.0573
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0830
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.69s
                      Time elapsed: 01:15:15
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 39597 steps/s (collection: 2.329s, learning 0.154s)
             Mean action noise std: 4.08
          Mean value_function loss: 113.8176
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.5862
                       Mean reward: 813.02
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 1.9721
     Episode_Reward/lifting_object: 173.1772
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0824
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.48s
                      Time elapsed: 01:15:18
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 37589 steps/s (collection: 2.472s, learning 0.143s)
             Mean action noise std: 4.08
          Mean value_function loss: 118.3220
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.5952
                       Mean reward: 876.48
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.9540
     Episode_Reward/lifting_object: 171.1785
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.62s
                      Time elapsed: 01:15:20
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 39423 steps/s (collection: 2.390s, learning 0.103s)
             Mean action noise std: 4.08
          Mean value_function loss: 153.1068
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.6036
                       Mean reward: 854.36
               Mean episode length: 228.70
    Episode_Reward/reaching_object: 1.9478
     Episode_Reward/lifting_object: 170.1556
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.49s
                      Time elapsed: 01:15:23
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 38855 steps/s (collection: 2.375s, learning 0.155s)
             Mean action noise std: 4.08
          Mean value_function loss: 141.0079
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.6133
                       Mean reward: 861.00
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 1.9876
     Episode_Reward/lifting_object: 174.0890
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.53s
                      Time elapsed: 01:15:25
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 38890 steps/s (collection: 2.390s, learning 0.137s)
             Mean action noise std: 4.09
          Mean value_function loss: 133.6640
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.6279
                       Mean reward: 826.83
               Mean episode length: 221.14
    Episode_Reward/reaching_object: 1.9497
     Episode_Reward/lifting_object: 170.4077
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.53s
                      Time elapsed: 01:15:28
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 39333 steps/s (collection: 2.361s, learning 0.139s)
             Mean action noise std: 4.09
          Mean value_function loss: 107.4367
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.6437
                       Mean reward: 861.92
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 171.0299
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.50s
                      Time elapsed: 01:15:30
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 40378 steps/s (collection: 2.321s, learning 0.114s)
             Mean action noise std: 4.09
          Mean value_function loss: 108.6838
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.6522
                       Mean reward: 894.01
               Mean episode length: 236.41
    Episode_Reward/reaching_object: 1.9886
     Episode_Reward/lifting_object: 174.2912
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.43s
                      Time elapsed: 01:15:33
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 35489 steps/s (collection: 2.623s, learning 0.147s)
             Mean action noise std: 4.09
          Mean value_function loss: 100.7475
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.6598
                       Mean reward: 877.63
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 2.0061
     Episode_Reward/lifting_object: 175.8130
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.77s
                      Time elapsed: 01:15:36
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 35754 steps/s (collection: 2.624s, learning 0.126s)
             Mean action noise std: 4.10
          Mean value_function loss: 107.6144
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.6746
                       Mean reward: 881.94
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 1.9686
     Episode_Reward/lifting_object: 172.2394
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.75s
                      Time elapsed: 01:15:38
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 36631 steps/s (collection: 2.508s, learning 0.176s)
             Mean action noise std: 4.10
          Mean value_function loss: 140.3046
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.6915
                       Mean reward: 870.49
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 1.9951
     Episode_Reward/lifting_object: 174.6669
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.68s
                      Time elapsed: 01:15:41
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 32021 steps/s (collection: 2.906s, learning 0.164s)
             Mean action noise std: 4.10
          Mean value_function loss: 153.5532
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.7021
                       Mean reward: 884.09
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.9857
     Episode_Reward/lifting_object: 173.7979
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 3.07s
                      Time elapsed: 01:15:44
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 34839 steps/s (collection: 2.645s, learning 0.177s)
             Mean action noise std: 4.10
          Mean value_function loss: 128.7011
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.7135
                       Mean reward: 860.63
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 1.9451
     Episode_Reward/lifting_object: 170.2132
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0835
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.82s
                      Time elapsed: 01:15:47
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 39181 steps/s (collection: 2.371s, learning 0.138s)
             Mean action noise std: 4.10
          Mean value_function loss: 92.2231
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.7279
                       Mean reward: 886.19
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.9806
     Episode_Reward/lifting_object: 173.3762
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.51s
                      Time elapsed: 01:15:49
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 38016 steps/s (collection: 2.455s, learning 0.131s)
             Mean action noise std: 4.11
          Mean value_function loss: 96.2871
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.7422
                       Mean reward: 897.28
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.9811
     Episode_Reward/lifting_object: 173.5632
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.59s
                      Time elapsed: 01:15:52
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 37362 steps/s (collection: 2.479s, learning 0.152s)
             Mean action noise std: 4.11
          Mean value_function loss: 116.6468
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.7533
                       Mean reward: 909.88
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.9794
     Episode_Reward/lifting_object: 173.6529
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.63s
                      Time elapsed: 01:15:55
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 37399 steps/s (collection: 2.435s, learning 0.193s)
             Mean action noise std: 4.11
          Mean value_function loss: 108.8847
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.7628
                       Mean reward: 879.35
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.9890
     Episode_Reward/lifting_object: 174.7805
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1011
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.63s
                      Time elapsed: 01:15:57
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 39319 steps/s (collection: 2.364s, learning 0.136s)
             Mean action noise std: 4.11
          Mean value_function loss: 154.5576
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.7724
                       Mean reward: 840.16
               Mean episode length: 224.62
    Episode_Reward/reaching_object: 1.9246
     Episode_Reward/lifting_object: 168.9828
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.50s
                      Time elapsed: 01:16:00
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 38014 steps/s (collection: 2.458s, learning 0.128s)
             Mean action noise std: 4.11
          Mean value_function loss: 162.1056
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 72.7828
                       Mean reward: 845.98
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.9123
     Episode_Reward/lifting_object: 167.6595
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.59s
                      Time elapsed: 01:16:02
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 38788 steps/s (collection: 2.397s, learning 0.138s)
             Mean action noise std: 4.11
          Mean value_function loss: 150.3602
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 72.7960
                       Mean reward: 856.70
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.8967
     Episode_Reward/lifting_object: 165.4543
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.53s
                      Time elapsed: 01:16:05
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 33265 steps/s (collection: 2.761s, learning 0.194s)
             Mean action noise std: 4.12
          Mean value_function loss: 116.9159
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.8024
                       Mean reward: 895.94
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 173.2455
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.96s
                      Time elapsed: 01:16:08
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 35331 steps/s (collection: 2.558s, learning 0.224s)
             Mean action noise std: 4.12
          Mean value_function loss: 147.5746
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.8103
                       Mean reward: 833.21
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.9573
     Episode_Reward/lifting_object: 171.0697
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.78s
                      Time elapsed: 01:16:11
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 36218 steps/s (collection: 2.574s, learning 0.140s)
             Mean action noise std: 4.12
          Mean value_function loss: 124.8119
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.8183
                       Mean reward: 893.96
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 1.9899
     Episode_Reward/lifting_object: 174.7664
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.71s
                      Time elapsed: 01:16:13
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 32726 steps/s (collection: 2.797s, learning 0.207s)
             Mean action noise std: 4.12
          Mean value_function loss: 115.8133
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.8324
                       Mean reward: 857.53
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.9765
     Episode_Reward/lifting_object: 173.1593
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 3.00s
                      Time elapsed: 01:16:16
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 36914 steps/s (collection: 2.490s, learning 0.173s)
             Mean action noise std: 4.12
          Mean value_function loss: 102.4860
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.8452
                       Mean reward: 896.49
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 1.9721
     Episode_Reward/lifting_object: 173.4186
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.66s
                      Time elapsed: 01:16:19
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 37181 steps/s (collection: 2.506s, learning 0.138s)
             Mean action noise std: 4.13
          Mean value_function loss: 145.7647
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.8563
                       Mean reward: 874.26
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.9494
     Episode_Reward/lifting_object: 171.1079
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.64s
                      Time elapsed: 01:16:22
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 37220 steps/s (collection: 2.488s, learning 0.153s)
             Mean action noise std: 4.13
          Mean value_function loss: 176.7892
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.8725
                       Mean reward: 861.37
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.9478
     Episode_Reward/lifting_object: 170.9349
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.64s
                      Time elapsed: 01:16:24
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 38865 steps/s (collection: 2.403s, learning 0.126s)
             Mean action noise std: 4.13
          Mean value_function loss: 143.2728
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 72.8869
                       Mean reward: 835.01
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 1.8821
     Episode_Reward/lifting_object: 164.6459
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.53s
                      Time elapsed: 01:16:27
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 37808 steps/s (collection: 2.426s, learning 0.174s)
             Mean action noise std: 4.13
          Mean value_function loss: 125.3644
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 72.8973
                       Mean reward: 840.65
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 1.9622
     Episode_Reward/lifting_object: 172.4179
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.60s
                      Time elapsed: 01:16:29
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 36663 steps/s (collection: 2.482s, learning 0.199s)
             Mean action noise std: 4.13
          Mean value_function loss: 166.2676
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 72.8994
                       Mean reward: 845.59
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 1.9200
     Episode_Reward/lifting_object: 168.2305
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.68s
                      Time elapsed: 01:16:32
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 36333 steps/s (collection: 2.530s, learning 0.176s)
             Mean action noise std: 4.13
          Mean value_function loss: 154.0484
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.9053
                       Mean reward: 875.49
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.9012
     Episode_Reward/lifting_object: 166.5754
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.71s
                      Time elapsed: 01:16:35
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 39428 steps/s (collection: 2.362s, learning 0.131s)
             Mean action noise std: 4.13
          Mean value_function loss: 126.5661
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.9177
                       Mean reward: 860.04
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.9745
     Episode_Reward/lifting_object: 173.3358
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.49s
                      Time elapsed: 01:16:37
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 38270 steps/s (collection: 2.395s, learning 0.174s)
             Mean action noise std: 4.14
          Mean value_function loss: 127.1866
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.9323
                       Mean reward: 852.60
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.9561
     Episode_Reward/lifting_object: 171.9232
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.57s
                      Time elapsed: 01:16:40
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 38714 steps/s (collection: 2.394s, learning 0.145s)
             Mean action noise std: 4.14
          Mean value_function loss: 131.7651
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 72.9479
                       Mean reward: 891.33
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.9715
     Episode_Reward/lifting_object: 173.1371
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.54s
                      Time elapsed: 01:16:42
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 37623 steps/s (collection: 2.416s, learning 0.197s)
             Mean action noise std: 4.14
          Mean value_function loss: 130.8228
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.9564
                       Mean reward: 848.83
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.9564
     Episode_Reward/lifting_object: 171.4631
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.61s
                      Time elapsed: 01:16:45
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 38197 steps/s (collection: 2.424s, learning 0.149s)
             Mean action noise std: 4.14
          Mean value_function loss: 141.3421
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.9650
                       Mean reward: 857.13
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.9428
     Episode_Reward/lifting_object: 169.9709
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.57s
                      Time elapsed: 01:16:48
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 39078 steps/s (collection: 2.380s, learning 0.136s)
             Mean action noise std: 4.14
          Mean value_function loss: 120.5889
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.9746
                       Mean reward: 855.42
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.9224
     Episode_Reward/lifting_object: 168.4410
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.52s
                      Time elapsed: 01:16:50
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 33682 steps/s (collection: 2.725s, learning 0.194s)
             Mean action noise std: 4.15
          Mean value_function loss: 138.9453
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.9845
                       Mean reward: 809.70
               Mean episode length: 216.46
    Episode_Reward/reaching_object: 1.9147
     Episode_Reward/lifting_object: 168.0102
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.92s
                      Time elapsed: 01:16:53
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 30531 steps/s (collection: 2.965s, learning 0.255s)
             Mean action noise std: 4.15
          Mean value_function loss: 123.5557
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.9962
                       Mean reward: 836.98
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 1.9719
     Episode_Reward/lifting_object: 172.8989
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0843
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 3.22s
                      Time elapsed: 01:16:56
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 35704 steps/s (collection: 2.605s, learning 0.149s)
             Mean action noise std: 4.15
          Mean value_function loss: 107.3932
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.0097
                       Mean reward: 905.91
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 1.9678
     Episode_Reward/lifting_object: 172.0893
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.75s
                      Time elapsed: 01:16:59
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 35304 steps/s (collection: 2.609s, learning 0.176s)
             Mean action noise std: 4.15
          Mean value_function loss: 132.1367
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.0203
                       Mean reward: 901.09
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.9933
     Episode_Reward/lifting_object: 175.1090
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.78s
                      Time elapsed: 01:17:02
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 37315 steps/s (collection: 2.466s, learning 0.169s)
             Mean action noise std: 4.15
          Mean value_function loss: 159.1344
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.0314
                       Mean reward: 843.62
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 1.9038
     Episode_Reward/lifting_object: 166.2261
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.63s
                      Time elapsed: 01:17:04
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 38383 steps/s (collection: 2.412s, learning 0.149s)
             Mean action noise std: 4.16
          Mean value_function loss: 150.8270
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.0425
                       Mean reward: 866.56
               Mean episode length: 229.81
    Episode_Reward/reaching_object: 1.9657
     Episode_Reward/lifting_object: 172.3200
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.56s
                      Time elapsed: 01:17:07
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 38504 steps/s (collection: 2.438s, learning 0.115s)
             Mean action noise std: 4.16
          Mean value_function loss: 119.1482
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.0521
                       Mean reward: 860.37
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.9862
     Episode_Reward/lifting_object: 174.2594
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.55s
                      Time elapsed: 01:17:10
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 37883 steps/s (collection: 2.441s, learning 0.154s)
             Mean action noise std: 4.16
          Mean value_function loss: 138.4173
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.0616
                       Mean reward: 836.81
               Mean episode length: 222.92
    Episode_Reward/reaching_object: 1.9066
     Episode_Reward/lifting_object: 166.5537
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.59s
                      Time elapsed: 01:17:12
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 37382 steps/s (collection: 2.503s, learning 0.127s)
             Mean action noise std: 4.16
          Mean value_function loss: 109.8929
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.0713
                       Mean reward: 848.35
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 1.9845
     Episode_Reward/lifting_object: 174.2345
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.63s
                      Time elapsed: 01:17:15
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 37958 steps/s (collection: 2.461s, learning 0.129s)
             Mean action noise std: 4.16
          Mean value_function loss: 118.3861
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.0811
                       Mean reward: 895.10
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.9576
     Episode_Reward/lifting_object: 172.0382
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.59s
                      Time elapsed: 01:17:17
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 38404 steps/s (collection: 2.410s, learning 0.150s)
             Mean action noise std: 4.16
          Mean value_function loss: 164.2353
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.0944
                       Mean reward: 846.06
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.9203
     Episode_Reward/lifting_object: 168.4985
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.56s
                      Time elapsed: 01:17:20
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 37113 steps/s (collection: 2.455s, learning 0.194s)
             Mean action noise std: 4.17
          Mean value_function loss: 154.4402
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.1022
                       Mean reward: 854.88
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 1.9477
     Episode_Reward/lifting_object: 170.6776
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.65s
                      Time elapsed: 01:17:23
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 33477 steps/s (collection: 2.735s, learning 0.201s)
             Mean action noise std: 4.17
          Mean value_function loss: 134.0554
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.1121
                       Mean reward: 854.83
               Mean episode length: 227.69
    Episode_Reward/reaching_object: 1.9006
     Episode_Reward/lifting_object: 166.7174
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.94s
                      Time elapsed: 01:17:26
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 36634 steps/s (collection: 2.540s, learning 0.143s)
             Mean action noise std: 4.17
          Mean value_function loss: 121.6157
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.1278
                       Mean reward: 922.89
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 1.9975
     Episode_Reward/lifting_object: 175.4391
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.68s
                      Time elapsed: 01:17:28
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 37491 steps/s (collection: 2.488s, learning 0.134s)
             Mean action noise std: 4.17
          Mean value_function loss: 110.3878
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.1403
                       Mean reward: 876.98
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.9955
     Episode_Reward/lifting_object: 174.9060
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.62s
                      Time elapsed: 01:17:31
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 37585 steps/s (collection: 2.439s, learning 0.177s)
             Mean action noise std: 4.17
          Mean value_function loss: 123.3064
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.1456
                       Mean reward: 806.14
               Mean episode length: 216.14
    Episode_Reward/reaching_object: 1.9118
     Episode_Reward/lifting_object: 167.3595
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.62s
                      Time elapsed: 01:17:33
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 36679 steps/s (collection: 2.484s, learning 0.196s)
             Mean action noise std: 4.17
          Mean value_function loss: 125.2734
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.1534
                       Mean reward: 839.91
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 1.9459
     Episode_Reward/lifting_object: 170.4186
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.68s
                      Time elapsed: 01:17:36
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 36619 steps/s (collection: 2.522s, learning 0.162s)
             Mean action noise std: 4.18
          Mean value_function loss: 136.2321
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.1608
                       Mean reward: 857.83
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 1.9165
     Episode_Reward/lifting_object: 167.8823
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1010
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.68s
                      Time elapsed: 01:17:39
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 33918 steps/s (collection: 2.748s, learning 0.151s)
             Mean action noise std: 4.18
          Mean value_function loss: 82.4726
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.1679
                       Mean reward: 895.03
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 2.0209
     Episode_Reward/lifting_object: 177.4311
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1060
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.90s
                      Time elapsed: 01:17:42
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 36300 steps/s (collection: 2.500s, learning 0.208s)
             Mean action noise std: 4.18
          Mean value_function loss: 139.8591
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.1783
                       Mean reward: 865.58
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 1.9624
     Episode_Reward/lifting_object: 171.8863
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.71s
                      Time elapsed: 01:17:44
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 37229 steps/s (collection: 2.493s, learning 0.148s)
             Mean action noise std: 4.18
          Mean value_function loss: 147.6125
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.1889
                       Mean reward: 891.27
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.9472
     Episode_Reward/lifting_object: 170.4461
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.64s
                      Time elapsed: 01:17:47
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 36767 steps/s (collection: 2.499s, learning 0.174s)
             Mean action noise std: 4.18
          Mean value_function loss: 138.2344
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.2042
                       Mean reward: 876.91
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 1.9341
     Episode_Reward/lifting_object: 169.3019
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.67s
                      Time elapsed: 01:17:50
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 38108 steps/s (collection: 2.432s, learning 0.148s)
             Mean action noise std: 4.19
          Mean value_function loss: 138.0359
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.2177
                       Mean reward: 851.55
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.9420
     Episode_Reward/lifting_object: 170.2325
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.58s
                      Time elapsed: 01:17:52
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 35951 steps/s (collection: 2.527s, learning 0.207s)
             Mean action noise std: 4.19
          Mean value_function loss: 109.2277
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.2292
                       Mean reward: 889.49
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.9844
     Episode_Reward/lifting_object: 173.7276
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.73s
                      Time elapsed: 01:17:55
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 37568 steps/s (collection: 2.460s, learning 0.157s)
             Mean action noise std: 4.19
          Mean value_function loss: 87.0076
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.2374
                       Mean reward: 888.75
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 2.0128
     Episode_Reward/lifting_object: 176.4472
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.62s
                      Time elapsed: 01:17:58
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 38286 steps/s (collection: 2.401s, learning 0.167s)
             Mean action noise std: 4.19
          Mean value_function loss: 119.8871
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.2497
                       Mean reward: 841.37
               Mean episode length: 222.56
    Episode_Reward/reaching_object: 1.9677
     Episode_Reward/lifting_object: 172.8722
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.57s
                      Time elapsed: 01:18:00
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 36639 steps/s (collection: 2.460s, learning 0.223s)
             Mean action noise std: 4.19
          Mean value_function loss: 92.1313
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.2614
                       Mean reward: 906.49
               Mean episode length: 239.07
    Episode_Reward/reaching_object: 2.0036
     Episode_Reward/lifting_object: 175.8488
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.68s
                      Time elapsed: 01:18:03
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 36713 steps/s (collection: 2.507s, learning 0.171s)
             Mean action noise std: 4.19
          Mean value_function loss: 123.7143
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.2701
                       Mean reward: 845.07
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 1.9436
     Episode_Reward/lifting_object: 170.3771
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.68s
                      Time elapsed: 01:18:06
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 37143 steps/s (collection: 2.487s, learning 0.160s)
             Mean action noise std: 4.19
          Mean value_function loss: 127.0995
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.2755
                       Mean reward: 877.89
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.9398
     Episode_Reward/lifting_object: 170.0545
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1034
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.65s
                      Time elapsed: 01:18:08
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 35558 steps/s (collection: 2.650s, learning 0.115s)
             Mean action noise std: 4.20
          Mean value_function loss: 115.8768
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.2820
                       Mean reward: 897.62
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 1.9351
     Episode_Reward/lifting_object: 169.4373
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.76s
                      Time elapsed: 01:18:11
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 34338 steps/s (collection: 2.682s, learning 0.181s)
             Mean action noise std: 4.20
          Mean value_function loss: 122.0657
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.2924
                       Mean reward: 889.68
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 1.9790
     Episode_Reward/lifting_object: 173.4780
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.86s
                      Time elapsed: 01:18:14
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 36018 steps/s (collection: 2.599s, learning 0.130s)
             Mean action noise std: 4.20
          Mean value_function loss: 162.4689
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.3012
                       Mean reward: 846.04
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.9594
     Episode_Reward/lifting_object: 171.9316
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.73s
                      Time elapsed: 01:18:17
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 38559 steps/s (collection: 2.443s, learning 0.106s)
             Mean action noise std: 4.20
          Mean value_function loss: 144.1178
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.3082
                       Mean reward: 839.23
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.9513
     Episode_Reward/lifting_object: 171.1345
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.55s
                      Time elapsed: 01:18:19
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 33977 steps/s (collection: 2.685s, learning 0.208s)
             Mean action noise std: 4.20
          Mean value_function loss: 138.8296
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.3160
                       Mean reward: 910.44
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.9620
     Episode_Reward/lifting_object: 172.3710
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.89s
                      Time elapsed: 01:18:22
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 34974 steps/s (collection: 2.688s, learning 0.123s)
             Mean action noise std: 4.20
          Mean value_function loss: 134.4377
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.3220
                       Mean reward: 893.58
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.9813
     Episode_Reward/lifting_object: 174.4427
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.81s
                      Time elapsed: 01:18:25
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 39138 steps/s (collection: 2.382s, learning 0.130s)
             Mean action noise std: 4.20
          Mean value_function loss: 132.3199
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.3280
                       Mean reward: 885.24
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 1.9404
     Episode_Reward/lifting_object: 170.4300
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.51s
                      Time elapsed: 01:18:27
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 38618 steps/s (collection: 2.409s, learning 0.136s)
             Mean action noise std: 4.21
          Mean value_function loss: 128.6962
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.3363
                       Mean reward: 836.76
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 1.9547
     Episode_Reward/lifting_object: 171.6038
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.55s
                      Time elapsed: 01:18:30
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 38301 steps/s (collection: 2.422s, learning 0.145s)
             Mean action noise std: 4.21
          Mean value_function loss: 130.9082
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.3511
                       Mean reward: 860.57
               Mean episode length: 228.82
    Episode_Reward/reaching_object: 1.9568
     Episode_Reward/lifting_object: 171.6215
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.57s
                      Time elapsed: 01:18:32
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 38737 steps/s (collection: 2.374s, learning 0.164s)
             Mean action noise std: 4.21
          Mean value_function loss: 140.3324
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.3611
                       Mean reward: 848.66
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.9283
     Episode_Reward/lifting_object: 169.3789
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.54s
                      Time elapsed: 01:18:35
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 38131 steps/s (collection: 2.356s, learning 0.222s)
             Mean action noise std: 4.21
          Mean value_function loss: 152.1605
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.3712
                       Mean reward: 811.54
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 1.8844
     Episode_Reward/lifting_object: 165.1205
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0852
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.58s
                      Time elapsed: 01:18:38
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 36313 steps/s (collection: 2.535s, learning 0.172s)
             Mean action noise std: 4.21
          Mean value_function loss: 136.6894
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.3860
                       Mean reward: 864.97
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.9482
     Episode_Reward/lifting_object: 170.8947
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.71s
                      Time elapsed: 01:18:40
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 36417 steps/s (collection: 2.534s, learning 0.166s)
             Mean action noise std: 4.22
          Mean value_function loss: 138.6701
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 73.3988
                       Mean reward: 860.00
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.9451
     Episode_Reward/lifting_object: 170.0896
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.70s
                      Time elapsed: 01:18:43
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 34878 steps/s (collection: 2.643s, learning 0.176s)
             Mean action noise std: 4.22
          Mean value_function loss: 145.6642
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.4127
                       Mean reward: 831.96
               Mean episode length: 221.82
    Episode_Reward/reaching_object: 1.9399
     Episode_Reward/lifting_object: 169.4581
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.82s
                      Time elapsed: 01:18:46
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 31475 steps/s (collection: 2.895s, learning 0.229s)
             Mean action noise std: 4.22
          Mean value_function loss: 143.2672
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.4250
                       Mean reward: 860.62
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 1.9481
     Episode_Reward/lifting_object: 170.1180
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0860
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 3.12s
                      Time elapsed: 01:18:49
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 37733 steps/s (collection: 2.469s, learning 0.137s)
             Mean action noise std: 4.22
          Mean value_function loss: 132.8064
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4354
                       Mean reward: 885.41
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 170.6191
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.61s
                      Time elapsed: 01:18:52
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 37798 steps/s (collection: 2.416s, learning 0.185s)
             Mean action noise std: 4.22
          Mean value_function loss: 122.7053
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.4462
                       Mean reward: 842.04
               Mean episode length: 224.35
    Episode_Reward/reaching_object: 1.9619
     Episode_Reward/lifting_object: 171.4514
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.60s
                      Time elapsed: 01:18:54
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 39419 steps/s (collection: 2.383s, learning 0.111s)
             Mean action noise std: 4.23
          Mean value_function loss: 111.9023
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.4563
                       Mean reward: 880.54
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.9766
     Episode_Reward/lifting_object: 172.5541
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.49s
                      Time elapsed: 01:18:57
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 38898 steps/s (collection: 2.410s, learning 0.118s)
             Mean action noise std: 4.23
          Mean value_function loss: 107.7166
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.4682
                       Mean reward: 934.92
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 2.0286
     Episode_Reward/lifting_object: 177.4619
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.53s
                      Time elapsed: 01:18:59
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 38451 steps/s (collection: 2.430s, learning 0.126s)
             Mean action noise std: 4.23
          Mean value_function loss: 216.0022
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.4771
                       Mean reward: 825.61
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 1.9001
     Episode_Reward/lifting_object: 165.5952
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.56s
                      Time elapsed: 01:19:02
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 38807 steps/s (collection: 2.381s, learning 0.152s)
             Mean action noise std: 4.23
          Mean value_function loss: 159.1532
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.4910
                       Mean reward: 873.79
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.9582
     Episode_Reward/lifting_object: 170.8102
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.53s
                      Time elapsed: 01:19:04
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 38221 steps/s (collection: 2.360s, learning 0.212s)
             Mean action noise std: 4.23
          Mean value_function loss: 125.2630
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.5048
                       Mean reward: 858.07
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.9626
     Episode_Reward/lifting_object: 171.4616
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.57s
                      Time elapsed: 01:19:07
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 38334 steps/s (collection: 2.398s, learning 0.167s)
             Mean action noise std: 4.24
          Mean value_function loss: 164.5625
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.5170
                       Mean reward: 850.36
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.9582
     Episode_Reward/lifting_object: 170.4088
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.56s
                      Time elapsed: 01:19:09
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 38888 steps/s (collection: 2.422s, learning 0.106s)
             Mean action noise std: 4.24
          Mean value_function loss: 123.4534
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.5268
                       Mean reward: 863.48
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.9554
     Episode_Reward/lifting_object: 170.5046
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.53s
                      Time elapsed: 01:19:12
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 37745 steps/s (collection: 2.466s, learning 0.139s)
             Mean action noise std: 4.24
          Mean value_function loss: 143.6333
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.5351
                       Mean reward: 904.52
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 1.9658
     Episode_Reward/lifting_object: 171.6264
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.60s
                      Time elapsed: 01:19:15
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 37059 steps/s (collection: 2.516s, learning 0.136s)
             Mean action noise std: 4.24
          Mean value_function loss: 158.0708
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.5457
                       Mean reward: 838.43
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 1.9131
     Episode_Reward/lifting_object: 166.9273
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1034
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.65s
                      Time elapsed: 01:19:17
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 37710 steps/s (collection: 2.429s, learning 0.178s)
             Mean action noise std: 4.24
          Mean value_function loss: 125.1209
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.5581
                       Mean reward: 899.11
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 1.8885
     Episode_Reward/lifting_object: 164.3952
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.61s
                      Time elapsed: 01:19:20
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 38507 steps/s (collection: 2.416s, learning 0.137s)
             Mean action noise std: 4.25
          Mean value_function loss: 125.0507
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.5761
                       Mean reward: 842.70
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.9456
     Episode_Reward/lifting_object: 169.8117
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.55s
                      Time elapsed: 01:19:22
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 39364 steps/s (collection: 2.365s, learning 0.132s)
             Mean action noise std: 4.25
          Mean value_function loss: 167.1500
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.5877
                       Mean reward: 819.07
               Mean episode length: 219.15
    Episode_Reward/reaching_object: 1.9343
     Episode_Reward/lifting_object: 168.7848
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.50s
                      Time elapsed: 01:19:25
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 38917 steps/s (collection: 2.368s, learning 0.158s)
             Mean action noise std: 4.25
          Mean value_function loss: 114.0115
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.5975
                       Mean reward: 867.48
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.9378
     Episode_Reward/lifting_object: 169.1500
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.53s
                      Time elapsed: 01:19:27
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 36181 steps/s (collection: 2.558s, learning 0.159s)
             Mean action noise std: 4.25
          Mean value_function loss: 120.6417
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.6058
                       Mean reward: 882.70
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 173.7440
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.72s
                      Time elapsed: 01:19:30
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 38049 steps/s (collection: 2.408s, learning 0.176s)
             Mean action noise std: 4.25
          Mean value_function loss: 134.1273
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.6121
                       Mean reward: 852.17
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.9590
     Episode_Reward/lifting_object: 171.0447
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.58s
                      Time elapsed: 01:19:33
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 35716 steps/s (collection: 2.635s, learning 0.118s)
             Mean action noise std: 4.25
          Mean value_function loss: 125.8040
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.6208
                       Mean reward: 887.24
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.9720
     Episode_Reward/lifting_object: 172.7094
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.75s
                      Time elapsed: 01:19:35
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 38169 steps/s (collection: 2.398s, learning 0.178s)
             Mean action noise std: 4.25
          Mean value_function loss: 129.0830
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.6304
                       Mean reward: 856.16
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.9544
     Episode_Reward/lifting_object: 171.1145
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.58s
                      Time elapsed: 01:19:38
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 38675 steps/s (collection: 2.418s, learning 0.124s)
             Mean action noise std: 4.26
          Mean value_function loss: 139.1062
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.6403
                       Mean reward: 901.15
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 1.9348
     Episode_Reward/lifting_object: 169.2753
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.54s
                      Time elapsed: 01:19:41
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 35842 steps/s (collection: 2.583s, learning 0.160s)
             Mean action noise std: 4.26
          Mean value_function loss: 155.1702
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.6496
                       Mean reward: 846.67
               Mean episode length: 226.44
    Episode_Reward/reaching_object: 1.9069
     Episode_Reward/lifting_object: 166.5075
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.74s
                      Time elapsed: 01:19:43
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 36950 steps/s (collection: 2.466s, learning 0.195s)
             Mean action noise std: 4.26
          Mean value_function loss: 143.6583
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.6581
                       Mean reward: 874.40
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.9469
     Episode_Reward/lifting_object: 170.6234
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.66s
                      Time elapsed: 01:19:46
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 35299 steps/s (collection: 2.582s, learning 0.203s)
             Mean action noise std: 4.26
          Mean value_function loss: 117.4535
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.6671
                       Mean reward: 867.75
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.9659
     Episode_Reward/lifting_object: 172.1684
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.78s
                      Time elapsed: 01:19:49
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 35103 steps/s (collection: 2.587s, learning 0.213s)
             Mean action noise std: 4.26
          Mean value_function loss: 110.5964
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.6711
                       Mean reward: 836.24
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 1.9382
     Episode_Reward/lifting_object: 169.9017
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.80s
                      Time elapsed: 01:19:52
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 36250 steps/s (collection: 2.536s, learning 0.176s)
             Mean action noise std: 4.26
          Mean value_function loss: 172.1522
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.6768
                       Mean reward: 765.58
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 1.8748
     Episode_Reward/lifting_object: 164.2106
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0867
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.71s
                      Time elapsed: 01:19:54
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 37177 steps/s (collection: 2.451s, learning 0.194s)
             Mean action noise std: 4.26
          Mean value_function loss: 133.2940
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6862
                       Mean reward: 848.14
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 1.9469
     Episode_Reward/lifting_object: 171.0278
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.64s
                      Time elapsed: 01:19:57
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 37372 steps/s (collection: 2.457s, learning 0.173s)
             Mean action noise std: 4.26
          Mean value_function loss: 127.2795
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6904
                       Mean reward: 826.11
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 1.9152
     Episode_Reward/lifting_object: 168.0413
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.63s
                      Time elapsed: 01:20:00
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 37436 steps/s (collection: 2.440s, learning 0.186s)
             Mean action noise std: 4.27
          Mean value_function loss: 146.5836
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.6965
                       Mean reward: 868.23
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.9564
     Episode_Reward/lifting_object: 171.4439
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.63s
                      Time elapsed: 01:20:02
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 35804 steps/s (collection: 2.565s, learning 0.181s)
             Mean action noise std: 4.27
          Mean value_function loss: 140.9327
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.7099
                       Mean reward: 849.45
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.8905
     Episode_Reward/lifting_object: 165.3686
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.75s
                      Time elapsed: 01:20:05
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 38497 steps/s (collection: 2.415s, learning 0.139s)
             Mean action noise std: 4.27
          Mean value_function loss: 186.6064
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.7266
                       Mean reward: 888.34
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 1.9203
     Episode_Reward/lifting_object: 168.1741
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.55s
                      Time elapsed: 01:20:07
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 37942 steps/s (collection: 2.434s, learning 0.157s)
             Mean action noise std: 4.27
          Mean value_function loss: 131.6376
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.7373
                       Mean reward: 915.24
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 1.9891
     Episode_Reward/lifting_object: 174.5002
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.59s
                      Time elapsed: 01:20:10
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 35864 steps/s (collection: 2.564s, learning 0.177s)
             Mean action noise std: 4.27
          Mean value_function loss: 115.9779
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.7410
                       Mean reward: 882.23
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.9235
     Episode_Reward/lifting_object: 168.5533
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0889
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.74s
                      Time elapsed: 01:20:13
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 35477 steps/s (collection: 2.582s, learning 0.189s)
             Mean action noise std: 4.27
          Mean value_function loss: 128.5361
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.7442
                       Mean reward: 837.67
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.9500
     Episode_Reward/lifting_object: 171.1686
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.77s
                      Time elapsed: 01:20:16
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 35290 steps/s (collection: 2.596s, learning 0.190s)
             Mean action noise std: 4.28
          Mean value_function loss: 148.9943
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.7515
                       Mean reward: 782.75
               Mean episode length: 211.43
    Episode_Reward/reaching_object: 1.8709
     Episode_Reward/lifting_object: 163.4327
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.79s
                      Time elapsed: 01:20:18
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 36409 steps/s (collection: 2.487s, learning 0.213s)
             Mean action noise std: 4.28
          Mean value_function loss: 129.4239
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.7614
                       Mean reward: 813.68
               Mean episode length: 217.46
    Episode_Reward/reaching_object: 1.9667
     Episode_Reward/lifting_object: 172.4588
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1082
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.70s
                      Time elapsed: 01:20:21
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 38411 steps/s (collection: 2.458s, learning 0.101s)
             Mean action noise std: 4.28
          Mean value_function loss: 139.4022
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.7693
                       Mean reward: 866.73
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 1.9326
     Episode_Reward/lifting_object: 169.2404
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.56s
                      Time elapsed: 01:20:24
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 36225 steps/s (collection: 2.550s, learning 0.164s)
             Mean action noise std: 4.28
          Mean value_function loss: 150.4344
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.7779
                       Mean reward: 848.95
               Mean episode length: 225.82
    Episode_Reward/reaching_object: 1.9390
     Episode_Reward/lifting_object: 169.8266
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1068
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.71s
                      Time elapsed: 01:20:26
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 39059 steps/s (collection: 2.390s, learning 0.127s)
             Mean action noise std: 4.28
          Mean value_function loss: 147.3382
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.7918
                       Mean reward: 888.34
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.9790
     Episode_Reward/lifting_object: 173.6436
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.52s
                      Time elapsed: 01:20:29
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 39260 steps/s (collection: 2.334s, learning 0.170s)
             Mean action noise std: 4.28
          Mean value_function loss: 147.4749
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.8087
                       Mean reward: 835.37
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 1.9186
     Episode_Reward/lifting_object: 167.0952
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.50s
                      Time elapsed: 01:20:31
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 38605 steps/s (collection: 2.360s, learning 0.186s)
             Mean action noise std: 4.29
          Mean value_function loss: 130.3292
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.8186
                       Mean reward: 838.96
               Mean episode length: 223.86
    Episode_Reward/reaching_object: 1.8965
     Episode_Reward/lifting_object: 165.6519
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.55s
                      Time elapsed: 01:20:34
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 36918 steps/s (collection: 2.487s, learning 0.176s)
             Mean action noise std: 4.29
          Mean value_function loss: 132.0449
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.8266
                       Mean reward: 839.84
               Mean episode length: 223.06
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 171.5433
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.66s
                      Time elapsed: 01:20:37
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 38179 steps/s (collection: 2.389s, learning 0.186s)
             Mean action noise std: 4.29
          Mean value_function loss: 116.0350
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.8367
                       Mean reward: 867.88
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.9586
     Episode_Reward/lifting_object: 171.4440
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0915
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.57s
                      Time elapsed: 01:20:39
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 34857 steps/s (collection: 2.627s, learning 0.193s)
             Mean action noise std: 4.29
          Mean value_function loss: 118.1350
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.8413
                       Mean reward: 899.60
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 1.9578
     Episode_Reward/lifting_object: 171.6565
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.82s
                      Time elapsed: 01:20:42
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 38186 steps/s (collection: 2.430s, learning 0.144s)
             Mean action noise std: 4.29
          Mean value_function loss: 130.9485
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8492
                       Mean reward: 865.14
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.9301
     Episode_Reward/lifting_object: 168.7008
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.57s
                      Time elapsed: 01:20:45
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 37365 steps/s (collection: 2.486s, learning 0.145s)
             Mean action noise std: 4.29
          Mean value_function loss: 116.7907
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.8585
                       Mean reward: 826.15
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 1.9846
     Episode_Reward/lifting_object: 174.1531
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.63s
                      Time elapsed: 01:20:47
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 37784 steps/s (collection: 2.463s, learning 0.139s)
             Mean action noise std: 4.30
          Mean value_function loss: 102.9947
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.8678
                       Mean reward: 878.42
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 1.9745
     Episode_Reward/lifting_object: 173.0229
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.60s
                      Time elapsed: 01:20:50
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 38976 steps/s (collection: 2.404s, learning 0.118s)
             Mean action noise std: 4.30
          Mean value_function loss: 138.5215
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.8784
                       Mean reward: 838.03
               Mean episode length: 224.44
    Episode_Reward/reaching_object: 1.9292
     Episode_Reward/lifting_object: 168.6760
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1078
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.52s
                      Time elapsed: 01:20:52
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 36393 steps/s (collection: 2.527s, learning 0.174s)
             Mean action noise std: 4.30
          Mean value_function loss: 91.6496
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.8864
                       Mean reward: 909.27
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.9686
     Episode_Reward/lifting_object: 172.5420
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.70s
                      Time elapsed: 01:20:55
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 38958 steps/s (collection: 2.395s, learning 0.129s)
             Mean action noise std: 4.30
          Mean value_function loss: 101.4143
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.8904
                       Mean reward: 887.39
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 1.9833
     Episode_Reward/lifting_object: 173.9979
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.52s
                      Time elapsed: 01:20:57
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 38344 steps/s (collection: 2.413s, learning 0.151s)
             Mean action noise std: 4.30
          Mean value_function loss: 144.0804
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.8994
                       Mean reward: 865.50
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 1.9427
     Episode_Reward/lifting_object: 170.6669
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.56s
                      Time elapsed: 01:21:00
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 38499 steps/s (collection: 2.405s, learning 0.148s)
             Mean action noise std: 4.30
          Mean value_function loss: 121.2721
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.9099
                       Mean reward: 824.73
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.9462
     Episode_Reward/lifting_object: 170.9495
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.55s
                      Time elapsed: 01:21:03
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 38913 steps/s (collection: 2.394s, learning 0.132s)
             Mean action noise std: 4.30
          Mean value_function loss: 144.7087
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.9166
                       Mean reward: 899.48
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 1.9672
     Episode_Reward/lifting_object: 172.6405
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.53s
                      Time elapsed: 01:21:05
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 39177 steps/s (collection: 2.378s, learning 0.132s)
             Mean action noise std: 4.30
          Mean value_function loss: 176.6275
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 73.9197
                       Mean reward: 825.10
               Mean episode length: 221.24
    Episode_Reward/reaching_object: 1.8808
     Episode_Reward/lifting_object: 164.6136
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.51s
                      Time elapsed: 01:21:08
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 38813 steps/s (collection: 2.409s, learning 0.124s)
             Mean action noise std: 4.30
          Mean value_function loss: 156.6297
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.9216
                       Mean reward: 844.31
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 1.9013
     Episode_Reward/lifting_object: 166.5078
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.53s
                      Time elapsed: 01:21:10
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 38422 steps/s (collection: 2.364s, learning 0.195s)
             Mean action noise std: 4.30
          Mean value_function loss: 155.2771
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.9254
                       Mean reward: 819.11
               Mean episode length: 218.31
    Episode_Reward/reaching_object: 1.9088
     Episode_Reward/lifting_object: 167.7080
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.56s
                      Time elapsed: 01:21:13
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 36362 steps/s (collection: 2.564s, learning 0.140s)
             Mean action noise std: 4.31
          Mean value_function loss: 156.2955
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.9346
                       Mean reward: 844.36
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 1.9306
     Episode_Reward/lifting_object: 169.3871
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.70s
                      Time elapsed: 01:21:15
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 39796 steps/s (collection: 2.357s, learning 0.114s)
             Mean action noise std: 4.31
          Mean value_function loss: 173.4564
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.9446
                       Mean reward: 845.99
               Mean episode length: 224.98
    Episode_Reward/reaching_object: 1.9584
     Episode_Reward/lifting_object: 171.6488
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.47s
                      Time elapsed: 01:21:18
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 38379 steps/s (collection: 2.367s, learning 0.195s)
             Mean action noise std: 4.31
          Mean value_function loss: 138.1994
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.9480
                       Mean reward: 850.97
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 1.9384
     Episode_Reward/lifting_object: 169.8491
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.56s
                      Time elapsed: 01:21:20
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 36961 steps/s (collection: 2.535s, learning 0.125s)
             Mean action noise std: 4.31
          Mean value_function loss: 135.3474
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.9524
                       Mean reward: 861.37
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.9583
     Episode_Reward/lifting_object: 171.4845
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.66s
                      Time elapsed: 01:21:23
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 36916 steps/s (collection: 2.506s, learning 0.157s)
             Mean action noise std: 4.31
          Mean value_function loss: 133.6184
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.9588
                       Mean reward: 867.40
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.9450
     Episode_Reward/lifting_object: 170.0046
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.66s
                      Time elapsed: 01:21:26
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 33689 steps/s (collection: 2.718s, learning 0.200s)
             Mean action noise std: 4.31
          Mean value_function loss: 134.6062
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.9676
                       Mean reward: 907.80
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 1.9600
     Episode_Reward/lifting_object: 171.2883
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.92s
                      Time elapsed: 01:21:29
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 33247 steps/s (collection: 2.797s, learning 0.160s)
             Mean action noise std: 4.31
          Mean value_function loss: 170.1756
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.9739
                       Mean reward: 867.57
               Mean episode length: 229.20
    Episode_Reward/reaching_object: 1.8805
     Episode_Reward/lifting_object: 164.3831
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.96s
                      Time elapsed: 01:21:32
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 36423 steps/s (collection: 2.552s, learning 0.147s)
             Mean action noise std: 4.32
          Mean value_function loss: 145.5716
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.9839
                       Mean reward: 824.57
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 1.9260
     Episode_Reward/lifting_object: 167.9877
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1087
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.70s
                      Time elapsed: 01:21:34
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 32182 steps/s (collection: 2.888s, learning 0.166s)
             Mean action noise std: 4.32
          Mean value_function loss: 131.9336
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.9892
                       Mean reward: 895.61
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.9241
     Episode_Reward/lifting_object: 167.9725
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 3.05s
                      Time elapsed: 01:21:37
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 32024 steps/s (collection: 2.921s, learning 0.149s)
             Mean action noise std: 4.32
          Mean value_function loss: 145.9411
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.9959
                       Mean reward: 856.62
               Mean episode length: 228.11
    Episode_Reward/reaching_object: 1.9289
     Episode_Reward/lifting_object: 168.6461
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 3.07s
                      Time elapsed: 01:21:40
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 36504 steps/s (collection: 2.520s, learning 0.173s)
             Mean action noise std: 4.32
          Mean value_function loss: 137.6939
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 74.0038
                       Mean reward: 822.67
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.8936
     Episode_Reward/lifting_object: 165.2748
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.69s
                      Time elapsed: 01:21:43
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 36024 steps/s (collection: 2.526s, learning 0.203s)
             Mean action noise std: 4.32
          Mean value_function loss: 105.8638
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.0120
                       Mean reward: 847.64
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 1.9551
     Episode_Reward/lifting_object: 170.9298
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.73s
                      Time elapsed: 01:21:46
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 33983 steps/s (collection: 2.748s, learning 0.145s)
             Mean action noise std: 4.32
          Mean value_function loss: 137.2334
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.0231
                       Mean reward: 917.34
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 1.9802
     Episode_Reward/lifting_object: 173.6101
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.89s
                      Time elapsed: 01:21:49
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 31290 steps/s (collection: 2.890s, learning 0.252s)
             Mean action noise std: 4.32
          Mean value_function loss: 124.2316
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.0358
                       Mean reward: 815.55
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.9357
     Episode_Reward/lifting_object: 169.3330
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 3.14s
                      Time elapsed: 01:21:52
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 29860 steps/s (collection: 3.044s, learning 0.248s)
             Mean action noise std: 4.33
          Mean value_function loss: 121.7064
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.0426
                       Mean reward: 840.42
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 1.9207
     Episode_Reward/lifting_object: 168.0796
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 3.29s
                      Time elapsed: 01:21:55
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 32663 steps/s (collection: 2.817s, learning 0.192s)
             Mean action noise std: 4.33
          Mean value_function loss: 104.7966
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.0514
                       Mean reward: 876.84
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.9702
     Episode_Reward/lifting_object: 172.6813
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 3.01s
                      Time elapsed: 01:21:58
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 31026 steps/s (collection: 2.979s, learning 0.190s)
             Mean action noise std: 4.33
          Mean value_function loss: 143.5346
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.0639
                       Mean reward: 841.29
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.9406
     Episode_Reward/lifting_object: 169.9920
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 3.17s
                      Time elapsed: 01:22:01
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 32005 steps/s (collection: 2.920s, learning 0.151s)
             Mean action noise std: 4.33
          Mean value_function loss: 127.9880
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0733
                       Mean reward: 849.54
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 1.9101
     Episode_Reward/lifting_object: 167.3669
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 3.07s
                      Time elapsed: 01:22:04
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 31748 steps/s (collection: 2.946s, learning 0.151s)
             Mean action noise std: 4.33
          Mean value_function loss: 146.3304
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.0806
                       Mean reward: 840.02
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 1.9166
     Episode_Reward/lifting_object: 168.3899
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 3.10s
                      Time elapsed: 01:22:08
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 32672 steps/s (collection: 2.880s, learning 0.129s)
             Mean action noise std: 4.34
          Mean value_function loss: 124.3235
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.0933
                       Mean reward: 852.16
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 1.9370
     Episode_Reward/lifting_object: 169.8479
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 3.01s
                      Time elapsed: 01:22:11
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 33451 steps/s (collection: 2.734s, learning 0.205s)
             Mean action noise std: 4.34
          Mean value_function loss: 109.0555
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.1090
                       Mean reward: 873.24
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 174.0890
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.94s
                      Time elapsed: 01:22:14
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 29478 steps/s (collection: 3.142s, learning 0.193s)
             Mean action noise std: 4.34
          Mean value_function loss: 131.3700
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.1214
                       Mean reward: 879.43
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 1.9451
     Episode_Reward/lifting_object: 171.2076
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 3.33s
                      Time elapsed: 01:22:17
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 31651 steps/s (collection: 2.938s, learning 0.168s)
             Mean action noise std: 4.34
          Mean value_function loss: 126.6762
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.1299
                       Mean reward: 859.94
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.9400
     Episode_Reward/lifting_object: 170.4792
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 3.11s
                      Time elapsed: 01:22:20
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 29838 steps/s (collection: 3.036s, learning 0.259s)
             Mean action noise std: 4.34
          Mean value_function loss: 139.4455
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.1402
                       Mean reward: 869.65
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 1.9294
     Episode_Reward/lifting_object: 170.0006
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 3.29s
                      Time elapsed: 01:22:23
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 30576 steps/s (collection: 3.014s, learning 0.201s)
             Mean action noise std: 4.35
          Mean value_function loss: 114.2538
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.1516
                       Mean reward: 906.68
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 1.9663
     Episode_Reward/lifting_object: 172.6656
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1116
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 3.22s
                      Time elapsed: 01:22:26
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 33869 steps/s (collection: 2.736s, learning 0.167s)
             Mean action noise std: 4.35
          Mean value_function loss: 157.9882
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.1674
                       Mean reward: 878.32
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.9497
     Episode_Reward/lifting_object: 171.7007
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.90s
                      Time elapsed: 01:22:29
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 31889 steps/s (collection: 2.772s, learning 0.310s)
             Mean action noise std: 4.35
          Mean value_function loss: 212.2163
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1856
                       Mean reward: 888.85
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.9758
     Episode_Reward/lifting_object: 173.8521
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 3.08s
                      Time elapsed: 01:22:32
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 27102 steps/s (collection: 3.408s, learning 0.220s)
             Mean action noise std: 4.35
          Mean value_function loss: 130.6767
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.1973
                       Mean reward: 850.10
               Mean episode length: 227.57
    Episode_Reward/reaching_object: 1.9468
     Episode_Reward/lifting_object: 170.9351
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 3.63s
                      Time elapsed: 01:22:36
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 26806 steps/s (collection: 3.206s, learning 0.461s)
             Mean action noise std: 4.36
          Mean value_function loss: 138.3639
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.2088
                       Mean reward: 838.31
               Mean episode length: 224.29
    Episode_Reward/reaching_object: 1.9314
     Episode_Reward/lifting_object: 169.7092
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 3.67s
                      Time elapsed: 01:22:40
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 21333 steps/s (collection: 4.197s, learning 0.411s)
             Mean action noise std: 4.36
          Mean value_function loss: 127.5112
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.2207
                       Mean reward: 854.50
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 1.9366
     Episode_Reward/lifting_object: 170.4169
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 4.61s
                      Time elapsed: 01:22:44
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 25448 steps/s (collection: 3.569s, learning 0.294s)
             Mean action noise std: 4.36
          Mean value_function loss: 103.3963
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.2315
                       Mean reward: 907.10
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 1.9865
     Episode_Reward/lifting_object: 174.9765
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 3.86s
                      Time elapsed: 01:22:48
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 24138 steps/s (collection: 3.749s, learning 0.323s)
             Mean action noise std: 4.36
          Mean value_function loss: 131.2811
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.2431
                       Mean reward: 847.23
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.9647
     Episode_Reward/lifting_object: 172.9438
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0910
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 4.07s
                      Time elapsed: 01:22:52
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 25859 steps/s (collection: 3.501s, learning 0.301s)
             Mean action noise std: 4.36
          Mean value_function loss: 150.2251
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.2502
                       Mean reward: 865.17
               Mean episode length: 230.75
    Episode_Reward/reaching_object: 1.9285
     Episode_Reward/lifting_object: 168.7680
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 3.80s
                      Time elapsed: 01:22:56
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 25929 steps/s (collection: 3.511s, learning 0.280s)
             Mean action noise std: 4.36
          Mean value_function loss: 129.5700
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2615
                       Mean reward: 894.33
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 1.9102
     Episode_Reward/lifting_object: 167.6446
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0886
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 3.79s
                      Time elapsed: 01:23:00
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 24382 steps/s (collection: 3.760s, learning 0.272s)
             Mean action noise std: 4.37
          Mean value_function loss: 116.9561
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.2680
                       Mean reward: 910.34
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.9500
     Episode_Reward/lifting_object: 171.3741
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 4.03s
                      Time elapsed: 01:23:04
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 25347 steps/s (collection: 3.601s, learning 0.278s)
             Mean action noise std: 4.37
          Mean value_function loss: 125.5524
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.2707
                       Mean reward: 904.25
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 1.9411
     Episode_Reward/lifting_object: 170.4853
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 3.88s
                      Time elapsed: 01:23:08
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 25999 steps/s (collection: 3.487s, learning 0.294s)
             Mean action noise std: 4.37
          Mean value_function loss: 131.6362
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.2737
                       Mean reward: 851.76
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.9403
     Episode_Reward/lifting_object: 170.6274
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 3.78s
                      Time elapsed: 01:23:12
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 25190 steps/s (collection: 3.598s, learning 0.305s)
             Mean action noise std: 4.37
          Mean value_function loss: 143.1525
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.2813
                       Mean reward: 850.84
               Mean episode length: 226.20
    Episode_Reward/reaching_object: 1.9626
     Episode_Reward/lifting_object: 172.4696
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1129
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 3.90s
                      Time elapsed: 01:23:15
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 26702 steps/s (collection: 3.445s, learning 0.236s)
             Mean action noise std: 4.37
          Mean value_function loss: 120.1581
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.2921
                       Mean reward: 842.72
               Mean episode length: 223.96
    Episode_Reward/reaching_object: 1.9313
     Episode_Reward/lifting_object: 169.3636
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0888
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 3.68s
                      Time elapsed: 01:23:19
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 28355 steps/s (collection: 3.206s, learning 0.260s)
             Mean action noise std: 4.37
          Mean value_function loss: 120.4185
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.3001
                       Mean reward: 904.08
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 1.9704
     Episode_Reward/lifting_object: 173.1264
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 3.47s
                      Time elapsed: 01:23:23
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 29334 steps/s (collection: 3.189s, learning 0.162s)
             Mean action noise std: 4.37
          Mean value_function loss: 147.5368
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.3048
                       Mean reward: 829.96
               Mean episode length: 224.35
    Episode_Reward/reaching_object: 1.9504
     Episode_Reward/lifting_object: 170.2088
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 3.35s
                      Time elapsed: 01:23:26
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 30417 steps/s (collection: 2.965s, learning 0.267s)
             Mean action noise std: 4.37
          Mean value_function loss: 109.7303
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.3080
                       Mean reward: 907.68
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 1.9724
     Episode_Reward/lifting_object: 173.1086
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 3.23s
                      Time elapsed: 01:23:29
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 27847 steps/s (collection: 3.319s, learning 0.211s)
             Mean action noise std: 4.38
          Mean value_function loss: 99.5476
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.3175
                       Mean reward: 884.95
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 2.0065
     Episode_Reward/lifting_object: 176.4697
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1150
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 3.53s
                      Time elapsed: 01:23:33
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 30639 steps/s (collection: 3.026s, learning 0.182s)
             Mean action noise std: 4.38
          Mean value_function loss: 133.7546
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.3303
                       Mean reward: 857.23
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.9640
     Episode_Reward/lifting_object: 172.4549
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 3.21s
                      Time elapsed: 01:23:36
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 34715 steps/s (collection: 2.640s, learning 0.192s)
             Mean action noise std: 4.38
          Mean value_function loss: 122.4957
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.3420
                       Mean reward: 868.94
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.9681
     Episode_Reward/lifting_object: 172.3829
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.83s
                      Time elapsed: 01:23:39
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 37840 steps/s (collection: 2.489s, learning 0.109s)
             Mean action noise std: 4.38
          Mean value_function loss: 150.9506
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.3519
                       Mean reward: 852.42
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.9045
     Episode_Reward/lifting_object: 166.4502
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.60s
                      Time elapsed: 01:23:41
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 38663 steps/s (collection: 2.360s, learning 0.182s)
             Mean action noise std: 4.38
          Mean value_function loss: 135.0163
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.3586
                       Mean reward: 884.63
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.9733
     Episode_Reward/lifting_object: 172.8704
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.54s
                      Time elapsed: 01:23:44
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 40213 steps/s (collection: 2.316s, learning 0.128s)
             Mean action noise std: 4.38
          Mean value_function loss: 210.2569
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.3694
                       Mean reward: 902.46
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.9463
     Episode_Reward/lifting_object: 170.7850
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.44s
                      Time elapsed: 01:23:46
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 38677 steps/s (collection: 2.398s, learning 0.144s)
             Mean action noise std: 4.39
          Mean value_function loss: 155.4067
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.3788
                       Mean reward: 900.27
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.9799
     Episode_Reward/lifting_object: 174.3553
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.54s
                      Time elapsed: 01:23:49
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 38653 steps/s (collection: 2.358s, learning 0.186s)
             Mean action noise std: 4.39
          Mean value_function loss: 134.5269
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.3885
                       Mean reward: 895.96
               Mean episode length: 236.71
    Episode_Reward/reaching_object: 1.9270
     Episode_Reward/lifting_object: 169.1403
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.54s
                      Time elapsed: 01:23:51
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 38436 steps/s (collection: 2.379s, learning 0.179s)
             Mean action noise std: 4.39
          Mean value_function loss: 150.2623
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.3983
                       Mean reward: 796.83
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.8953
     Episode_Reward/lifting_object: 166.6184
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.56s
                      Time elapsed: 01:23:54
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 38764 steps/s (collection: 2.413s, learning 0.123s)
             Mean action noise std: 4.39
          Mean value_function loss: 91.6078
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.4065
                       Mean reward: 906.66
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 1.9856
     Episode_Reward/lifting_object: 175.1968
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.54s
                      Time elapsed: 01:23:57
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 37039 steps/s (collection: 2.448s, learning 0.206s)
             Mean action noise std: 4.39
          Mean value_function loss: 150.1995
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.4155
                       Mean reward: 875.30
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 172.1992
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.65s
                      Time elapsed: 01:23:59
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 38103 steps/s (collection: 2.395s, learning 0.185s)
             Mean action noise std: 4.40
          Mean value_function loss: 151.4447
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.4290
                       Mean reward: 856.56
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.9560
     Episode_Reward/lifting_object: 172.2027
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.58s
                      Time elapsed: 01:24:02
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 37726 steps/s (collection: 2.407s, learning 0.199s)
             Mean action noise std: 4.40
          Mean value_function loss: 148.1117
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.4423
                       Mean reward: 825.23
               Mean episode length: 221.05
    Episode_Reward/reaching_object: 1.9144
     Episode_Reward/lifting_object: 167.9448
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.61s
                      Time elapsed: 01:24:04
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 37656 steps/s (collection: 2.493s, learning 0.118s)
             Mean action noise std: 4.40
          Mean value_function loss: 135.5364
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.4523
                       Mean reward: 842.90
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.9468
     Episode_Reward/lifting_object: 171.2358
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.61s
                      Time elapsed: 01:24:07
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 38137 steps/s (collection: 2.425s, learning 0.152s)
             Mean action noise std: 4.40
          Mean value_function loss: 134.3901
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.4627
                       Mean reward: 876.48
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.9228
     Episode_Reward/lifting_object: 168.6332
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1120
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.58s
                      Time elapsed: 01:24:10
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 35970 steps/s (collection: 2.494s, learning 0.239s)
             Mean action noise std: 4.40
          Mean value_function loss: 93.7898
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.4698
                       Mean reward: 865.96
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 2.0136
     Episode_Reward/lifting_object: 176.6979
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0924
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.73s
                      Time elapsed: 01:24:12
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 38033 steps/s (collection: 2.411s, learning 0.173s)
             Mean action noise std: 4.40
          Mean value_function loss: 135.0090
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.4769
                       Mean reward: 872.47
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.9769
     Episode_Reward/lifting_object: 173.8287
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.58s
                      Time elapsed: 01:24:15
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 34976 steps/s (collection: 2.604s, learning 0.206s)
             Mean action noise std: 4.41
          Mean value_function loss: 129.2782
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.4855
                       Mean reward: 891.52
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.9603
     Episode_Reward/lifting_object: 172.3592
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1135
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.81s
                      Time elapsed: 01:24:18
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 38166 steps/s (collection: 2.421s, learning 0.155s)
             Mean action noise std: 4.41
          Mean value_function loss: 115.9276
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.4960
                       Mean reward: 865.89
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.9714
     Episode_Reward/lifting_object: 173.0516
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.58s
                      Time elapsed: 01:24:20
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 37857 steps/s (collection: 2.424s, learning 0.173s)
             Mean action noise std: 4.41
          Mean value_function loss: 148.3848
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.5072
                       Mean reward: 840.54
               Mean episode length: 225.02
    Episode_Reward/reaching_object: 1.9528
     Episode_Reward/lifting_object: 171.5600
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.60s
                      Time elapsed: 01:24:23
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 39363 steps/s (collection: 2.392s, learning 0.106s)
             Mean action noise std: 4.41
          Mean value_function loss: 115.4081
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.5207
                       Mean reward: 858.69
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 1.9185
     Episode_Reward/lifting_object: 168.7538
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.50s
                      Time elapsed: 01:24:25
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 36493 steps/s (collection: 2.545s, learning 0.149s)
             Mean action noise std: 4.41
          Mean value_function loss: 134.1743
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.5330
                       Mean reward: 860.00
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.9593
     Episode_Reward/lifting_object: 172.6841
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.69s
                      Time elapsed: 01:24:28
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 35599 steps/s (collection: 2.546s, learning 0.216s)
             Mean action noise std: 4.42
          Mean value_function loss: 156.8265
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.5451
                       Mean reward: 819.63
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.9157
     Episode_Reward/lifting_object: 168.5245
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.76s
                      Time elapsed: 01:24:31
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 35591 steps/s (collection: 2.544s, learning 0.218s)
             Mean action noise std: 4.42
          Mean value_function loss: 115.7106
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.5544
                       Mean reward: 841.43
               Mean episode length: 224.37
    Episode_Reward/reaching_object: 1.9711
     Episode_Reward/lifting_object: 173.2241
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.76s
                      Time elapsed: 01:24:34
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 37231 steps/s (collection: 2.468s, learning 0.172s)
             Mean action noise std: 4.42
          Mean value_function loss: 132.7048
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.5615
                       Mean reward: 852.52
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.9460
     Episode_Reward/lifting_object: 171.2769
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.64s
                      Time elapsed: 01:24:36
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 37559 steps/s (collection: 2.428s, learning 0.189s)
             Mean action noise std: 4.42
          Mean value_function loss: 114.6112
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.5694
                       Mean reward: 876.14
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.9686
     Episode_Reward/lifting_object: 173.1597
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.62s
                      Time elapsed: 01:24:39
                               ETA: 00:00:02

