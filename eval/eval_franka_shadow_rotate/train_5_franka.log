################################################################################
                      [1m Learning iteration 0/1500 [0m                       

                       Computation: 17570 steps/s (collection: 5.367s, learning 0.228s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 11.3726
                       Mean reward: 0.00
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0002
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.59s
                      Time elapsed: 00:00:05
                               ETA: 02:19:52

################################################################################
                      [1m Learning iteration 1/1500 [0m                       

                       Computation: 35198 steps/s (collection: 2.646s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 11.4287
                       Mean reward: 0.00
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0009
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.79s
                      Time elapsed: 00:00:08
                               ETA: 01:44:46

################################################################################
                      [1m Learning iteration 2/1500 [0m                       

                       Computation: 34699 steps/s (collection: 2.673s, learning 0.160s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 11.4264
                       Mean reward: 0.01
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0019
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.83s
                      Time elapsed: 00:00:11
                               ETA: 01:33:22

################################################################################
                      [1m Learning iteration 3/1500 [0m                       

                       Computation: 33479 steps/s (collection: 2.801s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 11.4270
                       Mean reward: 0.01
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0029
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.94s
                      Time elapsed: 00:00:14
                               ETA: 01:28:18

################################################################################
                      [1m Learning iteration 4/1500 [0m                       

                       Computation: 35601 steps/s (collection: 2.624s, learning 0.137s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 11.4163
                       Mean reward: 0.01
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0037
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.76s
                      Time elapsed: 00:00:16
                               ETA: 01:24:21

################################################################################
                      [1m Learning iteration 5/1500 [0m                       

                       Computation: 34271 steps/s (collection: 2.714s, learning 0.154s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.4109
                       Mean reward: 0.01
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0048
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.87s
                      Time elapsed: 00:00:19
                               ETA: 01:22:10

################################################################################
                      [1m Learning iteration 6/1500 [0m                       

                       Computation: 34130 steps/s (collection: 2.729s, learning 0.151s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 11.4120
                       Mean reward: 0.02
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0057
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.88s
                      Time elapsed: 00:00:22
                               ETA: 01:20:37

################################################################################
                      [1m Learning iteration 7/1500 [0m                       

                       Computation: 35545 steps/s (collection: 2.619s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 11.4164
                       Mean reward: 0.02
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0070
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.77s
                      Time elapsed: 00:00:25
                               ETA: 01:19:06

################################################################################
                      [1m Learning iteration 8/1500 [0m                       

                       Computation: 26556 steps/s (collection: 3.560s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 11.4141
                       Mean reward: 0.03
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0077
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.70s
                      Time elapsed: 00:00:29
                               ETA: 01:20:29

################################################################################
                      [1m Learning iteration 9/1500 [0m                       

                       Computation: 130769 steps/s (collection: 0.648s, learning 0.104s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 11.4024
                       Mean reward: 0.04
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0109
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.75s
                      Time elapsed: 00:00:29
                               ETA: 01:14:15

################################################################################
                      [1m Learning iteration 10/1500 [0m                      

                       Computation: 139425 steps/s (collection: 0.594s, learning 0.111s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 11.3745
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0126
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.71s
                      Time elapsed: 00:00:30
                               ETA: 01:09:03

################################################################################
                      [1m Learning iteration 11/1500 [0m                      

                       Computation: 128650 steps/s (collection: 0.677s, learning 0.088s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 11.3828
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0162
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.76s
                      Time elapsed: 00:00:31
                               ETA: 01:04:50

################################################################################
                      [1m Learning iteration 12/1500 [0m                      

                       Computation: 133630 steps/s (collection: 0.645s, learning 0.091s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 11.3677
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0193
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.74s
                      Time elapsed: 00:00:32
                               ETA: 01:01:13

################################################################################
                      [1m Learning iteration 13/1500 [0m                      

                       Computation: 129553 steps/s (collection: 0.657s, learning 0.102s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 11.3472
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0221
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.76s
                      Time elapsed: 00:00:32
                               ETA: 00:58:09

################################################################################
                      [1m Learning iteration 14/1500 [0m                      

                       Computation: 131448 steps/s (collection: 0.646s, learning 0.102s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 11.3471
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0302
    Episode_Reward/rotating_object: 0.0002
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.75s
                      Time elapsed: 00:00:33
                               ETA: 00:55:28

################################################################################
                      [1m Learning iteration 15/1500 [0m                      

                       Computation: 132806 steps/s (collection: 0.654s, learning 0.086s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 11.3501
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0386
    Episode_Reward/rotating_object: 0.0006
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.74s
                      Time elapsed: 00:00:34
                               ETA: 00:53:06

################################################################################
                      [1m Learning iteration 16/1500 [0m                      

                       Computation: 125584 steps/s (collection: 0.691s, learning 0.092s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0058
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 11.3587
                       Mean reward: 0.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0531
    Episode_Reward/rotating_object: 0.0028
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.78s
                      Time elapsed: 00:00:35
                               ETA: 00:51:05

################################################################################
                      [1m Learning iteration 17/1500 [0m                      

                       Computation: 116286 steps/s (collection: 0.700s, learning 0.146s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0322
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.3957
                       Mean reward: 0.39
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.0646
    Episode_Reward/rotating_object: 0.0089
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.85s
                      Time elapsed: 00:00:35
                               ETA: 00:49:23

################################################################################
                      [1m Learning iteration 18/1500 [0m                      

                       Computation: 122896 steps/s (collection: 0.673s, learning 0.127s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0530
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 11.4620
                       Mean reward: 0.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0828
    Episode_Reward/rotating_object: 0.0353
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.80s
                      Time elapsed: 00:00:36
                               ETA: 00:47:47

################################################################################
                      [1m Learning iteration 19/1500 [0m                      

                       Computation: 122072 steps/s (collection: 0.689s, learning 0.116s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1032
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 11.5297
                       Mean reward: 0.82
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.1021
    Episode_Reward/rotating_object: 0.0356
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.81s
                      Time elapsed: 00:00:37
                               ETA: 00:46:22

################################################################################
                      [1m Learning iteration 20/1500 [0m                      

                       Computation: 124087 steps/s (collection: 0.702s, learning 0.090s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.3709
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.5680
                       Mean reward: 2.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1220
    Episode_Reward/rotating_object: 0.1489
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.79s
                      Time elapsed: 00:00:38
                               ETA: 00:45:03

################################################################################
                      [1m Learning iteration 21/1500 [0m                      

                       Computation: 123384 steps/s (collection: 0.700s, learning 0.097s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2099
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.6063
                       Mean reward: 1.34
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.1395
    Episode_Reward/rotating_object: 0.0977
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.80s
                      Time elapsed: 00:00:39
                               ETA: 00:43:52

################################################################################
                      [1m Learning iteration 22/1500 [0m                      

                       Computation: 108646 steps/s (collection: 0.769s, learning 0.136s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.1135
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 11.6709
                       Mean reward: 1.34
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.1552
    Episode_Reward/rotating_object: 0.2023
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.90s
                      Time elapsed: 00:00:40
                               ETA: 00:42:54

################################################################################
                      [1m Learning iteration 23/1500 [0m                      

                       Computation: 108583 steps/s (collection: 0.765s, learning 0.140s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.1125
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.7037
                       Mean reward: 2.26
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.1695
    Episode_Reward/rotating_object: 0.1897
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.91s
                      Time elapsed: 00:00:40
                               ETA: 00:42:01

################################################################################
                      [1m Learning iteration 24/1500 [0m                      

                       Computation: 114727 steps/s (collection: 0.720s, learning 0.137s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.3327
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 11.7536
                       Mean reward: 2.48
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.1829
    Episode_Reward/rotating_object: 0.3533
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                      Time elapsed: 00:00:41
                               ETA: 00:41:09

################################################################################
                      [1m Learning iteration 25/1500 [0m                      

                       Computation: 111918 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.2266
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 11.8336
                       Mean reward: 3.60
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.1961
    Episode_Reward/rotating_object: 0.4314
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.88s
                      Time elapsed: 00:00:42
                               ETA: 00:40:22

################################################################################
                      [1m Learning iteration 26/1500 [0m                      

                       Computation: 106408 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1930
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.8999
                       Mean reward: 2.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2014
    Episode_Reward/rotating_object: 0.2781
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.92s
                      Time elapsed: 00:00:43
                               ETA: 00:39:41

################################################################################
                      [1m Learning iteration 27/1500 [0m                      

                       Computation: 113578 steps/s (collection: 0.759s, learning 0.107s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2539
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 11.9667
                       Mean reward: 3.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2036
    Episode_Reward/rotating_object: 0.4353
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.87s
                      Time elapsed: 00:00:44
                               ETA: 00:39:00

################################################################################
                      [1m Learning iteration 28/1500 [0m                      

                       Computation: 96422 steps/s (collection: 0.848s, learning 0.172s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1849
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 12.0162
                       Mean reward: 3.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2085
    Episode_Reward/rotating_object: 0.3645
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.02s
                      Time elapsed: 00:00:45
                               ETA: 00:38:30

################################################################################
                      [1m Learning iteration 29/1500 [0m                      

                       Computation: 95949 steps/s (collection: 0.934s, learning 0.091s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0854
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 12.0591
                       Mean reward: 2.63
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.2146
    Episode_Reward/rotating_object: 0.4213
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.02s
                      Time elapsed: 00:00:46
                               ETA: 00:38:01

################################################################################
                      [1m Learning iteration 30/1500 [0m                      

                       Computation: 122782 steps/s (collection: 0.699s, learning 0.101s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.1282
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.1137
                       Mean reward: 2.58
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.2141
    Episode_Reward/rotating_object: 0.3632
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.80s
                      Time elapsed: 00:00:47
                               ETA: 00:37:24

################################################################################
                      [1m Learning iteration 31/1500 [0m                      

                       Computation: 118082 steps/s (collection: 0.747s, learning 0.086s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.2193
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 12.1582
                       Mean reward: 5.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2098
    Episode_Reward/rotating_object: 0.4883
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.83s
                      Time elapsed: 00:00:48
                               ETA: 00:36:51

################################################################################
                      [1m Learning iteration 32/1500 [0m                      

                       Computation: 110078 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.1337
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.2205
                       Mean reward: 2.88
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.2140
    Episode_Reward/rotating_object: 0.3573
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.89s
                      Time elapsed: 00:00:49
                               ETA: 00:36:22

################################################################################
                      [1m Learning iteration 33/1500 [0m                      

                       Computation: 117313 steps/s (collection: 0.737s, learning 0.101s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1143
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 12.2929
                       Mean reward: 2.45
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.2180
    Episode_Reward/rotating_object: 0.4026
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.84s
                      Time elapsed: 00:00:49
                               ETA: 00:35:53

################################################################################
                      [1m Learning iteration 34/1500 [0m                      

                       Computation: 108520 steps/s (collection: 0.753s, learning 0.153s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0999
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 12.3382
                       Mean reward: 3.07
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.2153
    Episode_Reward/rotating_object: 0.4708
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.91s
                      Time elapsed: 00:00:50
                               ETA: 00:35:28

################################################################################
                      [1m Learning iteration 35/1500 [0m                      

                       Computation: 121670 steps/s (collection: 0.721s, learning 0.087s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1055
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.4180
                       Mean reward: 3.30
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.2268
    Episode_Reward/rotating_object: 0.4431
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.81s
                      Time elapsed: 00:00:51
                               ETA: 00:35:00

################################################################################
                      [1m Learning iteration 36/1500 [0m                      

                       Computation: 123496 steps/s (collection: 0.705s, learning 0.091s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2501
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 12.4382
                       Mean reward: 4.78
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.2330
    Episode_Reward/rotating_object: 0.4863
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.80s
                      Time elapsed: 00:00:52
                               ETA: 00:34:33

################################################################################
                      [1m Learning iteration 37/1500 [0m                      

                       Computation: 111478 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.1814
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.4781
                       Mean reward: 3.60
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.2343
    Episode_Reward/rotating_object: 0.4686
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.88s
                      Time elapsed: 00:00:53
                               ETA: 00:34:11

################################################################################
                      [1m Learning iteration 38/1500 [0m                      

                       Computation: 119939 steps/s (collection: 0.714s, learning 0.106s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2688
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 12.5087
                       Mean reward: 3.22
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.2406
    Episode_Reward/rotating_object: 0.4402
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.82s
                      Time elapsed: 00:00:54
                               ETA: 00:33:48

################################################################################
                      [1m Learning iteration 39/1500 [0m                      

                       Computation: 114441 steps/s (collection: 0.711s, learning 0.148s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.4028
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 12.5500
                       Mean reward: 4.70
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.2430
    Episode_Reward/rotating_object: 0.5623
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.86s
                      Time elapsed: 00:00:54
                               ETA: 00:33:27

################################################################################
                      [1m Learning iteration 40/1500 [0m                      

                       Computation: 112083 steps/s (collection: 0.781s, learning 0.097s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.4557
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 12.5865
                       Mean reward: 4.31
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.2541
    Episode_Reward/rotating_object: 0.6113
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.88s
                      Time elapsed: 00:00:55
                               ETA: 00:33:08

################################################################################
                      [1m Learning iteration 41/1500 [0m                      

                       Computation: 106347 steps/s (collection: 0.754s, learning 0.170s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.5292
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 12.6581
                       Mean reward: 4.44
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.2613
    Episode_Reward/rotating_object: 0.5483
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.92s
                      Time elapsed: 00:00:56
                               ETA: 00:32:52

################################################################################
                      [1m Learning iteration 42/1500 [0m                      

                       Computation: 101649 steps/s (collection: 0.815s, learning 0.153s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.3404
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 12.6885
                       Mean reward: 6.48
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.2673
    Episode_Reward/rotating_object: 0.7515
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.97s
                      Time elapsed: 00:00:57
                               ETA: 00:32:37

################################################################################
                      [1m Learning iteration 43/1500 [0m                      

                       Computation: 98435 steps/s (collection: 0.798s, learning 0.201s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3155
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 12.7371
                       Mean reward: 6.88
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.2669
    Episode_Reward/rotating_object: 0.8652
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.00s
                      Time elapsed: 00:00:58
                               ETA: 00:32:25

################################################################################
                      [1m Learning iteration 44/1500 [0m                      

                       Computation: 100886 steps/s (collection: 0.835s, learning 0.139s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3862
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 12.7748
                       Mean reward: 4.20
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 0.8917
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.97s
                      Time elapsed: 00:00:59
                               ETA: 00:32:12

################################################################################
                      [1m Learning iteration 45/1500 [0m                      

                       Computation: 111530 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3487
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.8222
                       Mean reward: 5.79
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.2878
    Episode_Reward/rotating_object: 0.8290
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.88s
                      Time elapsed: 00:01:00
                               ETA: 00:31:56

################################################################################
                      [1m Learning iteration 46/1500 [0m                      

                       Computation: 112624 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.6241
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 12.8367
                       Mean reward: 6.03
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.2910
    Episode_Reward/rotating_object: 1.0619
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.87s
                      Time elapsed: 00:01:01
                               ETA: 00:31:41

################################################################################
                      [1m Learning iteration 47/1500 [0m                      

                       Computation: 114604 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5662
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.8706
                       Mean reward: 6.44
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.2949
    Episode_Reward/rotating_object: 1.1082
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.86s
                      Time elapsed: 00:01:02
                               ETA: 00:31:26

################################################################################
                      [1m Learning iteration 48/1500 [0m                      

                       Computation: 120683 steps/s (collection: 0.719s, learning 0.095s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.6980
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 12.8986
                       Mean reward: 9.82
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3063
    Episode_Reward/rotating_object: 1.2427
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.81s
                      Time elapsed: 00:01:03
                               ETA: 00:31:11

################################################################################
                      [1m Learning iteration 49/1500 [0m                      

                       Computation: 112493 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.7644
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 12.9310
                       Mean reward: 7.77
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 1.1586
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.87s
                      Time elapsed: 00:01:04
                               ETA: 00:30:57

################################################################################
                      [1m Learning iteration 50/1500 [0m                      

                       Computation: 100465 steps/s (collection: 0.786s, learning 0.192s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.5822
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.9815
                       Mean reward: 8.89
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 1.1640
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.98s
                      Time elapsed: 00:01:04
                               ETA: 00:30:47

################################################################################
                      [1m Learning iteration 51/1500 [0m                      

                       Computation: 99956 steps/s (collection: 0.763s, learning 0.221s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7506
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 13.0051
                       Mean reward: 6.54
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 1.1589
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.98s
                      Time elapsed: 00:01:05
                               ETA: 00:30:38

################################################################################
                      [1m Learning iteration 52/1500 [0m                      

                       Computation: 104056 steps/s (collection: 0.852s, learning 0.093s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.5294
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.0519
                       Mean reward: 11.20
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 1.4468
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.94s
                      Time elapsed: 00:01:06
                               ETA: 00:30:28

################################################################################
                      [1m Learning iteration 53/1500 [0m                      

                       Computation: 97484 steps/s (collection: 0.895s, learning 0.113s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.6816
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.0944
                       Mean reward: 11.03
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 1.5972
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.01s
                      Time elapsed: 00:01:07
                               ETA: 00:30:20

################################################################################
                      [1m Learning iteration 54/1500 [0m                      

                       Computation: 107617 steps/s (collection: 0.815s, learning 0.098s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.6754
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.1225
                       Mean reward: 8.79
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 1.2214
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.91s
                      Time elapsed: 00:01:08
                               ETA: 00:30:09

################################################################################
                      [1m Learning iteration 55/1500 [0m                      

                       Computation: 95477 steps/s (collection: 0.806s, learning 0.224s)
             Mean action noise std: 1.26
          Mean value_function loss: 1.0723
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 13.1689
                       Mean reward: 7.80
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 1.4317
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.03s
                      Time elapsed: 00:01:09
                               ETA: 00:30:02

################################################################################
                      [1m Learning iteration 56/1500 [0m                      

                       Computation: 81838 steps/s (collection: 1.061s, learning 0.140s)
             Mean action noise std: 1.27
          Mean value_function loss: 1.0819
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 13.1925
                       Mean reward: 9.13
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 1.3843
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.20s
                      Time elapsed: 00:01:11
                               ETA: 00:30:00

################################################################################
                      [1m Learning iteration 57/1500 [0m                      

                       Computation: 114392 steps/s (collection: 0.761s, learning 0.099s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.8757
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.2164
                       Mean reward: 10.71
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 1.6077
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.86s
                      Time elapsed: 00:01:11
                               ETA: 00:29:49

################################################################################
                      [1m Learning iteration 58/1500 [0m                      

                       Computation: 114198 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.8227
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 13.2465
                       Mean reward: 7.47
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 1.7422
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.86s
                      Time elapsed: 00:01:12
                               ETA: 00:29:39

################################################################################
                      [1m Learning iteration 59/1500 [0m                      

                       Computation: 113454 steps/s (collection: 0.745s, learning 0.121s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.5979
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 13.2930
                       Mean reward: 9.75
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.3258
    Episode_Reward/rotating_object: 1.6332
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.87s
                      Time elapsed: 00:01:13
                               ETA: 00:29:29

################################################################################
                      [1m Learning iteration 60/1500 [0m                      

                       Computation: 113067 steps/s (collection: 0.761s, learning 0.108s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.9349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.3163
                       Mean reward: 10.29
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 1.5538
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.87s
                      Time elapsed: 00:01:14
                               ETA: 00:29:19

################################################################################
                      [1m Learning iteration 61/1500 [0m                      

                       Computation: 114480 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.9300
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.3519
                       Mean reward: 8.17
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.3315
    Episode_Reward/rotating_object: 1.5845
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.86s
                      Time elapsed: 00:01:15
                               ETA: 00:29:09

################################################################################
                      [1m Learning iteration 62/1500 [0m                      

                       Computation: 112983 steps/s (collection: 0.781s, learning 0.089s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.9324
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 13.3698
                       Mean reward: 9.36
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.3260
    Episode_Reward/rotating_object: 1.5880
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.87s
                      Time elapsed: 00:01:16
                               ETA: 00:29:00

################################################################################
                      [1m Learning iteration 63/1500 [0m                      

                       Computation: 100076 steps/s (collection: 0.874s, learning 0.109s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.3048
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 13.3966
                       Mean reward: 7.71
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3333
    Episode_Reward/rotating_object: 1.3512
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.98s
                      Time elapsed: 00:01:17
                               ETA: 00:28:54

################################################################################
                      [1m Learning iteration 64/1500 [0m                      

                       Computation: 110314 steps/s (collection: 0.783s, learning 0.108s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.9536
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.4274
                       Mean reward: 11.01
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.3363
    Episode_Reward/rotating_object: 2.1032
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.89s
                      Time elapsed: 00:01:18
                               ETA: 00:28:46

################################################################################
                      [1m Learning iteration 65/1500 [0m                      

                       Computation: 103866 steps/s (collection: 0.794s, learning 0.152s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.1278
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.4379
                       Mean reward: 12.35
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 0.3338
    Episode_Reward/rotating_object: 1.9065
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.95s
                      Time elapsed: 00:01:19
                               ETA: 00:28:39

################################################################################
                      [1m Learning iteration 66/1500 [0m                      

                       Computation: 101084 steps/s (collection: 0.845s, learning 0.128s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.5306
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 13.4659
                       Mean reward: 11.25
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.3388
    Episode_Reward/rotating_object: 1.7360
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.97s
                      Time elapsed: 00:01:20
                               ETA: 00:28:33

################################################################################
                      [1m Learning iteration 67/1500 [0m                      

                       Computation: 93418 steps/s (collection: 0.953s, learning 0.100s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.1587
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 13.4968
                       Mean reward: 8.48
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 1.5876
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.05s
                      Time elapsed: 00:01:21
                               ETA: 00:28:29

################################################################################
                      [1m Learning iteration 68/1500 [0m                      

                       Computation: 96332 steps/s (collection: 0.921s, learning 0.099s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.5485
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 13.5495
                       Mean reward: 11.30
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3366
    Episode_Reward/rotating_object: 1.6912
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.02s
                      Time elapsed: 00:01:22
                               ETA: 00:28:24

################################################################################
                      [1m Learning iteration 69/1500 [0m                      

                       Computation: 106868 steps/s (collection: 0.805s, learning 0.115s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.7336
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.5739
                       Mean reward: 11.42
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.3403
    Episode_Reward/rotating_object: 1.8252
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.92s
                      Time elapsed: 00:01:23
                               ETA: 00:28:17

################################################################################
                      [1m Learning iteration 70/1500 [0m                      

                       Computation: 98517 steps/s (collection: 0.866s, learning 0.132s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.5979
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.6323
                       Mean reward: 9.46
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.3314
    Episode_Reward/rotating_object: 1.9105
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.00s
                      Time elapsed: 00:01:24
                               ETA: 00:28:12

################################################################################
                      [1m Learning iteration 71/1500 [0m                      

                       Computation: 106561 steps/s (collection: 0.804s, learning 0.118s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.9289
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.6569
                       Mean reward: 13.48
               Mean episode length: 232.19
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 2.0204
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.92s
                      Time elapsed: 00:01:24
                               ETA: 00:28:06

################################################################################
                      [1m Learning iteration 72/1500 [0m                      

                       Computation: 115742 steps/s (collection: 0.764s, learning 0.085s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.1092
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.6999
                       Mean reward: 10.70
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.3282
    Episode_Reward/rotating_object: 1.7935
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.85s
                      Time elapsed: 00:01:25
                               ETA: 00:27:58

################################################################################
                      [1m Learning iteration 73/1500 [0m                      

                       Computation: 104987 steps/s (collection: 0.787s, learning 0.149s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.2493
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.7547
                       Mean reward: 13.57
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3363
    Episode_Reward/rotating_object: 1.7093
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.94s
                      Time elapsed: 00:01:26
                               ETA: 00:27:52

################################################################################
                      [1m Learning iteration 74/1500 [0m                      

                       Computation: 114739 steps/s (collection: 0.756s, learning 0.100s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.0792
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 13.7989
                       Mean reward: 13.62
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.3371
    Episode_Reward/rotating_object: 2.3305
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.86s
                      Time elapsed: 00:01:27
                               ETA: 00:27:45

################################################################################
                      [1m Learning iteration 75/1500 [0m                      

                       Computation: 114322 steps/s (collection: 0.773s, learning 0.087s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.3208
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.8458
                       Mean reward: 12.43
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 0.3444
    Episode_Reward/rotating_object: 2.4483
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.86s
                      Time elapsed: 00:01:28
                               ETA: 00:27:38

################################################################################
                      [1m Learning iteration 76/1500 [0m                      

                       Computation: 111869 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.2053
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.8626
                       Mean reward: 13.69
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 2.4234
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.88s
                      Time elapsed: 00:01:29
                               ETA: 00:27:32

################################################################################
                      [1m Learning iteration 77/1500 [0m                      

                       Computation: 108722 steps/s (collection: 0.819s, learning 0.086s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.3548
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.8878
                       Mean reward: 10.89
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 0.3406
    Episode_Reward/rotating_object: 2.1920
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.90s
                      Time elapsed: 00:01:30
                               ETA: 00:27:26

################################################################################
                      [1m Learning iteration 78/1500 [0m                      

                       Computation: 103489 steps/s (collection: 0.843s, learning 0.107s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.5750
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 13.9062
                       Mean reward: 15.40
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 2.3163
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.95s
                      Time elapsed: 00:01:31
                               ETA: 00:27:21

################################################################################
                      [1m Learning iteration 79/1500 [0m                      

                       Computation: 103100 steps/s (collection: 0.858s, learning 0.095s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.5815
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 13.9208
                       Mean reward: 12.23
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.3304
    Episode_Reward/rotating_object: 2.3157
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.95s
                      Time elapsed: 00:01:32
                               ETA: 00:27:16

################################################################################
                      [1m Learning iteration 80/1500 [0m                      

                       Computation: 104087 steps/s (collection: 0.830s, learning 0.114s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.5788
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.9332
                       Mean reward: 14.38
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 2.3010
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.94s
                      Time elapsed: 00:01:33
                               ETA: 00:27:12

################################################################################
                      [1m Learning iteration 81/1500 [0m                      

                       Computation: 98102 steps/s (collection: 0.870s, learning 0.132s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.1415
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.9523
                       Mean reward: 10.30
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.3302
    Episode_Reward/rotating_object: 2.4655
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.00s
                      Time elapsed: 00:01:34
                               ETA: 00:27:08

################################################################################
                      [1m Learning iteration 82/1500 [0m                      

                       Computation: 110961 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.2845
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.9731
                       Mean reward: 18.42
               Mean episode length: 231.40
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 2.4428
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.89s
                      Time elapsed: 00:01:34
                               ETA: 00:27:02

################################################################################
                      [1m Learning iteration 83/1500 [0m                      

                       Computation: 106750 steps/s (collection: 0.760s, learning 0.161s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.6611
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.0133
                       Mean reward: 15.17
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 2.7613
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.92s
                      Time elapsed: 00:01:35
                               ETA: 00:26:57

################################################################################
                      [1m Learning iteration 84/1500 [0m                      

                       Computation: 108195 steps/s (collection: 0.783s, learning 0.125s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.9608
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.0572
                       Mean reward: 12.38
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 2.3983
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.91s
                      Time elapsed: 00:01:36
                               ETA: 00:26:52

################################################################################
                      [1m Learning iteration 85/1500 [0m                      

                       Computation: 103067 steps/s (collection: 0.818s, learning 0.136s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.5678
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.0683
                       Mean reward: 13.52
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 2.5498
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.95s
                      Time elapsed: 00:01:37
                               ETA: 00:26:48

################################################################################
                      [1m Learning iteration 86/1500 [0m                      

                       Computation: 91060 steps/s (collection: 0.882s, learning 0.197s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.5152
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.0901
                       Mean reward: 12.87
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 0.3240
    Episode_Reward/rotating_object: 2.4630
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.08s
                      Time elapsed: 00:01:38
                               ETA: 00:26:46

################################################################################
                      [1m Learning iteration 87/1500 [0m                      

                       Computation: 93064 steps/s (collection: 0.870s, learning 0.186s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.7511
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.1100
                       Mean reward: 18.04
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 2.7661
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.06s
                      Time elapsed: 00:01:39
                               ETA: 00:26:44

################################################################################
                      [1m Learning iteration 88/1500 [0m                      

                       Computation: 86983 steps/s (collection: 0.900s, learning 0.230s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.1249
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.1379
                       Mean reward: 16.79
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 0.3276
    Episode_Reward/rotating_object: 2.8288
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.13s
                      Time elapsed: 00:01:41
                               ETA: 00:26:42

################################################################################
                      [1m Learning iteration 89/1500 [0m                      

                       Computation: 110912 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.5206
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.1776
                       Mean reward: 17.32
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 0.3290
    Episode_Reward/rotating_object: 2.7760
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.89s
                      Time elapsed: 00:01:41
                               ETA: 00:26:37

################################################################################
                      [1m Learning iteration 90/1500 [0m                      

                       Computation: 97425 steps/s (collection: 0.860s, learning 0.149s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.3222
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.2023
                       Mean reward: 13.49
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 2.6146
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.01s
                      Time elapsed: 00:01:42
                               ETA: 00:26:34

################################################################################
                      [1m Learning iteration 91/1500 [0m                      

                       Computation: 99973 steps/s (collection: 0.883s, learning 0.100s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.5739
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.2537
                       Mean reward: 14.35
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3365
    Episode_Reward/rotating_object: 3.0138
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.98s
                      Time elapsed: 00:01:43
                               ETA: 00:26:31

################################################################################
                      [1m Learning iteration 92/1500 [0m                      

                       Computation: 93010 steps/s (collection: 0.894s, learning 0.163s)
             Mean action noise std: 1.47
          Mean value_function loss: 3.9258
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.2950
                       Mean reward: 16.55
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 2.7352
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.06s
                      Time elapsed: 00:01:44
                               ETA: 00:26:29

################################################################################
                      [1m Learning iteration 93/1500 [0m                      

                       Computation: 100033 steps/s (collection: 0.856s, learning 0.127s)
             Mean action noise std: 1.47
          Mean value_function loss: 4.1336
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.3161
                       Mean reward: 21.73
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 3.3960
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.98s
                      Time elapsed: 00:01:45
                               ETA: 00:26:25

################################################################################
                      [1m Learning iteration 94/1500 [0m                      

                       Computation: 93743 steps/s (collection: 0.812s, learning 0.237s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.7418
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.3386
                       Mean reward: 18.77
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 3.2394
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.05s
                      Time elapsed: 00:01:47
                               ETA: 00:26:23

################################################################################
                      [1m Learning iteration 95/1500 [0m                      

                       Computation: 96517 steps/s (collection: 0.896s, learning 0.123s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.7624
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 14.3575
                       Mean reward: 12.46
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 2.6997
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.02s
                      Time elapsed: 00:01:48
                               ETA: 00:26:20

################################################################################
                      [1m Learning iteration 96/1500 [0m                      

                       Computation: 102663 steps/s (collection: 0.826s, learning 0.131s)
             Mean action noise std: 1.48
          Mean value_function loss: 4.3823
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.3724
                       Mean reward: 12.72
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 2.8620
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.96s
                      Time elapsed: 00:01:48
                               ETA: 00:26:17

################################################################################
                      [1m Learning iteration 97/1500 [0m                      

                       Computation: 101861 steps/s (collection: 0.857s, learning 0.109s)
             Mean action noise std: 1.49
          Mean value_function loss: 4.9840
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.3985
                       Mean reward: 18.18
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 2.8315
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.97s
                      Time elapsed: 00:01:49
                               ETA: 00:26:13

################################################################################
                      [1m Learning iteration 98/1500 [0m                      

                       Computation: 102547 steps/s (collection: 0.847s, learning 0.112s)
             Mean action noise std: 1.49
          Mean value_function loss: 4.5724
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.4135
                       Mean reward: 21.60
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 3.4074
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.96s
                      Time elapsed: 00:01:50
                               ETA: 00:26:10

################################################################################
                      [1m Learning iteration 99/1500 [0m                      

                       Computation: 92970 steps/s (collection: 0.890s, learning 0.167s)
             Mean action noise std: 1.50
          Mean value_function loss: 4.8919
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.4420
                       Mean reward: 17.84
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 3.1133
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.06s
                      Time elapsed: 00:01:51
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 100/1500 [0m                      

                       Computation: 107123 steps/s (collection: 0.788s, learning 0.130s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.9098
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.4614
                       Mean reward: 23.03
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 3.3489
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.92s
                      Time elapsed: 00:01:52
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 101/1500 [0m                      

                       Computation: 102204 steps/s (collection: 0.809s, learning 0.153s)
             Mean action noise std: 1.50
          Mean value_function loss: 6.4658
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.4728
                       Mean reward: 25.12
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 3.8697
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.96s
                      Time elapsed: 00:01:53
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 102/1500 [0m                      

                       Computation: 107479 steps/s (collection: 0.786s, learning 0.129s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.1003
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 14.4768
                       Mean reward: 19.71
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 3.2345
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.91s
                      Time elapsed: 00:01:54
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 103/1500 [0m                      

                       Computation: 101678 steps/s (collection: 0.822s, learning 0.145s)
             Mean action noise std: 1.51
          Mean value_function loss: 4.6392
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 14.4887
                       Mean reward: 16.33
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 3.4722
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.97s
                      Time elapsed: 00:01:55
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 104/1500 [0m                      

                       Computation: 111969 steps/s (collection: 0.788s, learning 0.090s)
             Mean action noise std: 1.51
          Mean value_function loss: 5.4592
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.4994
                       Mean reward: 20.15
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 3.1670
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.88s
                      Time elapsed: 00:01:56
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 105/1500 [0m                      

                       Computation: 105047 steps/s (collection: 0.848s, learning 0.087s)
             Mean action noise std: 1.51
          Mean value_function loss: 5.5703
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.5061
                       Mean reward: 14.84
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 3.4001
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.94s
                      Time elapsed: 00:01:57
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 106/1500 [0m                      

                       Computation: 112009 steps/s (collection: 0.789s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.8392
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.5252
                       Mean reward: 20.65
               Mean episode length: 224.32
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 3.3395
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.88s
                      Time elapsed: 00:01:58
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 107/1500 [0m                      

                       Computation: 114642 steps/s (collection: 0.766s, learning 0.091s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.3698
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.5431
                       Mean reward: 18.16
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 3.6349
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.86s
                      Time elapsed: 00:01:59
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 108/1500 [0m                      

                       Computation: 103422 steps/s (collection: 0.815s, learning 0.135s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.2714
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 14.5486
                       Mean reward: 27.56
               Mean episode length: 229.33
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 4.6777
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.95s
                      Time elapsed: 00:02:00
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 109/1500 [0m                      

                       Computation: 109780 steps/s (collection: 0.789s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.6392
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.5599
                       Mean reward: 22.86
               Mean episode length: 232.12
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 4.1181
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.90s
                      Time elapsed: 00:02:01
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 110/1500 [0m                      

                       Computation: 107434 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.3172
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.5756
                       Mean reward: 14.26
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 3.2973
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.92s
                      Time elapsed: 00:02:02
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 111/1500 [0m                      

                       Computation: 106728 steps/s (collection: 0.822s, learning 0.099s)
             Mean action noise std: 1.53
          Mean value_function loss: 6.1242
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.5815
                       Mean reward: 19.70
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 3.3951
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.92s
                      Time elapsed: 00:02:02
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 112/1500 [0m                      

                       Computation: 112512 steps/s (collection: 0.786s, learning 0.088s)
             Mean action noise std: 1.53
          Mean value_function loss: 6.2596
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 14.6028
                       Mean reward: 22.45
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 3.7058
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.87s
                      Time elapsed: 00:02:03
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 113/1500 [0m                      

                       Computation: 106933 steps/s (collection: 0.779s, learning 0.140s)
             Mean action noise std: 1.54
          Mean value_function loss: 7.7231
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 14.6374
                       Mean reward: 16.58
               Mean episode length: 227.42
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 3.9690
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.92s
                      Time elapsed: 00:02:04
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 114/1500 [0m                      

                       Computation: 108429 steps/s (collection: 0.816s, learning 0.090s)
             Mean action noise std: 1.54
          Mean value_function loss: 7.9182
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.6576
                       Mean reward: 22.35
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 4.6402
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.91s
                      Time elapsed: 00:02:05
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 115/1500 [0m                      

                       Computation: 115108 steps/s (collection: 0.754s, learning 0.100s)
             Mean action noise std: 1.54
          Mean value_function loss: 7.9830
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 14.6737
                       Mean reward: 21.37
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 3.9889
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.85s
                      Time elapsed: 00:02:06
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 116/1500 [0m                      

                       Computation: 102793 steps/s (collection: 0.853s, learning 0.103s)
             Mean action noise std: 1.55
          Mean value_function loss: 8.0665
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.6868
                       Mean reward: 21.59
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 4.3103
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.96s
                      Time elapsed: 00:02:07
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 117/1500 [0m                      

                       Computation: 111054 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 9.1365
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.7124
                       Mean reward: 19.46
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 4.3231
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.89s
                      Time elapsed: 00:02:08
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 118/1500 [0m                      

                       Computation: 98767 steps/s (collection: 0.825s, learning 0.171s)
             Mean action noise std: 1.56
          Mean value_function loss: 8.0902
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.7357
                       Mean reward: 25.55
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 0.3025
    Episode_Reward/rotating_object: 4.0673
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.00s
                      Time elapsed: 00:02:09
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 119/1500 [0m                      

                       Computation: 99678 steps/s (collection: 0.878s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 8.1684
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.7477
                       Mean reward: 19.49
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 0.3077
    Episode_Reward/rotating_object: 4.1547
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.99s
                      Time elapsed: 00:02:10
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 120/1500 [0m                      

                       Computation: 101629 steps/s (collection: 0.852s, learning 0.115s)
             Mean action noise std: 1.56
          Mean value_function loss: 7.9675
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.7549
                       Mean reward: 19.79
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 0.2999
    Episode_Reward/rotating_object: 4.1095
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.97s
                      Time elapsed: 00:02:11
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 121/1500 [0m                      

                       Computation: 99540 steps/s (collection: 0.894s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 9.5821
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.7715
                       Mean reward: 22.01
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 0.3026
    Episode_Reward/rotating_object: 4.8330
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.99s
                      Time elapsed: 00:02:12
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 122/1500 [0m                      

                       Computation: 92609 steps/s (collection: 0.929s, learning 0.133s)
             Mean action noise std: 1.57
          Mean value_function loss: 11.3231
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7805
                       Mean reward: 31.61
               Mean episode length: 224.21
    Episode_Reward/reaching_object: 0.2931
    Episode_Reward/rotating_object: 4.5285
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.06s
                      Time elapsed: 00:02:13
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 123/1500 [0m                      

                       Computation: 105776 steps/s (collection: 0.840s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 13.9812
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.7863
                       Mean reward: 29.91
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 0.2995
    Episode_Reward/rotating_object: 4.8853
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.93s
                      Time elapsed: 00:02:14
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 124/1500 [0m                      

                       Computation: 113622 steps/s (collection: 0.777s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 15.1427
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 14.8002
                       Mean reward: 39.00
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 5.4023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.87s
                      Time elapsed: 00:02:15
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 125/1500 [0m                      

                       Computation: 105234 steps/s (collection: 0.831s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 13.9132
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 14.8198
                       Mean reward: 38.34
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 0.2940
    Episode_Reward/rotating_object: 5.8449
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.93s
                      Time elapsed: 00:02:16
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 126/1500 [0m                      

                       Computation: 107793 steps/s (collection: 0.816s, learning 0.096s)
             Mean action noise std: 1.58
          Mean value_function loss: 13.9854
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 14.8284
                       Mean reward: 34.64
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 0.2939
    Episode_Reward/rotating_object: 5.9980
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.91s
                      Time elapsed: 00:02:16
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 127/1500 [0m                      

                       Computation: 105274 steps/s (collection: 0.786s, learning 0.148s)
             Mean action noise std: 1.58
          Mean value_function loss: 14.1892
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 14.8404
                       Mean reward: 39.30
               Mean episode length: 212.87
    Episode_Reward/reaching_object: 0.2931
    Episode_Reward/rotating_object: 6.5651
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.93s
                      Time elapsed: 00:02:17
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 128/1500 [0m                      

                       Computation: 109794 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 1.59
          Mean value_function loss: 13.2194
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8527
                       Mean reward: 32.86
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 0.2920
    Episode_Reward/rotating_object: 6.4721
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.90s
                      Time elapsed: 00:02:18
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 129/1500 [0m                      

                       Computation: 100577 steps/s (collection: 0.835s, learning 0.143s)
             Mean action noise std: 1.59
          Mean value_function loss: 12.1965
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8649
                       Mean reward: 41.92
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.2964
    Episode_Reward/rotating_object: 6.9903
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.98s
                      Time elapsed: 00:02:19
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 130/1500 [0m                      

                       Computation: 100379 steps/s (collection: 0.830s, learning 0.149s)
             Mean action noise std: 1.59
          Mean value_function loss: 15.4785
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.8694
                       Mean reward: 37.09
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 0.3029
    Episode_Reward/rotating_object: 7.2998
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.98s
                      Time elapsed: 00:02:20
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 131/1500 [0m                      

                       Computation: 109741 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.59
          Mean value_function loss: 14.2651
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.8730
                       Mean reward: 39.56
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 0.2987
    Episode_Reward/rotating_object: 7.2391
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.90s
                      Time elapsed: 00:02:21
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 132/1500 [0m                      

                       Computation: 95344 steps/s (collection: 0.888s, learning 0.143s)
             Mean action noise std: 1.59
          Mean value_function loss: 15.7457
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8819
                       Mean reward: 43.30
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 0.3032
    Episode_Reward/rotating_object: 6.6746
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.03s
                      Time elapsed: 00:02:22
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 133/1500 [0m                      

                       Computation: 99517 steps/s (collection: 0.825s, learning 0.163s)
             Mean action noise std: 1.60
          Mean value_function loss: 15.8039
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.8892
                       Mean reward: 44.24
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 0.2992
    Episode_Reward/rotating_object: 6.7399
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.99s
                      Time elapsed: 00:02:23
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 134/1500 [0m                      

                       Computation: 99108 steps/s (collection: 0.830s, learning 0.162s)
             Mean action noise std: 1.60
          Mean value_function loss: 15.4511
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.9014
                       Mean reward: 31.09
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 0.2944
    Episode_Reward/rotating_object: 6.8791
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.99s
                      Time elapsed: 00:02:24
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 135/1500 [0m                      

                       Computation: 105119 steps/s (collection: 0.819s, learning 0.116s)
             Mean action noise std: 1.60
          Mean value_function loss: 16.6519
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.9104
                       Mean reward: 23.76
               Mean episode length: 215.27
    Episode_Reward/reaching_object: 0.2909
    Episode_Reward/rotating_object: 6.8521
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.94s
                      Time elapsed: 00:02:25
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 136/1500 [0m                      

                       Computation: 109483 steps/s (collection: 0.807s, learning 0.091s)
             Mean action noise std: 1.60
          Mean value_function loss: 17.7236
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 14.9213
                       Mean reward: 45.12
               Mean episode length: 223.76
    Episode_Reward/reaching_object: 0.2998
    Episode_Reward/rotating_object: 7.4053
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.90s
                      Time elapsed: 00:02:26
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 137/1500 [0m                      

                       Computation: 108702 steps/s (collection: 0.812s, learning 0.093s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.2616
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 14.9322
                       Mean reward: 26.61
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 0.2978
    Episode_Reward/rotating_object: 7.0840
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.90s
                      Time elapsed: 00:02:27
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 138/1500 [0m                      

                       Computation: 111116 steps/s (collection: 0.798s, learning 0.087s)
             Mean action noise std: 1.61
          Mean value_function loss: 20.0582
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.9425
                       Mean reward: 36.64
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 0.2891
    Episode_Reward/rotating_object: 6.3006
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.88s
                      Time elapsed: 00:02:28
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 139/1500 [0m                      

                       Computation: 110914 steps/s (collection: 0.787s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 19.2073
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9485
                       Mean reward: 41.77
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 0.3024
    Episode_Reward/rotating_object: 8.3141
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.89s
                      Time elapsed: 00:02:29
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 140/1500 [0m                      

                       Computation: 110647 steps/s (collection: 0.793s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 22.2591
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 14.9649
                       Mean reward: 48.75
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 0.2943
    Episode_Reward/rotating_object: 7.2795
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.89s
                      Time elapsed: 00:02:30
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 141/1500 [0m                      

                       Computation: 110792 steps/s (collection: 0.801s, learning 0.087s)
             Mean action noise std: 1.62
          Mean value_function loss: 20.4121
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 14.9736
                       Mean reward: 43.70
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 0.2896
    Episode_Reward/rotating_object: 7.0521
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.89s
                      Time elapsed: 00:02:30
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 142/1500 [0m                      

                       Computation: 110021 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 1.62
          Mean value_function loss: 17.7929
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 14.9834
                       Mean reward: 43.99
               Mean episode length: 235.18
    Episode_Reward/reaching_object: 0.2890
    Episode_Reward/rotating_object: 8.2616
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.89s
                      Time elapsed: 00:02:31
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 143/1500 [0m                      

                       Computation: 104231 steps/s (collection: 0.798s, learning 0.145s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.5480
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9989
                       Mean reward: 38.14
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.2919
    Episode_Reward/rotating_object: 7.5528
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.94s
                      Time elapsed: 00:02:32
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 144/1500 [0m                      

                       Computation: 114328 steps/s (collection: 0.754s, learning 0.106s)
             Mean action noise std: 1.62
          Mean value_function loss: 23.0769
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.9937
                       Mean reward: 50.18
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 0.2806
    Episode_Reward/rotating_object: 9.2534
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.86s
                      Time elapsed: 00:02:33
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 145/1500 [0m                      

                       Computation: 111383 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 1.62
          Mean value_function loss: 21.2237
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.0012
                       Mean reward: 61.24
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 0.2867
    Episode_Reward/rotating_object: 9.2183
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.88s
                      Time elapsed: 00:02:34
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 146/1500 [0m                      

                       Computation: 102564 steps/s (collection: 0.834s, learning 0.124s)
             Mean action noise std: 1.63
          Mean value_function loss: 22.8555
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.0056
                       Mean reward: 50.77
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.2845
    Episode_Reward/rotating_object: 9.8428
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.96s
                      Time elapsed: 00:02:35
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 147/1500 [0m                      

                       Computation: 113974 steps/s (collection: 0.774s, learning 0.089s)
             Mean action noise std: 1.63
          Mean value_function loss: 23.9481
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.0110
                       Mean reward: 38.18
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 0.2868
    Episode_Reward/rotating_object: 8.6831
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.86s
                      Time elapsed: 00:02:36
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 148/1500 [0m                      

                       Computation: 111732 steps/s (collection: 0.780s, learning 0.099s)
             Mean action noise std: 1.63
          Mean value_function loss: 24.9887
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.0142
                       Mean reward: 66.56
               Mean episode length: 230.12
    Episode_Reward/reaching_object: 0.2831
    Episode_Reward/rotating_object: 11.7746
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.88s
                      Time elapsed: 00:02:37
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 149/1500 [0m                      

                       Computation: 112525 steps/s (collection: 0.782s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 22.8427
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.0143
                       Mean reward: 77.41
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 0.2783
    Episode_Reward/rotating_object: 10.7076
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.87s
                      Time elapsed: 00:02:38
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 150/1500 [0m                      

                       Computation: 113206 steps/s (collection: 0.779s, learning 0.089s)
             Mean action noise std: 1.63
          Mean value_function loss: 22.7432
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.0222
                       Mean reward: 52.95
               Mean episode length: 220.56
    Episode_Reward/reaching_object: 0.2829
    Episode_Reward/rotating_object: 11.5973
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.87s
                      Time elapsed: 00:02:38
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 151/1500 [0m                      

                       Computation: 103197 steps/s (collection: 0.827s, learning 0.126s)
             Mean action noise std: 1.64
          Mean value_function loss: 22.2629
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.0350
                       Mean reward: 46.46
               Mean episode length: 221.85
    Episode_Reward/reaching_object: 0.2822
    Episode_Reward/rotating_object: 10.6293
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.95s
                      Time elapsed: 00:02:39
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 152/1500 [0m                      

                       Computation: 112634 steps/s (collection: 0.783s, learning 0.090s)
             Mean action noise std: 1.64
          Mean value_function loss: 25.0390
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.0379
                       Mean reward: 70.40
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 0.2866
    Episode_Reward/rotating_object: 13.5563
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.87s
                      Time elapsed: 00:02:40
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 153/1500 [0m                      

                       Computation: 103913 steps/s (collection: 0.754s, learning 0.192s)
             Mean action noise std: 1.64
          Mean value_function loss: 26.6459
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.0417
                       Mean reward: 59.05
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 0.2844
    Episode_Reward/rotating_object: 12.3186
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.95s
                      Time elapsed: 00:02:41
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 154/1500 [0m                      

                       Computation: 102504 steps/s (collection: 0.794s, learning 0.165s)
             Mean action noise std: 1.64
          Mean value_function loss: 24.2381
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0516
                       Mean reward: 63.25
               Mean episode length: 213.72
    Episode_Reward/reaching_object: 0.2808
    Episode_Reward/rotating_object: 10.5051
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.96s
                      Time elapsed: 00:02:42
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 155/1500 [0m                      

                       Computation: 109861 steps/s (collection: 0.770s, learning 0.125s)
             Mean action noise std: 1.64
          Mean value_function loss: 24.0766
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.0572
                       Mean reward: 43.66
               Mean episode length: 217.73
    Episode_Reward/reaching_object: 0.2820
    Episode_Reward/rotating_object: 10.3183
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.89s
                      Time elapsed: 00:02:43
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 156/1500 [0m                      

                       Computation: 85122 steps/s (collection: 0.895s, learning 0.260s)
             Mean action noise std: 1.65
          Mean value_function loss: 25.7825
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.0688
                       Mean reward: 67.59
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 13.6930
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.15s
                      Time elapsed: 00:02:44
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 157/1500 [0m                      

                       Computation: 100677 steps/s (collection: 0.842s, learning 0.134s)
             Mean action noise std: 1.65
          Mean value_function loss: 28.1546
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.0786
                       Mean reward: 66.70
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 0.2910
    Episode_Reward/rotating_object: 12.0501
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.98s
                      Time elapsed: 00:02:45
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 158/1500 [0m                      

                       Computation: 105873 steps/s (collection: 0.839s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 27.6484
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.0924
                       Mean reward: 53.19
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 0.2966
    Episode_Reward/rotating_object: 12.9233
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.93s
                      Time elapsed: 00:02:46
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 159/1500 [0m                      

                       Computation: 86577 steps/s (collection: 0.896s, learning 0.240s)
             Mean action noise std: 1.65
          Mean value_function loss: 25.6700
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0987
                       Mean reward: 54.32
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 0.2869
    Episode_Reward/rotating_object: 12.0668
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.14s
                      Time elapsed: 00:02:47
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 160/1500 [0m                      

                       Computation: 95183 steps/s (collection: 0.871s, learning 0.162s)
             Mean action noise std: 1.66
          Mean value_function loss: 25.4750
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.1060
                       Mean reward: 80.94
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 0.2928
    Episode_Reward/rotating_object: 15.6514
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.03s
                      Time elapsed: 00:02:48
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 161/1500 [0m                      

                       Computation: 112458 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 1.66
          Mean value_function loss: 30.1395
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1209
                       Mean reward: 58.40
               Mean episode length: 224.47
    Episode_Reward/reaching_object: 0.2844
    Episode_Reward/rotating_object: 12.4404
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.87s
                      Time elapsed: 00:02:49
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 162/1500 [0m                      

                       Computation: 104185 steps/s (collection: 0.780s, learning 0.163s)
             Mean action noise std: 1.66
          Mean value_function loss: 36.4690
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.1244
                       Mean reward: 79.59
               Mean episode length: 229.58
    Episode_Reward/reaching_object: 0.2935
    Episode_Reward/rotating_object: 13.4685
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.94s
                      Time elapsed: 00:02:50
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 163/1500 [0m                      

                       Computation: 105305 steps/s (collection: 0.834s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 30.5209
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 15.1242
                       Mean reward: 82.26
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 0.2908
    Episode_Reward/rotating_object: 14.4636
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.93s
                      Time elapsed: 00:02:51
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 164/1500 [0m                      

                       Computation: 115838 steps/s (collection: 0.753s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 21.7395
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.1245
                       Mean reward: 63.10
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 0.3010
    Episode_Reward/rotating_object: 13.5981
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.85s
                      Time elapsed: 00:02:52
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 165/1500 [0m                      

                       Computation: 103901 steps/s (collection: 0.855s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 21.1948
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.1258
                       Mean reward: 80.89
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 0.3000
    Episode_Reward/rotating_object: 14.9466
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.95s
                      Time elapsed: 00:02:53
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 166/1500 [0m                      

                       Computation: 113033 steps/s (collection: 0.768s, learning 0.102s)
             Mean action noise std: 1.66
          Mean value_function loss: 23.9988
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.1326
                       Mean reward: 66.44
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 0.3050
    Episode_Reward/rotating_object: 14.8809
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.87s
                      Time elapsed: 00:02:54
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 167/1500 [0m                      

                       Computation: 108352 steps/s (collection: 0.796s, learning 0.111s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.1114
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.1479
                       Mean reward: 79.33
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.2946
    Episode_Reward/rotating_object: 14.9196
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.91s
                      Time elapsed: 00:02:55
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 168/1500 [0m                      

                       Computation: 108455 steps/s (collection: 0.791s, learning 0.116s)
             Mean action noise std: 1.67
          Mean value_function loss: 24.9485
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 15.1503
                       Mean reward: 71.28
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 0.2929
    Episode_Reward/rotating_object: 14.0762
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.91s
                      Time elapsed: 00:02:56
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 169/1500 [0m                      

                       Computation: 103528 steps/s (collection: 0.841s, learning 0.109s)
             Mean action noise std: 1.67
          Mean value_function loss: 28.8852
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.1527
                       Mean reward: 64.13
               Mean episode length: 224.51
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 14.5113
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.95s
                      Time elapsed: 00:02:57
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 170/1500 [0m                      

                       Computation: 108947 steps/s (collection: 0.812s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 33.3253
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.1601
                       Mean reward: 65.94
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 0.2942
    Episode_Reward/rotating_object: 14.5927
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.90s
                      Time elapsed: 00:02:57
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 171/1500 [0m                      

                       Computation: 103291 steps/s (collection: 0.794s, learning 0.158s)
             Mean action noise std: 1.67
          Mean value_function loss: 38.3144
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1658
                       Mean reward: 83.80
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 0.2956
    Episode_Reward/rotating_object: 16.8796
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.95s
                      Time elapsed: 00:02:58
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 172/1500 [0m                      

                       Computation: 93540 steps/s (collection: 0.895s, learning 0.156s)
             Mean action noise std: 1.67
          Mean value_function loss: 43.3697
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.1741
                       Mean reward: 68.31
               Mean episode length: 221.52
    Episode_Reward/reaching_object: 0.2801
    Episode_Reward/rotating_object: 13.1727
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.05s
                      Time elapsed: 00:02:59
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 173/1500 [0m                      

                       Computation: 98128 steps/s (collection: 0.813s, learning 0.189s)
             Mean action noise std: 1.67
          Mean value_function loss: 44.8753
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.1843
                       Mean reward: 82.31
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 0.3020
    Episode_Reward/rotating_object: 16.6229
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.00s
                      Time elapsed: 00:03:00
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 174/1500 [0m                      

                       Computation: 102978 steps/s (collection: 0.832s, learning 0.122s)
             Mean action noise std: 1.68
          Mean value_function loss: 44.3888
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.1912
                       Mean reward: 67.10
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 0.2963
    Episode_Reward/rotating_object: 14.0145
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.95s
                      Time elapsed: 00:03:01
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 175/1500 [0m                      

                       Computation: 102035 steps/s (collection: 0.836s, learning 0.128s)
             Mean action noise std: 1.68
          Mean value_function loss: 41.6540
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.1941
                       Mean reward: 83.85
               Mean episode length: 220.05
    Episode_Reward/reaching_object: 0.2940
    Episode_Reward/rotating_object: 16.1903
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.96s
                      Time elapsed: 00:03:02
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 176/1500 [0m                      

                       Computation: 108222 steps/s (collection: 0.790s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 32.1515
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.1956
                       Mean reward: 81.30
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 0.2908
    Episode_Reward/rotating_object: 16.8743
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.91s
                      Time elapsed: 00:03:03
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 177/1500 [0m                      

                       Computation: 107560 steps/s (collection: 0.774s, learning 0.140s)
             Mean action noise std: 1.68
          Mean value_function loss: 31.7190
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.2045
                       Mean reward: 70.34
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.2902
    Episode_Reward/rotating_object: 15.3926
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.91s
                      Time elapsed: 00:03:04
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 178/1500 [0m                      

                       Computation: 109128 steps/s (collection: 0.766s, learning 0.135s)
             Mean action noise std: 1.68
          Mean value_function loss: 30.6792
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.2172
                       Mean reward: 71.08
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 0.2872
    Episode_Reward/rotating_object: 13.4824
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.90s
                      Time elapsed: 00:03:05
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 179/1500 [0m                      

                       Computation: 95321 steps/s (collection: 0.879s, learning 0.152s)
             Mean action noise std: 1.69
          Mean value_function loss: 32.4964
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2242
                       Mean reward: 74.29
               Mean episode length: 224.07
    Episode_Reward/reaching_object: 0.2899
    Episode_Reward/rotating_object: 16.1200
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.03s
                      Time elapsed: 00:03:06
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 180/1500 [0m                      

                       Computation: 115792 steps/s (collection: 0.758s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 38.0882
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2369
                       Mean reward: 145.61
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 16.8670
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.85s
                      Time elapsed: 00:03:07
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 181/1500 [0m                      

                       Computation: 113192 steps/s (collection: 0.777s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 37.8863
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.2555
                       Mean reward: 74.48
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.2904
    Episode_Reward/rotating_object: 15.8622
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.87s
                      Time elapsed: 00:03:08
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 182/1500 [0m                      

                       Computation: 109418 steps/s (collection: 0.799s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 36.8604
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2640
                       Mean reward: 67.57
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.2853
    Episode_Reward/rotating_object: 13.8788
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.90s
                      Time elapsed: 00:03:09
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 183/1500 [0m                      

                       Computation: 112044 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 37.4378
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.2735
                       Mean reward: 80.85
               Mean episode length: 222.42
    Episode_Reward/reaching_object: 0.2849
    Episode_Reward/rotating_object: 14.6145
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.88s
                      Time elapsed: 00:03:10
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 184/1500 [0m                      

                       Computation: 97210 steps/s (collection: 0.850s, learning 0.161s)
             Mean action noise std: 1.70
          Mean value_function loss: 38.6143
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2850
                       Mean reward: 104.40
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.2966
    Episode_Reward/rotating_object: 17.6008
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.01s
                      Time elapsed: 00:03:11
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 185/1500 [0m                      

                       Computation: 97734 steps/s (collection: 0.879s, learning 0.127s)
             Mean action noise std: 1.70
          Mean value_function loss: 36.9595
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.2847
                       Mean reward: 80.59
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 17.1883
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.01s
                      Time elapsed: 00:03:12
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 186/1500 [0m                      

                       Computation: 102739 steps/s (collection: 0.843s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 35.4517
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.2784
                       Mean reward: 76.39
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 0.3002
    Episode_Reward/rotating_object: 17.2602
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.96s
                      Time elapsed: 00:03:13
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 187/1500 [0m                      

                       Computation: 110940 steps/s (collection: 0.776s, learning 0.111s)
             Mean action noise std: 1.70
          Mean value_function loss: 35.2680
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.2796
                       Mean reward: 100.31
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.2957
    Episode_Reward/rotating_object: 18.8481
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.89s
                      Time elapsed: 00:03:13
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 188/1500 [0m                      

                       Computation: 105357 steps/s (collection: 0.783s, learning 0.150s)
             Mean action noise std: 1.71
          Mean value_function loss: 38.8527
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.2845
                       Mean reward: 88.24
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.2902
    Episode_Reward/rotating_object: 17.9226
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.93s
                      Time elapsed: 00:03:14
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 189/1500 [0m                      

                       Computation: 107199 steps/s (collection: 0.779s, learning 0.138s)
             Mean action noise std: 1.71
          Mean value_function loss: 43.1118
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2919
                       Mean reward: 70.79
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 0.2890
    Episode_Reward/rotating_object: 16.3534
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.92s
                      Time elapsed: 00:03:15
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 190/1500 [0m                      

                       Computation: 101642 steps/s (collection: 0.810s, learning 0.157s)
             Mean action noise std: 1.71
          Mean value_function loss: 39.1050
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.2932
                       Mean reward: 94.70
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.2916
    Episode_Reward/rotating_object: 18.7651
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.97s
                      Time elapsed: 00:03:16
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 191/1500 [0m                      

                       Computation: 108785 steps/s (collection: 0.769s, learning 0.135s)
             Mean action noise std: 1.71
          Mean value_function loss: 39.2027
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 15.2958
                       Mean reward: 114.40
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 0.2953
    Episode_Reward/rotating_object: 19.4270
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.90s
                      Time elapsed: 00:03:17
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 192/1500 [0m                      

                       Computation: 112655 steps/s (collection: 0.766s, learning 0.106s)
             Mean action noise std: 1.71
          Mean value_function loss: 42.1834
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 15.2961
                       Mean reward: 75.58
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 0.2897
    Episode_Reward/rotating_object: 19.6766
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.87s
                      Time elapsed: 00:03:18
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 193/1500 [0m                      

                       Computation: 110796 steps/s (collection: 0.789s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 47.9845
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 15.2965
                       Mean reward: 100.26
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 0.2821
    Episode_Reward/rotating_object: 18.0166
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.89s
                      Time elapsed: 00:03:19
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 194/1500 [0m                      

                       Computation: 117124 steps/s (collection: 0.751s, learning 0.089s)
             Mean action noise std: 1.71
          Mean value_function loss: 56.2086
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 15.2966
                       Mean reward: 83.53
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.2894
    Episode_Reward/rotating_object: 16.6255
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.84s
                      Time elapsed: 00:03:20
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 195/1500 [0m                      

                       Computation: 110494 steps/s (collection: 0.786s, learning 0.104s)
             Mean action noise std: 1.71
          Mean value_function loss: 49.4285
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 15.2973
                       Mean reward: 100.21
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.2966
    Episode_Reward/rotating_object: 19.8240
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.89s
                      Time elapsed: 00:03:21
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 196/1500 [0m                      

                       Computation: 103028 steps/s (collection: 0.835s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 40.0850
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.2987
                       Mean reward: 116.28
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 0.2971
    Episode_Reward/rotating_object: 20.3482
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.95s
                      Time elapsed: 00:03:22
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 197/1500 [0m                      

                       Computation: 111517 steps/s (collection: 0.790s, learning 0.091s)
             Mean action noise std: 1.71
          Mean value_function loss: 42.3921
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.3037
                       Mean reward: 103.91
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 0.2975
    Episode_Reward/rotating_object: 17.6909
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.88s
                      Time elapsed: 00:03:22
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 198/1500 [0m                      

                       Computation: 109772 steps/s (collection: 0.771s, learning 0.124s)
             Mean action noise std: 1.71
          Mean value_function loss: 39.8961
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.3108
                       Mean reward: 116.75
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 0.3002
    Episode_Reward/rotating_object: 21.3221
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.90s
                      Time elapsed: 00:03:23
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 199/1500 [0m                      

                       Computation: 100992 steps/s (collection: 0.839s, learning 0.135s)
             Mean action noise std: 1.71
          Mean value_function loss: 46.8275
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.3124
                       Mean reward: 107.27
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 19.6275
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.97s
                      Time elapsed: 00:03:24
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 200/1500 [0m                      

                       Computation: 106844 steps/s (collection: 0.798s, learning 0.122s)
             Mean action noise std: 1.71
          Mean value_function loss: 57.2072
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.3173
                       Mean reward: 131.49
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.2953
    Episode_Reward/rotating_object: 20.4491
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.92s
                      Time elapsed: 00:03:25
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 201/1500 [0m                      

                       Computation: 115294 steps/s (collection: 0.749s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 53.9767
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.3205
                       Mean reward: 96.17
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.2886
    Episode_Reward/rotating_object: 18.7334
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.85s
                      Time elapsed: 00:03:26
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 202/1500 [0m                      

                       Computation: 102148 steps/s (collection: 0.790s, learning 0.173s)
             Mean action noise std: 1.71
          Mean value_function loss: 57.1418
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3218
                       Mean reward: 97.72
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 0.2927
    Episode_Reward/rotating_object: 19.1073
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.96s
                      Time elapsed: 00:03:27
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 203/1500 [0m                      

                       Computation: 97371 steps/s (collection: 0.903s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 57.0579
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3224
                       Mean reward: 71.58
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 0.2964
    Episode_Reward/rotating_object: 18.0810
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.01s
                      Time elapsed: 00:03:28
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 204/1500 [0m                      

                       Computation: 109625 steps/s (collection: 0.778s, learning 0.119s)
             Mean action noise std: 1.72
          Mean value_function loss: 58.5186
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.3248
                       Mean reward: 84.99
               Mean episode length: 231.57
    Episode_Reward/reaching_object: 0.2863
    Episode_Reward/rotating_object: 18.3050
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.90s
                      Time elapsed: 00:03:29
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 205/1500 [0m                      

                       Computation: 105011 steps/s (collection: 0.824s, learning 0.113s)
             Mean action noise std: 1.72
          Mean value_function loss: 65.9982
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.3268
                       Mean reward: 113.05
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 0.2993
    Episode_Reward/rotating_object: 21.6814
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.94s
                      Time elapsed: 00:03:30
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 206/1500 [0m                      

                       Computation: 108010 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 1.72
          Mean value_function loss: 64.7128
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.3261
                       Mean reward: 134.20
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.2889
    Episode_Reward/rotating_object: 22.0718
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.91s
                      Time elapsed: 00:03:31
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 207/1500 [0m                      

                       Computation: 100190 steps/s (collection: 0.871s, learning 0.110s)
             Mean action noise std: 1.72
          Mean value_function loss: 63.6956
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.3336
                       Mean reward: 80.10
               Mean episode length: 222.66
    Episode_Reward/reaching_object: 0.2986
    Episode_Reward/rotating_object: 19.3601
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.98s
                      Time elapsed: 00:03:32
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 208/1500 [0m                      

                       Computation: 108215 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 51.4017
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.3406
                       Mean reward: 107.54
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 0.2939
    Episode_Reward/rotating_object: 18.5531
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.91s
                      Time elapsed: 00:03:33
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 209/1500 [0m                      

                       Computation: 107782 steps/s (collection: 0.804s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 60.6693
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.3414
                       Mean reward: 82.79
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 0.2887
    Episode_Reward/rotating_object: 19.8288
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.91s
                      Time elapsed: 00:03:34
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 210/1500 [0m                      

                       Computation: 110375 steps/s (collection: 0.802s, learning 0.089s)
             Mean action noise std: 1.72
          Mean value_function loss: 58.6728
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3379
                       Mean reward: 117.04
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 0.2869
    Episode_Reward/rotating_object: 20.5760
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.89s
                      Time elapsed: 00:03:35
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 211/1500 [0m                      

                       Computation: 108275 steps/s (collection: 0.812s, learning 0.096s)
             Mean action noise std: 1.73
          Mean value_function loss: 59.0282
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.3443
                       Mean reward: 99.99
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 0.2942
    Episode_Reward/rotating_object: 20.0363
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.91s
                      Time elapsed: 00:03:35
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 212/1500 [0m                      

                       Computation: 107055 steps/s (collection: 0.812s, learning 0.107s)
             Mean action noise std: 1.73
          Mean value_function loss: 58.8576
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.3539
                       Mean reward: 91.85
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.2894
    Episode_Reward/rotating_object: 20.6176
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.92s
                      Time elapsed: 00:03:36
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 213/1500 [0m                      

                       Computation: 104103 steps/s (collection: 0.820s, learning 0.125s)
             Mean action noise std: 1.73
          Mean value_function loss: 55.5867
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.3567
                       Mean reward: 105.79
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 0.2919
    Episode_Reward/rotating_object: 23.3796
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.94s
                      Time elapsed: 00:03:37
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 214/1500 [0m                      

                       Computation: 94219 steps/s (collection: 0.875s, learning 0.168s)
             Mean action noise std: 1.73
          Mean value_function loss: 57.3531
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3595
                       Mean reward: 120.46
               Mean episode length: 235.26
    Episode_Reward/reaching_object: 0.2959
    Episode_Reward/rotating_object: 21.8842
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.04s
                      Time elapsed: 00:03:38
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 215/1500 [0m                      

                       Computation: 100694 steps/s (collection: 0.878s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 60.1938
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3602
                       Mean reward: 120.71
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 0.2758
    Episode_Reward/rotating_object: 20.9865
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.98s
                      Time elapsed: 00:03:39
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 216/1500 [0m                      

                       Computation: 103118 steps/s (collection: 0.787s, learning 0.166s)
             Mean action noise std: 1.73
          Mean value_function loss: 67.9841
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.3617
                       Mean reward: 65.95
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 0.2805
    Episode_Reward/rotating_object: 18.5981
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.95s
                      Time elapsed: 00:03:40
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 217/1500 [0m                      

                       Computation: 106809 steps/s (collection: 0.816s, learning 0.104s)
             Mean action noise std: 1.73
          Mean value_function loss: 60.6592
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.3660
                       Mean reward: 101.78
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 0.2968
    Episode_Reward/rotating_object: 23.4680
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.92s
                      Time elapsed: 00:03:41
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 218/1500 [0m                      

                       Computation: 107908 steps/s (collection: 0.787s, learning 0.124s)
             Mean action noise std: 1.73
          Mean value_function loss: 56.2039
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3678
                       Mean reward: 124.65
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.3037
    Episode_Reward/rotating_object: 23.8832
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.91s
                      Time elapsed: 00:03:42
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 219/1500 [0m                      

                       Computation: 91935 steps/s (collection: 0.893s, learning 0.177s)
             Mean action noise std: 1.73
          Mean value_function loss: 57.5241
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3732
                       Mean reward: 95.17
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 0.2920
    Episode_Reward/rotating_object: 22.7698
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.07s
                      Time elapsed: 00:03:43
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 220/1500 [0m                      

                       Computation: 103557 steps/s (collection: 0.829s, learning 0.121s)
             Mean action noise std: 1.73
          Mean value_function loss: 54.7717
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.3734
                       Mean reward: 135.32
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.3037
    Episode_Reward/rotating_object: 25.7507
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.95s
                      Time elapsed: 00:03:44
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 221/1500 [0m                      

                       Computation: 105325 steps/s (collection: 0.796s, learning 0.137s)
             Mean action noise std: 1.74
          Mean value_function loss: 63.8396
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.3783
                       Mean reward: 131.47
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.2903
    Episode_Reward/rotating_object: 23.1187
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.93s
                      Time elapsed: 00:03:45
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 222/1500 [0m                      

                       Computation: 109238 steps/s (collection: 0.811s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 56.0396
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.3917
                       Mean reward: 105.30
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 0.2847
    Episode_Reward/rotating_object: 21.6588
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.90s
                      Time elapsed: 00:03:46
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 223/1500 [0m                      

                       Computation: 103903 steps/s (collection: 0.829s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 66.2914
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.4027
                       Mean reward: 123.44
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.2917
    Episode_Reward/rotating_object: 26.4872
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.95s
                      Time elapsed: 00:03:47
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 224/1500 [0m                      

                       Computation: 102885 steps/s (collection: 0.851s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 57.5966
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.4084
                       Mean reward: 127.61
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.2970
    Episode_Reward/rotating_object: 24.1062
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.96s
                      Time elapsed: 00:03:48
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 225/1500 [0m                      

                       Computation: 100201 steps/s (collection: 0.881s, learning 0.100s)
             Mean action noise std: 1.74
          Mean value_function loss: 51.1972
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.4104
                       Mean reward: 130.19
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.2979
    Episode_Reward/rotating_object: 24.0470
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.98s
                      Time elapsed: 00:03:49
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 226/1500 [0m                      

                       Computation: 100909 steps/s (collection: 0.871s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 53.7607
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.4112
                       Mean reward: 120.77
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.2920
    Episode_Reward/rotating_object: 24.0698
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.97s
                      Time elapsed: 00:03:50
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 227/1500 [0m                      

                       Computation: 103386 steps/s (collection: 0.818s, learning 0.133s)
             Mean action noise std: 1.74
          Mean value_function loss: 49.0397
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.4134
                       Mean reward: 120.72
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.2943
    Episode_Reward/rotating_object: 23.9037
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.95s
                      Time elapsed: 00:03:51
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 228/1500 [0m                      

                       Computation: 107754 steps/s (collection: 0.808s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 52.6503
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4188
                       Mean reward: 135.27
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.2923
    Episode_Reward/rotating_object: 24.0024
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.91s
                      Time elapsed: 00:03:52
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 229/1500 [0m                      

                       Computation: 89190 steps/s (collection: 0.914s, learning 0.189s)
             Mean action noise std: 1.75
          Mean value_function loss: 67.7695
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.4280
                       Mean reward: 101.07
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.2901
    Episode_Reward/rotating_object: 22.3029
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.10s
                      Time elapsed: 00:03:53
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 230/1500 [0m                      

                       Computation: 88296 steps/s (collection: 1.026s, learning 0.088s)
             Mean action noise std: 1.75
          Mean value_function loss: 61.3376
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.4368
                       Mean reward: 130.76
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.2989
    Episode_Reward/rotating_object: 25.2727
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.11s
                      Time elapsed: 00:03:54
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 231/1500 [0m                      

                       Computation: 108830 steps/s (collection: 0.794s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 63.1948
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.4383
                       Mean reward: 140.86
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 23.0546
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.90s
                      Time elapsed: 00:03:55
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 232/1500 [0m                      

                       Computation: 105305 steps/s (collection: 0.827s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 70.1584
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.4380
                       Mean reward: 130.21
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.2980
    Episode_Reward/rotating_object: 25.6122
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.93s
                      Time elapsed: 00:03:56
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 233/1500 [0m                      

                       Computation: 111082 steps/s (collection: 0.791s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 62.5156
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4365
                       Mean reward: 127.91
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.3058
    Episode_Reward/rotating_object: 22.6430
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.88s
                      Time elapsed: 00:03:57
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 234/1500 [0m                      

                       Computation: 104777 steps/s (collection: 0.824s, learning 0.114s)
             Mean action noise std: 1.75
          Mean value_function loss: 60.8395
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.4389
                       Mean reward: 143.24
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 0.3039
    Episode_Reward/rotating_object: 24.6380
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.94s
                      Time elapsed: 00:03:58
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 235/1500 [0m                      

                       Computation: 96988 steps/s (collection: 0.837s, learning 0.176s)
             Mean action noise std: 1.75
          Mean value_function loss: 61.3255
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.4428
                       Mean reward: 105.17
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 24.0702
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.01s
                      Time elapsed: 00:03:59
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 236/1500 [0m                      

                       Computation: 97357 steps/s (collection: 0.894s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 77.3782
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4456
                       Mean reward: 124.32
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 27.9036
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.01s
                      Time elapsed: 00:04:00
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 237/1500 [0m                      

                       Computation: 104032 steps/s (collection: 0.836s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 71.1787
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.4472
                       Mean reward: 155.03
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 0.3040
    Episode_Reward/rotating_object: 28.5616
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.94s
                      Time elapsed: 00:04:01
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 238/1500 [0m                      

                       Computation: 110571 steps/s (collection: 0.799s, learning 0.090s)
             Mean action noise std: 1.75
          Mean value_function loss: 67.7999
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.4476
                       Mean reward: 159.54
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 29.9166
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.89s
                      Time elapsed: 00:04:01
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 239/1500 [0m                      

                       Computation: 101513 steps/s (collection: 0.826s, learning 0.143s)
             Mean action noise std: 1.75
          Mean value_function loss: 77.7035
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4486
                       Mean reward: 148.64
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 25.9688
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.97s
                      Time elapsed: 00:04:02
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 240/1500 [0m                      

                       Computation: 97215 steps/s (collection: 0.858s, learning 0.153s)
             Mean action noise std: 1.76
          Mean value_function loss: 72.9258
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4512
                       Mean reward: 118.60
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 26.5580
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.01s
                      Time elapsed: 00:04:03
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 241/1500 [0m                      

                       Computation: 97185 steps/s (collection: 0.831s, learning 0.181s)
             Mean action noise std: 1.76
          Mean value_function loss: 81.5521
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.4467
                       Mean reward: 120.42
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 25.9337
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.01s
                      Time elapsed: 00:04:04
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 242/1500 [0m                      

                       Computation: 108739 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 91.4062
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4406
                       Mean reward: 159.15
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 29.4046
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.90s
                      Time elapsed: 00:04:05
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 243/1500 [0m                      

                       Computation: 104612 steps/s (collection: 0.799s, learning 0.141s)
             Mean action noise std: 1.76
          Mean value_function loss: 92.6894
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4403
                       Mean reward: 108.65
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 22.5442
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.94s
                      Time elapsed: 00:04:06
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 244/1500 [0m                      

                       Computation: 103150 steps/s (collection: 0.804s, learning 0.149s)
             Mean action noise std: 1.76
          Mean value_function loss: 83.1120
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.4388
                       Mean reward: 163.90
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 27.9998
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.95s
                      Time elapsed: 00:04:07
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 245/1500 [0m                      

                       Computation: 100987 steps/s (collection: 0.816s, learning 0.158s)
             Mean action noise std: 1.76
          Mean value_function loss: 95.5262
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4391
                       Mean reward: 128.64
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.3115
    Episode_Reward/rotating_object: 26.9133
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.97s
                      Time elapsed: 00:04:08
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 246/1500 [0m                      

                       Computation: 104085 steps/s (collection: 0.808s, learning 0.136s)
             Mean action noise std: 1.76
          Mean value_function loss: 79.2962
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.4386
                       Mean reward: 149.34
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 26.4790
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.94s
                      Time elapsed: 00:04:09
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 247/1500 [0m                      

                       Computation: 109124 steps/s (collection: 0.778s, learning 0.123s)
             Mean action noise std: 1.76
          Mean value_function loss: 72.8627
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.4454
                       Mean reward: 142.48
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 29.0626
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.90s
                      Time elapsed: 00:04:10
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 248/1500 [0m                      

                       Computation: 106564 steps/s (collection: 0.817s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 68.6343
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.4489
                       Mean reward: 134.07
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 0.2992
    Episode_Reward/rotating_object: 23.7769
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.92s
                      Time elapsed: 00:04:11
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 249/1500 [0m                      

                       Computation: 103575 steps/s (collection: 0.846s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 67.4829
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4534
                       Mean reward: 135.23
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 0.3033
    Episode_Reward/rotating_object: 26.6701
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.95s
                      Time elapsed: 00:04:12
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 250/1500 [0m                      

                       Computation: 96106 steps/s (collection: 0.894s, learning 0.129s)
             Mean action noise std: 1.76
          Mean value_function loss: 69.8867
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.4555
                       Mean reward: 137.58
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 25.9487
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.02s
                      Time elapsed: 00:04:13
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 251/1500 [0m                      

                       Computation: 96050 steps/s (collection: 0.907s, learning 0.116s)
             Mean action noise std: 1.76
          Mean value_function loss: 72.8519
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4579
                       Mean reward: 130.72
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 24.7414
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.02s
                      Time elapsed: 00:04:14
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 252/1500 [0m                      

                       Computation: 102372 steps/s (collection: 0.826s, learning 0.135s)
             Mean action noise std: 1.77
          Mean value_function loss: 68.4274
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.4608
                       Mean reward: 135.85
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.3024
    Episode_Reward/rotating_object: 28.3531
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.96s
                      Time elapsed: 00:04:15
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 253/1500 [0m                      

                       Computation: 95819 steps/s (collection: 0.881s, learning 0.145s)
             Mean action noise std: 1.77
          Mean value_function loss: 76.7779
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4639
                       Mean reward: 131.39
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.2953
    Episode_Reward/rotating_object: 25.2010
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.03s
                      Time elapsed: 00:04:16
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 254/1500 [0m                      

                       Computation: 113908 steps/s (collection: 0.771s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 74.5747
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.4581
                       Mean reward: 115.81
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 24.7521
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.86s
                      Time elapsed: 00:04:17
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 255/1500 [0m                      

                       Computation: 105642 steps/s (collection: 0.843s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 74.9242
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.4602
                       Mean reward: 149.32
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 0.3012
    Episode_Reward/rotating_object: 27.1073
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.93s
                      Time elapsed: 00:04:18
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 256/1500 [0m                      

                       Computation: 109688 steps/s (collection: 0.804s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 74.6491
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.4561
                       Mean reward: 95.93
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.2987
    Episode_Reward/rotating_object: 24.1259
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.90s
                      Time elapsed: 00:04:19
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 257/1500 [0m                      

                       Computation: 102796 steps/s (collection: 0.837s, learning 0.119s)
             Mean action noise std: 1.77
          Mean value_function loss: 76.4743
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4576
                       Mean reward: 127.95
               Mean episode length: 225.85
    Episode_Reward/reaching_object: 0.2995
    Episode_Reward/rotating_object: 27.5012
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.96s
                      Time elapsed: 00:04:20
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 258/1500 [0m                      

                       Computation: 109745 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 81.0130
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4650
                       Mean reward: 129.96
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.2993
    Episode_Reward/rotating_object: 27.0159
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.90s
                      Time elapsed: 00:04:20
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 259/1500 [0m                      

                       Computation: 108067 steps/s (collection: 0.796s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 85.2985
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.4768
                       Mean reward: 160.71
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 0.3027
    Episode_Reward/rotating_object: 30.6692
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.91s
                      Time elapsed: 00:04:21
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 260/1500 [0m                      

                       Computation: 97993 steps/s (collection: 0.830s, learning 0.173s)
             Mean action noise std: 1.78
          Mean value_function loss: 91.2799
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.4850
                       Mean reward: 135.17
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.3054
    Episode_Reward/rotating_object: 29.0308
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.00s
                      Time elapsed: 00:04:22
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 261/1500 [0m                      

                       Computation: 99416 steps/s (collection: 0.845s, learning 0.144s)
             Mean action noise std: 1.78
          Mean value_function loss: 84.9677
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4934
                       Mean reward: 132.43
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.3002
    Episode_Reward/rotating_object: 26.6645
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.99s
                      Time elapsed: 00:04:23
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 262/1500 [0m                      

                       Computation: 106667 steps/s (collection: 0.780s, learning 0.142s)
             Mean action noise std: 1.78
          Mean value_function loss: 86.2447
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.4964
                       Mean reward: 141.69
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 0.2971
    Episode_Reward/rotating_object: 27.5784
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.92s
                      Time elapsed: 00:04:24
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 263/1500 [0m                      

                       Computation: 110408 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 80.0033
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5031
                       Mean reward: 129.75
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 0.3011
    Episode_Reward/rotating_object: 28.0537
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.89s
                      Time elapsed: 00:04:25
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 264/1500 [0m                      

                       Computation: 97535 steps/s (collection: 0.857s, learning 0.151s)
             Mean action noise std: 1.78
          Mean value_function loss: 79.6401
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5111
                       Mean reward: 126.85
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.2983
    Episode_Reward/rotating_object: 28.2226
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.01s
                      Time elapsed: 00:04:26
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 265/1500 [0m                      

                       Computation: 110700 steps/s (collection: 0.794s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 80.1991
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5122
                       Mean reward: 157.43
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 0.3007
    Episode_Reward/rotating_object: 26.9813
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.89s
                      Time elapsed: 00:04:27
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 266/1500 [0m                      

                       Computation: 96039 steps/s (collection: 0.838s, learning 0.186s)
             Mean action noise std: 1.78
          Mean value_function loss: 88.2539
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5100
                       Mean reward: 130.75
               Mean episode length: 230.59
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 28.2476
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.02s
                      Time elapsed: 00:04:28
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 267/1500 [0m                      

                       Computation: 104770 steps/s (collection: 0.792s, learning 0.147s)
             Mean action noise std: 1.78
          Mean value_function loss: 88.7779
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5126
                       Mean reward: 146.90
               Mean episode length: 225.76
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 31.6857
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.94s
                      Time elapsed: 00:04:29
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 268/1500 [0m                      

                       Computation: 110496 steps/s (collection: 0.785s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 79.7484
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5181
                       Mean reward: 148.70
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 29.8275
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.89s
                      Time elapsed: 00:04:30
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 269/1500 [0m                      

                       Computation: 104382 steps/s (collection: 0.830s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 78.0263
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5233
                       Mean reward: 160.59
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 27.7676
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.94s
                      Time elapsed: 00:04:31
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 270/1500 [0m                      

                       Computation: 108847 steps/s (collection: 0.792s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 81.4820
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5284
                       Mean reward: 158.57
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 27.8633
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.90s
                      Time elapsed: 00:04:32
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 271/1500 [0m                      

                       Computation: 110329 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 80.6643
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5354
                       Mean reward: 137.59
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.3032
    Episode_Reward/rotating_object: 27.4039
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.89s
                      Time elapsed: 00:04:33
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 272/1500 [0m                      

                       Computation: 114133 steps/s (collection: 0.754s, learning 0.107s)
             Mean action noise std: 1.79
          Mean value_function loss: 84.2506
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.5462
                       Mean reward: 162.64
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.3095
    Episode_Reward/rotating_object: 32.1923
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.86s
                      Time elapsed: 00:04:34
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 273/1500 [0m                      

                       Computation: 109272 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 93.6195
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5538
                       Mean reward: 154.40
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 31.0334
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.90s
                      Time elapsed: 00:04:34
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 274/1500 [0m                      

                       Computation: 107851 steps/s (collection: 0.801s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 95.8786
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.5561
                       Mean reward: 186.75
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 29.3990
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.91s
                      Time elapsed: 00:04:35
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 275/1500 [0m                      

                       Computation: 103104 steps/s (collection: 0.783s, learning 0.171s)
             Mean action noise std: 1.80
          Mean value_function loss: 100.9268
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5572
                       Mean reward: 204.25
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 32.7612
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.95s
                      Time elapsed: 00:04:36
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 276/1500 [0m                      

                       Computation: 110323 steps/s (collection: 0.795s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 89.8877
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.5596
                       Mean reward: 180.46
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 32.7776
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.89s
                      Time elapsed: 00:04:37
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 277/1500 [0m                      

                       Computation: 89738 steps/s (collection: 0.952s, learning 0.143s)
             Mean action noise std: 1.80
          Mean value_function loss: 75.5147
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5590
                       Mean reward: 200.18
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.3095
    Episode_Reward/rotating_object: 31.2414
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.10s
                      Time elapsed: 00:04:38
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 278/1500 [0m                      

                       Computation: 102792 steps/s (collection: 0.826s, learning 0.131s)
             Mean action noise std: 1.80
          Mean value_function loss: 91.9365
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.5602
                       Mean reward: 121.17
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 27.1910
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.96s
                      Time elapsed: 00:04:39
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 279/1500 [0m                      

                       Computation: 104653 steps/s (collection: 0.781s, learning 0.158s)
             Mean action noise std: 1.80
          Mean value_function loss: 91.1322
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5668
                       Mean reward: 170.25
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 33.2825
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.94s
                      Time elapsed: 00:04:40
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 280/1500 [0m                      

                       Computation: 93985 steps/s (collection: 0.863s, learning 0.183s)
             Mean action noise std: 1.80
          Mean value_function loss: 102.4957
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5715
                       Mean reward: 178.78
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 31.9774
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.05s
                      Time elapsed: 00:04:41
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 281/1500 [0m                      

                       Computation: 108868 steps/s (collection: 0.792s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 102.0260
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5776
                       Mean reward: 168.02
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 28.8947
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.90s
                      Time elapsed: 00:04:42
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 282/1500 [0m                      

                       Computation: 104869 steps/s (collection: 0.810s, learning 0.127s)
             Mean action noise std: 1.80
          Mean value_function loss: 102.7250
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.5763
                       Mean reward: 146.55
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 31.3676
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.94s
                      Time elapsed: 00:04:43
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 283/1500 [0m                      

                       Computation: 99895 steps/s (collection: 0.863s, learning 0.122s)
             Mean action noise std: 1.80
          Mean value_function loss: 103.1173
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5775
                       Mean reward: 139.58
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 30.2437
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.98s
                      Time elapsed: 00:04:44
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 284/1500 [0m                      

                       Computation: 103693 steps/s (collection: 0.809s, learning 0.139s)
             Mean action noise std: 1.80
          Mean value_function loss: 99.8227
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5783
                       Mean reward: 140.04
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 32.0524
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.95s
                      Time elapsed: 00:04:45
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 285/1500 [0m                      

                       Computation: 104887 steps/s (collection: 0.831s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 96.6017
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.5750
                       Mean reward: 154.35
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 30.8067
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.94s
                      Time elapsed: 00:04:46
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 286/1500 [0m                      

                       Computation: 112810 steps/s (collection: 0.766s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 93.7375
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5770
                       Mean reward: 166.50
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 28.0460
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.87s
                      Time elapsed: 00:04:47
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 287/1500 [0m                      

                       Computation: 106595 steps/s (collection: 0.800s, learning 0.122s)
             Mean action noise std: 1.81
          Mean value_function loss: 97.8581
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5798
                       Mean reward: 159.77
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 30.3522
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.92s
                      Time elapsed: 00:04:48
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 288/1500 [0m                      

                       Computation: 101650 steps/s (collection: 0.879s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 103.8284
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.5841
                       Mean reward: 133.54
               Mean episode length: 242.29
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 28.5197
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.97s
                      Time elapsed: 00:04:49
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 289/1500 [0m                      

                       Computation: 100085 steps/s (collection: 0.881s, learning 0.101s)
             Mean action noise std: 1.81
          Mean value_function loss: 97.5495
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5918
                       Mean reward: 152.36
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 30.4905
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.98s
                      Time elapsed: 00:04:50
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 290/1500 [0m                      

                       Computation: 97644 steps/s (collection: 0.886s, learning 0.121s)
             Mean action noise std: 1.81
          Mean value_function loss: 83.7843
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5923
                       Mean reward: 128.18
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 0.3073
    Episode_Reward/rotating_object: 29.9021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.01s
                      Time elapsed: 00:04:51
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 291/1500 [0m                      

                       Computation: 110439 steps/s (collection: 0.779s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 78.9587
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.5941
                       Mean reward: 149.24
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 29.9954
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.89s
                      Time elapsed: 00:04:52
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 292/1500 [0m                      

                       Computation: 108433 steps/s (collection: 0.817s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 93.7456
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5980
                       Mean reward: 183.21
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 34.4056
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.91s
                      Time elapsed: 00:04:52
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 293/1500 [0m                      

                       Computation: 110661 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 102.7279
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6036
                       Mean reward: 160.17
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 30.9139
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.89s
                      Time elapsed: 00:04:53
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 294/1500 [0m                      

                       Computation: 107127 steps/s (collection: 0.800s, learning 0.118s)
             Mean action noise std: 1.81
          Mean value_function loss: 97.1752
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6107
                       Mean reward: 166.41
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 0.3073
    Episode_Reward/rotating_object: 30.5750
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.92s
                      Time elapsed: 00:04:54
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 295/1500 [0m                      

                       Computation: 104460 steps/s (collection: 0.791s, learning 0.150s)
             Mean action noise std: 1.81
          Mean value_function loss: 92.1972
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6025
                       Mean reward: 160.69
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 27.9874
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.94s
                      Time elapsed: 00:04:55
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 296/1500 [0m                      

                       Computation: 104655 steps/s (collection: 0.794s, learning 0.146s)
             Mean action noise std: 1.81
          Mean value_function loss: 90.2458
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5954
                       Mean reward: 141.84
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 32.4997
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.94s
                      Time elapsed: 00:04:56
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 297/1500 [0m                      

                       Computation: 108950 steps/s (collection: 0.785s, learning 0.117s)
             Mean action noise std: 1.82
          Mean value_function loss: 86.9854
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6008
                       Mean reward: 165.10
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3093
    Episode_Reward/rotating_object: 29.4270
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.90s
                      Time elapsed: 00:04:57
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 298/1500 [0m                      

                       Computation: 110385 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 83.5363
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6061
                       Mean reward: 148.95
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 32.2044
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.89s
                      Time elapsed: 00:04:58
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 299/1500 [0m                      

                       Computation: 110653 steps/s (collection: 0.791s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 83.0506
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6135
                       Mean reward: 175.11
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 29.7770
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.89s
                      Time elapsed: 00:04:59
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 300/1500 [0m                      

                       Computation: 101779 steps/s (collection: 0.870s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 82.7383
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.6172
                       Mean reward: 157.55
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 0.3085
    Episode_Reward/rotating_object: 29.3331
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.97s
                      Time elapsed: 00:05:00
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 301/1500 [0m                      

                       Computation: 108740 steps/s (collection: 0.814s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 85.5052
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.6200
                       Mean reward: 125.67
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 31.1697
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.90s
                      Time elapsed: 00:05:01
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 302/1500 [0m                      

                       Computation: 109217 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 84.7580
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.6257
                       Mean reward: 162.39
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 32.6882
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.90s
                      Time elapsed: 00:05:02
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 303/1500 [0m                      

                       Computation: 113676 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.6837
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.6272
                       Mean reward: 167.48
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 29.6798
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.86s
                      Time elapsed: 00:05:02
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 304/1500 [0m                      

                       Computation: 103555 steps/s (collection: 0.840s, learning 0.110s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.2291
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.6195
                       Mean reward: 202.39
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 35.4115
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.95s
                      Time elapsed: 00:05:03
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 305/1500 [0m                      

                       Computation: 113055 steps/s (collection: 0.772s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 79.7743
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6209
                       Mean reward: 152.68
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 33.6403
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.87s
                      Time elapsed: 00:05:04
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 306/1500 [0m                      

                       Computation: 103920 steps/s (collection: 0.826s, learning 0.120s)
             Mean action noise std: 1.82
          Mean value_function loss: 74.5553
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6245
                       Mean reward: 158.63
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 31.6631
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.95s
                      Time elapsed: 00:05:05
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 307/1500 [0m                      

                       Computation: 104249 steps/s (collection: 0.836s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.5835
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6243
                       Mean reward: 154.04
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 0.2972
    Episode_Reward/rotating_object: 28.1064
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.94s
                      Time elapsed: 00:05:06
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 308/1500 [0m                      

                       Computation: 102355 steps/s (collection: 0.792s, learning 0.169s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.9130
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.6255
                       Mean reward: 172.83
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 30.7203
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.96s
                      Time elapsed: 00:05:07
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 309/1500 [0m                      

                       Computation: 102658 steps/s (collection: 0.792s, learning 0.165s)
             Mean action noise std: 1.82
          Mean value_function loss: 86.1346
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.6245
                       Mean reward: 183.42
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 32.4723
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.96s
                      Time elapsed: 00:05:08
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 310/1500 [0m                      

                       Computation: 98918 steps/s (collection: 0.815s, learning 0.179s)
             Mean action noise std: 1.83
          Mean value_function loss: 76.9037
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6276
                       Mean reward: 146.73
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 29.5426
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.99s
                      Time elapsed: 00:05:09
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 311/1500 [0m                      

                       Computation: 94436 steps/s (collection: 0.888s, learning 0.153s)
             Mean action noise std: 1.83
          Mean value_function loss: 74.6568
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.6331
                       Mean reward: 181.07
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.2961
    Episode_Reward/rotating_object: 31.7570
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 1.04s
                      Time elapsed: 00:05:10
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 312/1500 [0m                      

                       Computation: 102471 steps/s (collection: 0.805s, learning 0.155s)
             Mean action noise std: 1.83
          Mean value_function loss: 66.8815
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6324
                       Mean reward: 144.50
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 31.7138
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.96s
                      Time elapsed: 00:05:11
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 313/1500 [0m                      

                       Computation: 109865 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 73.0949
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6301
                       Mean reward: 189.06
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 38.1090
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.89s
                      Time elapsed: 00:05:12
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 314/1500 [0m                      

                       Computation: 113770 steps/s (collection: 0.763s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 63.7212
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.6327
                       Mean reward: 170.02
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.3115
    Episode_Reward/rotating_object: 32.4045
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.86s
                      Time elapsed: 00:05:13
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 315/1500 [0m                      

                       Computation: 111611 steps/s (collection: 0.787s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 62.4791
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6357
                       Mean reward: 168.93
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.3021
    Episode_Reward/rotating_object: 29.0001
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.88s
                      Time elapsed: 00:05:14
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 316/1500 [0m                      

                       Computation: 109916 steps/s (collection: 0.808s, learning 0.087s)
             Mean action noise std: 1.83
          Mean value_function loss: 69.4786
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.6404
                       Mean reward: 215.90
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 35.4071
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.89s
                      Time elapsed: 00:05:15
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 317/1500 [0m                      

                       Computation: 111330 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 73.4173
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6438
                       Mean reward: 147.99
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 33.4251
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.88s
                      Time elapsed: 00:05:16
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 318/1500 [0m                      

                       Computation: 108319 steps/s (collection: 0.813s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 66.3349
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6513
                       Mean reward: 176.68
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.3074
    Episode_Reward/rotating_object: 33.8303
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.91s
                      Time elapsed: 00:05:16
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 319/1500 [0m                      

                       Computation: 111089 steps/s (collection: 0.780s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 76.8185
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6531
                       Mean reward: 173.93
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 31.9496
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.88s
                      Time elapsed: 00:05:17
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 320/1500 [0m                      

                       Computation: 106698 steps/s (collection: 0.778s, learning 0.143s)
             Mean action noise std: 1.84
          Mean value_function loss: 79.5884
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6573
                       Mean reward: 148.32
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 34.8552
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.92s
                      Time elapsed: 00:05:18
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 321/1500 [0m                      

                       Computation: 94900 steps/s (collection: 0.856s, learning 0.179s)
             Mean action noise std: 1.84
          Mean value_function loss: 83.4010
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6605
                       Mean reward: 178.66
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 33.9450
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 1.04s
                      Time elapsed: 00:05:19
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 322/1500 [0m                      

                       Computation: 93339 steps/s (collection: 0.841s, learning 0.212s)
             Mean action noise std: 1.84
          Mean value_function loss: 83.8240
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6643
                       Mean reward: 155.90
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 32.1022
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 1.05s
                      Time elapsed: 00:05:20
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 323/1500 [0m                      

                       Computation: 103758 steps/s (collection: 0.830s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 76.7329
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.6628
                       Mean reward: 169.94
               Mean episode length: 243.18
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 34.1708
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.95s
                      Time elapsed: 00:05:21
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 324/1500 [0m                      

                       Computation: 103782 steps/s (collection: 0.797s, learning 0.151s)
             Mean action noise std: 1.84
          Mean value_function loss: 85.3281
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6671
                       Mean reward: 153.46
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 32.0917
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.95s
                      Time elapsed: 00:05:22
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 325/1500 [0m                      

                       Computation: 107036 steps/s (collection: 0.792s, learning 0.126s)
             Mean action noise std: 1.84
          Mean value_function loss: 88.1194
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6694
                       Mean reward: 161.99
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 38.3109
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.92s
                      Time elapsed: 00:05:23
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 326/1500 [0m                      

                       Computation: 104922 steps/s (collection: 0.796s, learning 0.141s)
             Mean action noise std: 1.84
          Mean value_function loss: 78.7529
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6693
                       Mean reward: 153.43
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 34.9513
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.94s
                      Time elapsed: 00:05:24
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 327/1500 [0m                      

                       Computation: 104823 steps/s (collection: 0.791s, learning 0.147s)
             Mean action noise std: 1.84
          Mean value_function loss: 74.6237
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6851
                       Mean reward: 174.09
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 34.2242
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.94s
                      Time elapsed: 00:05:25
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 328/1500 [0m                      

                       Computation: 110945 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 69.7497
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7011
                       Mean reward: 140.00
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 33.1829
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.89s
                      Time elapsed: 00:05:26
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 329/1500 [0m                      

                       Computation: 115952 steps/s (collection: 0.758s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 72.7988
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7061
                       Mean reward: 178.34
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 35.7633
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.85s
                      Time elapsed: 00:05:27
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 330/1500 [0m                      

                       Computation: 99602 steps/s (collection: 0.864s, learning 0.123s)
             Mean action noise std: 1.85
          Mean value_function loss: 69.6720
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7103
                       Mean reward: 205.59
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 38.2642
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.99s
                      Time elapsed: 00:05:28
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 331/1500 [0m                      

                       Computation: 92532 steps/s (collection: 0.892s, learning 0.170s)
             Mean action noise std: 1.85
          Mean value_function loss: 74.5365
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7074
                       Mean reward: 192.55
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 37.1732
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 1.06s
                      Time elapsed: 00:05:29
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 332/1500 [0m                      

                       Computation: 88844 steps/s (collection: 0.934s, learning 0.172s)
             Mean action noise std: 1.85
          Mean value_function loss: 74.2259
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7091
                       Mean reward: 210.84
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 38.2295
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 1.11s
                      Time elapsed: 00:05:30
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 333/1500 [0m                      

                       Computation: 37155 steps/s (collection: 2.527s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 87.8824
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.7111
                       Mean reward: 187.97
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 35.0001
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.65s
                      Time elapsed: 00:05:33
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 334/1500 [0m                      

                       Computation: 33207 steps/s (collection: 2.836s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 87.2795
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7144
                       Mean reward: 177.85
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 37.1992
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.96s
                      Time elapsed: 00:05:36
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 335/1500 [0m                      

                       Computation: 31176 steps/s (collection: 3.021s, learning 0.133s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.9760
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7086
                       Mean reward: 206.51
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 34.3589
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.15s
                      Time elapsed: 00:05:39
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 336/1500 [0m                      

                       Computation: 30104 steps/s (collection: 3.075s, learning 0.191s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.3487
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7137
                       Mean reward: 199.62
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 37.1168
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.27s
                      Time elapsed: 00:05:42
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 337/1500 [0m                      

                       Computation: 31834 steps/s (collection: 2.961s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.0732
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7206
                       Mean reward: 175.23
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 39.6535
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.09s
                      Time elapsed: 00:05:45
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 338/1500 [0m                      

                       Computation: 32327 steps/s (collection: 2.908s, learning 0.133s)
             Mean action noise std: 1.86
          Mean value_function loss: 95.3258
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7224
                       Mean reward: 201.98
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 0.3268
    Episode_Reward/rotating_object: 38.3184
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.04s
                      Time elapsed: 00:05:48
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 339/1500 [0m                      

                       Computation: 33116 steps/s (collection: 2.849s, learning 0.120s)
             Mean action noise std: 1.86
          Mean value_function loss: 96.6510
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7244
                       Mean reward: 167.56
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.3312
    Episode_Reward/rotating_object: 38.9381
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 2.97s
                      Time elapsed: 00:05:51
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 340/1500 [0m                      

                       Computation: 31264 steps/s (collection: 2.977s, learning 0.168s)
             Mean action noise std: 1.86
          Mean value_function loss: 86.2119
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7245
                       Mean reward: 203.92
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.3355
    Episode_Reward/rotating_object: 38.5378
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.14s
                      Time elapsed: 00:05:54
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 341/1500 [0m                      

                       Computation: 21663 steps/s (collection: 4.439s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 100.3910
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.7267
                       Mean reward: 180.15
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 37.0048
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.54s
                      Time elapsed: 00:05:59
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 342/1500 [0m                      

                       Computation: 108751 steps/s (collection: 0.810s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 97.3160
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7291
                       Mean reward: 152.29
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.3287
    Episode_Reward/rotating_object: 37.2183
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.90s
                      Time elapsed: 00:06:00
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 343/1500 [0m                      

                       Computation: 97254 steps/s (collection: 0.851s, learning 0.160s)
             Mean action noise std: 1.86
          Mean value_function loss: 88.3487
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7318
                       Mean reward: 175.97
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 0.3378
    Episode_Reward/rotating_object: 39.8958
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.01s
                      Time elapsed: 00:06:01
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 344/1500 [0m                      

                       Computation: 102299 steps/s (collection: 0.850s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 86.1547
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7421
                       Mean reward: 211.44
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 0.3363
    Episode_Reward/rotating_object: 41.5563
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.96s
                      Time elapsed: 00:06:02
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 345/1500 [0m                      

                       Computation: 102534 steps/s (collection: 0.869s, learning 0.090s)
             Mean action noise std: 1.86
          Mean value_function loss: 93.7444
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7449
                       Mean reward: 209.92
               Mean episode length: 234.40
    Episode_Reward/reaching_object: 0.3363
    Episode_Reward/rotating_object: 41.0962
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.96s
                      Time elapsed: 00:06:03
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 346/1500 [0m                      

                       Computation: 112458 steps/s (collection: 0.789s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 95.9938
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7462
                       Mean reward: 231.06
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.3334
    Episode_Reward/rotating_object: 40.6328
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.87s
                      Time elapsed: 00:06:03
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 347/1500 [0m                      

                       Computation: 104115 steps/s (collection: 0.772s, learning 0.173s)
             Mean action noise std: 1.86
          Mean value_function loss: 102.6881
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7455
                       Mean reward: 163.87
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.3336
    Episode_Reward/rotating_object: 38.3795
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.94s
                      Time elapsed: 00:06:04
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 348/1500 [0m                      

                       Computation: 104013 steps/s (collection: 0.816s, learning 0.130s)
             Mean action noise std: 1.86
          Mean value_function loss: 97.5579
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7389
                       Mean reward: 199.13
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 37.6030
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.95s
                      Time elapsed: 00:06:05
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 349/1500 [0m                      

                       Computation: 97825 steps/s (collection: 0.844s, learning 0.161s)
             Mean action noise std: 1.87
          Mean value_function loss: 101.0448
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7417
                       Mean reward: 190.85
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3416
    Episode_Reward/rotating_object: 41.3447
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.00s
                      Time elapsed: 00:06:06
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 350/1500 [0m                      

                       Computation: 110285 steps/s (collection: 0.769s, learning 0.123s)
             Mean action noise std: 1.87
          Mean value_function loss: 97.1432
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7431
                       Mean reward: 203.68
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.3424
    Episode_Reward/rotating_object: 44.0189
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.89s
                      Time elapsed: 00:06:07
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 351/1500 [0m                      

                       Computation: 112867 steps/s (collection: 0.764s, learning 0.107s)
             Mean action noise std: 1.87
          Mean value_function loss: 91.2581
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7403
                       Mean reward: 205.45
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.3398
    Episode_Reward/rotating_object: 41.9431
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.87s
                      Time elapsed: 00:06:08
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 352/1500 [0m                      

                       Computation: 110412 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 88.9783
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7462
                       Mean reward: 202.94
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.3429
    Episode_Reward/rotating_object: 41.0777
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.89s
                      Time elapsed: 00:06:09
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 353/1500 [0m                      

                       Computation: 108786 steps/s (collection: 0.801s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 87.2446
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.7524
                       Mean reward: 204.52
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 0.3377
    Episode_Reward/rotating_object: 38.8317
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.90s
                      Time elapsed: 00:06:10
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 354/1500 [0m                      

                       Computation: 105324 steps/s (collection: 0.803s, learning 0.130s)
             Mean action noise std: 1.87
          Mean value_function loss: 88.6392
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7537
                       Mean reward: 215.64
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.3426
    Episode_Reward/rotating_object: 41.2146
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.93s
                      Time elapsed: 00:06:11
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 355/1500 [0m                      

                       Computation: 100143 steps/s (collection: 0.791s, learning 0.191s)
             Mean action noise std: 1.87
          Mean value_function loss: 91.3806
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.7543
                       Mean reward: 162.60
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 35.0372
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.98s
                      Time elapsed: 00:06:12
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 356/1500 [0m                      

                       Computation: 100746 steps/s (collection: 0.808s, learning 0.168s)
             Mean action noise std: 1.87
          Mean value_function loss: 80.4732
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7571
                       Mean reward: 232.83
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 43.1630
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.98s
                      Time elapsed: 00:06:13
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 357/1500 [0m                      

                       Computation: 105867 steps/s (collection: 0.830s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 90.1852
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7558
                       Mean reward: 208.57
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 0.3394
    Episode_Reward/rotating_object: 43.3820
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.93s
                      Time elapsed: 00:06:14
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 358/1500 [0m                      

                       Computation: 112332 steps/s (collection: 0.764s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 89.0308
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7536
                       Mean reward: 212.67
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 40.9258
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.88s
                      Time elapsed: 00:06:15
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 359/1500 [0m                      

                       Computation: 103090 steps/s (collection: 0.815s, learning 0.139s)
             Mean action noise std: 1.87
          Mean value_function loss: 93.0167
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7569
                       Mean reward: 234.43
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 0.3322
    Episode_Reward/rotating_object: 41.1940
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.95s
                      Time elapsed: 00:06:16
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 360/1500 [0m                      

                       Computation: 94509 steps/s (collection: 0.871s, learning 0.169s)
             Mean action noise std: 1.88
          Mean value_function loss: 89.9318
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7578
                       Mean reward: 236.62
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.3375
    Episode_Reward/rotating_object: 40.0106
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.04s
                      Time elapsed: 00:06:17
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 361/1500 [0m                      

                       Computation: 105651 steps/s (collection: 0.828s, learning 0.102s)
             Mean action noise std: 1.88
          Mean value_function loss: 87.0759
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7589
                       Mean reward: 236.08
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 41.3687
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.93s
                      Time elapsed: 00:06:17
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 362/1500 [0m                      

                       Computation: 108567 steps/s (collection: 0.774s, learning 0.132s)
             Mean action noise std: 1.88
          Mean value_function loss: 88.0331
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7612
                       Mean reward: 209.16
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.3315
    Episode_Reward/rotating_object: 41.7638
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.91s
                      Time elapsed: 00:06:18
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 363/1500 [0m                      

                       Computation: 112704 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 83.9866
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7625
                       Mean reward: 195.47
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.3319
    Episode_Reward/rotating_object: 41.6183
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.87s
                      Time elapsed: 00:06:19
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 364/1500 [0m                      

                       Computation: 103202 steps/s (collection: 0.771s, learning 0.182s)
             Mean action noise std: 1.88
          Mean value_function loss: 93.6417
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.7637
                       Mean reward: 214.97
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.3412
    Episode_Reward/rotating_object: 38.8010
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.95s
                      Time elapsed: 00:06:20
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 365/1500 [0m                      

                       Computation: 93945 steps/s (collection: 0.845s, learning 0.201s)
             Mean action noise std: 1.88
          Mean value_function loss: 88.8221
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7640
                       Mean reward: 211.48
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.3333
    Episode_Reward/rotating_object: 42.2292
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.05s
                      Time elapsed: 00:06:21
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 366/1500 [0m                      

                       Computation: 104325 steps/s (collection: 0.807s, learning 0.135s)
             Mean action noise std: 1.88
          Mean value_function loss: 88.0886
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7666
                       Mean reward: 206.06
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 46.0726
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.94s
                      Time elapsed: 00:06:22
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 367/1500 [0m                      

                       Computation: 114960 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 84.7666
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7716
                       Mean reward: 227.80
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 42.3087
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.86s
                      Time elapsed: 00:06:23
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 368/1500 [0m                      

                       Computation: 111511 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 1.88
          Mean value_function loss: 93.2752
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.7629
                       Mean reward: 213.50
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.3380
    Episode_Reward/rotating_object: 41.4737
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.88s
                      Time elapsed: 00:06:24
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 369/1500 [0m                      

                       Computation: 111424 steps/s (collection: 0.797s, learning 0.086s)
             Mean action noise std: 1.88
          Mean value_function loss: 79.4289
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.7591
                       Mean reward: 214.32
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.3380
    Episode_Reward/rotating_object: 44.9053
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.88s
                      Time elapsed: 00:06:25
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 370/1500 [0m                      

                       Computation: 97752 steps/s (collection: 0.870s, learning 0.136s)
             Mean action noise std: 1.88
          Mean value_function loss: 85.6801
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7594
                       Mean reward: 225.93
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 44.7434
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.01s
                      Time elapsed: 00:06:26
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 371/1500 [0m                      

                       Computation: 97955 steps/s (collection: 0.905s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 81.6898
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7563
                       Mean reward: 239.78
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.3391
    Episode_Reward/rotating_object: 46.9798
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.00s
                      Time elapsed: 00:06:27
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 372/1500 [0m                      

                       Computation: 111087 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 1.88
          Mean value_function loss: 92.9548
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7576
                       Mean reward: 254.52
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.3398
    Episode_Reward/rotating_object: 43.0785
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.88s
                      Time elapsed: 00:06:28
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 373/1500 [0m                      

                       Computation: 110934 steps/s (collection: 0.791s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 93.1882
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7646
                       Mean reward: 226.55
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 0.3351
    Episode_Reward/rotating_object: 45.1172
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.89s
                      Time elapsed: 00:06:29
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 374/1500 [0m                      

                       Computation: 107626 steps/s (collection: 0.787s, learning 0.127s)
             Mean action noise std: 1.88
          Mean value_function loss: 87.0588
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.7636
                       Mean reward: 204.29
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3364
    Episode_Reward/rotating_object: 44.4062
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.91s
                      Time elapsed: 00:06:30
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 375/1500 [0m                      

                       Computation: 99090 steps/s (collection: 0.785s, learning 0.207s)
             Mean action noise std: 1.89
          Mean value_function loss: 90.9090
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.7638
                       Mean reward: 221.12
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 0.3399
    Episode_Reward/rotating_object: 44.4158
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.99s
                      Time elapsed: 00:06:31
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 376/1500 [0m                      

                       Computation: 92676 steps/s (collection: 0.955s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 95.4013
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7694
                       Mean reward: 226.80
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.3359
    Episode_Reward/rotating_object: 45.8848
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.06s
                      Time elapsed: 00:06:32
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 377/1500 [0m                      

                       Computation: 107353 steps/s (collection: 0.820s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 100.4857
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7702
                       Mean reward: 214.87
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.3399
    Episode_Reward/rotating_object: 43.2926
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.92s
                      Time elapsed: 00:06:32
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 378/1500 [0m                      

                       Computation: 113194 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 102.2906
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7706
                       Mean reward: 221.28
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 45.9585
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.87s
                      Time elapsed: 00:06:33
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 379/1500 [0m                      

                       Computation: 106758 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 1.89
          Mean value_function loss: 105.4747
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7779
                       Mean reward: 242.35
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 43.9483
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.92s
                      Time elapsed: 00:06:34
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 380/1500 [0m                      

                       Computation: 105516 steps/s (collection: 0.797s, learning 0.135s)
             Mean action noise std: 1.89
          Mean value_function loss: 90.1330
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.7832
                       Mean reward: 217.73
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3421
    Episode_Reward/rotating_object: 42.2859
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.93s
                      Time elapsed: 00:06:35
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 381/1500 [0m                      

                       Computation: 100238 steps/s (collection: 0.852s, learning 0.129s)
             Mean action noise std: 1.89
          Mean value_function loss: 92.4376
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7849
                       Mean reward: 232.78
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.3483
    Episode_Reward/rotating_object: 46.0030
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.98s
                      Time elapsed: 00:06:36
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 382/1500 [0m                      

                       Computation: 98912 steps/s (collection: 0.841s, learning 0.153s)
             Mean action noise std: 1.89
          Mean value_function loss: 86.0622
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7882
                       Mean reward: 258.37
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.3426
    Episode_Reward/rotating_object: 45.8038
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.99s
                      Time elapsed: 00:06:37
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 383/1500 [0m                      

                       Computation: 97650 steps/s (collection: 0.859s, learning 0.148s)
             Mean action noise std: 1.89
          Mean value_function loss: 79.9257
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7941
                       Mean reward: 224.43
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 0.3410
    Episode_Reward/rotating_object: 46.3689
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.01s
                      Time elapsed: 00:06:38
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 384/1500 [0m                      

                       Computation: 115294 steps/s (collection: 0.760s, learning 0.093s)
             Mean action noise std: 1.89
          Mean value_function loss: 91.0023
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7919
                       Mean reward: 230.57
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3379
    Episode_Reward/rotating_object: 45.5943
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.85s
                      Time elapsed: 00:06:39
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 385/1500 [0m                      

                       Computation: 106867 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 1.89
          Mean value_function loss: 89.8584
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7882
                       Mean reward: 276.24
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.3393
    Episode_Reward/rotating_object: 48.6261
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.92s
                      Time elapsed: 00:06:40
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 386/1500 [0m                      

                       Computation: 102217 steps/s (collection: 0.814s, learning 0.148s)
             Mean action noise std: 1.89
          Mean value_function loss: 88.4711
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7821
                       Mean reward: 253.49
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.3456
    Episode_Reward/rotating_object: 48.9941
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.96s
                      Time elapsed: 00:06:41
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 387/1500 [0m                      

                       Computation: 94940 steps/s (collection: 0.891s, learning 0.145s)
             Mean action noise std: 1.89
          Mean value_function loss: 88.8907
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7814
                       Mean reward: 278.74
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 0.3457
    Episode_Reward/rotating_object: 49.7197
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.04s
                      Time elapsed: 00:06:42
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 388/1500 [0m                      

                       Computation: 108419 steps/s (collection: 0.799s, learning 0.108s)
             Mean action noise std: 1.89
          Mean value_function loss: 86.0105
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7862
                       Mean reward: 250.36
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 0.3382
    Episode_Reward/rotating_object: 46.9937
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.91s
                      Time elapsed: 00:06:43
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 389/1500 [0m                      

                       Computation: 109278 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 88.7771
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.7875
                       Mean reward: 238.18
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.3365
    Episode_Reward/rotating_object: 47.9490
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.90s
                      Time elapsed: 00:06:44
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 390/1500 [0m                      

                       Computation: 113733 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 87.7483
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7836
                       Mean reward: 243.74
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.3397
    Episode_Reward/rotating_object: 46.5289
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.86s
                      Time elapsed: 00:06:45
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 391/1500 [0m                      

                       Computation: 112093 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 1.89
          Mean value_function loss: 74.3416
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.7819
                       Mean reward: 227.51
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.3416
    Episode_Reward/rotating_object: 48.9584
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.88s
                      Time elapsed: 00:06:45
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 392/1500 [0m                      

                       Computation: 111342 steps/s (collection: 0.769s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 71.7273
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7891
                       Mean reward: 220.51
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.3438
    Episode_Reward/rotating_object: 47.7494
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.88s
                      Time elapsed: 00:06:46
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 393/1500 [0m                      

                       Computation: 104533 steps/s (collection: 0.820s, learning 0.120s)
             Mean action noise std: 1.90
          Mean value_function loss: 81.6154
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.7954
                       Mean reward: 241.65
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 0.3392
    Episode_Reward/rotating_object: 49.7938
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.94s
                      Time elapsed: 00:06:47
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 394/1500 [0m                      

                       Computation: 113656 steps/s (collection: 0.734s, learning 0.131s)
             Mean action noise std: 1.90
          Mean value_function loss: 82.0831
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7951
                       Mean reward: 276.29
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.3353
    Episode_Reward/rotating_object: 52.6170
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.86s
                      Time elapsed: 00:06:48
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 395/1500 [0m                      

                       Computation: 110657 steps/s (collection: 0.777s, learning 0.112s)
             Mean action noise std: 1.90
          Mean value_function loss: 76.6132
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.7970
                       Mean reward: 264.09
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3362
    Episode_Reward/rotating_object: 46.6801
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.89s
                      Time elapsed: 00:06:49
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 396/1500 [0m                      

                       Computation: 112228 steps/s (collection: 0.775s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 91.0237
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8019
                       Mean reward: 236.34
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 46.3524
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.88s
                      Time elapsed: 00:06:50
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 397/1500 [0m                      

                       Computation: 102292 steps/s (collection: 0.841s, learning 0.120s)
             Mean action noise std: 1.90
          Mean value_function loss: 94.3755
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8075
                       Mean reward: 244.91
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.3271
    Episode_Reward/rotating_object: 45.5028
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.96s
                      Time elapsed: 00:06:51
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 398/1500 [0m                      

                       Computation: 97710 steps/s (collection: 0.837s, learning 0.169s)
             Mean action noise std: 1.90
          Mean value_function loss: 91.4449
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8112
                       Mean reward: 195.97
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 0.3311
    Episode_Reward/rotating_object: 49.3663
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.01s
                      Time elapsed: 00:06:52
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 399/1500 [0m                      

                       Computation: 106793 steps/s (collection: 0.817s, learning 0.104s)
             Mean action noise std: 1.90
          Mean value_function loss: 83.5947
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.8179
                       Mean reward: 227.35
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.3250
    Episode_Reward/rotating_object: 46.2999
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.92s
                      Time elapsed: 00:06:53
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 400/1500 [0m                      

                       Computation: 106713 steps/s (collection: 0.814s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 103.6953
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.8232
                       Mean reward: 235.39
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.3354
    Episode_Reward/rotating_object: 46.8069
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.92s
                      Time elapsed: 00:06:54
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 401/1500 [0m                      

                       Computation: 106908 steps/s (collection: 0.806s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 88.0272
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.8247
                       Mean reward: 250.65
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.3394
    Episode_Reward/rotating_object: 50.6247
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.92s
                      Time elapsed: 00:06:55
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 402/1500 [0m                      

                       Computation: 106951 steps/s (collection: 0.821s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 78.5170
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8249
                       Mean reward: 248.68
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.3429
    Episode_Reward/rotating_object: 49.2135
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.92s
                      Time elapsed: 00:06:56
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 403/1500 [0m                      

                       Computation: 97365 steps/s (collection: 0.871s, learning 0.138s)
             Mean action noise std: 1.90
          Mean value_function loss: 87.2504
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8202
                       Mean reward: 241.70
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 48.1868
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.01s
                      Time elapsed: 00:06:57
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 404/1500 [0m                      

                       Computation: 101156 steps/s (collection: 0.877s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 82.3384
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.8158
                       Mean reward: 235.95
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 0.3397
    Episode_Reward/rotating_object: 53.9376
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.97s
                      Time elapsed: 00:06:58
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 405/1500 [0m                      

                       Computation: 107578 steps/s (collection: 0.801s, learning 0.112s)
             Mean action noise std: 1.90
          Mean value_function loss: 84.9771
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.8123
                       Mean reward: 251.76
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.3279
    Episode_Reward/rotating_object: 48.9603
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.91s
                      Time elapsed: 00:06:58
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 406/1500 [0m                      

                       Computation: 103455 steps/s (collection: 0.864s, learning 0.086s)
             Mean action noise std: 1.90
          Mean value_function loss: 83.0500
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8148
                       Mean reward: 269.79
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.3382
    Episode_Reward/rotating_object: 51.2351
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.95s
                      Time elapsed: 00:06:59
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 407/1500 [0m                      

                       Computation: 108410 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 90.5280
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8167
                       Mean reward: 246.26
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.3452
    Episode_Reward/rotating_object: 53.6165
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.91s
                      Time elapsed: 00:07:00
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 408/1500 [0m                      

                       Computation: 99686 steps/s (collection: 0.808s, learning 0.179s)
             Mean action noise std: 1.91
          Mean value_function loss: 81.0003
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8194
                       Mean reward: 237.14
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.3304
    Episode_Reward/rotating_object: 48.0546
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.99s
                      Time elapsed: 00:07:01
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 409/1500 [0m                      

                       Computation: 112181 steps/s (collection: 0.785s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 80.2883
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.8277
                       Mean reward: 262.06
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 48.6602
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.88s
                      Time elapsed: 00:07:02
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 410/1500 [0m                      

                       Computation: 103714 steps/s (collection: 0.774s, learning 0.173s)
             Mean action noise std: 1.91
          Mean value_function loss: 87.6938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8286
                       Mean reward: 273.06
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3357
    Episode_Reward/rotating_object: 52.7372
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.95s
                      Time elapsed: 00:07:03
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 411/1500 [0m                      

                       Computation: 107997 steps/s (collection: 0.822s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 88.7285
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8308
                       Mean reward: 251.34
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 51.8575
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.91s
                      Time elapsed: 00:07:04
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 412/1500 [0m                      

                       Computation: 109838 steps/s (collection: 0.768s, learning 0.127s)
             Mean action noise std: 1.91
          Mean value_function loss: 86.4807
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.8345
                       Mean reward: 288.12
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 53.0837
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.89s
                      Time elapsed: 00:07:05
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 413/1500 [0m                      

                       Computation: 108954 steps/s (collection: 0.811s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 88.1627
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8292
                       Mean reward: 283.56
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.3407
    Episode_Reward/rotating_object: 53.8498
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.90s
                      Time elapsed: 00:07:06
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 414/1500 [0m                      

                       Computation: 95037 steps/s (collection: 0.940s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 88.0916
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.8239
                       Mean reward: 286.26
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.3319
    Episode_Reward/rotating_object: 51.5123
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 1.03s
                      Time elapsed: 00:07:07
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 415/1500 [0m                      

                       Computation: 104555 steps/s (collection: 0.847s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 90.3060
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8285
                       Mean reward: 238.66
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.3371
    Episode_Reward/rotating_object: 48.9339
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.94s
                      Time elapsed: 00:07:08
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 416/1500 [0m                      

                       Computation: 109250 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 85.8980
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8329
                       Mean reward: 285.15
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 54.7436
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.90s
                      Time elapsed: 00:07:09
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 417/1500 [0m                      

                       Computation: 106040 steps/s (collection: 0.829s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 96.8403
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.8355
                       Mean reward: 305.34
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.3513
    Episode_Reward/rotating_object: 52.6486
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.93s
                      Time elapsed: 00:07:10
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 418/1500 [0m                      

                       Computation: 107861 steps/s (collection: 0.802s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 85.8797
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8324
                       Mean reward: 270.55
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.3447
    Episode_Reward/rotating_object: 53.8829
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.91s
                      Time elapsed: 00:07:11
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 419/1500 [0m                      

                       Computation: 105638 steps/s (collection: 0.813s, learning 0.118s)
             Mean action noise std: 1.92
          Mean value_function loss: 90.7922
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.8331
                       Mean reward: 277.19
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 54.8620
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.93s
                      Time elapsed: 00:07:12
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 420/1500 [0m                      

                       Computation: 110060 steps/s (collection: 0.793s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 107.9653
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.8378
                       Mean reward: 266.84
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 52.7627
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.89s
                      Time elapsed: 00:07:12
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 421/1500 [0m                      

                       Computation: 107334 steps/s (collection: 0.796s, learning 0.120s)
             Mean action noise std: 1.92
          Mean value_function loss: 87.9976
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8464
                       Mean reward: 261.42
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.3444
    Episode_Reward/rotating_object: 52.9169
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.92s
                      Time elapsed: 00:07:13
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 422/1500 [0m                      

                       Computation: 99783 steps/s (collection: 0.803s, learning 0.182s)
             Mean action noise std: 1.92
          Mean value_function loss: 94.5260
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8562
                       Mean reward: 226.39
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 0.3508
    Episode_Reward/rotating_object: 51.3872
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.99s
                      Time elapsed: 00:07:14
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 423/1500 [0m                      

                       Computation: 111144 steps/s (collection: 0.790s, learning 0.094s)
             Mean action noise std: 1.93
          Mean value_function loss: 79.5388
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8696
                       Mean reward: 304.45
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.3530
    Episode_Reward/rotating_object: 55.5147
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.88s
                      Time elapsed: 00:07:15
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 424/1500 [0m                      

                       Computation: 105493 steps/s (collection: 0.789s, learning 0.142s)
             Mean action noise std: 1.93
          Mean value_function loss: 92.2480
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8773
                       Mean reward: 257.57
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3483
    Episode_Reward/rotating_object: 57.8373
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.93s
                      Time elapsed: 00:07:16
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 425/1500 [0m                      

                       Computation: 109286 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 95.5197
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8820
                       Mean reward: 322.73
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3418
    Episode_Reward/rotating_object: 55.6415
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.90s
                      Time elapsed: 00:07:17
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 426/1500 [0m                      

                       Computation: 111079 steps/s (collection: 0.784s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 101.0644
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8897
                       Mean reward: 264.96
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.3405
    Episode_Reward/rotating_object: 54.7236
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.88s
                      Time elapsed: 00:07:18
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 427/1500 [0m                      

                       Computation: 103210 steps/s (collection: 0.835s, learning 0.117s)
             Mean action noise std: 1.93
          Mean value_function loss: 114.1646
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8913
                       Mean reward: 306.99
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.3446
    Episode_Reward/rotating_object: 54.5891
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.95s
                      Time elapsed: 00:07:19
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 428/1500 [0m                      

                       Computation: 107908 steps/s (collection: 0.798s, learning 0.113s)
             Mean action noise std: 1.93
          Mean value_function loss: 117.6419
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8967
                       Mean reward: 267.55
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3579
    Episode_Reward/rotating_object: 56.5755
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.91s
                      Time elapsed: 00:07:20
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 429/1500 [0m                      

                       Computation: 107884 steps/s (collection: 0.808s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 111.5932
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.8977
                       Mean reward: 260.54
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 0.3506
    Episode_Reward/rotating_object: 56.0573
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.91s
                      Time elapsed: 00:07:21
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 430/1500 [0m                      

                       Computation: 99223 steps/s (collection: 0.884s, learning 0.107s)
             Mean action noise std: 1.93
          Mean value_function loss: 104.5596
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8957
                       Mean reward: 282.05
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.3538
    Episode_Reward/rotating_object: 56.2794
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.99s
                      Time elapsed: 00:07:22
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 431/1500 [0m                      

                       Computation: 106581 steps/s (collection: 0.820s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 118.7201
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8971
                       Mean reward: 295.33
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.3538
    Episode_Reward/rotating_object: 57.2189
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.92s
                      Time elapsed: 00:07:23
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 432/1500 [0m                      

                       Computation: 106414 steps/s (collection: 0.822s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 121.1825
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9007
                       Mean reward: 291.00
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.3560
    Episode_Reward/rotating_object: 59.1708
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.92s
                      Time elapsed: 00:07:24
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 433/1500 [0m                      

                       Computation: 106419 steps/s (collection: 0.816s, learning 0.108s)
             Mean action noise std: 1.94
          Mean value_function loss: 118.1838
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.9046
                       Mean reward: 280.50
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.3603
    Episode_Reward/rotating_object: 56.6052
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.92s
                      Time elapsed: 00:07:24
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 434/1500 [0m                      

                       Computation: 105377 steps/s (collection: 0.799s, learning 0.133s)
             Mean action noise std: 1.94
          Mean value_function loss: 111.8110
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.9101
                       Mean reward: 262.84
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.3606
    Episode_Reward/rotating_object: 58.2885
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.93s
                      Time elapsed: 00:07:25
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 435/1500 [0m                      

                       Computation: 104043 steps/s (collection: 0.796s, learning 0.149s)
             Mean action noise std: 1.94
          Mean value_function loss: 121.3412
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9150
                       Mean reward: 293.84
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.3667
    Episode_Reward/rotating_object: 59.9976
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.94s
                      Time elapsed: 00:07:26
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 436/1500 [0m                      

                       Computation: 99524 steps/s (collection: 0.870s, learning 0.118s)
             Mean action noise std: 1.94
          Mean value_function loss: 106.8349
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.9157
                       Mean reward: 293.74
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 0.3543
    Episode_Reward/rotating_object: 54.1348
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.99s
                      Time elapsed: 00:07:27
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 437/1500 [0m                      

                       Computation: 107233 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 111.9312
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9142
                       Mean reward: 297.24
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 0.3700
    Episode_Reward/rotating_object: 62.4152
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.92s
                      Time elapsed: 00:07:28
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 438/1500 [0m                      

                       Computation: 100930 steps/s (collection: 0.830s, learning 0.144s)
             Mean action noise std: 1.94
          Mean value_function loss: 110.7516
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9142
                       Mean reward: 280.44
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.3693
    Episode_Reward/rotating_object: 57.2778
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.97s
                      Time elapsed: 00:07:29
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 439/1500 [0m                      

                       Computation: 99301 steps/s (collection: 0.885s, learning 0.105s)
             Mean action noise std: 1.94
          Mean value_function loss: 111.2573
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9167
                       Mean reward: 274.04
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.3622
    Episode_Reward/rotating_object: 55.7224
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.99s
                      Time elapsed: 00:07:30
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 440/1500 [0m                      

                       Computation: 93308 steps/s (collection: 0.847s, learning 0.206s)
             Mean action noise std: 1.95
          Mean value_function loss: 120.9554
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9292
                       Mean reward: 261.41
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 60.0328
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.05s
                      Time elapsed: 00:07:31
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 441/1500 [0m                      

                       Computation: 103844 steps/s (collection: 0.806s, learning 0.140s)
             Mean action noise std: 1.95
          Mean value_function loss: 128.3737
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.9340
                       Mean reward: 294.35
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 60.9150
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.95s
                      Time elapsed: 00:07:32
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 442/1500 [0m                      

                       Computation: 101683 steps/s (collection: 0.823s, learning 0.144s)
             Mean action noise std: 1.95
          Mean value_function loss: 119.0768
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.9338
                       Mean reward: 336.81
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 0.3700
    Episode_Reward/rotating_object: 62.7845
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.97s
                      Time elapsed: 00:07:33
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 443/1500 [0m                      

                       Computation: 101520 steps/s (collection: 0.836s, learning 0.132s)
             Mean action noise std: 1.95
          Mean value_function loss: 120.2353
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.9368
                       Mean reward: 263.34
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.3589
    Episode_Reward/rotating_object: 54.6570
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.97s
                      Time elapsed: 00:07:34
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 444/1500 [0m                      

                       Computation: 103845 steps/s (collection: 0.802s, learning 0.145s)
             Mean action noise std: 1.95
          Mean value_function loss: 117.2019
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9325
                       Mean reward: 325.61
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.3607
    Episode_Reward/rotating_object: 61.1498
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.95s
                      Time elapsed: 00:07:35
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 445/1500 [0m                      

                       Computation: 99527 steps/s (collection: 0.832s, learning 0.156s)
             Mean action noise std: 1.95
          Mean value_function loss: 123.7600
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9243
                       Mean reward: 318.54
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.3630
    Episode_Reward/rotating_object: 59.0201
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.99s
                      Time elapsed: 00:07:36
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 446/1500 [0m                      

                       Computation: 89542 steps/s (collection: 0.926s, learning 0.172s)
             Mean action noise std: 1.95
          Mean value_function loss: 122.2971
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9226
                       Mean reward: 287.08
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.3708
    Episode_Reward/rotating_object: 60.0807
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 1.10s
                      Time elapsed: 00:07:37
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 447/1500 [0m                      

                       Computation: 101376 steps/s (collection: 0.832s, learning 0.138s)
             Mean action noise std: 1.95
          Mean value_function loss: 131.8713
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9238
                       Mean reward: 323.46
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.3726
    Episode_Reward/rotating_object: 61.5039
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.97s
                      Time elapsed: 00:07:38
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 448/1500 [0m                      

                       Computation: 108380 steps/s (collection: 0.787s, learning 0.120s)
             Mean action noise std: 1.95
          Mean value_function loss: 129.8190
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9265
                       Mean reward: 303.35
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 58.9731
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.91s
                      Time elapsed: 00:07:39
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 449/1500 [0m                      

                       Computation: 106073 steps/s (collection: 0.797s, learning 0.130s)
             Mean action noise std: 1.95
          Mean value_function loss: 123.2599
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9231
                       Mean reward: 294.38
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.3736
    Episode_Reward/rotating_object: 60.6185
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.93s
                      Time elapsed: 00:07:40
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 450/1500 [0m                      

                       Computation: 98534 steps/s (collection: 0.829s, learning 0.169s)
             Mean action noise std: 1.95
          Mean value_function loss: 127.3365
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9203
                       Mean reward: 288.84
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.3792
    Episode_Reward/rotating_object: 57.3504
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 1.00s
                      Time elapsed: 00:07:41
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 451/1500 [0m                      

                       Computation: 92665 steps/s (collection: 0.931s, learning 0.129s)
             Mean action noise std: 1.95
          Mean value_function loss: 129.5181
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 15.9239
                       Mean reward: 332.71
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.3721
    Episode_Reward/rotating_object: 59.9239
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 1.06s
                      Time elapsed: 00:07:42
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 452/1500 [0m                      

                       Computation: 100145 steps/s (collection: 0.821s, learning 0.161s)
             Mean action noise std: 1.95
          Mean value_function loss: 131.4925
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.9211
                       Mean reward: 380.51
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.3751
    Episode_Reward/rotating_object: 64.3708
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.98s
                      Time elapsed: 00:07:43
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 453/1500 [0m                      

                       Computation: 99230 steps/s (collection: 0.845s, learning 0.146s)
             Mean action noise std: 1.95
          Mean value_function loss: 133.6544
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9224
                       Mean reward: 294.62
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.3795
    Episode_Reward/rotating_object: 63.2287
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.99s
                      Time elapsed: 00:07:44
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 454/1500 [0m                      

                       Computation: 98032 steps/s (collection: 0.887s, learning 0.116s)
             Mean action noise std: 1.95
          Mean value_function loss: 129.1548
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9277
                       Mean reward: 323.30
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 0.3730
    Episode_Reward/rotating_object: 62.9896
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 1.00s
                      Time elapsed: 00:07:45
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 455/1500 [0m                      

                       Computation: 105945 steps/s (collection: 0.802s, learning 0.126s)
             Mean action noise std: 1.96
          Mean value_function loss: 122.9845
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9282
                       Mean reward: 262.80
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.3677
    Episode_Reward/rotating_object: 55.3398
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.93s
                      Time elapsed: 00:07:46
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 456/1500 [0m                      

                       Computation: 103224 steps/s (collection: 0.843s, learning 0.110s)
             Mean action noise std: 1.96
          Mean value_function loss: 137.7619
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9296
                       Mean reward: 322.41
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.3818
    Episode_Reward/rotating_object: 65.7798
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.95s
                      Time elapsed: 00:07:47
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 457/1500 [0m                      

                       Computation: 105824 steps/s (collection: 0.825s, learning 0.104s)
             Mean action noise std: 1.96
          Mean value_function loss: 128.4903
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.9286
                       Mean reward: 302.33
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.3713
    Episode_Reward/rotating_object: 60.1528
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.93s
                      Time elapsed: 00:07:48
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 458/1500 [0m                      

                       Computation: 112463 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 1.96
          Mean value_function loss: 126.5904
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.9301
                       Mean reward: 303.01
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.3742
    Episode_Reward/rotating_object: 61.8928
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.87s
                      Time elapsed: 00:07:49
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 459/1500 [0m                      

                       Computation: 99110 steps/s (collection: 0.850s, learning 0.142s)
             Mean action noise std: 1.96
          Mean value_function loss: 136.1467
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9313
                       Mean reward: 266.42
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.3748
    Episode_Reward/rotating_object: 58.9459
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.99s
                      Time elapsed: 00:07:50
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 460/1500 [0m                      

                       Computation: 113053 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 122.3399
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9242
                       Mean reward: 315.51
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.3660
    Episode_Reward/rotating_object: 64.0013
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.87s
                      Time elapsed: 00:07:51
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 461/1500 [0m                      

                       Computation: 94216 steps/s (collection: 0.887s, learning 0.157s)
             Mean action noise std: 1.96
          Mean value_function loss: 123.0517
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9232
                       Mean reward: 305.38
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 61.1817
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 1.04s
                      Time elapsed: 00:07:52
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 462/1500 [0m                      

                       Computation: 106658 steps/s (collection: 0.834s, learning 0.088s)
             Mean action noise std: 1.96
          Mean value_function loss: 130.6670
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9265
                       Mean reward: 328.60
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3726
    Episode_Reward/rotating_object: 62.0278
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.92s
                      Time elapsed: 00:07:53
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 463/1500 [0m                      

                       Computation: 103244 steps/s (collection: 0.837s, learning 0.115s)
             Mean action noise std: 1.96
          Mean value_function loss: 121.5384
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9246
                       Mean reward: 298.27
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.3770
    Episode_Reward/rotating_object: 64.1849
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.95s
                      Time elapsed: 00:07:53
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 464/1500 [0m                      

                       Computation: 108386 steps/s (collection: 0.816s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 117.1411
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9174
                       Mean reward: 304.42
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 0.3709
    Episode_Reward/rotating_object: 60.6712
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.91s
                      Time elapsed: 00:07:54
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 465/1500 [0m                      

                       Computation: 100126 steps/s (collection: 0.829s, learning 0.153s)
             Mean action noise std: 1.96
          Mean value_function loss: 121.7877
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9170
                       Mean reward: 319.33
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 0.3695
    Episode_Reward/rotating_object: 66.7171
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.98s
                      Time elapsed: 00:07:55
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 466/1500 [0m                      

                       Computation: 106202 steps/s (collection: 0.817s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 112.9778
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.9145
                       Mean reward: 315.31
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.3775
    Episode_Reward/rotating_object: 64.0310
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.93s
                      Time elapsed: 00:07:56
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 467/1500 [0m                      

                       Computation: 100685 steps/s (collection: 0.814s, learning 0.162s)
             Mean action noise std: 1.96
          Mean value_function loss: 124.2511
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9099
                       Mean reward: 297.18
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 0.3686
    Episode_Reward/rotating_object: 61.9636
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.98s
                      Time elapsed: 00:07:57
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 468/1500 [0m                      

                       Computation: 108465 steps/s (collection: 0.803s, learning 0.104s)
             Mean action noise std: 1.96
          Mean value_function loss: 118.7613
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.9164
                       Mean reward: 297.90
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 65.2663
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.91s
                      Time elapsed: 00:07:58
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 469/1500 [0m                      

                       Computation: 104234 steps/s (collection: 0.783s, learning 0.160s)
             Mean action noise std: 1.96
          Mean value_function loss: 108.8288
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9150
                       Mean reward: 336.36
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.3717
    Episode_Reward/rotating_object: 65.5792
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.94s
                      Time elapsed: 00:07:59
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 470/1500 [0m                      

                       Computation: 111101 steps/s (collection: 0.788s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 110.6770
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9107
                       Mean reward: 330.81
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.3640
    Episode_Reward/rotating_object: 63.7491
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.88s
                      Time elapsed: 00:08:00
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 471/1500 [0m                      

                       Computation: 90220 steps/s (collection: 0.916s, learning 0.174s)
             Mean action noise std: 1.96
          Mean value_function loss: 114.9057
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9012
                       Mean reward: 323.86
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 0.3664
    Episode_Reward/rotating_object: 63.5240
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.09s
                      Time elapsed: 00:08:01
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 472/1500 [0m                      

                       Computation: 99504 steps/s (collection: 0.828s, learning 0.160s)
             Mean action noise std: 1.96
          Mean value_function loss: 134.3602
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9057
                       Mean reward: 295.72
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.3743
    Episode_Reward/rotating_object: 62.3695
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.99s
                      Time elapsed: 00:08:02
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 473/1500 [0m                      

                       Computation: 106672 steps/s (collection: 0.825s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 121.1139
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9099
                       Mean reward: 333.43
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.3619
    Episode_Reward/rotating_object: 62.0171
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.92s
                      Time elapsed: 00:08:03
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 474/1500 [0m                      

                       Computation: 108980 steps/s (collection: 0.797s, learning 0.105s)
             Mean action noise std: 1.97
          Mean value_function loss: 111.0291
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.9154
                       Mean reward: 326.14
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.3609
    Episode_Reward/rotating_object: 62.2730
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.90s
                      Time elapsed: 00:08:04
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 475/1500 [0m                      

                       Computation: 107796 steps/s (collection: 0.818s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 108.8674
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9116
                       Mean reward: 362.88
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3777
    Episode_Reward/rotating_object: 68.4022
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.91s
                      Time elapsed: 00:08:05
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 476/1500 [0m                      

                       Computation: 103633 steps/s (collection: 0.843s, learning 0.106s)
             Mean action noise std: 1.97
          Mean value_function loss: 114.1520
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9134
                       Mean reward: 322.47
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 0.3633
    Episode_Reward/rotating_object: 65.7529
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.95s
                      Time elapsed: 00:08:06
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 477/1500 [0m                      

                       Computation: 94600 steps/s (collection: 0.908s, learning 0.131s)
             Mean action noise std: 1.97
          Mean value_function loss: 114.8368
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9137
                       Mean reward: 326.38
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.3635
    Episode_Reward/rotating_object: 62.2636
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 1.04s
                      Time elapsed: 00:08:07
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 478/1500 [0m                      

                       Computation: 104117 steps/s (collection: 0.853s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 125.1639
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9271
                       Mean reward: 265.60
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 65.4006
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.94s
                      Time elapsed: 00:08:08
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 479/1500 [0m                      

                       Computation: 101581 steps/s (collection: 0.856s, learning 0.112s)
             Mean action noise std: 1.97
          Mean value_function loss: 115.8953
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9354
                       Mean reward: 324.25
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.3741
    Episode_Reward/rotating_object: 67.9241
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.97s
                      Time elapsed: 00:08:09
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 480/1500 [0m                      

                       Computation: 103462 steps/s (collection: 0.836s, learning 0.114s)
             Mean action noise std: 1.97
          Mean value_function loss: 111.3328
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9367
                       Mean reward: 318.35
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.3593
    Episode_Reward/rotating_object: 64.7385
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.95s
                      Time elapsed: 00:08:10
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 481/1500 [0m                      

                       Computation: 95858 steps/s (collection: 0.847s, learning 0.179s)
             Mean action noise std: 1.97
          Mean value_function loss: 123.0469
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9388
                       Mean reward: 330.59
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.3662
    Episode_Reward/rotating_object: 65.2762
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 1.03s
                      Time elapsed: 00:08:11
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 482/1500 [0m                      

                       Computation: 95518 steps/s (collection: 0.923s, learning 0.107s)
             Mean action noise std: 1.98
          Mean value_function loss: 119.9126
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.9334
                       Mean reward: 295.36
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 63.5280
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 1.03s
                      Time elapsed: 00:08:12
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 483/1500 [0m                      

                       Computation: 102478 steps/s (collection: 0.837s, learning 0.122s)
             Mean action noise std: 1.98
          Mean value_function loss: 123.0882
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.9342
                       Mean reward: 320.33
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 0.3718
    Episode_Reward/rotating_object: 65.8556
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.96s
                      Time elapsed: 00:08:13
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 484/1500 [0m                      

                       Computation: 97109 steps/s (collection: 0.904s, learning 0.109s)
             Mean action noise std: 1.98
          Mean value_function loss: 128.2394
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9336
                       Mean reward: 321.11
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 66.3341
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 1.01s
                      Time elapsed: 00:08:14
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 485/1500 [0m                      

                       Computation: 104475 steps/s (collection: 0.814s, learning 0.127s)
             Mean action noise std: 1.98
          Mean value_function loss: 128.5155
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9338
                       Mean reward: 368.85
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.3716
    Episode_Reward/rotating_object: 67.6157
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.94s
                      Time elapsed: 00:08:15
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 486/1500 [0m                      

                       Computation: 95699 steps/s (collection: 0.872s, learning 0.155s)
             Mean action noise std: 1.98
          Mean value_function loss: 130.5218
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.9336
                       Mean reward: 361.36
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 0.3718
    Episode_Reward/rotating_object: 65.9872
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 1.03s
                      Time elapsed: 00:08:16
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 487/1500 [0m                      

                       Computation: 93721 steps/s (collection: 0.932s, learning 0.117s)
             Mean action noise std: 1.98
          Mean value_function loss: 122.1402
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9303
                       Mean reward: 353.05
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 0.3755
    Episode_Reward/rotating_object: 68.5992
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 1.05s
                      Time elapsed: 00:08:17
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 488/1500 [0m                      

                       Computation: 105126 steps/s (collection: 0.811s, learning 0.124s)
             Mean action noise std: 1.98
          Mean value_function loss: 127.7095
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9280
                       Mean reward: 353.10
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.3836
    Episode_Reward/rotating_object: 68.4390
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.94s
                      Time elapsed: 00:08:18
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 489/1500 [0m                      

                       Computation: 108496 steps/s (collection: 0.815s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 118.2792
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.9283
                       Mean reward: 349.13
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 69.1816
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.91s
                      Time elapsed: 00:08:19
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 490/1500 [0m                      

                       Computation: 105595 steps/s (collection: 0.814s, learning 0.117s)
             Mean action noise std: 1.98
          Mean value_function loss: 111.3932
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.9214
                       Mean reward: 372.83
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.3879
    Episode_Reward/rotating_object: 68.0566
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.93s
                      Time elapsed: 00:08:19
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 491/1500 [0m                      

                       Computation: 103830 steps/s (collection: 0.842s, learning 0.104s)
             Mean action noise std: 1.98
          Mean value_function loss: 115.8239
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9285
                       Mean reward: 358.37
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.3753
    Episode_Reward/rotating_object: 64.9516
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.95s
                      Time elapsed: 00:08:20
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 492/1500 [0m                      

                       Computation: 106722 steps/s (collection: 0.825s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 114.9950
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9380
                       Mean reward: 301.14
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.3667
    Episode_Reward/rotating_object: 67.4432
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.92s
                      Time elapsed: 00:08:21
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 493/1500 [0m                      

                       Computation: 105724 steps/s (collection: 0.820s, learning 0.110s)
             Mean action noise std: 1.99
          Mean value_function loss: 116.5116
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.9486
                       Mean reward: 303.89
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3733
    Episode_Reward/rotating_object: 65.5945
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.93s
                      Time elapsed: 00:08:22
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 494/1500 [0m                      

                       Computation: 101319 steps/s (collection: 0.830s, learning 0.140s)
             Mean action noise std: 1.99
          Mean value_function loss: 122.6219
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9519
                       Mean reward: 357.30
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 71.1372
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.97s
                      Time elapsed: 00:08:23
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 495/1500 [0m                      

                       Computation: 101109 steps/s (collection: 0.817s, learning 0.156s)
             Mean action noise std: 1.99
          Mean value_function loss: 130.7427
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9560
                       Mean reward: 320.09
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3753
    Episode_Reward/rotating_object: 66.5020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.97s
                      Time elapsed: 00:08:24
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 496/1500 [0m                      

                       Computation: 105197 steps/s (collection: 0.820s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 102.6731
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.9598
                       Mean reward: 315.12
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.3680
    Episode_Reward/rotating_object: 65.7767
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.93s
                      Time elapsed: 00:08:25
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 497/1500 [0m                      

                       Computation: 104440 steps/s (collection: 0.795s, learning 0.146s)
             Mean action noise std: 1.99
          Mean value_function loss: 107.7203
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.9636
                       Mean reward: 349.85
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.3755
    Episode_Reward/rotating_object: 72.0454
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.94s
                      Time elapsed: 00:08:26
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 498/1500 [0m                      

                       Computation: 102322 steps/s (collection: 0.815s, learning 0.146s)
             Mean action noise std: 1.99
          Mean value_function loss: 113.5314
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9577
                       Mean reward: 330.79
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 66.7161
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.96s
                      Time elapsed: 00:08:27
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 499/1500 [0m                      

                       Computation: 105510 steps/s (collection: 0.817s, learning 0.115s)
             Mean action noise std: 1.99
          Mean value_function loss: 117.1470
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9526
                       Mean reward: 373.14
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 69.2156
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.93s
                      Time elapsed: 00:08:28
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 500/1500 [0m                      

                       Computation: 103299 steps/s (collection: 0.842s, learning 0.109s)
             Mean action noise std: 1.99
          Mean value_function loss: 134.2950
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9490
                       Mean reward: 397.01
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.3740
    Episode_Reward/rotating_object: 74.6460
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.95s
                      Time elapsed: 00:08:29
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 501/1500 [0m                      

                       Computation: 106540 steps/s (collection: 0.827s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 121.0888
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.9533
                       Mean reward: 312.53
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.3656
    Episode_Reward/rotating_object: 68.5869
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.92s
                      Time elapsed: 00:08:30
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 502/1500 [0m                      

                       Computation: 79340 steps/s (collection: 1.012s, learning 0.227s)
             Mean action noise std: 1.99
          Mean value_function loss: 119.2874
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.9573
                       Mean reward: 342.12
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.3635
    Episode_Reward/rotating_object: 68.3399
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 1.24s
                      Time elapsed: 00:08:31
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 503/1500 [0m                      

                       Computation: 102712 steps/s (collection: 0.833s, learning 0.125s)
             Mean action noise std: 1.99
          Mean value_function loss: 125.1397
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.9584
                       Mean reward: 306.31
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.3733
    Episode_Reward/rotating_object: 68.9424
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.96s
                      Time elapsed: 00:08:32
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 504/1500 [0m                      

                       Computation: 98229 steps/s (collection: 0.827s, learning 0.174s)
             Mean action noise std: 1.99
          Mean value_function loss: 128.0803
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9594
                       Mean reward: 411.21
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.3703
    Episode_Reward/rotating_object: 71.2288
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 1.00s
                      Time elapsed: 00:08:33
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 505/1500 [0m                      

                       Computation: 103075 steps/s (collection: 0.833s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 128.6718
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9595
                       Mean reward: 363.42
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 69.3011
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.95s
                      Time elapsed: 00:08:34
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 506/1500 [0m                      

                       Computation: 96487 steps/s (collection: 0.867s, learning 0.152s)
             Mean action noise std: 2.00
          Mean value_function loss: 121.2138
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9590
                       Mean reward: 331.33
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 68.8241
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 1.02s
                      Time elapsed: 00:08:35
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 507/1500 [0m                      

                       Computation: 85943 steps/s (collection: 0.941s, learning 0.203s)
             Mean action noise std: 2.00
          Mean value_function loss: 120.6401
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9603
                       Mean reward: 332.60
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 69.4755
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 1.14s
                      Time elapsed: 00:08:36
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 508/1500 [0m                      

                       Computation: 95485 steps/s (collection: 0.876s, learning 0.153s)
             Mean action noise std: 2.00
          Mean value_function loss: 126.0801
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9583
                       Mean reward: 355.46
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 0.3663
    Episode_Reward/rotating_object: 67.6696
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 1.03s
                      Time elapsed: 00:08:37
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 509/1500 [0m                      

                       Computation: 96378 steps/s (collection: 0.840s, learning 0.180s)
             Mean action noise std: 2.00
          Mean value_function loss: 131.1558
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9503
                       Mean reward: 357.19
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.3777
    Episode_Reward/rotating_object: 73.8190
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 1.02s
                      Time elapsed: 00:08:38
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 510/1500 [0m                      

                       Computation: 98515 steps/s (collection: 0.817s, learning 0.181s)
             Mean action noise std: 2.00
          Mean value_function loss: 114.4603
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.9454
                       Mean reward: 325.83
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.3650
    Episode_Reward/rotating_object: 65.7943
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.00s
                      Time elapsed: 00:08:39
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 511/1500 [0m                      

                       Computation: 108722 steps/s (collection: 0.807s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 105.4373
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9456
                       Mean reward: 344.16
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3716
    Episode_Reward/rotating_object: 69.6494
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.90s
                      Time elapsed: 00:08:40
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 512/1500 [0m                      

                       Computation: 89190 steps/s (collection: 0.952s, learning 0.150s)
             Mean action noise std: 2.00
          Mean value_function loss: 124.6081
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9447
                       Mean reward: 319.65
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 69.1677
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 1.10s
                      Time elapsed: 00:08:41
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 513/1500 [0m                      

                       Computation: 103031 steps/s (collection: 0.863s, learning 0.092s)
             Mean action noise std: 2.00
          Mean value_function loss: 121.0016
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.9453
                       Mean reward: 318.17
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.3706
    Episode_Reward/rotating_object: 65.2109
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.95s
                      Time elapsed: 00:08:42
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 514/1500 [0m                      

                       Computation: 103281 steps/s (collection: 0.825s, learning 0.126s)
             Mean action noise std: 2.00
          Mean value_function loss: 112.7364
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.9500
                       Mean reward: 355.77
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 69.3802
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.95s
                      Time elapsed: 00:08:43
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 515/1500 [0m                      

                       Computation: 96678 steps/s (collection: 0.825s, learning 0.192s)
             Mean action noise std: 2.00
          Mean value_function loss: 124.6996
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9442
                       Mean reward: 332.08
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3631
    Episode_Reward/rotating_object: 65.3695
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 1.02s
                      Time elapsed: 00:08:44
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 516/1500 [0m                      

                       Computation: 107954 steps/s (collection: 0.790s, learning 0.121s)
             Mean action noise std: 2.00
          Mean value_function loss: 114.7347
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9410
                       Mean reward: 350.33
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.3657
    Episode_Reward/rotating_object: 68.5691
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.91s
                      Time elapsed: 00:08:45
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 517/1500 [0m                      

                       Computation: 94070 steps/s (collection: 0.929s, learning 0.116s)
             Mean action noise std: 2.00
          Mean value_function loss: 108.5717
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9426
                       Mean reward: 407.38
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.3672
    Episode_Reward/rotating_object: 74.0352
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 1.05s
                      Time elapsed: 00:08:46
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 518/1500 [0m                      

                       Computation: 102262 steps/s (collection: 0.822s, learning 0.140s)
             Mean action noise std: 2.00
          Mean value_function loss: 112.2256
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9425
                       Mean reward: 408.51
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.3775
    Episode_Reward/rotating_object: 73.1226
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.96s
                      Time elapsed: 00:08:47
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 519/1500 [0m                      

                       Computation: 103522 steps/s (collection: 0.811s, learning 0.139s)
             Mean action noise std: 2.00
          Mean value_function loss: 111.4312
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9492
                       Mean reward: 357.97
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.3636
    Episode_Reward/rotating_object: 70.9271
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.95s
                      Time elapsed: 00:08:48
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 520/1500 [0m                      

                       Computation: 99092 steps/s (collection: 0.863s, learning 0.129s)
             Mean action noise std: 2.00
          Mean value_function loss: 106.8764
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.9539
                       Mean reward: 361.65
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.3676
    Episode_Reward/rotating_object: 70.2102
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.99s
                      Time elapsed: 00:08:49
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 521/1500 [0m                      

                       Computation: 105526 steps/s (collection: 0.825s, learning 0.107s)
             Mean action noise std: 2.00
          Mean value_function loss: 106.6628
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.9550
                       Mean reward: 365.13
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.3743
    Episode_Reward/rotating_object: 70.9544
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.93s
                      Time elapsed: 00:08:50
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 522/1500 [0m                      

                       Computation: 106487 steps/s (collection: 0.822s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 118.0852
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9523
                       Mean reward: 395.07
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.3805
    Episode_Reward/rotating_object: 77.3904
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.92s
                      Time elapsed: 00:08:51
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 523/1500 [0m                      

                       Computation: 107338 steps/s (collection: 0.815s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 117.2344
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.9585
                       Mean reward: 362.87
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.3733
    Episode_Reward/rotating_object: 73.3981
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.92s
                      Time elapsed: 00:08:52
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 524/1500 [0m                      

                       Computation: 106814 steps/s (collection: 0.826s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 108.8426
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9628
                       Mean reward: 403.54
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3737
    Episode_Reward/rotating_object: 74.3488
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.92s
                      Time elapsed: 00:08:53
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 525/1500 [0m                      

                       Computation: 110776 steps/s (collection: 0.799s, learning 0.089s)
             Mean action noise std: 2.01
          Mean value_function loss: 114.4551
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.9694
                       Mean reward: 412.23
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.3722
    Episode_Reward/rotating_object: 75.8373
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.89s
                      Time elapsed: 00:08:54
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 526/1500 [0m                      

                       Computation: 104970 steps/s (collection: 0.814s, learning 0.123s)
             Mean action noise std: 2.01
          Mean value_function loss: 106.2795
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9657
                       Mean reward: 363.47
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.3713
    Episode_Reward/rotating_object: 74.7854
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.94s
                      Time elapsed: 00:08:55
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 527/1500 [0m                      

                       Computation: 104065 steps/s (collection: 0.820s, learning 0.125s)
             Mean action noise std: 2.01
          Mean value_function loss: 108.7452
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9551
                       Mean reward: 356.26
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3769
    Episode_Reward/rotating_object: 75.5881
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.94s
                      Time elapsed: 00:08:55
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 528/1500 [0m                      

                       Computation: 106206 steps/s (collection: 0.831s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 106.0253
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.9555
                       Mean reward: 364.34
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 74.0926
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.93s
                      Time elapsed: 00:08:56
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 529/1500 [0m                      

                       Computation: 108324 steps/s (collection: 0.804s, learning 0.103s)
             Mean action noise std: 2.01
          Mean value_function loss: 106.7343
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.9586
                       Mean reward: 391.08
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.3697
    Episode_Reward/rotating_object: 74.9357
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.91s
                      Time elapsed: 00:08:57
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 530/1500 [0m                      

                       Computation: 105432 steps/s (collection: 0.828s, learning 0.105s)
             Mean action noise std: 2.01
          Mean value_function loss: 108.8062
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9627
                       Mean reward: 372.98
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 0.3715
    Episode_Reward/rotating_object: 74.7862
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.93s
                      Time elapsed: 00:08:58
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 531/1500 [0m                      

                       Computation: 96051 steps/s (collection: 0.846s, learning 0.178s)
             Mean action noise std: 2.01
          Mean value_function loss: 101.1595
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9661
                       Mean reward: 362.82
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 78.1896
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 1.02s
                      Time elapsed: 00:08:59
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 532/1500 [0m                      

                       Computation: 96447 steps/s (collection: 0.872s, learning 0.148s)
             Mean action noise std: 2.01
          Mean value_function loss: 99.8597
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9701
                       Mean reward: 405.31
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.3695
    Episode_Reward/rotating_object: 79.6616
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 1.02s
                      Time elapsed: 00:09:00
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 533/1500 [0m                      

                       Computation: 96995 steps/s (collection: 0.875s, learning 0.139s)
             Mean action noise std: 2.01
          Mean value_function loss: 99.9187
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.9728
                       Mean reward: 344.01
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 74.3272
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 1.01s
                      Time elapsed: 00:09:01
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 534/1500 [0m                      

                       Computation: 106128 steps/s (collection: 0.821s, learning 0.105s)
             Mean action noise std: 2.02
          Mean value_function loss: 105.5197
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9794
                       Mean reward: 402.71
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3686
    Episode_Reward/rotating_object: 74.8019
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.93s
                      Time elapsed: 00:09:02
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 535/1500 [0m                      

                       Computation: 97641 steps/s (collection: 0.826s, learning 0.181s)
             Mean action noise std: 2.02
          Mean value_function loss: 111.9762
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9896
                       Mean reward: 402.97
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.3739
    Episode_Reward/rotating_object: 73.2549
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 1.01s
                      Time elapsed: 00:09:03
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 536/1500 [0m                      

                       Computation: 102079 steps/s (collection: 0.811s, learning 0.152s)
             Mean action noise std: 2.02
          Mean value_function loss: 119.7465
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9961
                       Mean reward: 324.91
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3673
    Episode_Reward/rotating_object: 73.9673
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.96s
                      Time elapsed: 00:09:04
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 537/1500 [0m                      

                       Computation: 103240 steps/s (collection: 0.810s, learning 0.143s)
             Mean action noise std: 2.02
          Mean value_function loss: 118.5758
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.9999
                       Mean reward: 364.23
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 71.2715
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.95s
                      Time elapsed: 00:09:05
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 538/1500 [0m                      

                       Computation: 91179 steps/s (collection: 0.934s, learning 0.145s)
             Mean action noise std: 2.02
          Mean value_function loss: 112.6966
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0025
                       Mean reward: 384.08
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.3720
    Episode_Reward/rotating_object: 76.5528
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 1.08s
                      Time elapsed: 00:09:06
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 539/1500 [0m                      

                       Computation: 98124 steps/s (collection: 0.824s, learning 0.178s)
             Mean action noise std: 2.03
          Mean value_function loss: 113.3514
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.0074
                       Mean reward: 360.05
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.3643
    Episode_Reward/rotating_object: 73.6846
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 1.00s
                      Time elapsed: 00:09:07
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 540/1500 [0m                      

                       Computation: 104360 steps/s (collection: 0.821s, learning 0.121s)
             Mean action noise std: 2.03
          Mean value_function loss: 111.9686
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0202
                       Mean reward: 370.22
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 71.8318
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.94s
                      Time elapsed: 00:09:08
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 541/1500 [0m                      

                       Computation: 101914 steps/s (collection: 0.831s, learning 0.134s)
             Mean action noise std: 2.03
          Mean value_function loss: 107.6246
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0271
                       Mean reward: 417.18
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.3735
    Episode_Reward/rotating_object: 75.4221
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.96s
                      Time elapsed: 00:09:09
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 542/1500 [0m                      

                       Computation: 99782 steps/s (collection: 0.810s, learning 0.175s)
             Mean action noise std: 2.03
          Mean value_function loss: 114.5199
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0237
                       Mean reward: 358.13
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.3696
    Episode_Reward/rotating_object: 74.4922
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.99s
                      Time elapsed: 00:09:10
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 543/1500 [0m                      

                       Computation: 91154 steps/s (collection: 0.925s, learning 0.153s)
             Mean action noise std: 2.03
          Mean value_function loss: 110.1994
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0256
                       Mean reward: 341.74
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.3741
    Episode_Reward/rotating_object: 72.3353
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 1.08s
                      Time elapsed: 00:09:11
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 544/1500 [0m                      

                       Computation: 98072 steps/s (collection: 0.816s, learning 0.186s)
             Mean action noise std: 2.04
          Mean value_function loss: 121.8472
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0269
                       Mean reward: 404.50
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.3755
    Episode_Reward/rotating_object: 75.1826
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 1.00s
                      Time elapsed: 00:09:12
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 545/1500 [0m                      

                       Computation: 103396 steps/s (collection: 0.824s, learning 0.127s)
             Mean action noise std: 2.04
          Mean value_function loss: 120.4034
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0374
                       Mean reward: 428.59
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.3814
    Episode_Reward/rotating_object: 78.9111
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.95s
                      Time elapsed: 00:09:13
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 546/1500 [0m                      

                       Computation: 93601 steps/s (collection: 0.869s, learning 0.181s)
             Mean action noise std: 2.04
          Mean value_function loss: 109.8233
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0343
                       Mean reward: 360.54
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 0.3807
    Episode_Reward/rotating_object: 76.9463
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 1.05s
                      Time elapsed: 00:09:14
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 547/1500 [0m                      

                       Computation: 92093 steps/s (collection: 0.888s, learning 0.179s)
             Mean action noise std: 2.04
          Mean value_function loss: 125.6878
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.0352
                       Mean reward: 454.30
               Mean episode length: 239.74
    Episode_Reward/reaching_object: 0.3771
    Episode_Reward/rotating_object: 78.9591
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 1.07s
                      Time elapsed: 00:09:15
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 548/1500 [0m                      

                       Computation: 94187 steps/s (collection: 0.938s, learning 0.106s)
             Mean action noise std: 2.04
          Mean value_function loss: 120.5888
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0349
                       Mean reward: 381.06
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 0.3878
    Episode_Reward/rotating_object: 76.6883
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 1.04s
                      Time elapsed: 00:09:16
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 549/1500 [0m                      

                       Computation: 105087 steps/s (collection: 0.833s, learning 0.102s)
             Mean action noise std: 2.04
          Mean value_function loss: 112.0527
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0344
                       Mean reward: 371.24
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.3899
    Episode_Reward/rotating_object: 78.5879
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.94s
                      Time elapsed: 00:09:17
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 550/1500 [0m                      

                       Computation: 105946 steps/s (collection: 0.807s, learning 0.121s)
             Mean action noise std: 2.04
          Mean value_function loss: 119.3766
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.0344
                       Mean reward: 425.77
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.3877
    Episode_Reward/rotating_object: 78.9924
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.93s
                      Time elapsed: 00:09:18
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 551/1500 [0m                      

                       Computation: 98600 steps/s (collection: 0.815s, learning 0.182s)
             Mean action noise std: 2.04
          Mean value_function loss: 124.0774
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0365
                       Mean reward: 382.56
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.3828
    Episode_Reward/rotating_object: 77.1092
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 1.00s
                      Time elapsed: 00:09:19
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 552/1500 [0m                      

                       Computation: 109802 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 2.04
          Mean value_function loss: 105.3940
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0355
                       Mean reward: 431.95
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.3771
    Episode_Reward/rotating_object: 77.4771
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.90s
                      Time elapsed: 00:09:20
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 553/1500 [0m                      

                       Computation: 106887 steps/s (collection: 0.808s, learning 0.112s)
             Mean action noise std: 2.04
          Mean value_function loss: 120.8140
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0408
                       Mean reward: 403.82
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.3844
    Episode_Reward/rotating_object: 80.5857
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.92s
                      Time elapsed: 00:09:21
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 554/1500 [0m                      

                       Computation: 109495 steps/s (collection: 0.787s, learning 0.111s)
             Mean action noise std: 2.05
          Mean value_function loss: 119.3897
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0478
                       Mean reward: 351.64
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 0.3815
    Episode_Reward/rotating_object: 75.7574
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.90s
                      Time elapsed: 00:09:22
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 555/1500 [0m                      

                       Computation: 106310 steps/s (collection: 0.827s, learning 0.098s)
             Mean action noise std: 2.05
          Mean value_function loss: 122.7638
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.0548
                       Mean reward: 361.80
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 75.7276
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.92s
                      Time elapsed: 00:09:23
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 556/1500 [0m                      

                       Computation: 102777 steps/s (collection: 0.832s, learning 0.124s)
             Mean action noise std: 2.05
          Mean value_function loss: 120.5653
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.0549
                       Mean reward: 390.86
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3819
    Episode_Reward/rotating_object: 75.6499
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.96s
                      Time elapsed: 00:09:24
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 557/1500 [0m                      

                       Computation: 102045 steps/s (collection: 0.814s, learning 0.149s)
             Mean action noise std: 2.05
          Mean value_function loss: 125.8872
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0539
                       Mean reward: 389.20
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 0.3795
    Episode_Reward/rotating_object: 79.3521
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.96s
                      Time elapsed: 00:09:25
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 558/1500 [0m                      

                       Computation: 106140 steps/s (collection: 0.830s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 121.9064
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.0537
                       Mean reward: 418.39
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.3815
    Episode_Reward/rotating_object: 78.7024
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.93s
                      Time elapsed: 00:09:26
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 559/1500 [0m                      

                       Computation: 102139 steps/s (collection: 0.850s, learning 0.112s)
             Mean action noise std: 2.05
          Mean value_function loss: 133.0217
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0562
                       Mean reward: 370.72
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3776
    Episode_Reward/rotating_object: 77.4717
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.96s
                      Time elapsed: 00:09:27
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 560/1500 [0m                      

                       Computation: 105130 steps/s (collection: 0.821s, learning 0.114s)
             Mean action noise std: 2.05
          Mean value_function loss: 125.4884
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.0542
                       Mean reward: 396.04
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.3780
    Episode_Reward/rotating_object: 78.1645
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.94s
                      Time elapsed: 00:09:28
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 561/1500 [0m                      

                       Computation: 96744 steps/s (collection: 0.873s, learning 0.143s)
             Mean action noise std: 2.05
          Mean value_function loss: 124.0854
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.0533
                       Mean reward: 393.50
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.3801
    Episode_Reward/rotating_object: 78.5001
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 1.02s
                      Time elapsed: 00:09:29
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 562/1500 [0m                      

                       Computation: 99271 steps/s (collection: 0.889s, learning 0.101s)
             Mean action noise std: 2.05
          Mean value_function loss: 126.5207
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0481
                       Mean reward: 348.74
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 74.7492
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.99s
                      Time elapsed: 00:09:30
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 563/1500 [0m                      

                       Computation: 105644 steps/s (collection: 0.832s, learning 0.098s)
             Mean action noise std: 2.05
          Mean value_function loss: 117.6481
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0431
                       Mean reward: 366.98
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.3741
    Episode_Reward/rotating_object: 75.7481
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.93s
                      Time elapsed: 00:09:30
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 564/1500 [0m                      

                       Computation: 98460 steps/s (collection: 0.844s, learning 0.154s)
             Mean action noise std: 2.05
          Mean value_function loss: 125.8859
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0397
                       Mean reward: 363.10
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.3739
    Episode_Reward/rotating_object: 75.5469
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.00s
                      Time elapsed: 00:09:31
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 565/1500 [0m                      

                       Computation: 102801 steps/s (collection: 0.839s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 131.1389
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0425
                       Mean reward: 429.73
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 82.2631
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.96s
                      Time elapsed: 00:09:32
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 566/1500 [0m                      

                       Computation: 103960 steps/s (collection: 0.841s, learning 0.105s)
             Mean action noise std: 2.06
          Mean value_function loss: 132.9986
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0423
                       Mean reward: 400.96
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.3842
    Episode_Reward/rotating_object: 78.2047
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.95s
                      Time elapsed: 00:09:33
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 567/1500 [0m                      

                       Computation: 100158 steps/s (collection: 0.851s, learning 0.131s)
             Mean action noise std: 2.06
          Mean value_function loss: 127.5786
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0276
                       Mean reward: 396.67
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3808
    Episode_Reward/rotating_object: 80.6856
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.98s
                      Time elapsed: 00:09:34
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 568/1500 [0m                      

                       Computation: 98761 steps/s (collection: 0.845s, learning 0.151s)
             Mean action noise std: 2.05
          Mean value_function loss: 136.1107
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0159
                       Mean reward: 418.14
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.3930
    Episode_Reward/rotating_object: 84.0120
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 1.00s
                      Time elapsed: 00:09:35
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 569/1500 [0m                      

                       Computation: 98071 steps/s (collection: 0.885s, learning 0.118s)
             Mean action noise std: 2.06
          Mean value_function loss: 122.8891
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.0135
                       Mean reward: 388.94
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.3859
    Episode_Reward/rotating_object: 79.6672
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 1.00s
                      Time elapsed: 00:09:36
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 570/1500 [0m                      

                       Computation: 106490 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 125.0739
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0173
                       Mean reward: 405.39
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 80.0411
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.92s
                      Time elapsed: 00:09:37
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 571/1500 [0m                      

                       Computation: 97282 steps/s (collection: 0.854s, learning 0.157s)
             Mean action noise std: 2.06
          Mean value_function loss: 127.6049
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.0232
                       Mean reward: 390.68
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.3844
    Episode_Reward/rotating_object: 79.1735
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 1.01s
                      Time elapsed: 00:09:38
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 572/1500 [0m                      

                       Computation: 91913 steps/s (collection: 0.945s, learning 0.124s)
             Mean action noise std: 2.06
          Mean value_function loss: 127.8687
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.0223
                       Mean reward: 392.19
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.3893
    Episode_Reward/rotating_object: 80.7246
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 1.07s
                      Time elapsed: 00:09:39
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 573/1500 [0m                      

                       Computation: 101563 steps/s (collection: 0.849s, learning 0.119s)
             Mean action noise std: 2.06
          Mean value_function loss: 113.4804
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0192
                       Mean reward: 391.19
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.3862
    Episode_Reward/rotating_object: 80.9282
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.97s
                      Time elapsed: 00:09:40
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 574/1500 [0m                      

                       Computation: 101256 steps/s (collection: 0.856s, learning 0.115s)
             Mean action noise std: 2.06
          Mean value_function loss: 112.4536
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0244
                       Mean reward: 406.56
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.3844
    Episode_Reward/rotating_object: 82.5377
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.97s
                      Time elapsed: 00:09:41
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 575/1500 [0m                      

                       Computation: 104512 steps/s (collection: 0.838s, learning 0.103s)
             Mean action noise std: 2.06
          Mean value_function loss: 117.0446
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0235
                       Mean reward: 391.70
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.3873
    Episode_Reward/rotating_object: 78.4763
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.94s
                      Time elapsed: 00:09:42
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 576/1500 [0m                      

                       Computation: 106340 steps/s (collection: 0.833s, learning 0.091s)
             Mean action noise std: 2.06
          Mean value_function loss: 118.1090
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0254
                       Mean reward: 409.32
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 79.5381
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.92s
                      Time elapsed: 00:09:43
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 577/1500 [0m                      

                       Computation: 89027 steps/s (collection: 0.966s, learning 0.138s)
             Mean action noise std: 2.07
          Mean value_function loss: 110.3967
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0293
                       Mean reward: 426.88
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.3835
    Episode_Reward/rotating_object: 81.1956
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 1.10s
                      Time elapsed: 00:09:44
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 578/1500 [0m                      

                       Computation: 98708 steps/s (collection: 0.852s, learning 0.144s)
             Mean action noise std: 2.07
          Mean value_function loss: 113.8146
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0383
                       Mean reward: 452.91
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.3841
    Episode_Reward/rotating_object: 86.0037
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.00s
                      Time elapsed: 00:09:45
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 579/1500 [0m                      

                       Computation: 109624 steps/s (collection: 0.793s, learning 0.104s)
             Mean action noise std: 2.07
          Mean value_function loss: 99.7822
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.0368
                       Mean reward: 451.54
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.3806
    Episode_Reward/rotating_object: 83.0690
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.90s
                      Time elapsed: 00:09:46
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 580/1500 [0m                      

                       Computation: 100347 steps/s (collection: 0.838s, learning 0.142s)
             Mean action noise std: 2.07
          Mean value_function loss: 101.9396
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0338
                       Mean reward: 389.97
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.3741
    Episode_Reward/rotating_object: 79.6942
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.98s
                      Time elapsed: 00:09:47
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 581/1500 [0m                      

                       Computation: 104263 steps/s (collection: 0.808s, learning 0.135s)
             Mean action noise std: 2.07
          Mean value_function loss: 114.3591
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.0319
                       Mean reward: 386.55
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 80.2114
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.94s
                      Time elapsed: 00:09:48
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 582/1500 [0m                      

                       Computation: 107503 steps/s (collection: 0.797s, learning 0.118s)
             Mean action noise std: 2.07
          Mean value_function loss: 116.2406
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.0323
                       Mean reward: 397.32
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.3708
    Episode_Reward/rotating_object: 80.9457
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.91s
                      Time elapsed: 00:09:49
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 583/1500 [0m                      

                       Computation: 101085 steps/s (collection: 0.836s, learning 0.137s)
             Mean action noise std: 2.07
          Mean value_function loss: 102.6009
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 16.0253
                       Mean reward: 370.19
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 82.6246
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.97s
                      Time elapsed: 00:09:50
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 584/1500 [0m                      

                       Computation: 99463 steps/s (collection: 0.856s, learning 0.132s)
             Mean action noise std: 2.07
          Mean value_function loss: 107.5918
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0265
                       Mean reward: 409.85
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 78.6508
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.99s
                      Time elapsed: 00:09:51
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 585/1500 [0m                      

                       Computation: 105352 steps/s (collection: 0.810s, learning 0.123s)
             Mean action noise std: 2.07
          Mean value_function loss: 99.0442
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0299
                       Mean reward: 396.09
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3666
    Episode_Reward/rotating_object: 80.0514
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.93s
                      Time elapsed: 00:09:52
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 586/1500 [0m                      

                       Computation: 107154 steps/s (collection: 0.805s, learning 0.113s)
             Mean action noise std: 2.08
          Mean value_function loss: 109.3348
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0311
                       Mean reward: 394.26
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.3701
    Episode_Reward/rotating_object: 84.1976
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.92s
                      Time elapsed: 00:09:53
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 587/1500 [0m                      

                       Computation: 92612 steps/s (collection: 0.900s, learning 0.162s)
             Mean action noise std: 2.08
          Mean value_function loss: 105.8271
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0371
                       Mean reward: 449.47
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.3743
    Episode_Reward/rotating_object: 86.9172
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 1.06s
                      Time elapsed: 00:09:54
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 588/1500 [0m                      

                       Computation: 99165 steps/s (collection: 0.871s, learning 0.120s)
             Mean action noise std: 2.08
          Mean value_function loss: 117.1906
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.0515
                       Mean reward: 451.87
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.3771
    Episode_Reward/rotating_object: 87.2423
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.99s
                      Time elapsed: 00:09:55
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 589/1500 [0m                      

                       Computation: 95377 steps/s (collection: 0.859s, learning 0.172s)
             Mean action noise std: 2.08
          Mean value_function loss: 121.6147
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.0550
                       Mean reward: 427.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3760
    Episode_Reward/rotating_object: 85.6891
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 1.03s
                      Time elapsed: 00:09:56
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 590/1500 [0m                      

                       Computation: 101082 steps/s (collection: 0.800s, learning 0.172s)
             Mean action noise std: 2.08
          Mean value_function loss: 114.7271
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.0478
                       Mean reward: 414.12
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.3768
    Episode_Reward/rotating_object: 87.7468
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.97s
                      Time elapsed: 00:09:57
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 591/1500 [0m                      

                       Computation: 100045 steps/s (collection: 0.866s, learning 0.117s)
             Mean action noise std: 2.08
          Mean value_function loss: 113.4441
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0433
                       Mean reward: 412.74
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.3854
    Episode_Reward/rotating_object: 86.2544
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.98s
                      Time elapsed: 00:09:58
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 592/1500 [0m                      

                       Computation: 90442 steps/s (collection: 0.966s, learning 0.121s)
             Mean action noise std: 2.08
          Mean value_function loss: 122.6535
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.0406
                       Mean reward: 466.59
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.3831
    Episode_Reward/rotating_object: 88.6134
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 1.09s
                      Time elapsed: 00:09:59
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 593/1500 [0m                      

                       Computation: 89244 steps/s (collection: 0.956s, learning 0.146s)
             Mean action noise std: 2.08
          Mean value_function loss: 131.4287
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0348
                       Mean reward: 441.01
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 84.1969
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 1.10s
                      Time elapsed: 00:10:00
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 594/1500 [0m                      

                       Computation: 104851 steps/s (collection: 0.829s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 126.3128
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0274
                       Mean reward: 382.24
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3856
    Episode_Reward/rotating_object: 85.0887
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.94s
                      Time elapsed: 00:10:01
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 595/1500 [0m                      

                       Computation: 104596 steps/s (collection: 0.806s, learning 0.134s)
             Mean action noise std: 2.08
          Mean value_function loss: 133.7216
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0215
                       Mean reward: 465.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3971
    Episode_Reward/rotating_object: 84.7448
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.94s
                      Time elapsed: 00:10:02
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 596/1500 [0m                      

                       Computation: 103566 steps/s (collection: 0.815s, learning 0.135s)
             Mean action noise std: 2.08
          Mean value_function loss: 137.8505
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0217
                       Mean reward: 436.22
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.3886
    Episode_Reward/rotating_object: 86.3286
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.95s
                      Time elapsed: 00:10:03
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 597/1500 [0m                      

                       Computation: 95629 steps/s (collection: 0.877s, learning 0.151s)
             Mean action noise std: 2.08
          Mean value_function loss: 143.3519
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0166
                       Mean reward: 396.74
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.3867
    Episode_Reward/rotating_object: 79.8531
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 1.03s
                      Time elapsed: 00:10:04
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 598/1500 [0m                      

                       Computation: 105239 steps/s (collection: 0.844s, learning 0.091s)
             Mean action noise std: 2.08
          Mean value_function loss: 141.5475
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0074
                       Mean reward: 408.68
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.3862
    Episode_Reward/rotating_object: 79.3521
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.93s
                      Time elapsed: 00:10:05
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 599/1500 [0m                      

                       Computation: 109065 steps/s (collection: 0.797s, learning 0.104s)
             Mean action noise std: 2.09
          Mean value_function loss: 154.8344
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0097
                       Mean reward: 399.23
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.3806
    Episode_Reward/rotating_object: 79.6351
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.90s
                      Time elapsed: 00:10:06
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 600/1500 [0m                      

                       Computation: 101713 steps/s (collection: 0.849s, learning 0.118s)
             Mean action noise std: 2.09
          Mean value_function loss: 145.4761
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0106
                       Mean reward: 415.63
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 82.6007
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.97s
                      Time elapsed: 00:10:07
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 601/1500 [0m                      

                       Computation: 107439 steps/s (collection: 0.806s, learning 0.109s)
             Mean action noise std: 2.09
          Mean value_function loss: 135.5143
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0128
                       Mean reward: 468.58
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.4035
    Episode_Reward/rotating_object: 89.3171
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.91s
                      Time elapsed: 00:10:08
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 602/1500 [0m                      

                       Computation: 108040 steps/s (collection: 0.816s, learning 0.094s)
             Mean action noise std: 2.09
          Mean value_function loss: 148.2586
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0163
                       Mean reward: 439.51
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 84.4782
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.91s
                      Time elapsed: 00:10:09
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 603/1500 [0m                      

                       Computation: 105812 steps/s (collection: 0.824s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 125.3672
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0197
                       Mean reward: 424.23
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.4039
    Episode_Reward/rotating_object: 84.3913
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.93s
                      Time elapsed: 00:10:09
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 604/1500 [0m                      

                       Computation: 106564 steps/s (collection: 0.827s, learning 0.096s)
             Mean action noise std: 2.09
          Mean value_function loss: 121.8541
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0277
                       Mean reward: 421.47
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 0.3972
    Episode_Reward/rotating_object: 87.8125
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.92s
                      Time elapsed: 00:10:10
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 605/1500 [0m                      

                       Computation: 103870 steps/s (collection: 0.846s, learning 0.100s)
             Mean action noise std: 2.09
          Mean value_function loss: 130.5591
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.0331
                       Mean reward: 501.13
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.4047
    Episode_Reward/rotating_object: 91.8409
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.95s
                      Time elapsed: 00:10:11
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 606/1500 [0m                      

                       Computation: 109578 steps/s (collection: 0.796s, learning 0.102s)
             Mean action noise std: 2.10
          Mean value_function loss: 128.3985
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0410
                       Mean reward: 398.81
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 83.2111
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.90s
                      Time elapsed: 00:10:12
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 607/1500 [0m                      

                       Computation: 104369 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 2.10
          Mean value_function loss: 130.5359
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0474
                       Mean reward: 375.87
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 83.5311
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.94s
                      Time elapsed: 00:10:13
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 608/1500 [0m                      

                       Computation: 108440 steps/s (collection: 0.813s, learning 0.094s)
             Mean action noise std: 2.10
          Mean value_function loss: 131.8513
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.0537
                       Mean reward: 442.73
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 85.0844
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.91s
                      Time elapsed: 00:10:14
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 609/1500 [0m                      

                       Computation: 99693 steps/s (collection: 0.814s, learning 0.173s)
             Mean action noise std: 2.10
          Mean value_function loss: 129.2276
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0476
                       Mean reward: 471.28
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.3930
    Episode_Reward/rotating_object: 83.2405
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.99s
                      Time elapsed: 00:10:15
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 610/1500 [0m                      

                       Computation: 104530 steps/s (collection: 0.820s, learning 0.121s)
             Mean action noise std: 2.10
          Mean value_function loss: 131.6265
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0525
                       Mean reward: 417.15
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 84.5109
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.94s
                      Time elapsed: 00:10:16
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 611/1500 [0m                      

                       Computation: 101108 steps/s (collection: 0.863s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 120.1949
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0489
                       Mean reward: 417.86
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.3940
    Episode_Reward/rotating_object: 85.8947
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.97s
                      Time elapsed: 00:10:17
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 612/1500 [0m                      

                       Computation: 102374 steps/s (collection: 0.809s, learning 0.151s)
             Mean action noise std: 2.10
          Mean value_function loss: 134.4918
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0486
                       Mean reward: 437.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 86.7772
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.96s
                      Time elapsed: 00:10:18
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 613/1500 [0m                      

                       Computation: 95486 steps/s (collection: 0.895s, learning 0.135s)
             Mean action noise std: 2.10
          Mean value_function loss: 132.8054
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0459
                       Mean reward: 429.05
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.3814
    Episode_Reward/rotating_object: 83.2921
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 1.03s
                      Time elapsed: 00:10:19
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 614/1500 [0m                      

                       Computation: 101226 steps/s (collection: 0.813s, learning 0.158s)
             Mean action noise std: 2.10
          Mean value_function loss: 138.4578
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0433
                       Mean reward: 441.16
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 0.3971
    Episode_Reward/rotating_object: 89.2423
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.97s
                      Time elapsed: 00:10:20
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 615/1500 [0m                      

                       Computation: 102705 steps/s (collection: 0.814s, learning 0.143s)
             Mean action noise std: 2.10
          Mean value_function loss: 123.8915
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0389
                       Mean reward: 472.66
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.3851
    Episode_Reward/rotating_object: 85.5409
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.96s
                      Time elapsed: 00:10:21
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 616/1500 [0m                      

                       Computation: 109641 steps/s (collection: 0.798s, learning 0.099s)
             Mean action noise std: 2.10
          Mean value_function loss: 128.3617
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0386
                       Mean reward: 437.64
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.3966
    Episode_Reward/rotating_object: 86.5673
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.90s
                      Time elapsed: 00:10:22
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 617/1500 [0m                      

                       Computation: 107575 steps/s (collection: 0.788s, learning 0.126s)
             Mean action noise std: 2.11
          Mean value_function loss: 123.7414
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0417
                       Mean reward: 409.99
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.3983
    Episode_Reward/rotating_object: 86.6160
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.91s
                      Time elapsed: 00:10:23
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 618/1500 [0m                      

                       Computation: 107722 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 122.1715
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0436
                       Mean reward: 463.35
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.3931
    Episode_Reward/rotating_object: 84.8514
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.91s
                      Time elapsed: 00:10:24
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 619/1500 [0m                      

                       Computation: 88238 steps/s (collection: 0.930s, learning 0.184s)
             Mean action noise std: 2.11
          Mean value_function loss: 133.0087
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0407
                       Mean reward: 446.35
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3904
    Episode_Reward/rotating_object: 88.2016
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 1.11s
                      Time elapsed: 00:10:25
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 620/1500 [0m                      

                       Computation: 86369 steps/s (collection: 0.980s, learning 0.159s)
             Mean action noise std: 2.11
          Mean value_function loss: 120.5291
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0479
                       Mean reward: 413.65
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.3899
    Episode_Reward/rotating_object: 85.4075
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 1.14s
                      Time elapsed: 00:10:26
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 621/1500 [0m                      

                       Computation: 88673 steps/s (collection: 0.895s, learning 0.214s)
             Mean action noise std: 2.11
          Mean value_function loss: 126.7870
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0431
                       Mean reward: 485.91
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 89.7279
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 1.11s
                      Time elapsed: 00:10:27
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 622/1500 [0m                      

                       Computation: 92226 steps/s (collection: 0.895s, learning 0.171s)
             Mean action noise std: 2.11
          Mean value_function loss: 123.7692
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0370
                       Mean reward: 425.41
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3928
    Episode_Reward/rotating_object: 86.0415
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 1.07s
                      Time elapsed: 00:10:28
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 623/1500 [0m                      

                       Computation: 89272 steps/s (collection: 0.912s, learning 0.189s)
             Mean action noise std: 2.11
          Mean value_function loss: 127.2954
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0389
                       Mean reward: 438.21
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.3931
    Episode_Reward/rotating_object: 88.8706
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 1.10s
                      Time elapsed: 00:10:29
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 624/1500 [0m                      

                       Computation: 94383 steps/s (collection: 0.895s, learning 0.146s)
             Mean action noise std: 2.11
          Mean value_function loss: 124.0467
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.0481
                       Mean reward: 500.56
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.3966
    Episode_Reward/rotating_object: 91.3763
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 1.04s
                      Time elapsed: 00:10:30
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 625/1500 [0m                      

                       Computation: 91124 steps/s (collection: 0.952s, learning 0.127s)
             Mean action noise std: 2.11
          Mean value_function loss: 119.7962
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0466
                       Mean reward: 432.80
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 87.5929
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 1.08s
                      Time elapsed: 00:10:31
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 626/1500 [0m                      

                       Computation: 99637 steps/s (collection: 0.882s, learning 0.104s)
             Mean action noise std: 2.11
          Mean value_function loss: 107.0035
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0459
                       Mean reward: 483.56
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 91.4250
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.99s
                      Time elapsed: 00:10:32
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 627/1500 [0m                      

                       Computation: 100995 steps/s (collection: 0.874s, learning 0.100s)
             Mean action noise std: 2.11
          Mean value_function loss: 120.9979
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.0526
                       Mean reward: 432.48
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.3932
    Episode_Reward/rotating_object: 92.3894
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.97s
                      Time elapsed: 00:10:33
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 628/1500 [0m                      

                       Computation: 102303 steps/s (collection: 0.850s, learning 0.111s)
             Mean action noise std: 2.12
          Mean value_function loss: 111.1806
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0532
                       Mean reward: 486.33
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 92.9551
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.96s
                      Time elapsed: 00:10:34
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 629/1500 [0m                      

                       Computation: 88285 steps/s (collection: 0.923s, learning 0.190s)
             Mean action noise std: 2.12
          Mean value_function loss: 113.2502
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0536
                       Mean reward: 435.33
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.3926
    Episode_Reward/rotating_object: 88.7297
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 1.11s
                      Time elapsed: 00:10:35
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 630/1500 [0m                      

                       Computation: 103456 steps/s (collection: 0.835s, learning 0.115s)
             Mean action noise std: 2.12
          Mean value_function loss: 114.4207
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0563
                       Mean reward: 436.18
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 91.5633
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.95s
                      Time elapsed: 00:10:36
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 631/1500 [0m                      

                       Computation: 95998 steps/s (collection: 0.877s, learning 0.147s)
             Mean action noise std: 2.12
          Mean value_function loss: 106.5135
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0574
                       Mean reward: 465.30
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.3851
    Episode_Reward/rotating_object: 88.8844
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 1.02s
                      Time elapsed: 00:10:37
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 632/1500 [0m                      

                       Computation: 99856 steps/s (collection: 0.868s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 113.9568
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0646
                       Mean reward: 490.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3875
    Episode_Reward/rotating_object: 94.7654
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.98s
                      Time elapsed: 00:10:38
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 633/1500 [0m                      

                       Computation: 97592 steps/s (collection: 0.889s, learning 0.119s)
             Mean action noise std: 2.13
          Mean value_function loss: 106.7024
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.0757
                       Mean reward: 468.31
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3844
    Episode_Reward/rotating_object: 90.4258
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 1.01s
                      Time elapsed: 00:10:39
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 634/1500 [0m                      

                       Computation: 91353 steps/s (collection: 0.965s, learning 0.111s)
             Mean action noise std: 2.13
          Mean value_function loss: 110.9293
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0873
                       Mean reward: 444.56
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.3862
    Episode_Reward/rotating_object: 90.6083
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 1.08s
                      Time elapsed: 00:10:40
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 635/1500 [0m                      

                       Computation: 93611 steps/s (collection: 0.890s, learning 0.161s)
             Mean action noise std: 2.13
          Mean value_function loss: 104.8882
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0833
                       Mean reward: 489.08
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.3885
    Episode_Reward/rotating_object: 91.7441
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 1.05s
                      Time elapsed: 00:10:41
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 636/1500 [0m                      

                       Computation: 92869 steps/s (collection: 0.944s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 110.3959
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0795
                       Mean reward: 525.09
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 92.4960
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 1.06s
                      Time elapsed: 00:10:42
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 637/1500 [0m                      

                       Computation: 96194 steps/s (collection: 0.888s, learning 0.134s)
             Mean action noise std: 2.13
          Mean value_function loss: 105.9301
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.0826
                       Mean reward: 439.43
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 94.4605
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 1.02s
                      Time elapsed: 00:10:43
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 638/1500 [0m                      

                       Computation: 99530 steps/s (collection: 0.865s, learning 0.123s)
             Mean action noise std: 2.13
          Mean value_function loss: 101.1832
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0886
                       Mean reward: 455.36
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.3928
    Episode_Reward/rotating_object: 92.2094
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.99s
                      Time elapsed: 00:10:44
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 639/1500 [0m                      

                       Computation: 96703 steps/s (collection: 0.882s, learning 0.134s)
             Mean action noise std: 2.13
          Mean value_function loss: 104.1987
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0924
                       Mean reward: 455.87
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.3929
    Episode_Reward/rotating_object: 93.7720
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 1.02s
                      Time elapsed: 00:10:45
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 640/1500 [0m                      

                       Computation: 96976 steps/s (collection: 0.904s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 112.4038
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.0970
                       Mean reward: 450.40
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3874
    Episode_Reward/rotating_object: 90.7203
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 1.01s
                      Time elapsed: 00:10:46
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 641/1500 [0m                      

                       Computation: 97977 steps/s (collection: 0.876s, learning 0.127s)
             Mean action noise std: 2.14
          Mean value_function loss: 103.1884
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0993
                       Mean reward: 499.35
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.3848
    Episode_Reward/rotating_object: 90.6033
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 1.00s
                      Time elapsed: 00:10:47
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 642/1500 [0m                      

                       Computation: 93883 steps/s (collection: 0.918s, learning 0.129s)
             Mean action noise std: 2.14
          Mean value_function loss: 118.5919
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.1080
                       Mean reward: 439.05
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 88.6935
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 1.05s
                      Time elapsed: 00:10:49
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 643/1500 [0m                      

                       Computation: 93670 steps/s (collection: 0.913s, learning 0.136s)
             Mean action noise std: 2.14
          Mean value_function loss: 112.8684
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1109
                       Mean reward: 461.54
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 93.8917
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 1.05s
                      Time elapsed: 00:10:50
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 644/1500 [0m                      

                       Computation: 90777 steps/s (collection: 0.910s, learning 0.173s)
             Mean action noise std: 2.14
          Mean value_function loss: 117.3052
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.1132
                       Mean reward: 496.23
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 96.4211
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.08s
                      Time elapsed: 00:10:51
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 645/1500 [0m                      

                       Computation: 93039 steps/s (collection: 0.862s, learning 0.195s)
             Mean action noise std: 2.14
          Mean value_function loss: 120.8182
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.1011
                       Mean reward: 459.98
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 92.7991
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 1.06s
                      Time elapsed: 00:10:52
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 646/1500 [0m                      

                       Computation: 86846 steps/s (collection: 0.945s, learning 0.187s)
             Mean action noise std: 2.14
          Mean value_function loss: 116.6454
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0959
                       Mean reward: 500.44
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.4000
    Episode_Reward/rotating_object: 96.8622
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 1.13s
                      Time elapsed: 00:10:53
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 647/1500 [0m                      

                       Computation: 101775 steps/s (collection: 0.829s, learning 0.137s)
             Mean action noise std: 2.14
          Mean value_function loss: 116.1009
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.1053
                       Mean reward: 489.36
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.4000
    Episode_Reward/rotating_object: 95.0106
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.97s
                      Time elapsed: 00:10:54
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 648/1500 [0m                      

                       Computation: 91513 steps/s (collection: 0.941s, learning 0.133s)
             Mean action noise std: 2.15
          Mean value_function loss: 117.5099
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1099
                       Mean reward: 470.94
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.4005
    Episode_Reward/rotating_object: 96.3064
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 1.07s
                      Time elapsed: 00:10:55
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 649/1500 [0m                      

                       Computation: 90860 steps/s (collection: 0.960s, learning 0.122s)
             Mean action noise std: 2.15
          Mean value_function loss: 114.4992
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.1278
                       Mean reward: 493.69
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 98.3353
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 1.08s
                      Time elapsed: 00:10:56
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 650/1500 [0m                      

                       Computation: 104597 steps/s (collection: 0.824s, learning 0.116s)
             Mean action noise std: 2.15
          Mean value_function loss: 110.8406
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 16.1361
                       Mean reward: 448.13
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.3924
    Episode_Reward/rotating_object: 91.1639
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.94s
                      Time elapsed: 00:10:57
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 651/1500 [0m                      

                       Computation: 96151 steps/s (collection: 0.871s, learning 0.152s)
             Mean action noise std: 2.15
          Mean value_function loss: 115.1748
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.1449
                       Mean reward: 441.22
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 0.3991
    Episode_Reward/rotating_object: 94.0862
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 1.02s
                      Time elapsed: 00:10:58
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 652/1500 [0m                      

                       Computation: 94330 steps/s (collection: 0.864s, learning 0.179s)
             Mean action noise std: 2.15
          Mean value_function loss: 113.8025
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1485
                       Mean reward: 517.53
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.4027
    Episode_Reward/rotating_object: 100.1881
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 1.04s
                      Time elapsed: 00:10:59
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 653/1500 [0m                      

                       Computation: 92700 steps/s (collection: 0.911s, learning 0.149s)
             Mean action noise std: 2.15
          Mean value_function loss: 114.5761
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.1372
                       Mean reward: 459.15
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.3980
    Episode_Reward/rotating_object: 95.8712
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 1.06s
                      Time elapsed: 00:11:00
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 654/1500 [0m                      

                       Computation: 97029 steps/s (collection: 0.856s, learning 0.158s)
             Mean action noise std: 2.15
          Mean value_function loss: 119.7218
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.1344
                       Mean reward: 446.75
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.3880
    Episode_Reward/rotating_object: 91.4828
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 1.01s
                      Time elapsed: 00:11:01
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 655/1500 [0m                      

                       Computation: 91449 steps/s (collection: 0.913s, learning 0.162s)
             Mean action noise std: 2.15
          Mean value_function loss: 116.0837
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 16.1362
                       Mean reward: 443.50
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 91.4674
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 1.07s
                      Time elapsed: 00:11:02
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 656/1500 [0m                      

                       Computation: 92285 steps/s (collection: 0.901s, learning 0.165s)
             Mean action noise std: 2.16
          Mean value_function loss: 101.4635
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.1376
                       Mean reward: 446.00
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.4040
    Episode_Reward/rotating_object: 96.9794
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 1.07s
                      Time elapsed: 00:11:03
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 657/1500 [0m                      

                       Computation: 96571 steps/s (collection: 0.866s, learning 0.152s)
             Mean action noise std: 2.16
          Mean value_function loss: 103.1317
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1434
                       Mean reward: 462.11
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 0.3950
    Episode_Reward/rotating_object: 93.3350
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 1.02s
                      Time elapsed: 00:11:04
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 658/1500 [0m                      

                       Computation: 88437 steps/s (collection: 0.932s, learning 0.180s)
             Mean action noise std: 2.16
          Mean value_function loss: 109.6505
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1460
                       Mean reward: 506.99
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 98.5497
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.11s
                      Time elapsed: 00:11:05
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 659/1500 [0m                      

                       Computation: 97079 steps/s (collection: 0.868s, learning 0.145s)
             Mean action noise std: 2.16
          Mean value_function loss: 107.5934
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1469
                       Mean reward: 488.67
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.3934
    Episode_Reward/rotating_object: 91.5673
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 1.01s
                      Time elapsed: 00:11:06
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 660/1500 [0m                      

                       Computation: 97433 steps/s (collection: 0.880s, learning 0.129s)
             Mean action noise std: 2.16
          Mean value_function loss: 114.0678
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1504
                       Mean reward: 482.09
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.3952
    Episode_Reward/rotating_object: 94.2475
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 1.01s
                      Time elapsed: 00:11:07
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 661/1500 [0m                      

                       Computation: 92995 steps/s (collection: 0.892s, learning 0.165s)
             Mean action noise std: 2.17
          Mean value_function loss: 102.5453
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.1613
                       Mean reward: 487.67
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 95.2917
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 1.06s
                      Time elapsed: 00:11:08
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 662/1500 [0m                      

                       Computation: 100943 steps/s (collection: 0.837s, learning 0.137s)
             Mean action noise std: 2.17
          Mean value_function loss: 113.5276
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1647
                       Mean reward: 468.93
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 95.6078
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.97s
                      Time elapsed: 00:11:09
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 663/1500 [0m                      

                       Computation: 96545 steps/s (collection: 0.890s, learning 0.128s)
             Mean action noise std: 2.17
          Mean value_function loss: 117.1920
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1703
                       Mean reward: 518.12
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.3932
    Episode_Reward/rotating_object: 95.1785
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 1.02s
                      Time elapsed: 00:11:10
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 664/1500 [0m                      

                       Computation: 99346 steps/s (collection: 0.865s, learning 0.124s)
             Mean action noise std: 2.17
          Mean value_function loss: 112.5938
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1665
                       Mean reward: 463.35
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.3958
    Episode_Reward/rotating_object: 93.9339
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.99s
                      Time elapsed: 00:11:11
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 665/1500 [0m                      

                       Computation: 88606 steps/s (collection: 0.929s, learning 0.180s)
             Mean action noise std: 2.17
          Mean value_function loss: 131.2861
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.1729
                       Mean reward: 499.97
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 94.1049
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 1.11s
                      Time elapsed: 00:11:12
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 666/1500 [0m                      

                       Computation: 55803 steps/s (collection: 1.559s, learning 0.202s)
             Mean action noise std: 2.17
          Mean value_function loss: 120.0430
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1788
                       Mean reward: 512.32
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4048
    Episode_Reward/rotating_object: 97.9845
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.76s
                      Time elapsed: 00:11:14
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 667/1500 [0m                      

                       Computation: 31510 steps/s (collection: 2.997s, learning 0.123s)
             Mean action noise std: 2.18
          Mean value_function loss: 102.5528
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.1883
                       Mean reward: 463.67
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 0.3861
    Episode_Reward/rotating_object: 90.8059
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.12s
                      Time elapsed: 00:11:17
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 668/1500 [0m                      

                       Computation: 31489 steps/s (collection: 2.959s, learning 0.163s)
             Mean action noise std: 2.18
          Mean value_function loss: 101.5422
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1887
                       Mean reward: 482.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 93.6952
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.12s
                      Time elapsed: 00:11:20
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 669/1500 [0m                      

                       Computation: 31316 steps/s (collection: 2.998s, learning 0.141s)
             Mean action noise std: 2.18
          Mean value_function loss: 117.9534
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1975
                       Mean reward: 474.94
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 93.4451
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.14s
                      Time elapsed: 00:11:24
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 670/1500 [0m                      

                       Computation: 30566 steps/s (collection: 3.062s, learning 0.154s)
             Mean action noise std: 2.18
          Mean value_function loss: 115.3831
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1932
                       Mean reward: 499.75
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 96.3255
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.22s
                      Time elapsed: 00:11:27
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 671/1500 [0m                      

                       Computation: 31345 steps/s (collection: 2.951s, learning 0.186s)
             Mean action noise std: 2.18
          Mean value_function loss: 119.7538
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1967
                       Mean reward: 479.69
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.4015
    Episode_Reward/rotating_object: 98.2511
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.14s
                      Time elapsed: 00:11:30
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 672/1500 [0m                      

                       Computation: 30414 steps/s (collection: 3.078s, learning 0.155s)
             Mean action noise std: 2.19
          Mean value_function loss: 115.3171
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.2002
                       Mean reward: 463.18
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 92.3221
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.23s
                      Time elapsed: 00:11:33
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 673/1500 [0m                      

                       Computation: 31327 steps/s (collection: 2.993s, learning 0.145s)
             Mean action noise std: 2.19
          Mean value_function loss: 127.8984
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2102
                       Mean reward: 484.09
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.4063
    Episode_Reward/rotating_object: 97.0795
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.14s
                      Time elapsed: 00:11:36
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 674/1500 [0m                      

                       Computation: 31061 steps/s (collection: 3.046s, learning 0.119s)
             Mean action noise std: 2.19
          Mean value_function loss: 118.5762
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2261
                       Mean reward: 500.92
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.4046
    Episode_Reward/rotating_object: 96.9583
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.16s
                      Time elapsed: 00:11:39
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 675/1500 [0m                      

                       Computation: 23901 steps/s (collection: 3.959s, learning 0.154s)
             Mean action noise std: 2.19
          Mean value_function loss: 124.0402
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.2284
                       Mean reward: 467.85
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.4047
    Episode_Reward/rotating_object: 94.7865
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.11s
                      Time elapsed: 00:11:44
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 676/1500 [0m                      

                       Computation: 94299 steps/s (collection: 0.858s, learning 0.184s)
             Mean action noise std: 2.19
          Mean value_function loss: 129.6724
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.2344
                       Mean reward: 450.59
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.4003
    Episode_Reward/rotating_object: 93.8969
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 1.04s
                      Time elapsed: 00:11:45
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 677/1500 [0m                      

                       Computation: 95796 steps/s (collection: 0.859s, learning 0.167s)
             Mean action noise std: 2.20
          Mean value_function loss: 118.8940
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2368
                       Mean reward: 446.26
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.4078
    Episode_Reward/rotating_object: 94.3669
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.03s
                      Time elapsed: 00:11:46
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 678/1500 [0m                      

                       Computation: 92783 steps/s (collection: 0.863s, learning 0.196s)
             Mean action noise std: 2.20
          Mean value_function loss: 122.4133
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 16.2384
                       Mean reward: 502.28
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 0.4066
    Episode_Reward/rotating_object: 94.7241
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 1.06s
                      Time elapsed: 00:11:47
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 679/1500 [0m                      

                       Computation: 91858 steps/s (collection: 0.889s, learning 0.181s)
             Mean action noise std: 2.20
          Mean value_function loss: 123.5754
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.2425
                       Mean reward: 494.25
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 100.9139
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 1.07s
                      Time elapsed: 00:11:48
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 680/1500 [0m                      

                       Computation: 97577 steps/s (collection: 0.848s, learning 0.159s)
             Mean action noise std: 2.20
          Mean value_function loss: 133.5063
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 16.2466
                       Mean reward: 517.54
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.4146
    Episode_Reward/rotating_object: 99.0949
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 1.01s
                      Time elapsed: 00:11:49
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 681/1500 [0m                      

                       Computation: 87984 steps/s (collection: 0.896s, learning 0.222s)
             Mean action noise std: 2.20
          Mean value_function loss: 122.5878
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.2466
                       Mean reward: 456.47
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 88.4855
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 1.12s
                      Time elapsed: 00:11:50
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 682/1500 [0m                      

                       Computation: 91697 steps/s (collection: 0.965s, learning 0.107s)
             Mean action noise std: 2.20
          Mean value_function loss: 115.4036
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.2456
                       Mean reward: 455.62
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.4178
    Episode_Reward/rotating_object: 97.7734
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 1.07s
                      Time elapsed: 00:11:51
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 683/1500 [0m                      

                       Computation: 81368 steps/s (collection: 1.030s, learning 0.178s)
             Mean action noise std: 2.20
          Mean value_function loss: 108.0230
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2443
                       Mean reward: 510.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4210
    Episode_Reward/rotating_object: 99.9046
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 1.21s
                      Time elapsed: 00:11:52
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 684/1500 [0m                      

                       Computation: 91865 steps/s (collection: 0.925s, learning 0.145s)
             Mean action noise std: 2.20
          Mean value_function loss: 105.8052
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2489
                       Mean reward: 457.31
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.4012
    Episode_Reward/rotating_object: 88.6343
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 1.07s
                      Time elapsed: 00:11:53
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 685/1500 [0m                      

                       Computation: 92651 steps/s (collection: 0.925s, learning 0.136s)
             Mean action noise std: 2.20
          Mean value_function loss: 117.5013
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 16.2558
                       Mean reward: 536.77
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4167
    Episode_Reward/rotating_object: 101.3190
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 1.06s
                      Time elapsed: 00:11:54
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 686/1500 [0m                      

                       Computation: 94489 steps/s (collection: 0.892s, learning 0.149s)
             Mean action noise std: 2.20
          Mean value_function loss: 114.6901
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 16.2570
                       Mean reward: 477.02
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.4035
    Episode_Reward/rotating_object: 94.9483
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 1.04s
                      Time elapsed: 00:11:55
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 687/1500 [0m                      

                       Computation: 96995 steps/s (collection: 0.835s, learning 0.179s)
             Mean action noise std: 2.20
          Mean value_function loss: 114.2422
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.2571
                       Mean reward: 474.54
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.4077
    Episode_Reward/rotating_object: 93.3890
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 1.01s
                      Time elapsed: 00:11:56
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 688/1500 [0m                      

                       Computation: 91166 steps/s (collection: 0.903s, learning 0.176s)
             Mean action noise std: 2.20
          Mean value_function loss: 115.8935
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2595
                       Mean reward: 491.73
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.4096
    Episode_Reward/rotating_object: 97.2296
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 1.08s
                      Time elapsed: 00:11:57
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 689/1500 [0m                      

                       Computation: 94032 steps/s (collection: 0.869s, learning 0.176s)
             Mean action noise std: 2.20
          Mean value_function loss: 116.8050
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.2579
                       Mean reward: 527.80
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.4187
    Episode_Reward/rotating_object: 99.2840
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 1.05s
                      Time elapsed: 00:11:59
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 690/1500 [0m                      

                       Computation: 95956 steps/s (collection: 0.923s, learning 0.102s)
             Mean action noise std: 2.21
          Mean value_function loss: 111.4796
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.2611
                       Mean reward: 492.41
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.4117
    Episode_Reward/rotating_object: 97.6667
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 1.02s
                      Time elapsed: 00:12:00
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 691/1500 [0m                      

                       Computation: 87544 steps/s (collection: 0.955s, learning 0.168s)
             Mean action noise std: 2.21
          Mean value_function loss: 114.8992
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2707
                       Mean reward: 471.85
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4094
    Episode_Reward/rotating_object: 97.4807
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 1.12s
                      Time elapsed: 00:12:01
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 692/1500 [0m                      

                       Computation: 100007 steps/s (collection: 0.842s, learning 0.141s)
             Mean action noise std: 2.21
          Mean value_function loss: 120.3592
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.2883
                       Mean reward: 536.96
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.4112
    Episode_Reward/rotating_object: 101.2946
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.98s
                      Time elapsed: 00:12:02
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 693/1500 [0m                      

                       Computation: 87513 steps/s (collection: 0.893s, learning 0.230s)
             Mean action noise std: 2.21
          Mean value_function loss: 119.0863
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2899
                       Mean reward: 492.62
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.4033
    Episode_Reward/rotating_object: 95.0482
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 1.12s
                      Time elapsed: 00:12:03
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 694/1500 [0m                      

                       Computation: 93475 steps/s (collection: 0.940s, learning 0.112s)
             Mean action noise std: 2.21
          Mean value_function loss: 108.6537
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.2826
                       Mean reward: 485.46
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.4105
    Episode_Reward/rotating_object: 96.3567
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 1.05s
                      Time elapsed: 00:12:04
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 695/1500 [0m                      

                       Computation: 103213 steps/s (collection: 0.796s, learning 0.156s)
             Mean action noise std: 2.21
          Mean value_function loss: 115.6900
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.2781
                       Mean reward: 472.44
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.4110
    Episode_Reward/rotating_object: 94.5798
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.95s
                      Time elapsed: 00:12:05
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 696/1500 [0m                      

                       Computation: 103141 steps/s (collection: 0.843s, learning 0.111s)
             Mean action noise std: 2.21
          Mean value_function loss: 118.9328
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.2659
                       Mean reward: 505.19
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.4120
    Episode_Reward/rotating_object: 102.0280
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.95s
                      Time elapsed: 00:12:06
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 697/1500 [0m                      

                       Computation: 104590 steps/s (collection: 0.807s, learning 0.133s)
             Mean action noise std: 2.21
          Mean value_function loss: 108.4731
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.2674
                       Mean reward: 494.53
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.4089
    Episode_Reward/rotating_object: 96.4837
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.94s
                      Time elapsed: 00:12:07
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 698/1500 [0m                      

                       Computation: 102069 steps/s (collection: 0.830s, learning 0.133s)
             Mean action noise std: 2.21
          Mean value_function loss: 102.0798
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 16.2794
                       Mean reward: 506.62
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.4173
    Episode_Reward/rotating_object: 100.6614
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.96s
                      Time elapsed: 00:12:08
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 699/1500 [0m                      

                       Computation: 93182 steps/s (collection: 0.880s, learning 0.175s)
             Mean action noise std: 2.21
          Mean value_function loss: 101.3314
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2792
                       Mean reward: 489.65
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.4148
    Episode_Reward/rotating_object: 101.0577
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 1.05s
                      Time elapsed: 00:12:09
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 700/1500 [0m                      

                       Computation: 100055 steps/s (collection: 0.811s, learning 0.172s)
             Mean action noise std: 2.22
          Mean value_function loss: 104.3614
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.2802
                       Mean reward: 484.96
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.4103
    Episode_Reward/rotating_object: 97.0330
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.98s
                      Time elapsed: 00:12:10
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 701/1500 [0m                      

                       Computation: 99208 steps/s (collection: 0.803s, learning 0.188s)
             Mean action noise std: 2.22
          Mean value_function loss: 113.0168
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.2846
                       Mean reward: 463.60
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 102.7383
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.99s
                      Time elapsed: 00:12:11
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 702/1500 [0m                      

                       Computation: 93682 steps/s (collection: 0.924s, learning 0.125s)
             Mean action noise std: 2.22
          Mean value_function loss: 110.2475
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2710
                       Mean reward: 508.68
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.4103
    Episode_Reward/rotating_object: 98.1641
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 1.05s
                      Time elapsed: 00:12:12
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 703/1500 [0m                      

                       Computation: 97868 steps/s (collection: 0.848s, learning 0.156s)
             Mean action noise std: 2.22
          Mean value_function loss: 106.5077
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2840
                       Mean reward: 515.37
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 102.9063
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 1.00s
                      Time elapsed: 00:12:13
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 704/1500 [0m                      

                       Computation: 99127 steps/s (collection: 0.860s, learning 0.132s)
             Mean action noise std: 2.22
          Mean value_function loss: 104.7033
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.2861
                       Mean reward: 521.96
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.4100
    Episode_Reward/rotating_object: 100.3916
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.99s
                      Time elapsed: 00:12:14
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 705/1500 [0m                      

                       Computation: 96895 steps/s (collection: 0.851s, learning 0.163s)
             Mean action noise std: 2.22
          Mean value_function loss: 119.6388
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.2873
                       Mean reward: 501.04
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.4140
    Episode_Reward/rotating_object: 97.4452
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 1.01s
                      Time elapsed: 00:12:15
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 706/1500 [0m                      

                       Computation: 99563 steps/s (collection: 0.847s, learning 0.141s)
             Mean action noise std: 2.22
          Mean value_function loss: 123.0742
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2900
                       Mean reward: 491.23
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.4152
    Episode_Reward/rotating_object: 100.5226
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.99s
                      Time elapsed: 00:12:16
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 707/1500 [0m                      

                       Computation: 94598 steps/s (collection: 0.847s, learning 0.192s)
             Mean action noise std: 2.23
          Mean value_function loss: 118.7931
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.2972
                       Mean reward: 520.13
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.4163
    Episode_Reward/rotating_object: 102.9708
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.04s
                      Time elapsed: 00:12:17
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 708/1500 [0m                      

                       Computation: 98515 steps/s (collection: 0.829s, learning 0.169s)
             Mean action noise std: 2.23
          Mean value_function loss: 116.3714
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.3038
                       Mean reward: 510.22
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.4104
    Episode_Reward/rotating_object: 97.8557
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 1.00s
                      Time elapsed: 00:12:18
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 709/1500 [0m                      

                       Computation: 98346 steps/s (collection: 0.824s, learning 0.175s)
             Mean action noise std: 2.23
          Mean value_function loss: 102.7090
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.3086
                       Mean reward: 463.27
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.4042
    Episode_Reward/rotating_object: 97.4328
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 1.00s
                      Time elapsed: 00:12:19
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 710/1500 [0m                      

                       Computation: 100790 steps/s (collection: 0.812s, learning 0.164s)
             Mean action noise std: 2.23
          Mean value_function loss: 106.5009
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.2999
                       Mean reward: 459.44
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.4074
    Episode_Reward/rotating_object: 97.0517
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.98s
                      Time elapsed: 00:12:20
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 711/1500 [0m                      

                       Computation: 102390 steps/s (collection: 0.845s, learning 0.116s)
             Mean action noise std: 2.23
          Mean value_function loss: 114.5273
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.3016
                       Mean reward: 476.66
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 98.3721
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.96s
                      Time elapsed: 00:12:21
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 712/1500 [0m                      

                       Computation: 102333 steps/s (collection: 0.788s, learning 0.172s)
             Mean action noise std: 2.23
          Mean value_function loss: 102.4698
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.3016
                       Mean reward: 524.68
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.4132
    Episode_Reward/rotating_object: 100.9249
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.96s
                      Time elapsed: 00:12:22
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 713/1500 [0m                      

                       Computation: 102606 steps/s (collection: 0.803s, learning 0.155s)
             Mean action noise std: 2.23
          Mean value_function loss: 106.9136
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.3047
                       Mean reward: 510.16
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 100.3274
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.96s
                      Time elapsed: 00:12:23
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 714/1500 [0m                      

                       Computation: 93872 steps/s (collection: 0.862s, learning 0.186s)
             Mean action noise std: 2.23
          Mean value_function loss: 106.2939
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3022
                       Mean reward: 508.03
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.4061
    Episode_Reward/rotating_object: 99.6870
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.05s
                      Time elapsed: 00:12:24
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 715/1500 [0m                      

                       Computation: 91965 steps/s (collection: 0.923s, learning 0.145s)
             Mean action noise std: 2.24
          Mean value_function loss: 105.3297
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3085
                       Mean reward: 550.03
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.4056
    Episode_Reward/rotating_object: 98.1268
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.07s
                      Time elapsed: 00:12:25
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 716/1500 [0m                      

                       Computation: 101154 steps/s (collection: 0.821s, learning 0.151s)
             Mean action noise std: 2.24
          Mean value_function loss: 107.6468
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.3082
                       Mean reward: 478.14
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 92.2662
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.97s
                      Time elapsed: 00:12:26
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 717/1500 [0m                      

                       Computation: 97571 steps/s (collection: 0.849s, learning 0.158s)
             Mean action noise std: 2.24
          Mean value_function loss: 109.1717
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3086
                       Mean reward: 482.89
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 97.8634
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.01s
                      Time elapsed: 00:12:27
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 718/1500 [0m                      

                       Computation: 95259 steps/s (collection: 0.856s, learning 0.176s)
             Mean action noise std: 2.24
          Mean value_function loss: 108.5283
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3079
                       Mean reward: 564.86
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.4101
    Episode_Reward/rotating_object: 103.6749
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 1.03s
                      Time elapsed: 00:12:28
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 719/1500 [0m                      

                       Computation: 95221 steps/s (collection: 0.867s, learning 0.166s)
             Mean action noise std: 2.24
          Mean value_function loss: 111.7577
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.3145
                       Mean reward: 473.87
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4034
    Episode_Reward/rotating_object: 94.6910
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.03s
                      Time elapsed: 00:12:29
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 720/1500 [0m                      

                       Computation: 95975 steps/s (collection: 0.907s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 106.6625
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3150
                       Mean reward: 483.85
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 98.2850
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.02s
                      Time elapsed: 00:12:30
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 721/1500 [0m                      

                       Computation: 105940 steps/s (collection: 0.804s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 104.2700
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3204
                       Mean reward: 443.03
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.4027
    Episode_Reward/rotating_object: 93.7551
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.93s
                      Time elapsed: 00:12:31
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 722/1500 [0m                      

                       Computation: 101909 steps/s (collection: 0.820s, learning 0.145s)
             Mean action noise std: 2.25
          Mean value_function loss: 105.9708
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3219
                       Mean reward: 512.08
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 97.1751
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.96s
                      Time elapsed: 00:12:32
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 723/1500 [0m                      

                       Computation: 100258 steps/s (collection: 0.836s, learning 0.145s)
             Mean action noise std: 2.25
          Mean value_function loss: 101.9687
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3195
                       Mean reward: 518.98
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.4135
    Episode_Reward/rotating_object: 103.4483
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.98s
                      Time elapsed: 00:12:33
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 724/1500 [0m                      

                       Computation: 98694 steps/s (collection: 0.844s, learning 0.152s)
             Mean action noise std: 2.25
          Mean value_function loss: 98.7816
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.3300
                       Mean reward: 493.81
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.4114
    Episode_Reward/rotating_object: 100.5317
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.00s
                      Time elapsed: 00:12:34
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 725/1500 [0m                      

                       Computation: 100169 steps/s (collection: 0.845s, learning 0.137s)
             Mean action noise std: 2.25
          Mean value_function loss: 104.1061
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.3351
                       Mean reward: 521.97
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.4115
    Episode_Reward/rotating_object: 101.9026
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.98s
                      Time elapsed: 00:12:35
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 726/1500 [0m                      

                       Computation: 99695 steps/s (collection: 0.834s, learning 0.152s)
             Mean action noise std: 2.25
          Mean value_function loss: 108.9813
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 16.3435
                       Mean reward: 529.13
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.4134
    Episode_Reward/rotating_object: 103.4591
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.99s
                      Time elapsed: 00:12:36
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 727/1500 [0m                      

                       Computation: 100449 steps/s (collection: 0.824s, learning 0.154s)
             Mean action noise std: 2.25
          Mean value_function loss: 108.2631
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.3445
                       Mean reward: 545.52
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.4098
    Episode_Reward/rotating_object: 103.8340
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.98s
                      Time elapsed: 00:12:37
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 728/1500 [0m                      

                       Computation: 97822 steps/s (collection: 0.865s, learning 0.140s)
             Mean action noise std: 2.25
          Mean value_function loss: 96.4310
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.3395
                       Mean reward: 518.51
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.4074
    Episode_Reward/rotating_object: 97.3111
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.00s
                      Time elapsed: 00:12:38
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 729/1500 [0m                      

                       Computation: 107304 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 2.26
          Mean value_function loss: 96.2649
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3347
                       Mean reward: 527.25
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 101.7189
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.92s
                      Time elapsed: 00:12:39
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 730/1500 [0m                      

                       Computation: 105374 steps/s (collection: 0.795s, learning 0.138s)
             Mean action noise std: 2.26
          Mean value_function loss: 102.1154
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.3441
                       Mean reward: 538.36
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.4131
    Episode_Reward/rotating_object: 105.6270
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.93s
                      Time elapsed: 00:12:39
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 731/1500 [0m                      

                       Computation: 102074 steps/s (collection: 0.815s, learning 0.148s)
             Mean action noise std: 2.26
          Mean value_function loss: 103.5017
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.3409
                       Mean reward: 500.26
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.4075
    Episode_Reward/rotating_object: 101.1861
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.96s
                      Time elapsed: 00:12:40
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 732/1500 [0m                      

                       Computation: 101328 steps/s (collection: 0.846s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 91.2159
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3458
                       Mean reward: 510.39
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 103.8079
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.97s
                      Time elapsed: 00:12:41
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 733/1500 [0m                      

                       Computation: 103147 steps/s (collection: 0.820s, learning 0.133s)
             Mean action noise std: 2.26
          Mean value_function loss: 101.1612
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.3341
                       Mean reward: 500.42
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.3990
    Episode_Reward/rotating_object: 97.5865
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.95s
                      Time elapsed: 00:12:42
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 734/1500 [0m                      

                       Computation: 113603 steps/s (collection: 0.773s, learning 0.092s)
             Mean action noise std: 2.26
          Mean value_function loss: 109.9182
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3344
                       Mean reward: 480.13
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 101.8533
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.87s
                      Time elapsed: 00:12:43
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 735/1500 [0m                      

                       Computation: 112286 steps/s (collection: 0.758s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 103.3428
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.3376
                       Mean reward: 509.71
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4116
    Episode_Reward/rotating_object: 102.2000
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.88s
                      Time elapsed: 00:12:44
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 736/1500 [0m                      

                       Computation: 113736 steps/s (collection: 0.768s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 99.1580
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3460
                       Mean reward: 559.37
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.4153
    Episode_Reward/rotating_object: 103.4229
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.86s
                      Time elapsed: 00:12:45
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 737/1500 [0m                      

                       Computation: 116198 steps/s (collection: 0.746s, learning 0.100s)
             Mean action noise std: 2.27
          Mean value_function loss: 99.3903
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.3553
                       Mean reward: 516.73
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.4068
    Episode_Reward/rotating_object: 101.4228
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.85s
                      Time elapsed: 00:12:46
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 738/1500 [0m                      

                       Computation: 107564 steps/s (collection: 0.825s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.4396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3554
                       Mean reward: 511.47
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.4032
    Episode_Reward/rotating_object: 98.9091
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.91s
                      Time elapsed: 00:12:47
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 739/1500 [0m                      

                       Computation: 118173 steps/s (collection: 0.736s, learning 0.096s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.0627
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.3596
                       Mean reward: 520.76
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.4127
    Episode_Reward/rotating_object: 104.2556
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.83s
                      Time elapsed: 00:12:48
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 740/1500 [0m                      

                       Computation: 96914 steps/s (collection: 0.849s, learning 0.166s)
             Mean action noise std: 2.27
          Mean value_function loss: 98.6659
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3521
                       Mean reward: 540.13
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.4098
    Episode_Reward/rotating_object: 105.0013
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 1.01s
                      Time elapsed: 00:12:49
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 741/1500 [0m                      

                       Computation: 95200 steps/s (collection: 0.885s, learning 0.147s)
             Mean action noise std: 2.27
          Mean value_function loss: 99.5751
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3442
                       Mean reward: 531.94
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.4043
    Episode_Reward/rotating_object: 98.6033
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 1.03s
                      Time elapsed: 00:12:50
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 742/1500 [0m                      

                       Computation: 109155 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 2.27
          Mean value_function loss: 95.8986
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3481
                       Mean reward: 533.14
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.4067
    Episode_Reward/rotating_object: 103.0906
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.90s
                      Time elapsed: 00:12:50
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 743/1500 [0m                      

                       Computation: 105971 steps/s (collection: 0.789s, learning 0.139s)
             Mean action noise std: 2.27
          Mean value_function loss: 102.3935
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3454
                       Mean reward: 529.64
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 103.2083
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.93s
                      Time elapsed: 00:12:51
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 744/1500 [0m                      

                       Computation: 108980 steps/s (collection: 0.762s, learning 0.140s)
             Mean action noise std: 2.27
          Mean value_function loss: 99.3743
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.3457
                       Mean reward: 495.02
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.4092
    Episode_Reward/rotating_object: 99.2220
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.90s
                      Time elapsed: 00:12:52
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 745/1500 [0m                      

                       Computation: 106393 steps/s (collection: 0.777s, learning 0.147s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.3291
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.3483
                       Mean reward: 513.17
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.4153
    Episode_Reward/rotating_object: 104.9060
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.92s
                      Time elapsed: 00:12:53
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 746/1500 [0m                      

                       Computation: 111146 steps/s (collection: 0.766s, learning 0.118s)
             Mean action noise std: 2.28
          Mean value_function loss: 114.7173
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.3505
                       Mean reward: 475.83
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.4091
    Episode_Reward/rotating_object: 99.1078
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.88s
                      Time elapsed: 00:12:54
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 747/1500 [0m                      

                       Computation: 118563 steps/s (collection: 0.743s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 120.2465
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3515
                       Mean reward: 495.66
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.4110
    Episode_Reward/rotating_object: 99.9489
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.83s
                      Time elapsed: 00:12:55
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 748/1500 [0m                      

                       Computation: 118089 steps/s (collection: 0.730s, learning 0.102s)
             Mean action noise std: 2.28
          Mean value_function loss: 106.4067
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3525
                       Mean reward: 522.16
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 100.3398
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.83s
                      Time elapsed: 00:12:56
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 749/1500 [0m                      

                       Computation: 117660 steps/s (collection: 0.750s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 114.6875
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.3564
                       Mean reward: 507.98
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.4188
    Episode_Reward/rotating_object: 104.4970
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.84s
                      Time elapsed: 00:12:57
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 750/1500 [0m                      

                       Computation: 112956 steps/s (collection: 0.764s, learning 0.106s)
             Mean action noise std: 2.29
          Mean value_function loss: 97.8238
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3608
                       Mean reward: 471.30
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.4097
    Episode_Reward/rotating_object: 98.5487
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.87s
                      Time elapsed: 00:12:57
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 751/1500 [0m                      

                       Computation: 115389 steps/s (collection: 0.741s, learning 0.111s)
             Mean action noise std: 2.29
          Mean value_function loss: 102.5591
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3679
                       Mean reward: 523.51
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 103.8531
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.85s
                      Time elapsed: 00:12:58
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 752/1500 [0m                      

                       Computation: 110484 steps/s (collection: 0.749s, learning 0.141s)
             Mean action noise std: 2.29
          Mean value_function loss: 100.1777
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3855
                       Mean reward: 554.95
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 108.0628
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.89s
                      Time elapsed: 00:12:59
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 753/1500 [0m                      

                       Computation: 110839 steps/s (collection: 0.751s, learning 0.136s)
             Mean action noise std: 2.30
          Mean value_function loss: 102.1745
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3911
                       Mean reward: 501.87
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.4113
    Episode_Reward/rotating_object: 102.5758
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.89s
                      Time elapsed: 00:13:00
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 754/1500 [0m                      

                       Computation: 112198 steps/s (collection: 0.765s, learning 0.112s)
             Mean action noise std: 2.30
          Mean value_function loss: 102.2051
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.3895
                       Mean reward: 551.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4162
    Episode_Reward/rotating_object: 104.5058
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.88s
                      Time elapsed: 00:13:01
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 755/1500 [0m                      

                       Computation: 110028 steps/s (collection: 0.788s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 102.1525
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.3894
                       Mean reward: 508.40
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.4147
    Episode_Reward/rotating_object: 105.2295
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.89s
                      Time elapsed: 00:13:02
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 756/1500 [0m                      

                       Computation: 116294 steps/s (collection: 0.752s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 102.9408
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3895
                       Mean reward: 498.86
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.4121
    Episode_Reward/rotating_object: 102.4541
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.85s
                      Time elapsed: 00:13:03
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 757/1500 [0m                      

                       Computation: 112517 steps/s (collection: 0.756s, learning 0.117s)
             Mean action noise std: 2.30
          Mean value_function loss: 108.0452
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3856
                       Mean reward: 520.01
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.4111
    Episode_Reward/rotating_object: 101.4990
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.87s
                      Time elapsed: 00:13:04
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 758/1500 [0m                      

                       Computation: 110775 steps/s (collection: 0.773s, learning 0.115s)
             Mean action noise std: 2.30
          Mean value_function loss: 103.7803
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.3873
                       Mean reward: 541.87
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.4120
    Episode_Reward/rotating_object: 106.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.89s
                      Time elapsed: 00:13:04
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 759/1500 [0m                      

                       Computation: 106659 steps/s (collection: 0.794s, learning 0.128s)
             Mean action noise std: 2.30
          Mean value_function loss: 99.0355
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.3876
                       Mean reward: 508.49
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4058
    Episode_Reward/rotating_object: 100.8589
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.92s
                      Time elapsed: 00:13:05
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 760/1500 [0m                      

                       Computation: 106988 steps/s (collection: 0.798s, learning 0.121s)
             Mean action noise std: 2.30
          Mean value_function loss: 100.5665
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3914
                       Mean reward: 529.76
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 106.3115
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.92s
                      Time elapsed: 00:13:06
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 761/1500 [0m                      

                       Computation: 115816 steps/s (collection: 0.750s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 93.3380
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3812
                       Mean reward: 537.24
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4163
    Episode_Reward/rotating_object: 105.9134
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.85s
                      Time elapsed: 00:13:07
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 762/1500 [0m                      

                       Computation: 110653 steps/s (collection: 0.741s, learning 0.148s)
             Mean action noise std: 2.31
          Mean value_function loss: 97.1881
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3923
                       Mean reward: 512.09
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.4160
    Episode_Reward/rotating_object: 103.6474
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.89s
                      Time elapsed: 00:13:08
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 763/1500 [0m                      

                       Computation: 111779 steps/s (collection: 0.789s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 103.7375
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3926
                       Mean reward: 556.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4157
    Episode_Reward/rotating_object: 107.2245
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.88s
                      Time elapsed: 00:13:09
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 764/1500 [0m                      

                       Computation: 117036 steps/s (collection: 0.752s, learning 0.087s)
             Mean action noise std: 2.31
          Mean value_function loss: 94.6586
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3963
                       Mean reward: 520.16
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.4091
    Episode_Reward/rotating_object: 103.8627
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.84s
                      Time elapsed: 00:13:10
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 765/1500 [0m                      

                       Computation: 107697 steps/s (collection: 0.787s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 102.1357
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.4029
                       Mean reward: 554.71
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 104.6471
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.91s
                      Time elapsed: 00:13:11
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 766/1500 [0m                      

                       Computation: 113929 steps/s (collection: 0.738s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 113.5647
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.4098
                       Mean reward: 494.34
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.4033
    Episode_Reward/rotating_object: 101.1093
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.86s
                      Time elapsed: 00:13:12
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 767/1500 [0m                      

                       Computation: 113453 steps/s (collection: 0.774s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 99.8881
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.4046
                       Mean reward: 531.79
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.4206
    Episode_Reward/rotating_object: 108.1185
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.87s
                      Time elapsed: 00:13:12
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 768/1500 [0m                      

                       Computation: 113962 steps/s (collection: 0.764s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 106.5365
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4058
                       Mean reward: 536.25
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.4118
    Episode_Reward/rotating_object: 104.8005
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.86s
                      Time elapsed: 00:13:13
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 769/1500 [0m                      

                       Computation: 108794 steps/s (collection: 0.756s, learning 0.148s)
             Mean action noise std: 2.32
          Mean value_function loss: 101.2219
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.4141
                       Mean reward: 529.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4116
    Episode_Reward/rotating_object: 103.7592
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.90s
                      Time elapsed: 00:13:14
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 770/1500 [0m                      

                       Computation: 107717 steps/s (collection: 0.791s, learning 0.122s)
             Mean action noise std: 2.32
          Mean value_function loss: 104.9513
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4269
                       Mean reward: 531.07
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.4188
    Episode_Reward/rotating_object: 106.8785
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.91s
                      Time elapsed: 00:13:15
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 771/1500 [0m                      

                       Computation: 106246 steps/s (collection: 0.787s, learning 0.139s)
             Mean action noise std: 2.33
          Mean value_function loss: 84.3670
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.4296
                       Mean reward: 537.06
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4117
    Episode_Reward/rotating_object: 101.7111
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.93s
                      Time elapsed: 00:13:16
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 772/1500 [0m                      

                       Computation: 111277 steps/s (collection: 0.772s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 88.2808
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.4303
                       Mean reward: 528.39
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.4136
    Episode_Reward/rotating_object: 102.1576
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.88s
                      Time elapsed: 00:13:17
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 773/1500 [0m                      

                       Computation: 107160 steps/s (collection: 0.793s, learning 0.125s)
             Mean action noise std: 2.33
          Mean value_function loss: 98.6434
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.4278
                       Mean reward: 551.66
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.4144
    Episode_Reward/rotating_object: 104.2384
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.92s
                      Time elapsed: 00:13:18
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 774/1500 [0m                      

                       Computation: 110982 steps/s (collection: 0.766s, learning 0.120s)
             Mean action noise std: 2.33
          Mean value_function loss: 91.4786
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4357
                       Mean reward: 497.37
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.4101
    Episode_Reward/rotating_object: 101.9206
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.89s
                      Time elapsed: 00:13:19
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 775/1500 [0m                      

                       Computation: 112610 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 102.0673
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.4400
                       Mean reward: 491.61
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.4145
    Episode_Reward/rotating_object: 103.2719
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.87s
                      Time elapsed: 00:13:20
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 776/1500 [0m                      

                       Computation: 116492 steps/s (collection: 0.743s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 105.2338
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4426
                       Mean reward: 556.38
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4075
    Episode_Reward/rotating_object: 105.0302
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.84s
                      Time elapsed: 00:13:20
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 777/1500 [0m                      

                       Computation: 105534 steps/s (collection: 0.800s, learning 0.132s)
             Mean action noise std: 2.33
          Mean value_function loss: 103.3740
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.4519
                       Mean reward: 532.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4118
    Episode_Reward/rotating_object: 104.1602
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.93s
                      Time elapsed: 00:13:21
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 778/1500 [0m                      

                       Computation: 110615 steps/s (collection: 0.775s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 101.9110
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4510
                       Mean reward: 500.32
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.4038
    Episode_Reward/rotating_object: 100.1801
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.89s
                      Time elapsed: 00:13:22
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 779/1500 [0m                      

                       Computation: 103055 steps/s (collection: 0.746s, learning 0.207s)
             Mean action noise std: 2.34
          Mean value_function loss: 97.5506
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.4457
                       Mean reward: 527.10
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 105.0180
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.95s
                      Time elapsed: 00:13:23
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 780/1500 [0m                      

                       Computation: 103526 steps/s (collection: 0.808s, learning 0.142s)
             Mean action noise std: 2.34
          Mean value_function loss: 91.9154
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4489
                       Mean reward: 495.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4057
    Episode_Reward/rotating_object: 101.9264
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.95s
                      Time elapsed: 00:13:24
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 781/1500 [0m                      

                       Computation: 101778 steps/s (collection: 0.788s, learning 0.178s)
             Mean action noise std: 2.34
          Mean value_function loss: 86.8480
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.4597
                       Mean reward: 547.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4111
    Episode_Reward/rotating_object: 104.4748
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.97s
                      Time elapsed: 00:13:25
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 782/1500 [0m                      

                       Computation: 109176 steps/s (collection: 0.772s, learning 0.129s)
             Mean action noise std: 2.34
          Mean value_function loss: 82.9286
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.4522
                       Mean reward: 528.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4059
    Episode_Reward/rotating_object: 106.3244
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.90s
                      Time elapsed: 00:13:26
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 783/1500 [0m                      

                       Computation: 116374 steps/s (collection: 0.739s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 97.6369
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.4489
                       Mean reward: 537.83
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.4050
    Episode_Reward/rotating_object: 105.1136
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.84s
                      Time elapsed: 00:13:27
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 784/1500 [0m                      

                       Computation: 112961 steps/s (collection: 0.771s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 91.6480
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4501
                       Mean reward: 525.13
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 102.1336
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.87s
                      Time elapsed: 00:13:28
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 785/1500 [0m                      

                       Computation: 115639 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 2.34
          Mean value_function loss: 98.7496
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.4550
                       Mean reward: 553.39
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.4032
    Episode_Reward/rotating_object: 103.7299
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.85s
                      Time elapsed: 00:13:29
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 786/1500 [0m                      

                       Computation: 112131 steps/s (collection: 0.771s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 106.0655
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.4530
                       Mean reward: 513.06
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.4038
    Episode_Reward/rotating_object: 107.5777
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.88s
                      Time elapsed: 00:13:29
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 787/1500 [0m                      

                       Computation: 107792 steps/s (collection: 0.786s, learning 0.126s)
             Mean action noise std: 2.35
          Mean value_function loss: 92.3945
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4441
                       Mean reward: 576.24
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4068
    Episode_Reward/rotating_object: 106.6407
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.91s
                      Time elapsed: 00:13:30
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 788/1500 [0m                      

                       Computation: 107928 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 2.35
          Mean value_function loss: 94.7383
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.4481
                       Mean reward: 558.33
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 107.1415
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.91s
                      Time elapsed: 00:13:31
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 789/1500 [0m                      

                       Computation: 109478 steps/s (collection: 0.749s, learning 0.149s)
             Mean action noise std: 2.35
          Mean value_function loss: 96.2418
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.4603
                       Mean reward: 548.96
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 106.3447
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.90s
                      Time elapsed: 00:13:32
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 790/1500 [0m                      

                       Computation: 118045 steps/s (collection: 0.740s, learning 0.093s)
             Mean action noise std: 2.35
          Mean value_function loss: 94.0309
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.4601
                       Mean reward: 572.99
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.4104
    Episode_Reward/rotating_object: 110.2141
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.83s
                      Time elapsed: 00:13:33
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 791/1500 [0m                      

                       Computation: 103287 steps/s (collection: 0.809s, learning 0.143s)
             Mean action noise std: 2.35
          Mean value_function loss: 94.4232
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4683
                       Mean reward: 541.55
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.4026
    Episode_Reward/rotating_object: 101.7261
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.95s
                      Time elapsed: 00:13:34
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 792/1500 [0m                      

                       Computation: 116877 steps/s (collection: 0.757s, learning 0.084s)
             Mean action noise std: 2.35
          Mean value_function loss: 84.6693
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.4711
                       Mean reward: 533.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4068
    Episode_Reward/rotating_object: 105.5621
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.84s
                      Time elapsed: 00:13:35
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 793/1500 [0m                      

                       Computation: 114240 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 96.1252
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 16.4743
                       Mean reward: 529.24
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 106.7586
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.86s
                      Time elapsed: 00:13:36
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 794/1500 [0m                      

                       Computation: 109022 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 2.35
          Mean value_function loss: 95.6949
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.4750
                       Mean reward: 541.16
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4105
    Episode_Reward/rotating_object: 104.0528
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.90s
                      Time elapsed: 00:13:37
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 795/1500 [0m                      

                       Computation: 118751 steps/s (collection: 0.742s, learning 0.086s)
             Mean action noise std: 2.36
          Mean value_function loss: 95.9801
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.4775
                       Mean reward: 552.16
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.4127
    Episode_Reward/rotating_object: 108.5618
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.83s
                      Time elapsed: 00:13:37
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 796/1500 [0m                      

                       Computation: 109639 steps/s (collection: 0.762s, learning 0.135s)
             Mean action noise std: 2.36
          Mean value_function loss: 107.8989
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4844
                       Mean reward: 514.42
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.4127
    Episode_Reward/rotating_object: 106.4114
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.90s
                      Time elapsed: 00:13:38
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 797/1500 [0m                      

                       Computation: 117062 steps/s (collection: 0.753s, learning 0.087s)
             Mean action noise std: 2.36
          Mean value_function loss: 110.6232
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4797
                       Mean reward: 543.89
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.4082
    Episode_Reward/rotating_object: 106.6075
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.84s
                      Time elapsed: 00:13:39
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 798/1500 [0m                      

                       Computation: 110534 steps/s (collection: 0.760s, learning 0.129s)
             Mean action noise std: 2.36
          Mean value_function loss: 110.8279
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.4778
                       Mean reward: 531.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4177
    Episode_Reward/rotating_object: 104.7552
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.89s
                      Time elapsed: 00:13:40
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 799/1500 [0m                      

                       Computation: 116078 steps/s (collection: 0.749s, learning 0.098s)
             Mean action noise std: 2.36
          Mean value_function loss: 99.3645
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.4889
                       Mean reward: 530.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4238
    Episode_Reward/rotating_object: 109.1680
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.85s
                      Time elapsed: 00:13:41
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 800/1500 [0m                      

                       Computation: 116316 steps/s (collection: 0.753s, learning 0.092s)
             Mean action noise std: 2.36
          Mean value_function loss: 100.6965
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.4942
                       Mean reward: 517.54
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.4235
    Episode_Reward/rotating_object: 105.6546
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.85s
                      Time elapsed: 00:13:42
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 801/1500 [0m                      

                       Computation: 111844 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 105.7122
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5052
                       Mean reward: 547.74
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.4233
    Episode_Reward/rotating_object: 106.6775
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.88s
                      Time elapsed: 00:13:43
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 802/1500 [0m                      

                       Computation: 115130 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 2.37
          Mean value_function loss: 97.0136
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 16.5133
                       Mean reward: 538.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4209
    Episode_Reward/rotating_object: 107.2958
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.85s
                      Time elapsed: 00:13:43
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 803/1500 [0m                      

                       Computation: 108988 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 2.37
          Mean value_function loss: 101.5152
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.5196
                       Mean reward: 540.88
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.4252
    Episode_Reward/rotating_object: 107.3647
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.90s
                      Time elapsed: 00:13:44
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 804/1500 [0m                      

                       Computation: 104395 steps/s (collection: 0.778s, learning 0.164s)
             Mean action noise std: 2.37
          Mean value_function loss: 102.5862
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.5299
                       Mean reward: 564.12
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.4285
    Episode_Reward/rotating_object: 107.9098
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.94s
                      Time elapsed: 00:13:45
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 805/1500 [0m                      

                       Computation: 114031 steps/s (collection: 0.756s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 101.2948
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.5392
                       Mean reward: 516.66
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.4159
    Episode_Reward/rotating_object: 107.1357
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.86s
                      Time elapsed: 00:13:46
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 806/1500 [0m                      

                       Computation: 106076 steps/s (collection: 0.763s, learning 0.164s)
             Mean action noise std: 2.38
          Mean value_function loss: 101.7284
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.5452
                       Mean reward: 542.90
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.4345
    Episode_Reward/rotating_object: 110.1969
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.93s
                      Time elapsed: 00:13:47
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 807/1500 [0m                      

                       Computation: 111231 steps/s (collection: 0.786s, learning 0.098s)
             Mean action noise std: 2.38
          Mean value_function loss: 115.6055
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.5561
                       Mean reward: 571.66
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.4221
    Episode_Reward/rotating_object: 108.6467
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.88s
                      Time elapsed: 00:13:48
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 808/1500 [0m                      

                       Computation: 117254 steps/s (collection: 0.751s, learning 0.087s)
             Mean action noise std: 2.38
          Mean value_function loss: 106.5988
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.5521
                       Mean reward: 529.73
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.4232
    Episode_Reward/rotating_object: 107.4673
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.84s
                      Time elapsed: 00:13:49
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 809/1500 [0m                      

                       Computation: 111218 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 101.2797
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.5555
                       Mean reward: 551.27
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.4235
    Episode_Reward/rotating_object: 105.9686
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.88s
                      Time elapsed: 00:13:50
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 810/1500 [0m                      

                       Computation: 110731 steps/s (collection: 0.790s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 105.1512
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.5621
                       Mean reward: 516.40
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.4174
    Episode_Reward/rotating_object: 106.5028
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.89s
                      Time elapsed: 00:13:51
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 811/1500 [0m                      

                       Computation: 107456 steps/s (collection: 0.807s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 119.0739
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.5745
                       Mean reward: 544.06
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.4274
    Episode_Reward/rotating_object: 109.6512
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.91s
                      Time elapsed: 00:13:52
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 812/1500 [0m                      

                       Computation: 116801 steps/s (collection: 0.746s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 115.8344
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.5739
                       Mean reward: 507.91
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 104.1674
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.84s
                      Time elapsed: 00:13:52
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 813/1500 [0m                      

                       Computation: 103413 steps/s (collection: 0.836s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 106.9761
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5743
                       Mean reward: 536.94
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.4214
    Episode_Reward/rotating_object: 105.0127
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.95s
                      Time elapsed: 00:13:53
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 814/1500 [0m                      

                       Computation: 111344 steps/s (collection: 0.771s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 101.9002
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5690
                       Mean reward: 501.76
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.4220
    Episode_Reward/rotating_object: 104.7844
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.88s
                      Time elapsed: 00:13:54
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 815/1500 [0m                      

                       Computation: 110458 steps/s (collection: 0.763s, learning 0.127s)
             Mean action noise std: 2.40
          Mean value_function loss: 108.4717
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5705
                       Mean reward: 532.33
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.4237
    Episode_Reward/rotating_object: 105.7412
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.89s
                      Time elapsed: 00:13:55
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 816/1500 [0m                      

                       Computation: 113668 steps/s (collection: 0.763s, learning 0.102s)
             Mean action noise std: 2.40
          Mean value_function loss: 103.9669
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.5765
                       Mean reward: 548.49
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 106.4372
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.86s
                      Time elapsed: 00:13:56
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 817/1500 [0m                      

                       Computation: 109112 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 2.40
          Mean value_function loss: 109.1242
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5764
                       Mean reward: 508.16
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.4253
    Episode_Reward/rotating_object: 108.3018
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.90s
                      Time elapsed: 00:13:57
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 818/1500 [0m                      

                       Computation: 108676 steps/s (collection: 0.795s, learning 0.110s)
             Mean action noise std: 2.40
          Mean value_function loss: 107.0429
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.5776
                       Mean reward: 532.95
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.4235
    Episode_Reward/rotating_object: 106.4842
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.90s
                      Time elapsed: 00:13:58
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 819/1500 [0m                      

                       Computation: 103054 steps/s (collection: 0.843s, learning 0.111s)
             Mean action noise std: 2.40
          Mean value_function loss: 102.6523
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.5799
                       Mean reward: 560.84
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.4229
    Episode_Reward/rotating_object: 105.8121
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.95s
                      Time elapsed: 00:13:59
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 820/1500 [0m                      

                       Computation: 115486 steps/s (collection: 0.748s, learning 0.104s)
             Mean action noise std: 2.40
          Mean value_function loss: 95.5708
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5837
                       Mean reward: 547.79
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 108.4942
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.85s
                      Time elapsed: 00:14:00
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 821/1500 [0m                      

                       Computation: 113578 steps/s (collection: 0.780s, learning 0.086s)
             Mean action noise std: 2.41
          Mean value_function loss: 91.8528
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5871
                       Mean reward: 534.25
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.4170
    Episode_Reward/rotating_object: 104.0896
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.87s
                      Time elapsed: 00:14:00
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 822/1500 [0m                      

                       Computation: 115873 steps/s (collection: 0.761s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 102.3920
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.5913
                       Mean reward: 547.14
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.4211
    Episode_Reward/rotating_object: 110.8526
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.85s
                      Time elapsed: 00:14:01
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 823/1500 [0m                      

                       Computation: 110311 steps/s (collection: 0.762s, learning 0.129s)
             Mean action noise std: 2.41
          Mean value_function loss: 88.9069
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6008
                       Mean reward: 561.08
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.4152
    Episode_Reward/rotating_object: 108.0781
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.89s
                      Time elapsed: 00:14:02
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 824/1500 [0m                      

                       Computation: 112316 steps/s (collection: 0.743s, learning 0.132s)
             Mean action noise std: 2.41
          Mean value_function loss: 102.5010
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.6090
                       Mean reward: 549.95
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.4088
    Episode_Reward/rotating_object: 106.9648
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.88s
                      Time elapsed: 00:14:03
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 825/1500 [0m                      

                       Computation: 118934 steps/s (collection: 0.739s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 101.2127
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.6111
                       Mean reward: 557.69
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 109.0796
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.83s
                      Time elapsed: 00:14:04
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 826/1500 [0m                      

                       Computation: 107765 steps/s (collection: 0.824s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 100.8892
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6121
                       Mean reward: 546.02
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.4221
    Episode_Reward/rotating_object: 109.9002
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.91s
                      Time elapsed: 00:14:05
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 827/1500 [0m                      

                       Computation: 112977 steps/s (collection: 0.778s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 99.3464
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6059
                       Mean reward: 534.44
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 108.9407
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.87s
                      Time elapsed: 00:14:06
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 828/1500 [0m                      

                       Computation: 115794 steps/s (collection: 0.764s, learning 0.085s)
             Mean action noise std: 2.42
          Mean value_function loss: 106.6039
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6102
                       Mean reward: 538.83
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.4204
    Episode_Reward/rotating_object: 109.0452
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.85s
                      Time elapsed: 00:14:06
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 829/1500 [0m                      

                       Computation: 115774 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 107.9237
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6137
                       Mean reward: 559.65
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.4144
    Episode_Reward/rotating_object: 107.5914
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.85s
                      Time elapsed: 00:14:07
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 830/1500 [0m                      

                       Computation: 115620 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 100.0365
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6109
                       Mean reward: 573.09
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.4194
    Episode_Reward/rotating_object: 109.0382
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.85s
                      Time elapsed: 00:14:08
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 831/1500 [0m                      

                       Computation: 113162 steps/s (collection: 0.742s, learning 0.127s)
             Mean action noise std: 2.42
          Mean value_function loss: 97.5733
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.6231
                       Mean reward: 547.74
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.4187
    Episode_Reward/rotating_object: 107.1434
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.87s
                      Time elapsed: 00:14:09
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 832/1500 [0m                      

                       Computation: 108932 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 2.43
          Mean value_function loss: 101.4219
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.6302
                       Mean reward: 540.77
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.4281
    Episode_Reward/rotating_object: 111.2170
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.90s
                      Time elapsed: 00:14:10
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 833/1500 [0m                      

                       Computation: 111297 steps/s (collection: 0.786s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 99.7493
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.6302
                       Mean reward: 556.19
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.4222
    Episode_Reward/rotating_object: 108.6432
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.88s
                      Time elapsed: 00:14:11
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 834/1500 [0m                      

                       Computation: 109001 steps/s (collection: 0.806s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 97.1120
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6249
                       Mean reward: 539.31
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 107.6304
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.90s
                      Time elapsed: 00:14:12
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 835/1500 [0m                      

                       Computation: 118112 steps/s (collection: 0.742s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 101.2651
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.6240
                       Mean reward: 587.76
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.4259
    Episode_Reward/rotating_object: 112.4815
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.83s
                      Time elapsed: 00:14:13
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 836/1500 [0m                      

                       Computation: 113915 steps/s (collection: 0.777s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 104.7450
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6266
                       Mean reward: 570.96
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4260
    Episode_Reward/rotating_object: 110.1938
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.86s
                      Time elapsed: 00:14:13
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 837/1500 [0m                      

                       Computation: 110544 steps/s (collection: 0.800s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 97.5333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.6363
                       Mean reward: 526.82
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 105.8044
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.89s
                      Time elapsed: 00:14:14
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 838/1500 [0m                      

                       Computation: 110458 steps/s (collection: 0.759s, learning 0.131s)
             Mean action noise std: 2.43
          Mean value_function loss: 108.5353
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6394
                       Mean reward: 520.55
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 108.3100
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.89s
                      Time elapsed: 00:14:15
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 839/1500 [0m                      

                       Computation: 110360 steps/s (collection: 0.773s, learning 0.118s)
             Mean action noise std: 2.44
          Mean value_function loss: 110.1483
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6404
                       Mean reward: 574.11
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.4298
    Episode_Reward/rotating_object: 110.1100
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.89s
                      Time elapsed: 00:14:16
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 840/1500 [0m                      

                       Computation: 107363 steps/s (collection: 0.759s, learning 0.157s)
             Mean action noise std: 2.44
          Mean value_function loss: 115.0720
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6429
                       Mean reward: 519.32
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.4187
    Episode_Reward/rotating_object: 109.2024
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.92s
                      Time elapsed: 00:14:17
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 841/1500 [0m                      

                       Computation: 110405 steps/s (collection: 0.766s, learning 0.124s)
             Mean action noise std: 2.44
          Mean value_function loss: 110.4087
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6452
                       Mean reward: 513.71
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 103.1302
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.89s
                      Time elapsed: 00:14:18
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 842/1500 [0m                      

                       Computation: 107136 steps/s (collection: 0.811s, learning 0.107s)
             Mean action noise std: 2.44
          Mean value_function loss: 109.1965
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6434
                       Mean reward: 524.04
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.4246
    Episode_Reward/rotating_object: 109.3378
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.92s
                      Time elapsed: 00:14:19
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 843/1500 [0m                      

                       Computation: 105327 steps/s (collection: 0.843s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 104.7059
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.6433
                       Mean reward: 545.30
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.4209
    Episode_Reward/rotating_object: 107.1909
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.93s
                      Time elapsed: 00:14:20
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 844/1500 [0m                      

                       Computation: 103678 steps/s (collection: 0.851s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 109.9073
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.6454
                       Mean reward: 536.54
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 108.5933
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.95s
                      Time elapsed: 00:14:21
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 845/1500 [0m                      

                       Computation: 108515 steps/s (collection: 0.795s, learning 0.111s)
             Mean action noise std: 2.45
          Mean value_function loss: 92.7775
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.6483
                       Mean reward: 580.63
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.4306
    Episode_Reward/rotating_object: 111.5749
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.91s
                      Time elapsed: 00:14:22
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 846/1500 [0m                      

                       Computation: 107537 steps/s (collection: 0.793s, learning 0.121s)
             Mean action noise std: 2.45
          Mean value_function loss: 101.5228
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 16.6558
                       Mean reward: 572.05
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.4309
    Episode_Reward/rotating_object: 111.7504
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.91s
                      Time elapsed: 00:14:23
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 847/1500 [0m                      

                       Computation: 96062 steps/s (collection: 0.854s, learning 0.170s)
             Mean action noise std: 2.45
          Mean value_function loss: 100.9661
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.6575
                       Mean reward: 531.43
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.4266
    Episode_Reward/rotating_object: 107.8840
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 1.02s
                      Time elapsed: 00:14:24
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 848/1500 [0m                      

                       Computation: 98033 steps/s (collection: 0.898s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 93.6101
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6685
                       Mean reward: 536.69
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.4255
    Episode_Reward/rotating_object: 108.0407
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 1.00s
                      Time elapsed: 00:14:25
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 849/1500 [0m                      

                       Computation: 110039 steps/s (collection: 0.804s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 96.9814
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.6820
                       Mean reward: 562.49
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 110.0425
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.89s
                      Time elapsed: 00:14:25
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 850/1500 [0m                      

                       Computation: 109769 steps/s (collection: 0.808s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 97.3032
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.6811
                       Mean reward: 533.37
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.4241
    Episode_Reward/rotating_object: 107.9097
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.90s
                      Time elapsed: 00:14:26
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 851/1500 [0m                      

                       Computation: 105278 steps/s (collection: 0.819s, learning 0.115s)
             Mean action noise std: 2.46
          Mean value_function loss: 94.3167
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.6833
                       Mean reward: 458.72
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 108.6786
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.93s
                      Time elapsed: 00:14:27
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 852/1500 [0m                      

                       Computation: 99722 steps/s (collection: 0.877s, learning 0.109s)
             Mean action noise std: 2.46
          Mean value_function loss: 99.1852
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.6857
                       Mean reward: 568.94
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.4260
    Episode_Reward/rotating_object: 110.4534
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.99s
                      Time elapsed: 00:14:28
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 853/1500 [0m                      

                       Computation: 107966 steps/s (collection: 0.812s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 94.8777
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.6948
                       Mean reward: 528.89
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 103.7271
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.91s
                      Time elapsed: 00:14:29
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 854/1500 [0m                      

                       Computation: 107397 steps/s (collection: 0.778s, learning 0.137s)
             Mean action noise std: 2.46
          Mean value_function loss: 87.7204
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.6991
                       Mean reward: 568.43
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.4253
    Episode_Reward/rotating_object: 111.0603
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.92s
                      Time elapsed: 00:14:30
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 855/1500 [0m                      

                       Computation: 108735 steps/s (collection: 0.776s, learning 0.128s)
             Mean action noise std: 2.46
          Mean value_function loss: 97.3375
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.6994
                       Mean reward: 533.17
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 106.4362
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.90s
                      Time elapsed: 00:14:31
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 856/1500 [0m                      

                       Computation: 115558 steps/s (collection: 0.760s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 93.4159
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.7008
                       Mean reward: 541.10
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.4312
    Episode_Reward/rotating_object: 109.0872
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.85s
                      Time elapsed: 00:14:32
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 857/1500 [0m                      

                       Computation: 115155 steps/s (collection: 0.756s, learning 0.098s)
             Mean action noise std: 2.47
          Mean value_function loss: 88.9784
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.7059
                       Mean reward: 501.93
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.4161
    Episode_Reward/rotating_object: 104.2591
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.85s
                      Time elapsed: 00:14:33
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 858/1500 [0m                      

                       Computation: 102693 steps/s (collection: 0.841s, learning 0.116s)
             Mean action noise std: 2.47
          Mean value_function loss: 97.6909
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7088
                       Mean reward: 525.68
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 109.7009
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.96s
                      Time elapsed: 00:14:34
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 859/1500 [0m                      

                       Computation: 108514 steps/s (collection: 0.790s, learning 0.116s)
             Mean action noise std: 2.48
          Mean value_function loss: 97.7528
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7175
                       Mean reward: 578.70
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.4287
    Episode_Reward/rotating_object: 108.7570
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.91s
                      Time elapsed: 00:14:35
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 860/1500 [0m                      

                       Computation: 107217 steps/s (collection: 0.783s, learning 0.134s)
             Mean action noise std: 2.48
          Mean value_function loss: 94.1527
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.7252
                       Mean reward: 557.56
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 110.5972
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.92s
                      Time elapsed: 00:14:35
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 861/1500 [0m                      

                       Computation: 115407 steps/s (collection: 0.755s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 107.1189
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.7277
                       Mean reward: 535.39
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.4263
    Episode_Reward/rotating_object: 109.4520
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.85s
                      Time elapsed: 00:14:36
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 862/1500 [0m                      

                       Computation: 107360 steps/s (collection: 0.758s, learning 0.157s)
             Mean action noise std: 2.48
          Mean value_function loss: 98.2586
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.7325
                       Mean reward: 583.88
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 112.7749
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.92s
                      Time elapsed: 00:14:37
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 863/1500 [0m                      

                       Computation: 117049 steps/s (collection: 0.754s, learning 0.086s)
             Mean action noise std: 2.48
          Mean value_function loss: 88.1005
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7369
                       Mean reward: 535.56
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.4194
    Episode_Reward/rotating_object: 106.4746
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.84s
                      Time elapsed: 00:14:38
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 864/1500 [0m                      

                       Computation: 117786 steps/s (collection: 0.746s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 84.8906
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7452
                       Mean reward: 541.74
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.4283
    Episode_Reward/rotating_object: 111.4743
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.83s
                      Time elapsed: 00:14:39
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 865/1500 [0m                      

                       Computation: 113918 steps/s (collection: 0.770s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 79.6811
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7557
                       Mean reward: 578.80
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.4304
    Episode_Reward/rotating_object: 113.0150
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.86s
                      Time elapsed: 00:14:40
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 866/1500 [0m                      

                       Computation: 115540 steps/s (collection: 0.757s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.3410
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.7686
                       Mean reward: 551.91
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 112.1693
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.85s
                      Time elapsed: 00:14:41
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 867/1500 [0m                      

                       Computation: 116877 steps/s (collection: 0.754s, learning 0.087s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.2115
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.7715
                       Mean reward: 549.25
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.4270
    Episode_Reward/rotating_object: 112.4725
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.84s
                      Time elapsed: 00:14:41
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 868/1500 [0m                      

                       Computation: 111646 steps/s (collection: 0.760s, learning 0.121s)
             Mean action noise std: 2.50
          Mean value_function loss: 84.4385
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.7806
                       Mean reward: 570.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4293
    Episode_Reward/rotating_object: 111.8058
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.88s
                      Time elapsed: 00:14:42
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 869/1500 [0m                      

                       Computation: 111623 steps/s (collection: 0.755s, learning 0.126s)
             Mean action noise std: 2.50
          Mean value_function loss: 95.5052
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.7871
                       Mean reward: 550.23
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.4170
    Episode_Reward/rotating_object: 107.8622
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.88s
                      Time elapsed: 00:14:43
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 870/1500 [0m                      

                       Computation: 107700 steps/s (collection: 0.803s, learning 0.110s)
             Mean action noise std: 2.50
          Mean value_function loss: 93.9246
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.7836
                       Mean reward: 573.80
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 111.3700
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.91s
                      Time elapsed: 00:14:44
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 871/1500 [0m                      

                       Computation: 108219 steps/s (collection: 0.763s, learning 0.146s)
             Mean action noise std: 2.50
          Mean value_function loss: 90.4429
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.7951
                       Mean reward: 578.64
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 0.4194
    Episode_Reward/rotating_object: 111.7459
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.91s
                      Time elapsed: 00:14:45
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 872/1500 [0m                      

                       Computation: 115689 steps/s (collection: 0.750s, learning 0.100s)
             Mean action noise std: 2.50
          Mean value_function loss: 98.8850
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.8002
                       Mean reward: 566.74
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.4266
    Episode_Reward/rotating_object: 115.8737
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.85s
                      Time elapsed: 00:14:46
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 873/1500 [0m                      

                       Computation: 108358 steps/s (collection: 0.787s, learning 0.121s)
             Mean action noise std: 2.51
          Mean value_function loss: 78.0548
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8041
                       Mean reward: 572.58
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 112.2854
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.91s
                      Time elapsed: 00:14:47
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 874/1500 [0m                      

                       Computation: 111872 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 2.51
          Mean value_function loss: 90.9958
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.8069
                       Mean reward: 550.57
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.4195
    Episode_Reward/rotating_object: 111.8962
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.88s
                      Time elapsed: 00:14:48
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 875/1500 [0m                      

                       Computation: 108548 steps/s (collection: 0.808s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 84.6140
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8102
                       Mean reward: 538.87
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.4200
    Episode_Reward/rotating_object: 112.1628
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.91s
                      Time elapsed: 00:14:49
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 876/1500 [0m                      

                       Computation: 116978 steps/s (collection: 0.743s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 92.0209
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8151
                       Mean reward: 550.09
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4199
    Episode_Reward/rotating_object: 109.9591
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.84s
                      Time elapsed: 00:14:49
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 877/1500 [0m                      

                       Computation: 111882 steps/s (collection: 0.757s, learning 0.122s)
             Mean action noise std: 2.51
          Mean value_function loss: 89.5518
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.8170
                       Mean reward: 552.05
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 112.1616
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.88s
                      Time elapsed: 00:14:50
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 878/1500 [0m                      

                       Computation: 114643 steps/s (collection: 0.754s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 99.0258
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.8225
                       Mean reward: 582.10
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 110.2641
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.86s
                      Time elapsed: 00:14:51
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 879/1500 [0m                      

                       Computation: 113561 steps/s (collection: 0.757s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 90.3403
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8257
                       Mean reward: 567.55
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.4221
    Episode_Reward/rotating_object: 111.0353
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.87s
                      Time elapsed: 00:14:52
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 880/1500 [0m                      

                       Computation: 107527 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 96.0293
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8337
                       Mean reward: 576.04
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 115.7771
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.91s
                      Time elapsed: 00:14:53
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 881/1500 [0m                      

                       Computation: 114970 steps/s (collection: 0.766s, learning 0.089s)
             Mean action noise std: 2.52
          Mean value_function loss: 94.9478
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.8322
                       Mean reward: 490.72
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.4186
    Episode_Reward/rotating_object: 106.2197
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.86s
                      Time elapsed: 00:14:54
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 882/1500 [0m                      

                       Computation: 112522 steps/s (collection: 0.789s, learning 0.085s)
             Mean action noise std: 2.52
          Mean value_function loss: 95.3016
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.8302
                       Mean reward: 571.23
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 112.4215
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.87s
                      Time elapsed: 00:14:55
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 883/1500 [0m                      

                       Computation: 113308 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 2.52
          Mean value_function loss: 82.0488
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.8256
                       Mean reward: 556.72
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4223
    Episode_Reward/rotating_object: 111.1003
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.87s
                      Time elapsed: 00:14:56
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 884/1500 [0m                      

                       Computation: 113303 steps/s (collection: 0.773s, learning 0.095s)
             Mean action noise std: 2.53
          Mean value_function loss: 93.0380
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8276
                       Mean reward: 556.20
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.4249
    Episode_Reward/rotating_object: 108.2075
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.87s
                      Time elapsed: 00:14:56
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 885/1500 [0m                      

                       Computation: 107398 steps/s (collection: 0.785s, learning 0.131s)
             Mean action noise std: 2.53
          Mean value_function loss: 79.0609
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8356
                       Mean reward: 561.67
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.4257
    Episode_Reward/rotating_object: 113.3657
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.92s
                      Time elapsed: 00:14:57
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 886/1500 [0m                      

                       Computation: 117114 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 2.53
          Mean value_function loss: 95.3601
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.8387
                       Mean reward: 557.67
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.4207
    Episode_Reward/rotating_object: 111.4096
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.84s
                      Time elapsed: 00:14:58
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 887/1500 [0m                      

                       Computation: 106689 steps/s (collection: 0.766s, learning 0.155s)
             Mean action noise std: 2.53
          Mean value_function loss: 90.8891
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.8354
                       Mean reward: 548.53
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.4203
    Episode_Reward/rotating_object: 108.7030
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.92s
                      Time elapsed: 00:14:59
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 888/1500 [0m                      

                       Computation: 109645 steps/s (collection: 0.805s, learning 0.092s)
             Mean action noise std: 2.53
          Mean value_function loss: 86.9020
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.8257
                       Mean reward: 558.34
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 112.4577
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.90s
                      Time elapsed: 00:15:00
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 889/1500 [0m                      

                       Computation: 112679 steps/s (collection: 0.773s, learning 0.099s)
             Mean action noise std: 2.53
          Mean value_function loss: 90.1713
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.8243
                       Mean reward: 552.78
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.4174
    Episode_Reward/rotating_object: 106.6305
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.87s
                      Time elapsed: 00:15:01
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 890/1500 [0m                      

                       Computation: 112944 steps/s (collection: 0.782s, learning 0.089s)
             Mean action noise std: 2.53
          Mean value_function loss: 92.3874
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.8252
                       Mean reward: 565.20
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.4285
    Episode_Reward/rotating_object: 113.8182
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.87s
                      Time elapsed: 00:15:02
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 891/1500 [0m                      

                       Computation: 116419 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 2.54
          Mean value_function loss: 94.6412
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.8315
                       Mean reward: 531.39
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.4222
    Episode_Reward/rotating_object: 111.5520
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.84s
                      Time elapsed: 00:15:03
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 892/1500 [0m                      

                       Computation: 113352 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 93.7187
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8372
                       Mean reward: 570.95
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 112.6658
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.87s
                      Time elapsed: 00:15:03
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 893/1500 [0m                      

                       Computation: 108811 steps/s (collection: 0.771s, learning 0.133s)
             Mean action noise std: 2.54
          Mean value_function loss: 95.1316
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.8472
                       Mean reward: 587.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4248
    Episode_Reward/rotating_object: 112.0692
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.90s
                      Time elapsed: 00:15:04
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 894/1500 [0m                      

                       Computation: 106787 steps/s (collection: 0.785s, learning 0.135s)
             Mean action noise std: 2.54
          Mean value_function loss: 84.7606
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.8601
                       Mean reward: 531.41
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.4303
    Episode_Reward/rotating_object: 110.9583
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.92s
                      Time elapsed: 00:15:05
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 895/1500 [0m                      

                       Computation: 113374 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 86.3892
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.8649
                       Mean reward: 568.74
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.4169
    Episode_Reward/rotating_object: 110.0108
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.87s
                      Time elapsed: 00:15:06
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 896/1500 [0m                      

                       Computation: 107912 steps/s (collection: 0.777s, learning 0.134s)
             Mean action noise std: 2.54
          Mean value_function loss: 85.3647
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.8671
                       Mean reward: 592.81
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.4263
    Episode_Reward/rotating_object: 113.6459
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.91s
                      Time elapsed: 00:15:07
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 897/1500 [0m                      

                       Computation: 115399 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 91.2734
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.8635
                       Mean reward: 532.28
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 110.8940
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.85s
                      Time elapsed: 00:15:08
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 898/1500 [0m                      

                       Computation: 109603 steps/s (collection: 0.773s, learning 0.124s)
             Mean action noise std: 2.54
          Mean value_function loss: 96.5071
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.8586
                       Mean reward: 564.85
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.4216
    Episode_Reward/rotating_object: 114.0236
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.90s
                      Time elapsed: 00:15:09
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 899/1500 [0m                      

                       Computation: 105938 steps/s (collection: 0.834s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 92.7338
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8492
                       Mean reward: 536.28
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.4195
    Episode_Reward/rotating_object: 111.8503
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.93s
                      Time elapsed: 00:15:10
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 900/1500 [0m                      

                       Computation: 115094 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 2.55
          Mean value_function loss: 90.8123
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.8502
                       Mean reward: 553.97
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 115.1777
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.85s
                      Time elapsed: 00:15:11
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 901/1500 [0m                      

                       Computation: 110381 steps/s (collection: 0.775s, learning 0.116s)
             Mean action noise std: 2.55
          Mean value_function loss: 88.6653
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.8475
                       Mean reward: 546.07
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.4166
    Episode_Reward/rotating_object: 110.8616
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.89s
                      Time elapsed: 00:15:11
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 902/1500 [0m                      

                       Computation: 109755 steps/s (collection: 0.785s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 88.0668
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8498
                       Mean reward: 587.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 113.3519
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.90s
                      Time elapsed: 00:15:12
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 903/1500 [0m                      

                       Computation: 110673 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 90.5881
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8549
                       Mean reward: 582.97
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.4189
    Episode_Reward/rotating_object: 111.7541
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.89s
                      Time elapsed: 00:15:13
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 904/1500 [0m                      

                       Computation: 112554 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 2.55
          Mean value_function loss: 88.3096
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.8709
                       Mean reward: 570.16
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.4194
    Episode_Reward/rotating_object: 114.8669
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.87s
                      Time elapsed: 00:15:14
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 905/1500 [0m                      

                       Computation: 108348 steps/s (collection: 0.767s, learning 0.140s)
             Mean action noise std: 2.56
          Mean value_function loss: 88.0770
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.8830
                       Mean reward: 540.29
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.4176
    Episode_Reward/rotating_object: 108.9634
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.91s
                      Time elapsed: 00:15:15
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 906/1500 [0m                      

                       Computation: 115533 steps/s (collection: 0.757s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 83.1900
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.8961
                       Mean reward: 546.93
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.4150
    Episode_Reward/rotating_object: 109.1370
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.85s
                      Time elapsed: 00:15:16
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 907/1500 [0m                      

                       Computation: 112028 steps/s (collection: 0.789s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 85.7674
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.9062
                       Mean reward: 559.85
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.4186
    Episode_Reward/rotating_object: 115.3938
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.88s
                      Time elapsed: 00:15:17
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 908/1500 [0m                      

                       Computation: 111545 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 2.56
          Mean value_function loss: 92.0064
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.9118
                       Mean reward: 608.01
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 0.4223
    Episode_Reward/rotating_object: 114.6026
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.88s
                      Time elapsed: 00:15:18
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 909/1500 [0m                      

                       Computation: 109638 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 2.56
          Mean value_function loss: 83.3481
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.9080
                       Mean reward: 571.71
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 112.0213
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.90s
                      Time elapsed: 00:15:19
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 910/1500 [0m                      

                       Computation: 116348 steps/s (collection: 0.750s, learning 0.095s)
             Mean action noise std: 2.56
          Mean value_function loss: 86.1151
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.9083
                       Mean reward: 603.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 113.7422
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.84s
                      Time elapsed: 00:15:19
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 911/1500 [0m                      

                       Computation: 106922 steps/s (collection: 0.791s, learning 0.128s)
             Mean action noise std: 2.56
          Mean value_function loss: 93.6895
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.9128
                       Mean reward: 523.29
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.4184
    Episode_Reward/rotating_object: 108.7616
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.92s
                      Time elapsed: 00:15:20
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 912/1500 [0m                      

                       Computation: 106405 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 2.57
          Mean value_function loss: 90.1597
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.9230
                       Mean reward: 570.43
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.4268
    Episode_Reward/rotating_object: 114.7321
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.92s
                      Time elapsed: 00:15:21
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 913/1500 [0m                      

                       Computation: 110348 steps/s (collection: 0.754s, learning 0.137s)
             Mean action noise std: 2.57
          Mean value_function loss: 97.8183
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.9370
                       Mean reward: 539.28
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.4209
    Episode_Reward/rotating_object: 112.8748
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.89s
                      Time elapsed: 00:15:22
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 914/1500 [0m                      

                       Computation: 96973 steps/s (collection: 0.852s, learning 0.162s)
             Mean action noise std: 2.57
          Mean value_function loss: 92.0775
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.9416
                       Mean reward: 605.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 114.0435
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 1.01s
                      Time elapsed: 00:15:23
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 915/1500 [0m                      

                       Computation: 95383 steps/s (collection: 0.871s, learning 0.160s)
             Mean action noise std: 2.57
          Mean value_function loss: 92.1059
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.9547
                       Mean reward: 590.15
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.4287
    Episode_Reward/rotating_object: 113.7480
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 1.03s
                      Time elapsed: 00:15:24
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 916/1500 [0m                      

                       Computation: 94472 steps/s (collection: 0.886s, learning 0.154s)
             Mean action noise std: 2.58
          Mean value_function loss: 92.0595
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.9610
                       Mean reward: 551.15
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.4224
    Episode_Reward/rotating_object: 111.8686
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 1.04s
                      Time elapsed: 00:15:25
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 917/1500 [0m                      

                       Computation: 97231 steps/s (collection: 0.877s, learning 0.134s)
             Mean action noise std: 2.58
          Mean value_function loss: 90.3872
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.9546
                       Mean reward: 570.38
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 113.6299
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 1.01s
                      Time elapsed: 00:15:26
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 918/1500 [0m                      

                       Computation: 97632 steps/s (collection: 0.807s, learning 0.200s)
             Mean action noise std: 2.58
          Mean value_function loss: 93.3602
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9507
                       Mean reward: 576.39
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.4266
    Episode_Reward/rotating_object: 114.8163
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 1.01s
                      Time elapsed: 00:15:27
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 919/1500 [0m                      

                       Computation: 99926 steps/s (collection: 0.868s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 84.7430
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.9518
                       Mean reward: 527.86
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 111.5439
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.98s
                      Time elapsed: 00:15:28
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 920/1500 [0m                      

                       Computation: 93425 steps/s (collection: 0.822s, learning 0.230s)
             Mean action noise std: 2.58
          Mean value_function loss: 88.9269
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.9520
                       Mean reward: 565.45
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.4324
    Episode_Reward/rotating_object: 113.2033
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 1.05s
                      Time elapsed: 00:15:29
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 921/1500 [0m                      

                       Computation: 95343 steps/s (collection: 0.919s, learning 0.112s)
             Mean action noise std: 2.58
          Mean value_function loss: 89.8316
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.9551
                       Mean reward: 539.31
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.4290
    Episode_Reward/rotating_object: 113.8560
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 1.03s
                      Time elapsed: 00:15:30
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 922/1500 [0m                      

                       Computation: 94423 steps/s (collection: 0.874s, learning 0.167s)
             Mean action noise std: 2.58
          Mean value_function loss: 101.0011
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.9540
                       Mean reward: 558.44
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.4333
    Episode_Reward/rotating_object: 112.6910
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 1.04s
                      Time elapsed: 00:15:31
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 923/1500 [0m                      

                       Computation: 92566 steps/s (collection: 0.899s, learning 0.163s)
             Mean action noise std: 2.59
          Mean value_function loss: 111.0453
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9520
                       Mean reward: 594.95
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.4362
    Episode_Reward/rotating_object: 115.8276
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 1.06s
                      Time elapsed: 00:15:32
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 924/1500 [0m                      

                       Computation: 94095 steps/s (collection: 0.927s, learning 0.118s)
             Mean action noise std: 2.59
          Mean value_function loss: 86.2742
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.9552
                       Mean reward: 565.69
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.4330
    Episode_Reward/rotating_object: 115.9209
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 1.04s
                      Time elapsed: 00:15:33
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 925/1500 [0m                      

                       Computation: 93527 steps/s (collection: 0.937s, learning 0.114s)
             Mean action noise std: 2.59
          Mean value_function loss: 96.3209
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.9579
                       Mean reward: 556.32
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.4303
    Episode_Reward/rotating_object: 113.3168
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 1.05s
                      Time elapsed: 00:15:34
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 926/1500 [0m                      

                       Computation: 106683 steps/s (collection: 0.816s, learning 0.106s)
             Mean action noise std: 2.59
          Mean value_function loss: 95.5928
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.9605
                       Mean reward: 572.63
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.4330
    Episode_Reward/rotating_object: 111.8549
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.92s
                      Time elapsed: 00:15:35
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 927/1500 [0m                      

                       Computation: 99977 steps/s (collection: 0.839s, learning 0.144s)
             Mean action noise std: 2.59
          Mean value_function loss: 94.4895
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9651
                       Mean reward: 616.06
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.4269
    Episode_Reward/rotating_object: 114.4813
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.98s
                      Time elapsed: 00:15:36
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 928/1500 [0m                      

                       Computation: 93698 steps/s (collection: 0.905s, learning 0.144s)
             Mean action noise std: 2.59
          Mean value_function loss: 95.5715
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9732
                       Mean reward: 590.09
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.4379
    Episode_Reward/rotating_object: 117.8366
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 1.05s
                      Time elapsed: 00:15:37
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 929/1500 [0m                      

                       Computation: 98459 steps/s (collection: 0.889s, learning 0.110s)
             Mean action noise std: 2.59
          Mean value_function loss: 102.2369
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.9712
                       Mean reward: 550.96
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.4384
    Episode_Reward/rotating_object: 114.9360
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 1.00s
                      Time elapsed: 00:15:38
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 930/1500 [0m                      

                       Computation: 111778 steps/s (collection: 0.765s, learning 0.115s)
             Mean action noise std: 2.59
          Mean value_function loss: 95.4520
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.9695
                       Mean reward: 574.80
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 107.8576
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.88s
                      Time elapsed: 00:15:39
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 931/1500 [0m                      

                       Computation: 85025 steps/s (collection: 0.945s, learning 0.211s)
             Mean action noise std: 2.59
          Mean value_function loss: 92.1232
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.9688
                       Mean reward: 566.82
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 113.3358
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 1.16s
                      Time elapsed: 00:15:40
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 932/1500 [0m                      

                       Computation: 95662 steps/s (collection: 0.879s, learning 0.149s)
             Mean action noise std: 2.59
          Mean value_function loss: 97.5977
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.9712
                       Mean reward: 580.52
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.4302
    Episode_Reward/rotating_object: 111.7236
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 1.03s
                      Time elapsed: 00:15:42
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 933/1500 [0m                      

                       Computation: 95002 steps/s (collection: 0.851s, learning 0.184s)
             Mean action noise std: 2.59
          Mean value_function loss: 89.3516
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.9679
                       Mean reward: 604.80
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.4381
    Episode_Reward/rotating_object: 114.5160
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 1.03s
                      Time elapsed: 00:15:43
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 934/1500 [0m                      

                       Computation: 86301 steps/s (collection: 0.949s, learning 0.190s)
             Mean action noise std: 2.60
          Mean value_function loss: 83.6623
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.9662
                       Mean reward: 575.46
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.4296
    Episode_Reward/rotating_object: 112.7828
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 1.14s
                      Time elapsed: 00:15:44
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 935/1500 [0m                      

                       Computation: 90743 steps/s (collection: 0.902s, learning 0.182s)
             Mean action noise std: 2.60
          Mean value_function loss: 84.3028
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.9691
                       Mean reward: 583.07
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4299
    Episode_Reward/rotating_object: 114.7044
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 1.08s
                      Time elapsed: 00:15:45
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 936/1500 [0m                      

                       Computation: 92660 steps/s (collection: 0.901s, learning 0.160s)
             Mean action noise std: 2.60
          Mean value_function loss: 88.3721
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9703
                       Mean reward: 558.86
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.4261
    Episode_Reward/rotating_object: 113.3809
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 1.06s
                      Time elapsed: 00:15:46
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 937/1500 [0m                      

                       Computation: 85446 steps/s (collection: 0.988s, learning 0.162s)
             Mean action noise std: 2.60
          Mean value_function loss: 84.7113
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9724
                       Mean reward: 593.41
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.4324
    Episode_Reward/rotating_object: 116.9714
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 1.15s
                      Time elapsed: 00:15:47
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 938/1500 [0m                      

                       Computation: 84187 steps/s (collection: 0.926s, learning 0.242s)
             Mean action noise std: 2.60
          Mean value_function loss: 86.3301
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.9814
                       Mean reward: 604.72
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.4278
    Episode_Reward/rotating_object: 115.3300
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 1.17s
                      Time elapsed: 00:15:48
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 939/1500 [0m                      

                       Computation: 90616 steps/s (collection: 0.891s, learning 0.194s)
             Mean action noise std: 2.61
          Mean value_function loss: 88.8601
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.9857
                       Mean reward: 605.15
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.4260
    Episode_Reward/rotating_object: 115.0990
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 1.08s
                      Time elapsed: 00:15:49
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 940/1500 [0m                      

                       Computation: 94830 steps/s (collection: 0.906s, learning 0.131s)
             Mean action noise std: 2.61
          Mean value_function loss: 89.8986
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.9905
                       Mean reward: 582.72
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.4282
    Episode_Reward/rotating_object: 114.7770
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 1.04s
                      Time elapsed: 00:15:50
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 941/1500 [0m                      

                       Computation: 87576 steps/s (collection: 0.916s, learning 0.206s)
             Mean action noise std: 2.61
          Mean value_function loss: 79.7692
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.9925
                       Mean reward: 550.18
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 112.7218
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 1.12s
                      Time elapsed: 00:15:51
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 942/1500 [0m                      

                       Computation: 92799 steps/s (collection: 0.896s, learning 0.163s)
             Mean action noise std: 2.61
          Mean value_function loss: 83.6132
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9920
                       Mean reward: 602.90
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.4278
    Episode_Reward/rotating_object: 114.2766
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 1.06s
                      Time elapsed: 00:15:52
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 943/1500 [0m                      

                       Computation: 96249 steps/s (collection: 0.880s, learning 0.142s)
             Mean action noise std: 2.61
          Mean value_function loss: 83.7068
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.9945
                       Mean reward: 608.38
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 116.1129
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 1.02s
                      Time elapsed: 00:15:53
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 944/1500 [0m                      

                       Computation: 99322 steps/s (collection: 0.881s, learning 0.108s)
             Mean action noise std: 2.62
          Mean value_function loss: 84.9500
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.0027
                       Mean reward: 574.12
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.4229
    Episode_Reward/rotating_object: 111.9397
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.99s
                      Time elapsed: 00:15:54
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 945/1500 [0m                      

                       Computation: 105931 steps/s (collection: 0.835s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 89.1643
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0026
                       Mean reward: 595.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4269
    Episode_Reward/rotating_object: 116.7053
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.93s
                      Time elapsed: 00:15:55
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 946/1500 [0m                      

                       Computation: 102318 steps/s (collection: 0.869s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 86.6999
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0137
                       Mean reward: 557.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4242
    Episode_Reward/rotating_object: 111.2332
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.96s
                      Time elapsed: 00:15:56
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 947/1500 [0m                      

                       Computation: 100258 steps/s (collection: 0.850s, learning 0.130s)
             Mean action noise std: 2.62
          Mean value_function loss: 80.7570
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.0321
                       Mean reward: 564.60
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 114.5621
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.98s
                      Time elapsed: 00:15:57
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 948/1500 [0m                      

                       Computation: 102521 steps/s (collection: 0.827s, learning 0.131s)
             Mean action noise std: 2.63
          Mean value_function loss: 74.2642
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.0451
                       Mean reward: 590.88
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.4260
    Episode_Reward/rotating_object: 116.7353
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.96s
                      Time elapsed: 00:15:58
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 949/1500 [0m                      

                       Computation: 110830 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 2.63
          Mean value_function loss: 80.8574
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0521
                       Mean reward: 579.07
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.4267
    Episode_Reward/rotating_object: 115.9481
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.89s
                      Time elapsed: 00:15:59
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 950/1500 [0m                      

                       Computation: 98481 steps/s (collection: 0.864s, learning 0.134s)
             Mean action noise std: 2.63
          Mean value_function loss: 85.1797
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.0553
                       Mean reward: 577.72
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 113.9108
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 1.00s
                      Time elapsed: 00:16:00
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 951/1500 [0m                      

                       Computation: 93371 steps/s (collection: 0.891s, learning 0.162s)
             Mean action noise std: 2.63
          Mean value_function loss: 86.9159
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0558
                       Mean reward: 541.09
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.4366
    Episode_Reward/rotating_object: 116.1595
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 1.05s
                      Time elapsed: 00:16:01
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 952/1500 [0m                      

                       Computation: 98835 steps/s (collection: 0.834s, learning 0.161s)
             Mean action noise std: 2.63
          Mean value_function loss: 88.7225
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.0625
                       Mean reward: 586.72
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.4346
    Episode_Reward/rotating_object: 118.7767
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.99s
                      Time elapsed: 00:16:02
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 953/1500 [0m                      

                       Computation: 103894 steps/s (collection: 0.833s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 85.6306
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.0695
                       Mean reward: 597.71
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4364
    Episode_Reward/rotating_object: 118.6759
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.95s
                      Time elapsed: 00:16:03
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 954/1500 [0m                      

                       Computation: 105613 steps/s (collection: 0.781s, learning 0.150s)
             Mean action noise std: 2.64
          Mean value_function loss: 83.6278
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0735
                       Mean reward: 547.12
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.4245
    Episode_Reward/rotating_object: 114.0028
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.93s
                      Time elapsed: 00:16:04
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 955/1500 [0m                      

                       Computation: 108276 steps/s (collection: 0.785s, learning 0.123s)
             Mean action noise std: 2.64
          Mean value_function loss: 87.1278
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.0823
                       Mean reward: 616.43
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 118.1298
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.91s
                      Time elapsed: 00:16:05
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 956/1500 [0m                      

                       Computation: 105878 steps/s (collection: 0.798s, learning 0.130s)
             Mean action noise std: 2.64
          Mean value_function loss: 91.4865
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.0909
                       Mean reward: 579.03
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 114.0275
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.93s
                      Time elapsed: 00:16:06
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 957/1500 [0m                      

                       Computation: 106943 steps/s (collection: 0.815s, learning 0.104s)
             Mean action noise std: 2.64
          Mean value_function loss: 84.4602
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.1009
                       Mean reward: 594.88
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 117.1563
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.92s
                      Time elapsed: 00:16:07
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 958/1500 [0m                      

                       Computation: 108753 steps/s (collection: 0.777s, learning 0.127s)
             Mean action noise std: 2.65
          Mean value_function loss: 84.8263
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1046
                       Mean reward: 587.78
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 115.7758
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.90s
                      Time elapsed: 00:16:08
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 959/1500 [0m                      

                       Computation: 105372 steps/s (collection: 0.824s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 80.5338
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.1050
                       Mean reward: 583.98
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.4279
    Episode_Reward/rotating_object: 116.4506
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.93s
                      Time elapsed: 00:16:09
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 960/1500 [0m                      

                       Computation: 110987 steps/s (collection: 0.794s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 83.0307
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1078
                       Mean reward: 548.96
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.4263
    Episode_Reward/rotating_object: 114.5633
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.89s
                      Time elapsed: 00:16:10
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 961/1500 [0m                      

                       Computation: 106017 steps/s (collection: 0.815s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 76.0308
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1138
                       Mean reward: 589.93
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.4254
    Episode_Reward/rotating_object: 115.9277
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.93s
                      Time elapsed: 00:16:10
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 962/1500 [0m                      

                       Computation: 111847 steps/s (collection: 0.791s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 69.8193
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.1218
                       Mean reward: 603.31
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.4257
    Episode_Reward/rotating_object: 117.3890
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.88s
                      Time elapsed: 00:16:11
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 963/1500 [0m                      

                       Computation: 102044 steps/s (collection: 0.796s, learning 0.167s)
             Mean action noise std: 2.66
          Mean value_function loss: 75.0336
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1356
                       Mean reward: 574.30
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 113.9510
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.96s
                      Time elapsed: 00:16:12
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 964/1500 [0m                      

                       Computation: 101280 steps/s (collection: 0.829s, learning 0.142s)
             Mean action noise std: 2.66
          Mean value_function loss: 85.1407
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1441
                       Mean reward: 584.52
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.4236
    Episode_Reward/rotating_object: 117.9070
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.97s
                      Time elapsed: 00:16:13
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 965/1500 [0m                      

                       Computation: 103414 steps/s (collection: 0.799s, learning 0.151s)
             Mean action noise std: 2.66
          Mean value_function loss: 84.4980
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1514
                       Mean reward: 570.69
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 116.6094
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.95s
                      Time elapsed: 00:16:14
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 966/1500 [0m                      

                       Computation: 103899 steps/s (collection: 0.822s, learning 0.124s)
             Mean action noise std: 2.66
          Mean value_function loss: 84.1464
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1511
                       Mean reward: 555.80
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.4233
    Episode_Reward/rotating_object: 115.6989
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.95s
                      Time elapsed: 00:16:15
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 967/1500 [0m                      

                       Computation: 105866 steps/s (collection: 0.780s, learning 0.149s)
             Mean action noise std: 2.66
          Mean value_function loss: 87.0588
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1502
                       Mean reward: 575.96
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.4212
    Episode_Reward/rotating_object: 118.0313
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.93s
                      Time elapsed: 00:16:16
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 968/1500 [0m                      

                       Computation: 106388 steps/s (collection: 0.786s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 83.3880
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1449
                       Mean reward: 597.25
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.4228
    Episode_Reward/rotating_object: 118.5777
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.92s
                      Time elapsed: 00:16:17
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 969/1500 [0m                      

                       Computation: 103215 steps/s (collection: 0.836s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 77.2208
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1474
                       Mean reward: 585.70
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4175
    Episode_Reward/rotating_object: 112.6590
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.95s
                      Time elapsed: 00:16:18
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 970/1500 [0m                      

                       Computation: 102435 steps/s (collection: 0.801s, learning 0.158s)
             Mean action noise std: 2.67
          Mean value_function loss: 90.7217
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1455
                       Mean reward: 605.06
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 119.9878
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.96s
                      Time elapsed: 00:16:19
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 971/1500 [0m                      

                       Computation: 107351 steps/s (collection: 0.792s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 85.9742
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.1524
                       Mean reward: 595.81
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.4253
    Episode_Reward/rotating_object: 118.6486
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.92s
                      Time elapsed: 00:16:20
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 972/1500 [0m                      

                       Computation: 116588 steps/s (collection: 0.754s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 67.9663
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.1683
                       Mean reward: 584.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4219
    Episode_Reward/rotating_object: 115.6656
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.84s
                      Time elapsed: 00:16:21
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 973/1500 [0m                      

                       Computation: 110096 steps/s (collection: 0.794s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 75.6705
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1756
                       Mean reward: 575.97
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.4259
    Episode_Reward/rotating_object: 117.1156
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.89s
                      Time elapsed: 00:16:22
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 974/1500 [0m                      

                       Computation: 98406 steps/s (collection: 0.857s, learning 0.142s)
             Mean action noise std: 2.68
          Mean value_function loss: 91.8989
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1847
                       Mean reward: 569.78
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.4216
    Episode_Reward/rotating_object: 114.7759
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 1.00s
                      Time elapsed: 00:16:23
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 975/1500 [0m                      

                       Computation: 96986 steps/s (collection: 0.889s, learning 0.124s)
             Mean action noise std: 2.68
          Mean value_function loss: 85.1285
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.1977
                       Mean reward: 567.68
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.4226
    Episode_Reward/rotating_object: 114.3098
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 1.01s
                      Time elapsed: 00:16:24
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 976/1500 [0m                      

                       Computation: 99746 steps/s (collection: 0.867s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 94.2364
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.2009
                       Mean reward: 561.12
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 117.6506
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.99s
                      Time elapsed: 00:16:25
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 977/1500 [0m                      

                       Computation: 105641 steps/s (collection: 0.822s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 81.1015
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.2094
                       Mean reward: 576.07
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.4151
    Episode_Reward/rotating_object: 113.8650
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.93s
                      Time elapsed: 00:16:26
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 978/1500 [0m                      

                       Computation: 106250 steps/s (collection: 0.826s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 77.2170
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.2137
                       Mean reward: 608.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4224
    Episode_Reward/rotating_object: 116.6972
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.93s
                      Time elapsed: 00:16:26
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 979/1500 [0m                      

                       Computation: 112334 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 79.2465
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.2168
                       Mean reward: 569.71
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4205
    Episode_Reward/rotating_object: 114.9202
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.88s
                      Time elapsed: 00:16:27
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 980/1500 [0m                      

                       Computation: 105416 steps/s (collection: 0.810s, learning 0.123s)
             Mean action noise std: 2.69
          Mean value_function loss: 81.2823
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.2157
                       Mean reward: 558.08
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.4246
    Episode_Reward/rotating_object: 113.9788
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.93s
                      Time elapsed: 00:16:28
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 981/1500 [0m                      

                       Computation: 109162 steps/s (collection: 0.790s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 77.7533
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2216
                       Mean reward: 605.97
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.4249
    Episode_Reward/rotating_object: 116.9910
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.90s
                      Time elapsed: 00:16:29
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 982/1500 [0m                      

                       Computation: 102627 steps/s (collection: 0.776s, learning 0.182s)
             Mean action noise std: 2.69
          Mean value_function loss: 75.2864
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.2315
                       Mean reward: 606.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4245
    Episode_Reward/rotating_object: 115.1257
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.96s
                      Time elapsed: 00:16:30
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 983/1500 [0m                      

                       Computation: 107800 steps/s (collection: 0.777s, learning 0.135s)
             Mean action noise std: 2.69
          Mean value_function loss: 78.2179
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2398
                       Mean reward: 568.64
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 115.0070
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.91s
                      Time elapsed: 00:16:31
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 984/1500 [0m                      

                       Computation: 105523 steps/s (collection: 0.807s, learning 0.125s)
             Mean action noise std: 2.70
          Mean value_function loss: 87.0571
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.2476
                       Mean reward: 616.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4268
    Episode_Reward/rotating_object: 118.7507
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.93s
                      Time elapsed: 00:16:32
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 985/1500 [0m                      

                       Computation: 107043 steps/s (collection: 0.801s, learning 0.117s)
             Mean action noise std: 2.70
          Mean value_function loss: 78.4701
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.2568
                       Mean reward: 548.01
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.4230
    Episode_Reward/rotating_object: 115.1513
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.92s
                      Time elapsed: 00:16:33
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 986/1500 [0m                      

                       Computation: 105607 steps/s (collection: 0.810s, learning 0.121s)
             Mean action noise std: 2.70
          Mean value_function loss: 71.5932
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2644
                       Mean reward: 590.69
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 120.3724
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.93s
                      Time elapsed: 00:16:34
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 987/1500 [0m                      

                       Computation: 108867 steps/s (collection: 0.779s, learning 0.124s)
             Mean action noise std: 2.70
          Mean value_function loss: 80.8391
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.2689
                       Mean reward: 547.78
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.4228
    Episode_Reward/rotating_object: 115.2101
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.90s
                      Time elapsed: 00:16:35
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 988/1500 [0m                      

                       Computation: 98202 steps/s (collection: 0.841s, learning 0.160s)
             Mean action noise std: 2.71
          Mean value_function loss: 78.4813
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.2823
                       Mean reward: 592.76
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.4238
    Episode_Reward/rotating_object: 117.4812
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 1.00s
                      Time elapsed: 00:16:36
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 989/1500 [0m                      

                       Computation: 104808 steps/s (collection: 0.813s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 89.5343
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2955
                       Mean reward: 604.20
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.4254
    Episode_Reward/rotating_object: 119.1747
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.94s
                      Time elapsed: 00:16:37
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 990/1500 [0m                      

                       Computation: 96001 steps/s (collection: 0.884s, learning 0.140s)
             Mean action noise std: 2.71
          Mean value_function loss: 82.5945
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.3065
                       Mean reward: 579.81
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.4248
    Episode_Reward/rotating_object: 115.9239
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 1.02s
                      Time elapsed: 00:16:38
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 991/1500 [0m                      

                       Computation: 112481 steps/s (collection: 0.787s, learning 0.087s)
             Mean action noise std: 2.72
          Mean value_function loss: 88.9032
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.3120
                       Mean reward: 597.09
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.4190
    Episode_Reward/rotating_object: 115.7971
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.87s
                      Time elapsed: 00:16:39
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 992/1500 [0m                      

                       Computation: 107938 steps/s (collection: 0.818s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 84.5078
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.3234
                       Mean reward: 593.49
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4246
    Episode_Reward/rotating_object: 117.8238
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.91s
                      Time elapsed: 00:16:39
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 993/1500 [0m                      

                       Computation: 110651 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 2.72
          Mean value_function loss: 82.4566
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3301
                       Mean reward: 605.33
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.4180
    Episode_Reward/rotating_object: 111.8843
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.89s
                      Time elapsed: 00:16:40
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 994/1500 [0m                      

                       Computation: 107412 steps/s (collection: 0.822s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 84.8207
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.3322
                       Mean reward: 608.40
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.4309
    Episode_Reward/rotating_object: 121.9951
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.92s
                      Time elapsed: 00:16:41
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 995/1500 [0m                      

                       Computation: 103755 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 87.8419
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.3291
                       Mean reward: 588.92
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 120.1107
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.95s
                      Time elapsed: 00:16:42
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 996/1500 [0m                      

                       Computation: 94326 steps/s (collection: 0.911s, learning 0.131s)
             Mean action noise std: 2.73
          Mean value_function loss: 75.0599
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.3261
                       Mean reward: 607.54
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 114.9663
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 1.04s
                      Time elapsed: 00:16:43
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 997/1500 [0m                      

                       Computation: 96243 steps/s (collection: 0.823s, learning 0.198s)
             Mean action noise std: 2.73
          Mean value_function loss: 82.4471
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3416
                       Mean reward: 600.41
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.4258
    Episode_Reward/rotating_object: 114.2377
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 1.02s
                      Time elapsed: 00:16:44
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 998/1500 [0m                      

                       Computation: 88033 steps/s (collection: 1.012s, learning 0.105s)
             Mean action noise std: 2.73
          Mean value_function loss: 87.7230
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3571
                       Mean reward: 576.86
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.4295
    Episode_Reward/rotating_object: 117.6967
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 1.12s
                      Time elapsed: 00:16:45
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 999/1500 [0m                      

                       Computation: 104473 steps/s (collection: 0.837s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 97.8649
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.3601
                       Mean reward: 569.22
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4181
    Episode_Reward/rotating_object: 111.4358
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.94s
                      Time elapsed: 00:16:46
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1000/1500 [0m                     

                       Computation: 34281 steps/s (collection: 2.741s, learning 0.127s)
             Mean action noise std: 2.74
          Mean value_function loss: 89.4435
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.3596
                       Mean reward: 593.75
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4289
    Episode_Reward/rotating_object: 115.6171
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.87s
                      Time elapsed: 00:16:49
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1001/1500 [0m                     

                       Computation: 32485 steps/s (collection: 2.870s, learning 0.156s)
             Mean action noise std: 2.74
          Mean value_function loss: 86.7228
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3582
                       Mean reward: 600.81
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 116.6403
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.03s
                      Time elapsed: 00:16:52
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1002/1500 [0m                     

                       Computation: 30409 steps/s (collection: 3.114s, learning 0.119s)
             Mean action noise std: 2.74
          Mean value_function loss: 92.7598
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3591
                       Mean reward: 553.91
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.4239
    Episode_Reward/rotating_object: 115.9085
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.23s
                      Time elapsed: 00:16:55
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1003/1500 [0m                     

                       Computation: 32628 steps/s (collection: 2.883s, learning 0.130s)
             Mean action noise std: 2.74
          Mean value_function loss: 84.8019
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.3645
                       Mean reward: 581.22
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.4318
    Episode_Reward/rotating_object: 115.9760
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.01s
                      Time elapsed: 00:16:58
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1004/1500 [0m                     

                       Computation: 31161 steps/s (collection: 3.018s, learning 0.137s)
             Mean action noise std: 2.75
          Mean value_function loss: 91.0004
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.3734
                       Mean reward: 569.80
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.4311
    Episode_Reward/rotating_object: 116.6463
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.15s
                      Time elapsed: 00:17:02
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1005/1500 [0m                     

                       Computation: 33490 steps/s (collection: 2.817s, learning 0.119s)
             Mean action noise std: 2.75
          Mean value_function loss: 89.5759
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.3759
                       Mean reward: 611.30
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.4365
    Episode_Reward/rotating_object: 118.2443
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 2.94s
                      Time elapsed: 00:17:05
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1006/1500 [0m                     

                       Computation: 32667 steps/s (collection: 2.892s, learning 0.117s)
             Mean action noise std: 2.75
          Mean value_function loss: 86.3022
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.3789
                       Mean reward: 591.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 116.2994
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.01s
                      Time elapsed: 00:17:08
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1007/1500 [0m                     

                       Computation: 32282 steps/s (collection: 2.925s, learning 0.120s)
             Mean action noise std: 2.75
          Mean value_function loss: 77.5037
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3892
                       Mean reward: 556.97
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 115.0166
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.05s
                      Time elapsed: 00:17:11
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1008/1500 [0m                     

                       Computation: 24951 steps/s (collection: 3.815s, learning 0.125s)
             Mean action noise std: 2.75
          Mean value_function loss: 88.2863
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3952
                       Mean reward: 612.88
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.4270
    Episode_Reward/rotating_object: 115.9647
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.94s
                      Time elapsed: 00:17:15
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1009/1500 [0m                     

                       Computation: 88123 steps/s (collection: 0.910s, learning 0.205s)
             Mean action noise std: 2.76
          Mean value_function loss: 86.8561
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3980
                       Mean reward: 613.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 115.9289
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 1.12s
                      Time elapsed: 00:17:16
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1010/1500 [0m                     

                       Computation: 85054 steps/s (collection: 1.014s, learning 0.142s)
             Mean action noise std: 2.76
          Mean value_function loss: 89.6793
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.3976
                       Mean reward: 578.50
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.4338
    Episode_Reward/rotating_object: 121.4918
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.16s
                      Time elapsed: 00:17:17
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1011/1500 [0m                     

                       Computation: 115514 steps/s (collection: 0.755s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 76.4025
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.3982
                       Mean reward: 585.57
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 113.7875
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.85s
                      Time elapsed: 00:17:18
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1012/1500 [0m                     

                       Computation: 109025 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 2.76
          Mean value_function loss: 81.2119
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3998
                       Mean reward: 597.68
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.4256
    Episode_Reward/rotating_object: 116.3250
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.90s
                      Time elapsed: 00:17:19
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1013/1500 [0m                     

                       Computation: 103220 steps/s (collection: 0.822s, learning 0.131s)
             Mean action noise std: 2.76
          Mean value_function loss: 76.4416
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.4122
                       Mean reward: 609.55
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.4217
    Episode_Reward/rotating_object: 114.2069
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.95s
                      Time elapsed: 00:17:20
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1014/1500 [0m                     

                       Computation: 106443 steps/s (collection: 0.801s, learning 0.122s)
             Mean action noise std: 2.76
          Mean value_function loss: 88.2810
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4082
                       Mean reward: 590.67
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.4286
    Episode_Reward/rotating_object: 118.7028
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.92s
                      Time elapsed: 00:17:20
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1015/1500 [0m                     

                       Computation: 102541 steps/s (collection: 0.837s, learning 0.122s)
             Mean action noise std: 2.77
          Mean value_function loss: 87.0109
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4096
                       Mean reward: 594.30
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 114.9278
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.96s
                      Time elapsed: 00:17:21
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1016/1500 [0m                     

                       Computation: 109106 steps/s (collection: 0.812s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 77.5138
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.4080
                       Mean reward: 567.18
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.4331
    Episode_Reward/rotating_object: 117.8265
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.90s
                      Time elapsed: 00:17:22
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1017/1500 [0m                     

                       Computation: 102947 steps/s (collection: 0.788s, learning 0.167s)
             Mean action noise std: 2.77
          Mean value_function loss: 79.9219
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.4132
                       Mean reward: 614.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4308
    Episode_Reward/rotating_object: 119.1556
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.95s
                      Time elapsed: 00:17:23
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1018/1500 [0m                     

                       Computation: 103473 steps/s (collection: 0.830s, learning 0.120s)
             Mean action noise std: 2.77
          Mean value_function loss: 67.3372
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4169
                       Mean reward: 597.12
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.4304
    Episode_Reward/rotating_object: 118.4986
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.95s
                      Time elapsed: 00:17:24
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1019/1500 [0m                     

                       Computation: 99338 steps/s (collection: 0.778s, learning 0.211s)
             Mean action noise std: 2.77
          Mean value_function loss: 82.4548
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4152
                       Mean reward: 598.50
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.4328
    Episode_Reward/rotating_object: 118.9519
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.99s
                      Time elapsed: 00:17:25
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1020/1500 [0m                     

                       Computation: 115380 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 76.9294
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4108
                       Mean reward: 538.01
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 117.4225
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.85s
                      Time elapsed: 00:17:26
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1021/1500 [0m                     

                       Computation: 109067 steps/s (collection: 0.788s, learning 0.114s)
             Mean action noise std: 2.77
          Mean value_function loss: 69.0245
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.4114
                       Mean reward: 574.02
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 118.1823
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.90s
                      Time elapsed: 00:17:27
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1022/1500 [0m                     

                       Computation: 108761 steps/s (collection: 0.795s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 79.5387
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4142
                       Mean reward: 562.46
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 0.4239
    Episode_Reward/rotating_object: 117.3321
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.90s
                      Time elapsed: 00:17:28
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1023/1500 [0m                     

                       Computation: 105607 steps/s (collection: 0.803s, learning 0.128s)
             Mean action noise std: 2.78
          Mean value_function loss: 78.2016
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4119
                       Mean reward: 567.24
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.4274
    Episode_Reward/rotating_object: 117.8521
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.93s
                      Time elapsed: 00:17:29
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1024/1500 [0m                     

                       Computation: 101520 steps/s (collection: 0.850s, learning 0.119s)
             Mean action noise std: 2.78
          Mean value_function loss: 69.3875
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4082
                       Mean reward: 598.44
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.4223
    Episode_Reward/rotating_object: 116.0326
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.97s
                      Time elapsed: 00:17:30
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1025/1500 [0m                     

                       Computation: 106449 steps/s (collection: 0.821s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 76.9533
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.4149
                       Mean reward: 595.42
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 122.1882
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.92s
                      Time elapsed: 00:17:31
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1026/1500 [0m                     

                       Computation: 98713 steps/s (collection: 0.811s, learning 0.185s)
             Mean action noise std: 2.78
          Mean value_function loss: 73.0884
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.4190
                       Mean reward: 594.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4282
    Episode_Reward/rotating_object: 117.2068
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.00s
                      Time elapsed: 00:17:32
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1027/1500 [0m                     

                       Computation: 102886 steps/s (collection: 0.831s, learning 0.124s)
             Mean action noise std: 2.79
          Mean value_function loss: 74.4411
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4289
                       Mean reward: 601.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 120.7558
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.96s
                      Time elapsed: 00:17:33
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1028/1500 [0m                     

                       Computation: 108569 steps/s (collection: 0.808s, learning 0.097s)
             Mean action noise std: 2.79
          Mean value_function loss: 83.3462
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.4394
                       Mean reward: 623.11
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 119.8782
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.91s
                      Time elapsed: 00:17:34
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1029/1500 [0m                     

                       Computation: 105069 steps/s (collection: 0.824s, learning 0.111s)
             Mean action noise std: 2.79
          Mean value_function loss: 84.5903
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.4436
                       Mean reward: 599.05
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.4346
    Episode_Reward/rotating_object: 119.5019
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.94s
                      Time elapsed: 00:17:35
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1030/1500 [0m                     

                       Computation: 110710 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 2.79
          Mean value_function loss: 82.5407
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.4464
                       Mean reward: 625.00
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.4340
    Episode_Reward/rotating_object: 121.1723
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.89s
                      Time elapsed: 00:17:35
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1031/1500 [0m                     

                       Computation: 103654 steps/s (collection: 0.793s, learning 0.156s)
             Mean action noise std: 2.79
          Mean value_function loss: 77.1383
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4475
                       Mean reward: 552.50
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 116.5765
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.95s
                      Time elapsed: 00:17:36
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1032/1500 [0m                     

                       Computation: 107956 steps/s (collection: 0.780s, learning 0.131s)
             Mean action noise std: 2.79
          Mean value_function loss: 79.2915
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.4472
                       Mean reward: 569.69
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.4290
    Episode_Reward/rotating_object: 118.1052
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.91s
                      Time elapsed: 00:17:37
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1033/1500 [0m                     

                       Computation: 100975 steps/s (collection: 0.785s, learning 0.189s)
             Mean action noise std: 2.79
          Mean value_function loss: 82.9513
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.4486
                       Mean reward: 615.97
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.4375
    Episode_Reward/rotating_object: 121.5940
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.97s
                      Time elapsed: 00:17:38
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1034/1500 [0m                     

                       Computation: 105479 steps/s (collection: 0.806s, learning 0.126s)
             Mean action noise std: 2.80
          Mean value_function loss: 77.8590
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.4565
                       Mean reward: 591.00
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.4345
    Episode_Reward/rotating_object: 118.6067
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.93s
                      Time elapsed: 00:17:39
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1035/1500 [0m                     

                       Computation: 106629 steps/s (collection: 0.765s, learning 0.157s)
             Mean action noise std: 2.80
          Mean value_function loss: 80.0194
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.4626
                       Mean reward: 599.41
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.4330
    Episode_Reward/rotating_object: 117.8389
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.92s
                      Time elapsed: 00:17:40
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1036/1500 [0m                     

                       Computation: 108089 steps/s (collection: 0.808s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 83.1699
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4649
                       Mean reward: 619.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4385
    Episode_Reward/rotating_object: 119.0407
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.91s
                      Time elapsed: 00:17:41
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1037/1500 [0m                     

                       Computation: 115989 steps/s (collection: 0.753s, learning 0.095s)
             Mean action noise std: 2.80
          Mean value_function loss: 77.3271
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4657
                       Mean reward: 585.85
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.4351
    Episode_Reward/rotating_object: 119.3293
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.85s
                      Time elapsed: 00:17:42
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1038/1500 [0m                     

                       Computation: 107909 steps/s (collection: 0.792s, learning 0.119s)
             Mean action noise std: 2.80
          Mean value_function loss: 62.8410
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4680
                       Mean reward: 593.81
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.4328
    Episode_Reward/rotating_object: 117.8254
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.91s
                      Time elapsed: 00:17:43
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1039/1500 [0m                     

                       Computation: 112096 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 2.80
          Mean value_function loss: 83.4404
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.4717
                       Mean reward: 612.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4366
    Episode_Reward/rotating_object: 119.5234
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.88s
                      Time elapsed: 00:17:44
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1040/1500 [0m                     

                       Computation: 108928 steps/s (collection: 0.781s, learning 0.122s)
             Mean action noise std: 2.81
          Mean value_function loss: 74.3063
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.4746
                       Mean reward: 607.67
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.4416
    Episode_Reward/rotating_object: 119.6009
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.90s
                      Time elapsed: 00:17:45
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1041/1500 [0m                     

                       Computation: 111346 steps/s (collection: 0.777s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 74.9076
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4759
                       Mean reward: 634.18
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.4356
    Episode_Reward/rotating_object: 121.7238
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.88s
                      Time elapsed: 00:17:45
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1042/1500 [0m                     

                       Computation: 104977 steps/s (collection: 0.794s, learning 0.142s)
             Mean action noise std: 2.81
          Mean value_function loss: 75.3198
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 17.4753
                       Mean reward: 608.91
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.4363
    Episode_Reward/rotating_object: 120.7531
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.94s
                      Time elapsed: 00:17:46
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1043/1500 [0m                     

                       Computation: 106694 steps/s (collection: 0.799s, learning 0.123s)
             Mean action noise std: 2.81
          Mean value_function loss: 76.0287
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.4723
                       Mean reward: 569.48
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.4278
    Episode_Reward/rotating_object: 119.1036
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.92s
                      Time elapsed: 00:17:47
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1044/1500 [0m                     

                       Computation: 101912 steps/s (collection: 0.816s, learning 0.149s)
             Mean action noise std: 2.81
          Mean value_function loss: 79.7427
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4797
                       Mean reward: 605.33
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 119.6921
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.96s
                      Time elapsed: 00:17:48
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1045/1500 [0m                     

                       Computation: 103603 steps/s (collection: 0.786s, learning 0.163s)
             Mean action noise std: 2.81
          Mean value_function loss: 77.6948
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4893
                       Mean reward: 614.62
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.4333
    Episode_Reward/rotating_object: 121.1478
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.95s
                      Time elapsed: 00:17:49
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1046/1500 [0m                     

                       Computation: 110934 steps/s (collection: 0.788s, learning 0.099s)
             Mean action noise std: 2.81
          Mean value_function loss: 73.2086
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4905
                       Mean reward: 614.00
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.4254
    Episode_Reward/rotating_object: 120.6275
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.89s
                      Time elapsed: 00:17:50
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1047/1500 [0m                     

                       Computation: 107970 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 72.6416
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.4894
                       Mean reward: 579.26
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.4317
    Episode_Reward/rotating_object: 121.0578
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.91s
                      Time elapsed: 00:17:51
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1048/1500 [0m                     

                       Computation: 96710 steps/s (collection: 0.865s, learning 0.152s)
             Mean action noise std: 2.82
          Mean value_function loss: 71.4865
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.4925
                       Mean reward: 608.71
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.4318
    Episode_Reward/rotating_object: 120.2857
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.02s
                      Time elapsed: 00:17:52
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1049/1500 [0m                     

                       Computation: 104827 steps/s (collection: 0.824s, learning 0.114s)
             Mean action noise std: 2.82
          Mean value_function loss: 77.8847
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.4935
                       Mean reward: 588.56
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.4331
    Episode_Reward/rotating_object: 120.4935
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.94s
                      Time elapsed: 00:17:53
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1050/1500 [0m                     

                       Computation: 105484 steps/s (collection: 0.828s, learning 0.104s)
             Mean action noise std: 2.82
          Mean value_function loss: 73.3419
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4931
                       Mean reward: 579.13
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.4292
    Episode_Reward/rotating_object: 119.9026
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.93s
                      Time elapsed: 00:17:54
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1051/1500 [0m                     

                       Computation: 104875 steps/s (collection: 0.818s, learning 0.119s)
             Mean action noise std: 2.82
          Mean value_function loss: 72.9349
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.4894
                       Mean reward: 605.73
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.4266
    Episode_Reward/rotating_object: 118.6264
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.94s
                      Time elapsed: 00:17:55
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1052/1500 [0m                     

                       Computation: 104180 steps/s (collection: 0.832s, learning 0.112s)
             Mean action noise std: 2.82
          Mean value_function loss: 69.7821
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.4828
                       Mean reward: 577.13
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 120.8875
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.94s
                      Time elapsed: 00:17:56
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1053/1500 [0m                     

                       Computation: 110264 steps/s (collection: 0.799s, learning 0.092s)
             Mean action noise std: 2.82
          Mean value_function loss: 82.1311
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.4865
                       Mean reward: 600.50
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.4317
    Episode_Reward/rotating_object: 117.7629
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.89s
                      Time elapsed: 00:17:57
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1054/1500 [0m                     

                       Computation: 110115 steps/s (collection: 0.775s, learning 0.117s)
             Mean action noise std: 2.82
          Mean value_function loss: 76.7285
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4900
                       Mean reward: 622.94
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.4298
    Episode_Reward/rotating_object: 121.0528
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.89s
                      Time elapsed: 00:17:58
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1055/1500 [0m                     

                       Computation: 107380 steps/s (collection: 0.813s, learning 0.102s)
             Mean action noise std: 2.83
          Mean value_function loss: 80.0505
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.4979
                       Mean reward: 590.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4338
    Episode_Reward/rotating_object: 119.9693
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.92s
                      Time elapsed: 00:17:58
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1056/1500 [0m                     

                       Computation: 96797 steps/s (collection: 0.903s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 84.4500
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.5139
                       Mean reward: 537.84
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.4244
    Episode_Reward/rotating_object: 114.3777
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 1.02s
                      Time elapsed: 00:17:59
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1057/1500 [0m                     

                       Computation: 99717 steps/s (collection: 0.837s, learning 0.149s)
             Mean action noise std: 2.83
          Mean value_function loss: 81.5806
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.5169
                       Mean reward: 586.73
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4334
    Episode_Reward/rotating_object: 117.8903
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.99s
                      Time elapsed: 00:18:00
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1058/1500 [0m                     

                       Computation: 110836 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 2.83
          Mean value_function loss: 74.7368
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.5142
                       Mean reward: 588.96
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.4352
    Episode_Reward/rotating_object: 119.8448
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.89s
                      Time elapsed: 00:18:01
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1059/1500 [0m                     

                       Computation: 102048 steps/s (collection: 0.830s, learning 0.133s)
             Mean action noise std: 2.83
          Mean value_function loss: 74.6646
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5155
                       Mean reward: 603.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4363
    Episode_Reward/rotating_object: 118.3964
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.96s
                      Time elapsed: 00:18:02
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1060/1500 [0m                     

                       Computation: 108892 steps/s (collection: 0.786s, learning 0.117s)
             Mean action noise std: 2.84
          Mean value_function loss: 74.0091
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.5204
                       Mean reward: 615.40
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.4377
    Episode_Reward/rotating_object: 122.6993
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.90s
                      Time elapsed: 00:18:03
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1061/1500 [0m                     

                       Computation: 110182 steps/s (collection: 0.774s, learning 0.118s)
             Mean action noise std: 2.84
          Mean value_function loss: 77.1452
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5222
                       Mean reward: 576.49
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.4300
    Episode_Reward/rotating_object: 117.4328
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.89s
                      Time elapsed: 00:18:04
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1062/1500 [0m                     

                       Computation: 114769 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 2.84
          Mean value_function loss: 74.7280
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5295
                       Mean reward: 602.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4344
    Episode_Reward/rotating_object: 118.9169
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.86s
                      Time elapsed: 00:18:05
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1063/1500 [0m                     

                       Computation: 110812 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 2.84
          Mean value_function loss: 73.8768
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.5352
                       Mean reward: 592.43
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.4332
    Episode_Reward/rotating_object: 119.0374
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.89s
                      Time elapsed: 00:18:06
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1064/1500 [0m                     

                       Computation: 109931 steps/s (collection: 0.792s, learning 0.103s)
             Mean action noise std: 2.84
          Mean value_function loss: 71.1351
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.5392
                       Mean reward: 569.76
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.4353
    Episode_Reward/rotating_object: 118.0739
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.89s
                      Time elapsed: 00:18:07
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1065/1500 [0m                     

                       Computation: 113572 steps/s (collection: 0.777s, learning 0.089s)
             Mean action noise std: 2.84
          Mean value_function loss: 74.4118
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5349
                       Mean reward: 607.33
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.4360
    Episode_Reward/rotating_object: 120.6654
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.87s
                      Time elapsed: 00:18:08
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1066/1500 [0m                     

                       Computation: 112802 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 2.85
          Mean value_function loss: 64.3379
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5372
                       Mean reward: 579.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4356
    Episode_Reward/rotating_object: 120.4368
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.87s
                      Time elapsed: 00:18:08
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1067/1500 [0m                     

                       Computation: 107447 steps/s (collection: 0.798s, learning 0.117s)
             Mean action noise std: 2.85
          Mean value_function loss: 79.1558
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.5463
                       Mean reward: 650.98
               Mean episode length: 249.82
    Episode_Reward/reaching_object: 0.4361
    Episode_Reward/rotating_object: 119.6247
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.91s
                      Time elapsed: 00:18:09
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1068/1500 [0m                     

                       Computation: 112638 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 2.85
          Mean value_function loss: 72.8938
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.5476
                       Mean reward: 629.62
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.4334
    Episode_Reward/rotating_object: 121.8192
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.87s
                      Time elapsed: 00:18:10
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1069/1500 [0m                     

                       Computation: 101629 steps/s (collection: 0.771s, learning 0.196s)
             Mean action noise std: 2.85
          Mean value_function loss: 72.4104
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.5569
                       Mean reward: 562.16
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.4308
    Episode_Reward/rotating_object: 117.9030
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.97s
                      Time elapsed: 00:18:11
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1070/1500 [0m                     

                       Computation: 111698 steps/s (collection: 0.767s, learning 0.113s)
             Mean action noise std: 2.86
          Mean value_function loss: 73.1428
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.5584
                       Mean reward: 609.50
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.4257
    Episode_Reward/rotating_object: 120.4113
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.88s
                      Time elapsed: 00:18:12
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1071/1500 [0m                     

                       Computation: 110456 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 2.86
          Mean value_function loss: 70.4844
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.5642
                       Mean reward: 611.43
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.4378
    Episode_Reward/rotating_object: 122.6241
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.89s
                      Time elapsed: 00:18:13
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1072/1500 [0m                     

                       Computation: 103369 steps/s (collection: 0.810s, learning 0.141s)
             Mean action noise std: 2.86
          Mean value_function loss: 71.2480
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.5714
                       Mean reward: 588.84
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.4287
    Episode_Reward/rotating_object: 118.3521
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.95s
                      Time elapsed: 00:18:14
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1073/1500 [0m                     

                       Computation: 108249 steps/s (collection: 0.815s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 78.0364
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.5809
                       Mean reward: 614.71
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 118.0077
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.91s
                      Time elapsed: 00:18:15
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1074/1500 [0m                     

                       Computation: 97620 steps/s (collection: 0.893s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 76.6669
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.5875
                       Mean reward: 618.86
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 0.4296
    Episode_Reward/rotating_object: 120.8919
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.01s
                      Time elapsed: 00:18:16
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1075/1500 [0m                     

                       Computation: 102127 steps/s (collection: 0.848s, learning 0.115s)
             Mean action noise std: 2.87
          Mean value_function loss: 73.3332
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 17.5948
                       Mean reward: 567.87
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 118.8495
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.96s
                      Time elapsed: 00:18:17
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1076/1500 [0m                     

                       Computation: 106525 steps/s (collection: 0.832s, learning 0.091s)
             Mean action noise std: 2.87
          Mean value_function loss: 74.6910
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.5944
                       Mean reward: 544.53
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.4235
    Episode_Reward/rotating_object: 116.0389
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.92s
                      Time elapsed: 00:18:18
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1077/1500 [0m                     

                       Computation: 108738 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 2.87
          Mean value_function loss: 74.6113
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.5884
                       Mean reward: 598.79
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.4281
    Episode_Reward/rotating_object: 119.7963
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.90s
                      Time elapsed: 00:18:19
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1078/1500 [0m                     

                       Computation: 104977 steps/s (collection: 0.819s, learning 0.117s)
             Mean action noise std: 2.87
          Mean value_function loss: 82.0486
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5876
                       Mean reward: 574.79
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 120.4137
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.94s
                      Time elapsed: 00:18:20
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1079/1500 [0m                     

                       Computation: 107274 steps/s (collection: 0.802s, learning 0.115s)
             Mean action noise std: 2.87
          Mean value_function loss: 77.3766
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.5889
                       Mean reward: 590.82
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.4312
    Episode_Reward/rotating_object: 118.0554
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.92s
                      Time elapsed: 00:18:20
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1080/1500 [0m                     

                       Computation: 111924 steps/s (collection: 0.762s, learning 0.117s)
             Mean action noise std: 2.87
          Mean value_function loss: 58.7384
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.5937
                       Mean reward: 580.01
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 113.1317
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.88s
                      Time elapsed: 00:18:21
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1081/1500 [0m                     

                       Computation: 105818 steps/s (collection: 0.781s, learning 0.148s)
             Mean action noise std: 2.87
          Mean value_function loss: 66.1008
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6026
                       Mean reward: 601.18
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.4220
    Episode_Reward/rotating_object: 118.4975
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.93s
                      Time elapsed: 00:18:22
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1082/1500 [0m                     

                       Computation: 108066 steps/s (collection: 0.784s, learning 0.126s)
             Mean action noise std: 2.87
          Mean value_function loss: 70.9813
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6078
                       Mean reward: 604.66
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.4239
    Episode_Reward/rotating_object: 117.3553
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.91s
                      Time elapsed: 00:18:23
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1083/1500 [0m                     

                       Computation: 103055 steps/s (collection: 0.798s, learning 0.156s)
             Mean action noise std: 2.88
          Mean value_function loss: 69.4719
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.6052
                       Mean reward: 629.49
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 118.8200
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.95s
                      Time elapsed: 00:18:24
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1084/1500 [0m                     

                       Computation: 104951 steps/s (collection: 0.775s, learning 0.161s)
             Mean action noise std: 2.88
          Mean value_function loss: 67.0076
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6064
                       Mean reward: 607.85
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.4239
    Episode_Reward/rotating_object: 118.6988
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.94s
                      Time elapsed: 00:18:25
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1085/1500 [0m                     

                       Computation: 110599 steps/s (collection: 0.760s, learning 0.129s)
             Mean action noise std: 2.88
          Mean value_function loss: 69.0592
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6110
                       Mean reward: 575.42
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 117.9271
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.89s
                      Time elapsed: 00:18:26
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1086/1500 [0m                     

                       Computation: 106577 steps/s (collection: 0.810s, learning 0.113s)
             Mean action noise std: 2.88
          Mean value_function loss: 66.1171
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.6129
                       Mean reward: 582.91
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.4300
    Episode_Reward/rotating_object: 120.3515
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.92s
                      Time elapsed: 00:18:27
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1087/1500 [0m                     

                       Computation: 112088 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 2.88
          Mean value_function loss: 67.6337
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.6118
                       Mean reward: 619.44
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.4261
    Episode_Reward/rotating_object: 117.7505
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.88s
                      Time elapsed: 00:18:28
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1088/1500 [0m                     

                       Computation: 111317 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 2.88
          Mean value_function loss: 73.5031
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6107
                       Mean reward: 616.70
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 0.4302
    Episode_Reward/rotating_object: 121.4266
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.88s
                      Time elapsed: 00:18:29
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1089/1500 [0m                     

                       Computation: 110673 steps/s (collection: 0.792s, learning 0.096s)
             Mean action noise std: 2.88
          Mean value_function loss: 69.9147
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.6146
                       Mean reward: 606.19
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.4306
    Episode_Reward/rotating_object: 120.6568
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.89s
                      Time elapsed: 00:18:30
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1090/1500 [0m                     

                       Computation: 112250 steps/s (collection: 0.777s, learning 0.099s)
             Mean action noise std: 2.88
          Mean value_function loss: 71.2395
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.6148
                       Mean reward: 560.15
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.4298
    Episode_Reward/rotating_object: 116.6165
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.88s
                      Time elapsed: 00:18:30
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1091/1500 [0m                     

                       Computation: 101695 steps/s (collection: 0.838s, learning 0.129s)
             Mean action noise std: 2.89
          Mean value_function loss: 74.1963
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.6176
                       Mean reward: 619.98
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.4356
    Episode_Reward/rotating_object: 123.5287
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.97s
                      Time elapsed: 00:18:31
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1092/1500 [0m                     

                       Computation: 110884 steps/s (collection: 0.794s, learning 0.093s)
             Mean action noise std: 2.89
          Mean value_function loss: 59.9341
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.6233
                       Mean reward: 593.24
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.4270
    Episode_Reward/rotating_object: 117.3551
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.89s
                      Time elapsed: 00:18:32
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1093/1500 [0m                     

                       Computation: 112521 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 66.8727
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.6246
                       Mean reward: 600.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4316
    Episode_Reward/rotating_object: 123.0582
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.87s
                      Time elapsed: 00:18:33
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1094/1500 [0m                     

                       Computation: 106079 steps/s (collection: 0.802s, learning 0.125s)
             Mean action noise std: 2.89
          Mean value_function loss: 59.6989
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6298
                       Mean reward: 620.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4324
    Episode_Reward/rotating_object: 121.6724
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.93s
                      Time elapsed: 00:18:34
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1095/1500 [0m                     

                       Computation: 112745 steps/s (collection: 0.771s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 72.9542
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.6385
                       Mean reward: 627.63
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.4329
    Episode_Reward/rotating_object: 123.5811
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.87s
                      Time elapsed: 00:18:35
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1096/1500 [0m                     

                       Computation: 108272 steps/s (collection: 0.815s, learning 0.093s)
             Mean action noise std: 2.89
          Mean value_function loss: 74.8494
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.6389
                       Mean reward: 588.13
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 120.6696
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.91s
                      Time elapsed: 00:18:36
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1097/1500 [0m                     

                       Computation: 113225 steps/s (collection: 0.769s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 69.2583
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.6449
                       Mean reward: 620.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4349
    Episode_Reward/rotating_object: 123.5575
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.87s
                      Time elapsed: 00:18:37
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1098/1500 [0m                     

                       Computation: 103752 steps/s (collection: 0.808s, learning 0.140s)
             Mean action noise std: 2.90
          Mean value_function loss: 69.7260
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6504
                       Mean reward: 621.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4310
    Episode_Reward/rotating_object: 123.1927
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.95s
                      Time elapsed: 00:18:38
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1099/1500 [0m                     

                       Computation: 110365 steps/s (collection: 0.783s, learning 0.108s)
             Mean action noise std: 2.90
          Mean value_function loss: 84.4663
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.6497
                       Mean reward: 638.91
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 0.4329
    Episode_Reward/rotating_object: 121.9391
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.89s
                      Time elapsed: 00:18:39
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1100/1500 [0m                     

                       Computation: 106576 steps/s (collection: 0.809s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 70.3284
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.6537
                       Mean reward: 603.24
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 121.0721
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.92s
                      Time elapsed: 00:18:40
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1101/1500 [0m                     

                       Computation: 114916 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 2.90
          Mean value_function loss: 68.1098
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6593
                       Mean reward: 610.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4349
    Episode_Reward/rotating_object: 123.5300
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.86s
                      Time elapsed: 00:18:40
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1102/1500 [0m                     

                       Computation: 112165 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 2.90
          Mean value_function loss: 69.4389
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.6583
                       Mean reward: 602.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4333
    Episode_Reward/rotating_object: 119.5211
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.88s
                      Time elapsed: 00:18:41
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1103/1500 [0m                     

                       Computation: 112246 steps/s (collection: 0.780s, learning 0.096s)
             Mean action noise std: 2.90
          Mean value_function loss: 90.3037
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6526
                       Mean reward: 637.61
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.4398
    Episode_Reward/rotating_object: 124.3985
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.88s
                      Time elapsed: 00:18:42
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1104/1500 [0m                     

                       Computation: 107187 steps/s (collection: 0.784s, learning 0.134s)
             Mean action noise std: 2.90
          Mean value_function loss: 72.0754
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6481
                       Mean reward: 629.11
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.4287
    Episode_Reward/rotating_object: 120.1817
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.92s
                      Time elapsed: 00:18:43
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1105/1500 [0m                     

                       Computation: 108334 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 2.91
          Mean value_function loss: 67.0494
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.6516
                       Mean reward: 610.52
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.4412
    Episode_Reward/rotating_object: 122.1620
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.91s
                      Time elapsed: 00:18:44
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1106/1500 [0m                     

                       Computation: 108646 steps/s (collection: 0.789s, learning 0.116s)
             Mean action noise std: 2.91
          Mean value_function loss: 90.2713
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.6584
                       Mean reward: 613.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4419
    Episode_Reward/rotating_object: 119.7140
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.90s
                      Time elapsed: 00:18:45
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1107/1500 [0m                     

                       Computation: 113165 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 2.91
          Mean value_function loss: 75.3380
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.6593
                       Mean reward: 609.30
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.4395
    Episode_Reward/rotating_object: 118.5754
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.87s
                      Time elapsed: 00:18:46
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1108/1500 [0m                     

                       Computation: 108604 steps/s (collection: 0.806s, learning 0.100s)
             Mean action noise std: 2.91
          Mean value_function loss: 72.9206
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.6666
                       Mean reward: 586.69
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.4467
    Episode_Reward/rotating_object: 118.9846
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.91s
                      Time elapsed: 00:18:47
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1109/1500 [0m                     

                       Computation: 110040 steps/s (collection: 0.778s, learning 0.115s)
             Mean action noise std: 2.92
          Mean value_function loss: 84.5173
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.6738
                       Mean reward: 646.27
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.4417
    Episode_Reward/rotating_object: 124.6511
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.89s
                      Time elapsed: 00:18:48
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1110/1500 [0m                     

                       Computation: 111978 steps/s (collection: 0.781s, learning 0.097s)
             Mean action noise std: 2.92
          Mean value_function loss: 84.3162
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6798
                       Mean reward: 646.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4502
    Episode_Reward/rotating_object: 120.9441
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.88s
                      Time elapsed: 00:18:48
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1111/1500 [0m                     

                       Computation: 106707 steps/s (collection: 0.788s, learning 0.134s)
             Mean action noise std: 2.92
          Mean value_function loss: 78.6108
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 17.6791
                       Mean reward: 619.62
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.4457
    Episode_Reward/rotating_object: 122.3075
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.92s
                      Time elapsed: 00:18:49
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1112/1500 [0m                     

                       Computation: 109594 steps/s (collection: 0.771s, learning 0.126s)
             Mean action noise std: 2.92
          Mean value_function loss: 86.1750
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6776
                       Mean reward: 643.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4535
    Episode_Reward/rotating_object: 124.1735
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.90s
                      Time elapsed: 00:18:50
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1113/1500 [0m                     

                       Computation: 107680 steps/s (collection: 0.795s, learning 0.118s)
             Mean action noise std: 2.92
          Mean value_function loss: 84.4202
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.6743
                       Mean reward: 606.18
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.4442
    Episode_Reward/rotating_object: 118.2590
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.91s
                      Time elapsed: 00:18:51
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1114/1500 [0m                     

                       Computation: 108779 steps/s (collection: 0.780s, learning 0.124s)
             Mean action noise std: 2.92
          Mean value_function loss: 88.3978
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6776
                       Mean reward: 553.80
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.4419
    Episode_Reward/rotating_object: 118.4818
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.90s
                      Time elapsed: 00:18:52
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1115/1500 [0m                     

                       Computation: 109783 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 2.92
          Mean value_function loss: 74.9700
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.6811
                       Mean reward: 630.48
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.4507
    Episode_Reward/rotating_object: 124.6516
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.90s
                      Time elapsed: 00:18:53
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1116/1500 [0m                     

                       Computation: 108636 steps/s (collection: 0.815s, learning 0.090s)
             Mean action noise std: 2.92
          Mean value_function loss: 75.2657
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6827
                       Mean reward: 604.59
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.4389
    Episode_Reward/rotating_object: 122.2697
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.90s
                      Time elapsed: 00:18:54
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1117/1500 [0m                     

                       Computation: 111118 steps/s (collection: 0.790s, learning 0.095s)
             Mean action noise std: 2.92
          Mean value_function loss: 70.7365
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6761
                       Mean reward: 655.19
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.4419
    Episode_Reward/rotating_object: 121.1932
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.88s
                      Time elapsed: 00:18:55
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1118/1500 [0m                     

                       Computation: 110112 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 2.93
          Mean value_function loss: 83.1948
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6768
                       Mean reward: 576.35
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.4389
    Episode_Reward/rotating_object: 120.7908
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.89s
                      Time elapsed: 00:18:56
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1119/1500 [0m                     

                       Computation: 111099 steps/s (collection: 0.774s, learning 0.111s)
             Mean action noise std: 2.93
          Mean value_function loss: 75.1944
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.6758
                       Mean reward: 628.91
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.4430
    Episode_Reward/rotating_object: 120.3296
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.88s
                      Time elapsed: 00:18:56
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1120/1500 [0m                     

                       Computation: 98026 steps/s (collection: 0.874s, learning 0.129s)
             Mean action noise std: 2.93
          Mean value_function loss: 70.5169
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.6769
                       Mean reward: 626.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4469
    Episode_Reward/rotating_object: 123.0819
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 1.00s
                      Time elapsed: 00:18:57
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1121/1500 [0m                     

                       Computation: 107436 steps/s (collection: 0.805s, learning 0.110s)
             Mean action noise std: 2.93
          Mean value_function loss: 72.6651
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6808
                       Mean reward: 598.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4441
    Episode_Reward/rotating_object: 121.2164
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.91s
                      Time elapsed: 00:18:58
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1122/1500 [0m                     

                       Computation: 107629 steps/s (collection: 0.808s, learning 0.106s)
             Mean action noise std: 2.93
          Mean value_function loss: 69.5253
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6829
                       Mean reward: 638.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4461
    Episode_Reward/rotating_object: 125.9238
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.91s
                      Time elapsed: 00:18:59
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1123/1500 [0m                     

                       Computation: 111363 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 2.93
          Mean value_function loss: 75.6521
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.6872
                       Mean reward: 637.47
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.4451
    Episode_Reward/rotating_object: 122.3984
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.88s
                      Time elapsed: 00:19:00
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1124/1500 [0m                     

                       Computation: 98475 steps/s (collection: 0.832s, learning 0.166s)
             Mean action noise std: 2.93
          Mean value_function loss: 79.7762
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6838
                       Mean reward: 620.23
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.4438
    Episode_Reward/rotating_object: 124.2353
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.00s
                      Time elapsed: 00:19:01
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1125/1500 [0m                     

                       Computation: 109535 steps/s (collection: 0.776s, learning 0.122s)
             Mean action noise std: 2.94
          Mean value_function loss: 80.6645
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.6814
                       Mean reward: 640.14
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.4375
    Episode_Reward/rotating_object: 121.5665
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.90s
                      Time elapsed: 00:19:02
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1126/1500 [0m                     

                       Computation: 109600 steps/s (collection: 0.778s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 70.6656
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.6807
                       Mean reward: 591.89
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.4380
    Episode_Reward/rotating_object: 119.8644
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.90s
                      Time elapsed: 00:19:03
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1127/1500 [0m                     

                       Computation: 107598 steps/s (collection: 0.802s, learning 0.112s)
             Mean action noise std: 2.94
          Mean value_function loss: 74.9498
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.6790
                       Mean reward: 622.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4373
    Episode_Reward/rotating_object: 124.2292
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.91s
                      Time elapsed: 00:19:04
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1128/1500 [0m                     

                       Computation: 108988 steps/s (collection: 0.800s, learning 0.102s)
             Mean action noise std: 2.94
          Mean value_function loss: 63.8786
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.6821
                       Mean reward: 627.63
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.4342
    Episode_Reward/rotating_object: 120.2485
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.90s
                      Time elapsed: 00:19:05
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1129/1500 [0m                     

                       Computation: 95241 steps/s (collection: 0.925s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 70.2979
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6890
                       Mean reward: 633.20
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.4440
    Episode_Reward/rotating_object: 126.8115
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 1.03s
                      Time elapsed: 00:19:06
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1130/1500 [0m                     

                       Computation: 106366 steps/s (collection: 0.835s, learning 0.089s)
             Mean action noise std: 2.95
          Mean value_function loss: 75.1810
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6926
                       Mean reward: 558.40
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.4344
    Episode_Reward/rotating_object: 122.5617
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.92s
                      Time elapsed: 00:19:07
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1131/1500 [0m                     

                       Computation: 111449 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 2.95
          Mean value_function loss: 73.7046
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.6971
                       Mean reward: 614.93
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.4374
    Episode_Reward/rotating_object: 123.5303
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.88s
                      Time elapsed: 00:19:08
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1132/1500 [0m                     

                       Computation: 107908 steps/s (collection: 0.798s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 80.1305
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 17.6985
                       Mean reward: 568.55
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4347
    Episode_Reward/rotating_object: 117.3808
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.91s
                      Time elapsed: 00:19:09
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1133/1500 [0m                     

                       Computation: 110997 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 2.95
          Mean value_function loss: 86.2014
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7012
                       Mean reward: 615.52
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.4346
    Episode_Reward/rotating_object: 120.2475
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.89s
                      Time elapsed: 00:19:09
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1134/1500 [0m                     

                       Computation: 106337 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 2.95
          Mean value_function loss: 83.5117
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.7183
                       Mean reward: 633.68
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.4417
    Episode_Reward/rotating_object: 124.2260
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.92s
                      Time elapsed: 00:19:10
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1135/1500 [0m                     

                       Computation: 110771 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 89.4068
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7137
                       Mean reward: 621.20
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.4475
    Episode_Reward/rotating_object: 124.6896
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.89s
                      Time elapsed: 00:19:11
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1136/1500 [0m                     

                       Computation: 99399 steps/s (collection: 0.806s, learning 0.183s)
             Mean action noise std: 2.95
          Mean value_function loss: 75.9673
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7179
                       Mean reward: 615.66
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4440
    Episode_Reward/rotating_object: 122.9397
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.99s
                      Time elapsed: 00:19:12
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1137/1500 [0m                     

                       Computation: 108922 steps/s (collection: 0.805s, learning 0.098s)
             Mean action noise std: 2.95
          Mean value_function loss: 77.3785
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7168
                       Mean reward: 626.57
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.4486
    Episode_Reward/rotating_object: 123.2136
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.90s
                      Time elapsed: 00:19:13
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1138/1500 [0m                     

                       Computation: 101201 steps/s (collection: 0.804s, learning 0.168s)
             Mean action noise std: 2.96
          Mean value_function loss: 89.9824
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.7099
                       Mean reward: 621.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4460
    Episode_Reward/rotating_object: 124.4568
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.97s
                      Time elapsed: 00:19:14
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1139/1500 [0m                     

                       Computation: 103638 steps/s (collection: 0.803s, learning 0.145s)
             Mean action noise std: 2.96
          Mean value_function loss: 70.2427
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7056
                       Mean reward: 636.65
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.4513
    Episode_Reward/rotating_object: 122.1557
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.95s
                      Time elapsed: 00:19:15
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1140/1500 [0m                     

                       Computation: 109719 steps/s (collection: 0.780s, learning 0.116s)
             Mean action noise std: 2.96
          Mean value_function loss: 70.0267
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.7042
                       Mean reward: 600.41
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.4502
    Episode_Reward/rotating_object: 124.5659
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.90s
                      Time elapsed: 00:19:16
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1141/1500 [0m                     

                       Computation: 105802 steps/s (collection: 0.810s, learning 0.119s)
             Mean action noise std: 2.96
          Mean value_function loss: 78.3694
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.7090
                       Mean reward: 651.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4559
    Episode_Reward/rotating_object: 126.9835
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.93s
                      Time elapsed: 00:19:17
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1142/1500 [0m                     

                       Computation: 111002 steps/s (collection: 0.797s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 76.3202
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7156
                       Mean reward: 604.25
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.4448
    Episode_Reward/rotating_object: 120.7472
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.89s
                      Time elapsed: 00:19:18
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1143/1500 [0m                     

                       Computation: 110107 steps/s (collection: 0.785s, learning 0.108s)
             Mean action noise std: 2.96
          Mean value_function loss: 67.4322
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.7122
                       Mean reward: 661.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4468
    Episode_Reward/rotating_object: 123.2022
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.89s
                      Time elapsed: 00:19:19
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1144/1500 [0m                     

                       Computation: 108429 steps/s (collection: 0.812s, learning 0.095s)
             Mean action noise std: 2.96
          Mean value_function loss: 76.2460
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7142
                       Mean reward: 632.24
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.4464
    Episode_Reward/rotating_object: 125.3693
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.91s
                      Time elapsed: 00:19:20
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1145/1500 [0m                     

                       Computation: 113978 steps/s (collection: 0.768s, learning 0.095s)
             Mean action noise std: 2.97
          Mean value_function loss: 70.7254
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7203
                       Mean reward: 619.04
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.4472
    Episode_Reward/rotating_object: 123.3176
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.86s
                      Time elapsed: 00:19:20
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1146/1500 [0m                     

                       Computation: 106245 steps/s (collection: 0.799s, learning 0.126s)
             Mean action noise std: 2.97
          Mean value_function loss: 68.1474
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.7180
                       Mean reward: 618.76
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.4513
    Episode_Reward/rotating_object: 121.9092
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.93s
                      Time elapsed: 00:19:21
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1147/1500 [0m                     

                       Computation: 108996 steps/s (collection: 0.775s, learning 0.127s)
             Mean action noise std: 2.97
          Mean value_function loss: 76.8891
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.7147
                       Mean reward: 626.95
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.4438
    Episode_Reward/rotating_object: 123.3134
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.90s
                      Time elapsed: 00:19:22
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1148/1500 [0m                     

                       Computation: 102658 steps/s (collection: 0.790s, learning 0.168s)
             Mean action noise std: 2.97
          Mean value_function loss: 77.1090
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.7141
                       Mean reward: 641.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4539
    Episode_Reward/rotating_object: 126.4108
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.96s
                      Time elapsed: 00:19:23
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1149/1500 [0m                     

                       Computation: 102670 steps/s (collection: 0.832s, learning 0.126s)
             Mean action noise std: 2.97
          Mean value_function loss: 79.9723
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7109
                       Mean reward: 603.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4523
    Episode_Reward/rotating_object: 124.8852
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.96s
                      Time elapsed: 00:19:24
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1150/1500 [0m                     

                       Computation: 94512 steps/s (collection: 0.845s, learning 0.195s)
             Mean action noise std: 2.97
          Mean value_function loss: 83.1992
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7012
                       Mean reward: 616.33
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.4507
    Episode_Reward/rotating_object: 124.6952
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 1.04s
                      Time elapsed: 00:19:25
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1151/1500 [0m                     

                       Computation: 103862 steps/s (collection: 0.850s, learning 0.096s)
             Mean action noise std: 2.97
          Mean value_function loss: 90.8218
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7062
                       Mean reward: 623.96
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.4534
    Episode_Reward/rotating_object: 125.6805
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.95s
                      Time elapsed: 00:19:26
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1152/1500 [0m                     

                       Computation: 102217 steps/s (collection: 0.800s, learning 0.162s)
             Mean action noise std: 2.97
          Mean value_function loss: 84.4622
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7088
                       Mean reward: 647.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4618
    Episode_Reward/rotating_object: 128.7318
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.96s
                      Time elapsed: 00:19:27
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1153/1500 [0m                     

                       Computation: 105960 steps/s (collection: 0.804s, learning 0.124s)
             Mean action noise std: 2.97
          Mean value_function loss: 84.2118
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7119
                       Mean reward: 629.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4559
    Episode_Reward/rotating_object: 124.7164
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.93s
                      Time elapsed: 00:19:28
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1154/1500 [0m                     

                       Computation: 105981 steps/s (collection: 0.784s, learning 0.143s)
             Mean action noise std: 2.98
          Mean value_function loss: 83.1683
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.7072
                       Mean reward: 622.33
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.4591
    Episode_Reward/rotating_object: 125.9783
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.93s
                      Time elapsed: 00:19:29
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1155/1500 [0m                     

                       Computation: 109791 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 2.98
          Mean value_function loss: 81.7290
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7096
                       Mean reward: 650.85
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.4585
    Episode_Reward/rotating_object: 126.3212
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.90s
                      Time elapsed: 00:19:30
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1156/1500 [0m                     

                       Computation: 105198 steps/s (collection: 0.830s, learning 0.105s)
             Mean action noise std: 2.98
          Mean value_function loss: 79.9851
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.7088
                       Mean reward: 630.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4605
    Episode_Reward/rotating_object: 124.9736
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.93s
                      Time elapsed: 00:19:31
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1157/1500 [0m                     

                       Computation: 107384 steps/s (collection: 0.813s, learning 0.103s)
             Mean action noise std: 2.98
          Mean value_function loss: 85.5869
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.7054
                       Mean reward: 622.54
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.4603
    Episode_Reward/rotating_object: 121.1679
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.92s
                      Time elapsed: 00:19:32
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1158/1500 [0m                     

                       Computation: 111006 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 2.98
          Mean value_function loss: 78.2085
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.7108
                       Mean reward: 631.24
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.4587
    Episode_Reward/rotating_object: 122.3563
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.89s
                      Time elapsed: 00:19:33
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1159/1500 [0m                     

                       Computation: 108243 steps/s (collection: 0.783s, learning 0.125s)
             Mean action noise std: 2.98
          Mean value_function loss: 72.9887
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7115
                       Mean reward: 618.29
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.4564
    Episode_Reward/rotating_object: 121.8215
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.91s
                      Time elapsed: 00:19:34
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1160/1500 [0m                     

                       Computation: 105975 steps/s (collection: 0.820s, learning 0.107s)
             Mean action noise std: 2.98
          Mean value_function loss: 81.3769
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7062
                       Mean reward: 592.52
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.4504
    Episode_Reward/rotating_object: 122.1244
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.93s
                      Time elapsed: 00:19:34
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1161/1500 [0m                     

                       Computation: 94305 steps/s (collection: 0.861s, learning 0.182s)
             Mean action noise std: 2.98
          Mean value_function loss: 74.6628
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.7097
                       Mean reward: 633.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4517
    Episode_Reward/rotating_object: 122.3029
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 1.04s
                      Time elapsed: 00:19:35
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1162/1500 [0m                     

                       Computation: 103774 steps/s (collection: 0.831s, learning 0.117s)
             Mean action noise std: 2.98
          Mean value_function loss: 79.1710
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7167
                       Mean reward: 607.08
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.4604
    Episode_Reward/rotating_object: 124.2159
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.95s
                      Time elapsed: 00:19:36
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1163/1500 [0m                     

                       Computation: 109813 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 2.98
          Mean value_function loss: 70.9704
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7128
                       Mean reward: 649.15
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.4566
    Episode_Reward/rotating_object: 128.0594
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.90s
                      Time elapsed: 00:19:37
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1164/1500 [0m                     

                       Computation: 101465 steps/s (collection: 0.846s, learning 0.123s)
             Mean action noise std: 2.99
          Mean value_function loss: 73.0665
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7092
                       Mean reward: 612.94
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.4575
    Episode_Reward/rotating_object: 125.4512
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.97s
                      Time elapsed: 00:19:38
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1165/1500 [0m                     

                       Computation: 108112 steps/s (collection: 0.810s, learning 0.099s)
             Mean action noise std: 2.99
          Mean value_function loss: 66.6558
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.7100
                       Mean reward: 627.84
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.4531
    Episode_Reward/rotating_object: 124.2569
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.91s
                      Time elapsed: 00:19:39
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1166/1500 [0m                     

                       Computation: 93441 steps/s (collection: 0.835s, learning 0.217s)
             Mean action noise std: 2.99
          Mean value_function loss: 71.3522
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.7130
                       Mean reward: 628.13
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.4508
    Episode_Reward/rotating_object: 125.3634
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.05s
                      Time elapsed: 00:19:40
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1167/1500 [0m                     

                       Computation: 100210 steps/s (collection: 0.858s, learning 0.123s)
             Mean action noise std: 2.99
          Mean value_function loss: 72.7086
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.7175
                       Mean reward: 608.59
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.4536
    Episode_Reward/rotating_object: 125.7226
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.98s
                      Time elapsed: 00:19:41
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1168/1500 [0m                     

                       Computation: 99279 steps/s (collection: 0.839s, learning 0.151s)
             Mean action noise std: 2.99
          Mean value_function loss: 77.9029
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.7194
                       Mean reward: 614.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4478
    Episode_Reward/rotating_object: 123.5286
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.99s
                      Time elapsed: 00:19:42
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1169/1500 [0m                     

                       Computation: 102134 steps/s (collection: 0.840s, learning 0.123s)
             Mean action noise std: 2.99
          Mean value_function loss: 71.7059
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.7229
                       Mean reward: 648.08
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.4536
    Episode_Reward/rotating_object: 124.4467
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.96s
                      Time elapsed: 00:19:43
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1170/1500 [0m                     

                       Computation: 100415 steps/s (collection: 0.798s, learning 0.181s)
             Mean action noise std: 2.99
          Mean value_function loss: 66.7229
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7296
                       Mean reward: 665.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4576
    Episode_Reward/rotating_object: 129.0465
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.98s
                      Time elapsed: 00:19:44
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1171/1500 [0m                     

                       Computation: 83453 steps/s (collection: 0.941s, learning 0.237s)
             Mean action noise std: 2.99
          Mean value_function loss: 79.4603
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.7300
                       Mean reward: 660.75
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.4568
    Episode_Reward/rotating_object: 126.5497
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.18s
                      Time elapsed: 00:19:45
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1172/1500 [0m                     

                       Computation: 102743 steps/s (collection: 0.842s, learning 0.115s)
             Mean action noise std: 3.00
          Mean value_function loss: 70.9660
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.7321
                       Mean reward: 614.66
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.4533
    Episode_Reward/rotating_object: 123.9867
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.96s
                      Time elapsed: 00:19:46
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1173/1500 [0m                     

                       Computation: 92515 steps/s (collection: 0.949s, learning 0.113s)
             Mean action noise std: 3.00
          Mean value_function loss: 67.6856
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.7365
                       Mean reward: 632.45
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.4579
    Episode_Reward/rotating_object: 129.5226
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.06s
                      Time elapsed: 00:19:47
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1174/1500 [0m                     

                       Computation: 89312 steps/s (collection: 0.967s, learning 0.133s)
             Mean action noise std: 3.00
          Mean value_function loss: 68.8488
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7427
                       Mean reward: 667.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4587
    Episode_Reward/rotating_object: 126.9256
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.10s
                      Time elapsed: 00:19:48
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1175/1500 [0m                     

                       Computation: 102630 steps/s (collection: 0.866s, learning 0.092s)
             Mean action noise std: 3.00
          Mean value_function loss: 66.9086
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7525
                       Mean reward: 613.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4590
    Episode_Reward/rotating_object: 126.9853
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.96s
                      Time elapsed: 00:19:49
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1176/1500 [0m                     

                       Computation: 106375 steps/s (collection: 0.816s, learning 0.108s)
             Mean action noise std: 3.00
          Mean value_function loss: 72.9118
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7540
                       Mean reward: 620.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4581
    Episode_Reward/rotating_object: 125.2929
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.92s
                      Time elapsed: 00:19:50
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1177/1500 [0m                     

                       Computation: 89684 steps/s (collection: 0.926s, learning 0.170s)
             Mean action noise std: 3.00
          Mean value_function loss: 61.9403
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.7497
                       Mean reward: 635.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4581
    Episode_Reward/rotating_object: 128.0759
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 1.10s
                      Time elapsed: 00:19:51
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1178/1500 [0m                     

                       Computation: 92589 steps/s (collection: 0.919s, learning 0.143s)
             Mean action noise std: 3.01
          Mean value_function loss: 73.3212
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.7565
                       Mean reward: 632.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4568
    Episode_Reward/rotating_object: 129.1370
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.06s
                      Time elapsed: 00:19:53
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1179/1500 [0m                     

                       Computation: 104969 steps/s (collection: 0.827s, learning 0.109s)
             Mean action noise std: 3.01
          Mean value_function loss: 80.3920
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.7597
                       Mean reward: 626.31
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.4474
    Episode_Reward/rotating_object: 122.4707
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.94s
                      Time elapsed: 00:19:53
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1180/1500 [0m                     

                       Computation: 111864 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 3.01
          Mean value_function loss: 72.8324
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.7566
                       Mean reward: 604.24
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4587
    Episode_Reward/rotating_object: 125.5973
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.88s
                      Time elapsed: 00:19:54
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1181/1500 [0m                     

                       Computation: 104541 steps/s (collection: 0.830s, learning 0.111s)
             Mean action noise std: 3.01
          Mean value_function loss: 77.7558
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.7556
                       Mean reward: 612.64
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.4571
    Episode_Reward/rotating_object: 124.8789
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.94s
                      Time elapsed: 00:19:55
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1182/1500 [0m                     

                       Computation: 108693 steps/s (collection: 0.792s, learning 0.112s)
             Mean action noise std: 3.01
          Mean value_function loss: 71.4429
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7585
                       Mean reward: 638.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4547
    Episode_Reward/rotating_object: 128.3883
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.90s
                      Time elapsed: 00:19:56
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1183/1500 [0m                     

                       Computation: 111541 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 3.01
          Mean value_function loss: 74.1599
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7614
                       Mean reward: 639.14
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.4566
    Episode_Reward/rotating_object: 128.4357
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.88s
                      Time elapsed: 00:19:57
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1184/1500 [0m                     

                       Computation: 110659 steps/s (collection: 0.777s, learning 0.112s)
             Mean action noise std: 3.01
          Mean value_function loss: 73.9294
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.7680
                       Mean reward: 635.25
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.4553
    Episode_Reward/rotating_object: 129.9590
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.89s
                      Time elapsed: 00:19:58
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1185/1500 [0m                     

                       Computation: 110404 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 3.02
          Mean value_function loss: 82.1946
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7697
                       Mean reward: 649.29
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.4528
    Episode_Reward/rotating_object: 124.3736
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.89s
                      Time elapsed: 00:19:59
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1186/1500 [0m                     

                       Computation: 112650 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 3.02
          Mean value_function loss: 68.3295
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.7816
                       Mean reward: 624.97
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.4572
    Episode_Reward/rotating_object: 128.2260
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.87s
                      Time elapsed: 00:20:00
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1187/1500 [0m                     

                       Computation: 108235 steps/s (collection: 0.808s, learning 0.101s)
             Mean action noise std: 3.02
          Mean value_function loss: 68.9341
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7881
                       Mean reward: 635.38
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.4515
    Episode_Reward/rotating_object: 124.4293
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.91s
                      Time elapsed: 00:20:01
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1188/1500 [0m                     

                       Computation: 109281 steps/s (collection: 0.805s, learning 0.094s)
             Mean action noise std: 3.02
          Mean value_function loss: 70.2756
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7912
                       Mean reward: 646.76
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.4546
    Episode_Reward/rotating_object: 127.6758
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.90s
                      Time elapsed: 00:20:02
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1189/1500 [0m                     

                       Computation: 108837 steps/s (collection: 0.789s, learning 0.115s)
             Mean action noise std: 3.02
          Mean value_function loss: 67.4955
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7953
                       Mean reward: 636.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4533
    Episode_Reward/rotating_object: 128.0090
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.90s
                      Time elapsed: 00:20:02
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1190/1500 [0m                     

                       Computation: 110469 steps/s (collection: 0.794s, learning 0.096s)
             Mean action noise std: 3.02
          Mean value_function loss: 62.3486
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8010
                       Mean reward: 650.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4595
    Episode_Reward/rotating_object: 130.5148
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.89s
                      Time elapsed: 00:20:03
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1191/1500 [0m                     

                       Computation: 110347 steps/s (collection: 0.782s, learning 0.109s)
             Mean action noise std: 3.03
          Mean value_function loss: 68.8241
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8127
                       Mean reward: 611.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4465
    Episode_Reward/rotating_object: 125.1366
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.89s
                      Time elapsed: 00:20:04
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1192/1500 [0m                     

                       Computation: 110233 steps/s (collection: 0.780s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 63.0597
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.8140
                       Mean reward: 630.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4499
    Episode_Reward/rotating_object: 129.4142
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.89s
                      Time elapsed: 00:20:05
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1193/1500 [0m                     

                       Computation: 111347 steps/s (collection: 0.786s, learning 0.097s)
             Mean action noise std: 3.03
          Mean value_function loss: 56.5799
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8092
                       Mean reward: 637.81
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.4474
    Episode_Reward/rotating_object: 127.6143
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.88s
                      Time elapsed: 00:20:06
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1194/1500 [0m                     

                       Computation: 113416 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 60.2987
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8091
                       Mean reward: 642.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4405
    Episode_Reward/rotating_object: 122.6524
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.87s
                      Time elapsed: 00:20:07
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1195/1500 [0m                     

                       Computation: 109788 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 3.03
          Mean value_function loss: 71.0205
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8204
                       Mean reward: 662.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4488
    Episode_Reward/rotating_object: 129.0150
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.90s
                      Time elapsed: 00:20:08
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1196/1500 [0m                     

                       Computation: 114332 steps/s (collection: 0.769s, learning 0.091s)
             Mean action noise std: 3.03
          Mean value_function loss: 66.5818
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.8211
                       Mean reward: 624.67
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.4498
    Episode_Reward/rotating_object: 127.9499
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.86s
                      Time elapsed: 00:20:09
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1197/1500 [0m                     

                       Computation: 110050 steps/s (collection: 0.799s, learning 0.095s)
             Mean action noise std: 3.03
          Mean value_function loss: 65.3349
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.8189
                       Mean reward: 669.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4512
    Episode_Reward/rotating_object: 128.0964
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.89s
                      Time elapsed: 00:20:09
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1198/1500 [0m                     

                       Computation: 113696 steps/s (collection: 0.764s, learning 0.101s)
             Mean action noise std: 3.04
          Mean value_function loss: 62.1120
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8165
                       Mean reward: 635.55
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.4539
    Episode_Reward/rotating_object: 130.0070
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.86s
                      Time elapsed: 00:20:10
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1199/1500 [0m                     

                       Computation: 110663 steps/s (collection: 0.774s, learning 0.115s)
             Mean action noise std: 3.04
          Mean value_function loss: 61.4952
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8251
                       Mean reward: 637.98
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.4536
    Episode_Reward/rotating_object: 131.2672
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.89s
                      Time elapsed: 00:20:11
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1200/1500 [0m                     

                       Computation: 108222 steps/s (collection: 0.798s, learning 0.110s)
             Mean action noise std: 3.04
          Mean value_function loss: 64.2136
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.8250
                       Mean reward: 651.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4463
    Episode_Reward/rotating_object: 127.0738
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.91s
                      Time elapsed: 00:20:12
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1201/1500 [0m                     

                       Computation: 114761 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 3.04
          Mean value_function loss: 65.1615
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8306
                       Mean reward: 645.89
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.4518
    Episode_Reward/rotating_object: 129.6962
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.86s
                      Time elapsed: 00:20:13
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1202/1500 [0m                     

                       Computation: 112675 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 3.04
          Mean value_function loss: 65.8428
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.8299
                       Mean reward: 636.55
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.4492
    Episode_Reward/rotating_object: 128.2457
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.87s
                      Time elapsed: 00:20:14
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1203/1500 [0m                     

                       Computation: 109406 steps/s (collection: 0.803s, learning 0.095s)
             Mean action noise std: 3.04
          Mean value_function loss: 62.4559
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8286
                       Mean reward: 624.08
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.4486
    Episode_Reward/rotating_object: 125.2874
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.90s
                      Time elapsed: 00:20:15
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1204/1500 [0m                     

                       Computation: 107269 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 3.04
          Mean value_function loss: 65.4084
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.8341
                       Mean reward: 612.11
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.4477
    Episode_Reward/rotating_object: 127.5241
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.92s
                      Time elapsed: 00:20:16
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1205/1500 [0m                     

                       Computation: 109279 steps/s (collection: 0.808s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 56.0432
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8289
                       Mean reward: 612.19
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.4432
    Episode_Reward/rotating_object: 128.0979
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.90s
                      Time elapsed: 00:20:17
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1206/1500 [0m                     

                       Computation: 113046 steps/s (collection: 0.774s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 57.7329
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8273
                       Mean reward: 638.55
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.4458
    Episode_Reward/rotating_object: 126.0895
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.87s
                      Time elapsed: 00:20:17
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1207/1500 [0m                     

                       Computation: 110915 steps/s (collection: 0.778s, learning 0.108s)
             Mean action noise std: 3.05
          Mean value_function loss: 67.1252
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.8305
                       Mean reward: 664.53
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.4514
    Episode_Reward/rotating_object: 131.0344
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.89s
                      Time elapsed: 00:20:18
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1208/1500 [0m                     

                       Computation: 114394 steps/s (collection: 0.766s, learning 0.094s)
             Mean action noise std: 3.05
          Mean value_function loss: 71.2789
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8382
                       Mean reward: 613.65
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.4340
    Episode_Reward/rotating_object: 123.4830
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.86s
                      Time elapsed: 00:20:19
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1209/1500 [0m                     

                       Computation: 110871 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 3.05
          Mean value_function loss: 64.2404
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.8490
                       Mean reward: 623.59
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.4363
    Episode_Reward/rotating_object: 123.4598
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.89s
                      Time elapsed: 00:20:20
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1210/1500 [0m                     

                       Computation: 110936 steps/s (collection: 0.772s, learning 0.115s)
             Mean action noise std: 3.06
          Mean value_function loss: 72.1066
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.8579
                       Mean reward: 640.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4472
    Episode_Reward/rotating_object: 128.2991
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.89s
                      Time elapsed: 00:20:21
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1211/1500 [0m                     

                       Computation: 112881 steps/s (collection: 0.761s, learning 0.110s)
             Mean action noise std: 3.06
          Mean value_function loss: 65.1071
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.8638
                       Mean reward: 658.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4457
    Episode_Reward/rotating_object: 128.9568
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.87s
                      Time elapsed: 00:20:22
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1212/1500 [0m                     

                       Computation: 110830 steps/s (collection: 0.779s, learning 0.108s)
             Mean action noise std: 3.06
          Mean value_function loss: 65.1947
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.8618
                       Mean reward: 647.91
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.4448
    Episode_Reward/rotating_object: 128.5989
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.89s
                      Time elapsed: 00:20:23
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1213/1500 [0m                     

                       Computation: 110010 steps/s (collection: 0.789s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 71.7948
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8592
                       Mean reward: 674.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4509
    Episode_Reward/rotating_object: 129.8518
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.89s
                      Time elapsed: 00:20:24
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1214/1500 [0m                     

                       Computation: 108139 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 3.06
          Mean value_function loss: 70.0199
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8636
                       Mean reward: 636.05
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4479
    Episode_Reward/rotating_object: 130.3978
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.91s
                      Time elapsed: 00:20:25
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1215/1500 [0m                     

                       Computation: 108907 steps/s (collection: 0.784s, learning 0.119s)
             Mean action noise std: 3.06
          Mean value_function loss: 61.5692
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.8754
                       Mean reward: 643.46
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.4488
    Episode_Reward/rotating_object: 128.2747
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.90s
                      Time elapsed: 00:20:25
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1216/1500 [0m                     

                       Computation: 110210 steps/s (collection: 0.777s, learning 0.115s)
             Mean action noise std: 3.07
          Mean value_function loss: 60.6735
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8824
                       Mean reward: 630.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4484
    Episode_Reward/rotating_object: 129.0954
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.89s
                      Time elapsed: 00:20:26
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1217/1500 [0m                     

                       Computation: 112280 steps/s (collection: 0.771s, learning 0.105s)
             Mean action noise std: 3.07
          Mean value_function loss: 55.5887
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.8825
                       Mean reward: 639.78
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.4437
    Episode_Reward/rotating_object: 123.4124
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.88s
                      Time elapsed: 00:20:27
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1218/1500 [0m                     

                       Computation: 110175 steps/s (collection: 0.785s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 75.0354
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8809
                       Mean reward: 635.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4488
    Episode_Reward/rotating_object: 128.8601
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.89s
                      Time elapsed: 00:20:28
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1219/1500 [0m                     

                       Computation: 110931 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 66.9250
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.8858
                       Mean reward: 598.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4465
    Episode_Reward/rotating_object: 125.4185
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.89s
                      Time elapsed: 00:20:29
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1220/1500 [0m                     

                       Computation: 111930 steps/s (collection: 0.776s, learning 0.103s)
             Mean action noise std: 3.08
          Mean value_function loss: 56.9042
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.8955
                       Mean reward: 660.28
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.4523
    Episode_Reward/rotating_object: 131.8639
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.88s
                      Time elapsed: 00:20:30
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1221/1500 [0m                     

                       Computation: 109262 steps/s (collection: 0.789s, learning 0.111s)
             Mean action noise std: 3.08
          Mean value_function loss: 64.1636
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8993
                       Mean reward: 652.36
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.4483
    Episode_Reward/rotating_object: 127.3986
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.90s
                      Time elapsed: 00:20:31
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1222/1500 [0m                     

                       Computation: 109427 steps/s (collection: 0.787s, learning 0.112s)
             Mean action noise std: 3.08
          Mean value_function loss: 72.2429
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.8994
                       Mean reward: 630.21
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.4475
    Episode_Reward/rotating_object: 126.4622
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.90s
                      Time elapsed: 00:20:32
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1223/1500 [0m                     

                       Computation: 111356 steps/s (collection: 0.773s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 66.5624
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9135
                       Mean reward: 651.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4467
    Episode_Reward/rotating_object: 127.5679
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.88s
                      Time elapsed: 00:20:33
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1224/1500 [0m                     

                       Computation: 94856 steps/s (collection: 0.914s, learning 0.123s)
             Mean action noise std: 3.08
          Mean value_function loss: 64.2738
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9254
                       Mean reward: 657.50
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.4446
    Episode_Reward/rotating_object: 128.5108
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 1.04s
                      Time elapsed: 00:20:34
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1225/1500 [0m                     

                       Computation: 86557 steps/s (collection: 0.977s, learning 0.159s)
             Mean action noise std: 3.09
          Mean value_function loss: 68.1636
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.9269
                       Mean reward: 633.90
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.4490
    Episode_Reward/rotating_object: 127.3845
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 1.14s
                      Time elapsed: 00:20:35
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1226/1500 [0m                     

                       Computation: 94066 steps/s (collection: 0.938s, learning 0.107s)
             Mean action noise std: 3.09
          Mean value_function loss: 58.3323
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.9273
                       Mean reward: 653.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4462
    Episode_Reward/rotating_object: 125.9976
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 1.05s
                      Time elapsed: 00:20:36
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1227/1500 [0m                     

                       Computation: 106908 steps/s (collection: 0.816s, learning 0.104s)
             Mean action noise std: 3.09
          Mean value_function loss: 67.5324
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.9248
                       Mean reward: 630.33
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.4491
    Episode_Reward/rotating_object: 126.4563
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.92s
                      Time elapsed: 00:20:37
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1228/1500 [0m                     

                       Computation: 111781 steps/s (collection: 0.784s, learning 0.095s)
             Mean action noise std: 3.09
          Mean value_function loss: 66.2060
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.9296
                       Mean reward: 686.39
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.4533
    Episode_Reward/rotating_object: 131.5657
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.88s
                      Time elapsed: 00:20:38
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1229/1500 [0m                     

                       Computation: 109247 steps/s (collection: 0.797s, learning 0.103s)
             Mean action noise std: 3.09
          Mean value_function loss: 69.3148
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.9348
                       Mean reward: 626.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4590
    Episode_Reward/rotating_object: 131.5053
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.90s
                      Time elapsed: 00:20:38
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1230/1500 [0m                     

                       Computation: 108395 steps/s (collection: 0.805s, learning 0.102s)
             Mean action noise std: 3.09
          Mean value_function loss: 63.6612
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9401
                       Mean reward: 644.07
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.4521
    Episode_Reward/rotating_object: 128.1087
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.91s
                      Time elapsed: 00:20:39
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1231/1500 [0m                     

                       Computation: 108784 steps/s (collection: 0.812s, learning 0.092s)
             Mean action noise std: 3.09
          Mean value_function loss: 60.8469
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.9434
                       Mean reward: 667.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4547
    Episode_Reward/rotating_object: 129.2755
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.90s
                      Time elapsed: 00:20:40
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1232/1500 [0m                     

                       Computation: 112716 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 3.10
          Mean value_function loss: 61.8216
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9453
                       Mean reward: 647.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4565
    Episode_Reward/rotating_object: 126.7283
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.87s
                      Time elapsed: 00:20:41
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1233/1500 [0m                     

                       Computation: 110178 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 73.9812
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9454
                       Mean reward: 638.52
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.4558
    Episode_Reward/rotating_object: 130.2705
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.89s
                      Time elapsed: 00:20:42
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1234/1500 [0m                     

                       Computation: 112365 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 3.10
          Mean value_function loss: 64.9216
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.9450
                       Mean reward: 642.68
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.4441
    Episode_Reward/rotating_object: 126.9019
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.87s
                      Time elapsed: 00:20:43
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1235/1500 [0m                     

                       Computation: 110779 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 69.6215
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.9495
                       Mean reward: 626.75
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.4515
    Episode_Reward/rotating_object: 128.8038
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.89s
                      Time elapsed: 00:20:44
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1236/1500 [0m                     

                       Computation: 113463 steps/s (collection: 0.772s, learning 0.095s)
             Mean action noise std: 3.10
          Mean value_function loss: 68.1991
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.9499
                       Mean reward: 654.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4577
    Episode_Reward/rotating_object: 130.0495
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.87s
                      Time elapsed: 00:20:45
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1237/1500 [0m                     

                       Computation: 109182 steps/s (collection: 0.801s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 69.9271
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.9507
                       Mean reward: 649.98
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.4527
    Episode_Reward/rotating_object: 127.7748
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.90s
                      Time elapsed: 00:20:46
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1238/1500 [0m                     

                       Computation: 111878 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 63.5016
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.9573
                       Mean reward: 651.72
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4478
    Episode_Reward/rotating_object: 126.9491
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.88s
                      Time elapsed: 00:20:46
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1239/1500 [0m                     

                       Computation: 112878 steps/s (collection: 0.771s, learning 0.100s)
             Mean action noise std: 3.11
          Mean value_function loss: 60.4671
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.9538
                       Mean reward: 650.39
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.4610
    Episode_Reward/rotating_object: 132.4412
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.87s
                      Time elapsed: 00:20:47
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1240/1500 [0m                     

                       Computation: 114096 steps/s (collection: 0.765s, learning 0.097s)
             Mean action noise std: 3.11
          Mean value_function loss: 67.4887
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9557
                       Mean reward: 643.36
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.4544
    Episode_Reward/rotating_object: 127.1021
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.86s
                      Time elapsed: 00:20:48
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1241/1500 [0m                     

                       Computation: 109763 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 3.11
          Mean value_function loss: 64.9887
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.9655
                       Mean reward: 610.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4512
    Episode_Reward/rotating_object: 127.5183
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.90s
                      Time elapsed: 00:20:49
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1242/1500 [0m                     

                       Computation: 110311 steps/s (collection: 0.790s, learning 0.101s)
             Mean action noise std: 3.11
          Mean value_function loss: 63.9033
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 17.9693
                       Mean reward: 632.07
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.4478
    Episode_Reward/rotating_object: 124.3059
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.89s
                      Time elapsed: 00:20:50
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1243/1500 [0m                     

                       Computation: 111413 steps/s (collection: 0.781s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 71.6478
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9705
                       Mean reward: 651.11
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 130.8293
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.88s
                      Time elapsed: 00:20:51
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1244/1500 [0m                     

                       Computation: 113433 steps/s (collection: 0.778s, learning 0.089s)
             Mean action noise std: 3.11
          Mean value_function loss: 72.7987
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 17.9749
                       Mean reward: 623.83
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.4533
    Episode_Reward/rotating_object: 127.3905
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.87s
                      Time elapsed: 00:20:52
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1245/1500 [0m                     

                       Computation: 109855 steps/s (collection: 0.785s, learning 0.110s)
             Mean action noise std: 3.12
          Mean value_function loss: 70.8387
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.9804
                       Mean reward: 640.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4517
    Episode_Reward/rotating_object: 126.7723
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.89s
                      Time elapsed: 00:20:53
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1246/1500 [0m                     

                       Computation: 108390 steps/s (collection: 0.790s, learning 0.117s)
             Mean action noise std: 3.12
          Mean value_function loss: 70.8269
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9891
                       Mean reward: 660.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4578
    Episode_Reward/rotating_object: 129.9634
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.91s
                      Time elapsed: 00:20:54
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1247/1500 [0m                     

                       Computation: 104989 steps/s (collection: 0.818s, learning 0.119s)
             Mean action noise std: 3.12
          Mean value_function loss: 73.4955
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.9941
                       Mean reward: 673.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4609
    Episode_Reward/rotating_object: 130.6620
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.94s
                      Time elapsed: 00:20:54
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1248/1500 [0m                     

                       Computation: 108940 steps/s (collection: 0.788s, learning 0.114s)
             Mean action noise std: 3.12
          Mean value_function loss: 76.9102
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.9965
                       Mean reward: 650.27
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.4591
    Episode_Reward/rotating_object: 129.5623
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.90s
                      Time elapsed: 00:20:55
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1249/1500 [0m                     

                       Computation: 109485 steps/s (collection: 0.782s, learning 0.116s)
             Mean action noise std: 3.12
          Mean value_function loss: 70.0761
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.9968
                       Mean reward: 651.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4643
    Episode_Reward/rotating_object: 127.0060
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.90s
                      Time elapsed: 00:20:56
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1250/1500 [0m                     

                       Computation: 110854 steps/s (collection: 0.776s, learning 0.111s)
             Mean action noise std: 3.13
          Mean value_function loss: 71.8776
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.9995
                       Mean reward: 621.28
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.4531
    Episode_Reward/rotating_object: 126.9906
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.89s
                      Time elapsed: 00:20:57
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1251/1500 [0m                     

                       Computation: 108909 steps/s (collection: 0.783s, learning 0.120s)
             Mean action noise std: 3.13
          Mean value_function loss: 70.1872
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0130
                       Mean reward: 615.32
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.4519
    Episode_Reward/rotating_object: 127.2472
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.90s
                      Time elapsed: 00:20:58
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1252/1500 [0m                     

                       Computation: 114530 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 62.1283
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.0205
                       Mean reward: 647.41
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.4538
    Episode_Reward/rotating_object: 127.4974
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.86s
                      Time elapsed: 00:20:59
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1253/1500 [0m                     

                       Computation: 111176 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 3.13
          Mean value_function loss: 64.6757
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.0162
                       Mean reward: 633.59
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.4505
    Episode_Reward/rotating_object: 126.8783
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.88s
                      Time elapsed: 00:21:00
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1254/1500 [0m                     

                       Computation: 111800 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 71.4849
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0094
                       Mean reward: 631.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4535
    Episode_Reward/rotating_object: 126.0623
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.88s
                      Time elapsed: 00:21:01
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1255/1500 [0m                     

                       Computation: 111078 steps/s (collection: 0.790s, learning 0.095s)
             Mean action noise std: 3.13
          Mean value_function loss: 72.1358
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.0094
                       Mean reward: 616.41
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.4492
    Episode_Reward/rotating_object: 125.9031
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.88s
                      Time elapsed: 00:21:02
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1256/1500 [0m                     

                       Computation: 114327 steps/s (collection: 0.770s, learning 0.090s)
             Mean action noise std: 3.14
          Mean value_function loss: 69.3931
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0169
                       Mean reward: 626.15
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.4459
    Episode_Reward/rotating_object: 127.5302
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.86s
                      Time elapsed: 00:21:02
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1257/1500 [0m                     

                       Computation: 110623 steps/s (collection: 0.786s, learning 0.103s)
             Mean action noise std: 3.14
          Mean value_function loss: 70.1132
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.0231
                       Mean reward: 653.81
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.4542
    Episode_Reward/rotating_object: 127.4621
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.89s
                      Time elapsed: 00:21:03
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1258/1500 [0m                     

                       Computation: 111803 steps/s (collection: 0.784s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 62.3790
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0320
                       Mean reward: 646.61
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.4509
    Episode_Reward/rotating_object: 130.1085
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.88s
                      Time elapsed: 00:21:04
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1259/1500 [0m                     

                       Computation: 110262 steps/s (collection: 0.781s, learning 0.110s)
             Mean action noise std: 3.14
          Mean value_function loss: 72.4491
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0431
                       Mean reward: 648.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4567
    Episode_Reward/rotating_object: 127.2851
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.89s
                      Time elapsed: 00:21:05
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1260/1500 [0m                     

                       Computation: 111603 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 64.4216
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0527
                       Mean reward: 646.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4600
    Episode_Reward/rotating_object: 131.5385
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.88s
                      Time elapsed: 00:21:06
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1261/1500 [0m                     

                       Computation: 115059 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 62.8227
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 18.0570
                       Mean reward: 607.30
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.4485
    Episode_Reward/rotating_object: 125.0997
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.85s
                      Time elapsed: 00:21:07
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1262/1500 [0m                     

                       Computation: 114343 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 3.15
          Mean value_function loss: 60.3697
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0553
                       Mean reward: 653.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4590
    Episode_Reward/rotating_object: 131.2909
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.86s
                      Time elapsed: 00:21:08
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1263/1500 [0m                     

                       Computation: 111836 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 61.5896
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.0495
                       Mean reward: 659.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 128.9636
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.88s
                      Time elapsed: 00:21:09
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1264/1500 [0m                     

                       Computation: 110497 steps/s (collection: 0.786s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 65.4089
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0465
                       Mean reward: 611.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4535
    Episode_Reward/rotating_object: 129.5756
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.89s
                      Time elapsed: 00:21:09
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1265/1500 [0m                     

                       Computation: 107001 steps/s (collection: 0.805s, learning 0.114s)
             Mean action noise std: 3.15
          Mean value_function loss: 69.8656
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0533
                       Mean reward: 648.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4553
    Episode_Reward/rotating_object: 127.7404
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.92s
                      Time elapsed: 00:21:10
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1266/1500 [0m                     

                       Computation: 113181 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 64.4681
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0554
                       Mean reward: 668.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4552
    Episode_Reward/rotating_object: 130.0635
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.87s
                      Time elapsed: 00:21:11
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1267/1500 [0m                     

                       Computation: 112810 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 62.2769
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0474
                       Mean reward: 662.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4569
    Episode_Reward/rotating_object: 130.9109
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.87s
                      Time elapsed: 00:21:12
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1268/1500 [0m                     

                       Computation: 113451 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 3.15
          Mean value_function loss: 61.2333
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0408
                       Mean reward: 662.31
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 130.0786
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.87s
                      Time elapsed: 00:21:13
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1269/1500 [0m                     

                       Computation: 112982 steps/s (collection: 0.769s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 69.1373
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0348
                       Mean reward: 663.71
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.4542
    Episode_Reward/rotating_object: 131.8100
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.87s
                      Time elapsed: 00:21:14
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1270/1500 [0m                     

                       Computation: 110060 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 3.16
          Mean value_function loss: 69.2174
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0344
                       Mean reward: 646.62
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.4497
    Episode_Reward/rotating_object: 129.3569
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.89s
                      Time elapsed: 00:21:15
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1271/1500 [0m                     

                       Computation: 111069 steps/s (collection: 0.791s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 66.6967
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0395
                       Mean reward: 654.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4476
    Episode_Reward/rotating_object: 127.4260
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.89s
                      Time elapsed: 00:21:16
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1272/1500 [0m                     

                       Computation: 109788 steps/s (collection: 0.793s, learning 0.102s)
             Mean action noise std: 3.16
          Mean value_function loss: 65.1215
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0437
                       Mean reward: 609.34
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.4508
    Episode_Reward/rotating_object: 126.8021
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.90s
                      Time elapsed: 00:21:17
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1273/1500 [0m                     

                       Computation: 110280 steps/s (collection: 0.778s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 63.4728
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0429
                       Mean reward: 652.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4524
    Episode_Reward/rotating_object: 132.4330
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.89s
                      Time elapsed: 00:21:17
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1274/1500 [0m                     

                       Computation: 111311 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 3.16
          Mean value_function loss: 68.6195
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0438
                       Mean reward: 661.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4530
    Episode_Reward/rotating_object: 130.1391
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.88s
                      Time elapsed: 00:21:18
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1275/1500 [0m                     

                       Computation: 111122 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 60.3890
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.0502
                       Mean reward: 678.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4431
    Episode_Reward/rotating_object: 129.9719
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.88s
                      Time elapsed: 00:21:19
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1276/1500 [0m                     

                       Computation: 111064 steps/s (collection: 0.788s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 57.9056
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.0563
                       Mean reward: 660.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4587
    Episode_Reward/rotating_object: 131.9822
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.89s
                      Time elapsed: 00:21:20
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1277/1500 [0m                     

                       Computation: 109251 steps/s (collection: 0.790s, learning 0.110s)
             Mean action noise std: 3.17
          Mean value_function loss: 60.0157
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.0514
                       Mean reward: 634.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4546
    Episode_Reward/rotating_object: 129.3601
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.90s
                      Time elapsed: 00:21:21
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1278/1500 [0m                     

                       Computation: 110201 steps/s (collection: 0.786s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 56.8054
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 18.0550
                       Mean reward: 625.04
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.4472
    Episode_Reward/rotating_object: 126.7248
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.89s
                      Time elapsed: 00:21:22
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1279/1500 [0m                     

                       Computation: 112570 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 53.5482
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0623
                       Mean reward: 676.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4483
    Episode_Reward/rotating_object: 129.5753
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.87s
                      Time elapsed: 00:21:23
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1280/1500 [0m                     

                       Computation: 108743 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 52.5596
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0710
                       Mean reward: 600.33
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.4481
    Episode_Reward/rotating_object: 126.1678
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.90s
                      Time elapsed: 00:21:24
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1281/1500 [0m                     

                       Computation: 108217 steps/s (collection: 0.793s, learning 0.115s)
             Mean action noise std: 3.17
          Mean value_function loss: 55.0569
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0716
                       Mean reward: 673.61
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.4477
    Episode_Reward/rotating_object: 130.1233
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.91s
                      Time elapsed: 00:21:25
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1282/1500 [0m                     

                       Computation: 108225 steps/s (collection: 0.795s, learning 0.114s)
             Mean action noise std: 3.17
          Mean value_function loss: 62.0601
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0710
                       Mean reward: 671.07
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.4489
    Episode_Reward/rotating_object: 129.9703
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.91s
                      Time elapsed: 00:21:25
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1283/1500 [0m                     

                       Computation: 110731 steps/s (collection: 0.762s, learning 0.126s)
             Mean action noise std: 3.18
          Mean value_function loss: 58.2770
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 18.0737
                       Mean reward: 672.86
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4521
    Episode_Reward/rotating_object: 131.1041
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.89s
                      Time elapsed: 00:21:26
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1284/1500 [0m                     

                       Computation: 106390 steps/s (collection: 0.814s, learning 0.110s)
             Mean action noise std: 3.18
          Mean value_function loss: 62.1661
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0762
                       Mean reward: 683.13
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.4531
    Episode_Reward/rotating_object: 131.4132
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.92s
                      Time elapsed: 00:21:27
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1285/1500 [0m                     

                       Computation: 109497 steps/s (collection: 0.791s, learning 0.107s)
             Mean action noise std: 3.18
          Mean value_function loss: 69.0825
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0769
                       Mean reward: 638.39
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.4510
    Episode_Reward/rotating_object: 128.8151
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.90s
                      Time elapsed: 00:21:28
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1286/1500 [0m                     

                       Computation: 110461 steps/s (collection: 0.779s, learning 0.111s)
             Mean action noise std: 3.18
          Mean value_function loss: 55.4958
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0739
                       Mean reward: 665.42
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.4574
    Episode_Reward/rotating_object: 130.8229
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.89s
                      Time elapsed: 00:21:29
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1287/1500 [0m                     

                       Computation: 110652 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 58.0872
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0808
                       Mean reward: 668.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4567
    Episode_Reward/rotating_object: 133.0606
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.89s
                      Time elapsed: 00:21:30
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1288/1500 [0m                     

                       Computation: 109313 steps/s (collection: 0.786s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 62.9701
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0863
                       Mean reward: 685.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4540
    Episode_Reward/rotating_object: 131.5244
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.90s
                      Time elapsed: 00:21:31
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1289/1500 [0m                     

                       Computation: 113056 steps/s (collection: 0.759s, learning 0.111s)
             Mean action noise std: 3.19
          Mean value_function loss: 58.4277
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.0807
                       Mean reward: 654.47
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.4570
    Episode_Reward/rotating_object: 130.3424
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.87s
                      Time elapsed: 00:21:32
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1290/1500 [0m                     

                       Computation: 114364 steps/s (collection: 0.762s, learning 0.098s)
             Mean action noise std: 3.19
          Mean value_function loss: 60.5223
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0801
                       Mean reward: 647.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4542
    Episode_Reward/rotating_object: 130.7811
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.86s
                      Time elapsed: 00:21:33
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1291/1500 [0m                     

                       Computation: 114053 steps/s (collection: 0.769s, learning 0.093s)
             Mean action noise std: 3.19
          Mean value_function loss: 59.7425
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.0798
                       Mean reward: 657.39
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.4534
    Episode_Reward/rotating_object: 132.9970
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.86s
                      Time elapsed: 00:21:33
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1292/1500 [0m                     

                       Computation: 104716 steps/s (collection: 0.823s, learning 0.116s)
             Mean action noise std: 3.19
          Mean value_function loss: 49.0541
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0813
                       Mean reward: 658.84
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.4483
    Episode_Reward/rotating_object: 131.7809
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.94s
                      Time elapsed: 00:21:34
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1293/1500 [0m                     

                       Computation: 100764 steps/s (collection: 0.828s, learning 0.148s)
             Mean action noise std: 3.19
          Mean value_function loss: 59.5756
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0865
                       Mean reward: 636.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4470
    Episode_Reward/rotating_object: 129.0573
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.98s
                      Time elapsed: 00:21:35
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1294/1500 [0m                     

                       Computation: 106567 steps/s (collection: 0.822s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 52.9503
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0961
                       Mean reward: 675.50
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.4510
    Episode_Reward/rotating_object: 131.5082
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.92s
                      Time elapsed: 00:21:36
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1295/1500 [0m                     

                       Computation: 112057 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 56.6970
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0961
                       Mean reward: 645.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4483
    Episode_Reward/rotating_object: 130.2732
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.88s
                      Time elapsed: 00:21:37
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1296/1500 [0m                     

                       Computation: 112110 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 3.20
          Mean value_function loss: 59.6186
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0934
                       Mean reward: 652.52
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.4457
    Episode_Reward/rotating_object: 127.7366
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.88s
                      Time elapsed: 00:21:38
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1297/1500 [0m                     

                       Computation: 109471 steps/s (collection: 0.782s, learning 0.116s)
             Mean action noise std: 3.20
          Mean value_function loss: 55.0696
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0882
                       Mean reward: 682.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4563
    Episode_Reward/rotating_object: 134.7267
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.90s
                      Time elapsed: 00:21:39
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1298/1500 [0m                     

                       Computation: 103320 steps/s (collection: 0.812s, learning 0.140s)
             Mean action noise std: 3.20
          Mean value_function loss: 62.2812
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0858
                       Mean reward: 635.72
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.4518
    Episode_Reward/rotating_object: 130.5259
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.95s
                      Time elapsed: 00:21:40
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1299/1500 [0m                     

                       Computation: 100347 steps/s (collection: 0.864s, learning 0.116s)
             Mean action noise std: 3.20
          Mean value_function loss: 56.1019
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0886
                       Mean reward: 649.45
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4505
    Episode_Reward/rotating_object: 131.7454
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.98s
                      Time elapsed: 00:21:41
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1300/1500 [0m                     

                       Computation: 112435 steps/s (collection: 0.772s, learning 0.103s)
             Mean action noise std: 3.20
          Mean value_function loss: 52.8680
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0908
                       Mean reward: 648.85
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.4519
    Episode_Reward/rotating_object: 129.3815
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.87s
                      Time elapsed: 00:21:42
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1301/1500 [0m                     

                       Computation: 110967 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 3.20
          Mean value_function loss: 56.4719
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0975
                       Mean reward: 649.39
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 129.7120
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.89s
                      Time elapsed: 00:21:43
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1302/1500 [0m                     

                       Computation: 109037 steps/s (collection: 0.783s, learning 0.118s)
             Mean action noise std: 3.20
          Mean value_function loss: 50.6765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1049
                       Mean reward: 671.58
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4574
    Episode_Reward/rotating_object: 134.3447
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.90s
                      Time elapsed: 00:21:43
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1303/1500 [0m                     

                       Computation: 109651 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 3.21
          Mean value_function loss: 56.5982
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1150
                       Mean reward: 627.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4513
    Episode_Reward/rotating_object: 128.3262
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.90s
                      Time elapsed: 00:21:44
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1304/1500 [0m                     

                       Computation: 103491 steps/s (collection: 0.835s, learning 0.115s)
             Mean action noise std: 3.21
          Mean value_function loss: 50.4532
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.1207
                       Mean reward: 660.38
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.4541
    Episode_Reward/rotating_object: 130.5784
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.95s
                      Time elapsed: 00:21:45
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1305/1500 [0m                     

                       Computation: 114422 steps/s (collection: 0.755s, learning 0.105s)
             Mean action noise std: 3.21
          Mean value_function loss: 52.3340
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.1294
                       Mean reward: 656.61
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.4478
    Episode_Reward/rotating_object: 129.2203
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.86s
                      Time elapsed: 00:21:46
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1306/1500 [0m                     

                       Computation: 110591 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 3.21
          Mean value_function loss: 57.2497
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1354
                       Mean reward: 658.46
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.4538
    Episode_Reward/rotating_object: 135.0439
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.89s
                      Time elapsed: 00:21:47
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1307/1500 [0m                     

                       Computation: 113801 steps/s (collection: 0.754s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 60.2060
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.1484
                       Mean reward: 651.48
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.4499
    Episode_Reward/rotating_object: 127.6892
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.86s
                      Time elapsed: 00:21:48
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1308/1500 [0m                     

                       Computation: 112519 steps/s (collection: 0.769s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 63.1105
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.1476
                       Mean reward: 649.06
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.4539
    Episode_Reward/rotating_object: 132.5147
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.87s
                      Time elapsed: 00:21:49
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1309/1500 [0m                     

                       Computation: 106952 steps/s (collection: 0.818s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 55.7577
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1381
                       Mean reward: 651.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4527
    Episode_Reward/rotating_object: 130.9090
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.92s
                      Time elapsed: 00:21:50
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1310/1500 [0m                     

                       Computation: 109993 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 3.22
          Mean value_function loss: 54.4688
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.1461
                       Mean reward: 651.74
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.4482
    Episode_Reward/rotating_object: 131.0370
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.89s
                      Time elapsed: 00:21:51
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1311/1500 [0m                     

                       Computation: 109193 steps/s (collection: 0.800s, learning 0.100s)
             Mean action noise std: 3.22
          Mean value_function loss: 58.5868
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.1488
                       Mean reward: 651.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4565
    Episode_Reward/rotating_object: 130.4810
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.90s
                      Time elapsed: 00:21:52
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1312/1500 [0m                     

                       Computation: 111690 steps/s (collection: 0.778s, learning 0.103s)
             Mean action noise std: 3.22
          Mean value_function loss: 60.3775
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.1504
                       Mean reward: 681.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4615
    Episode_Reward/rotating_object: 133.7564
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.88s
                      Time elapsed: 00:21:52
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1313/1500 [0m                     

                       Computation: 109715 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 61.8841
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.1512
                       Mean reward: 663.33
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.4573
    Episode_Reward/rotating_object: 131.4547
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.90s
                      Time elapsed: 00:21:53
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1314/1500 [0m                     

                       Computation: 111107 steps/s (collection: 0.771s, learning 0.114s)
             Mean action noise std: 3.22
          Mean value_function loss: 53.5051
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.1515
                       Mean reward: 677.51
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.4641
    Episode_Reward/rotating_object: 131.1920
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.88s
                      Time elapsed: 00:21:54
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1315/1500 [0m                     

                       Computation: 111298 steps/s (collection: 0.773s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 61.3231
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.1535
                       Mean reward: 607.54
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.4526
    Episode_Reward/rotating_object: 129.1637
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.88s
                      Time elapsed: 00:21:55
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1316/1500 [0m                     

                       Computation: 112879 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 3.22
          Mean value_function loss: 59.5882
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1522
                       Mean reward: 641.95
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.4593
    Episode_Reward/rotating_object: 128.4703
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.87s
                      Time elapsed: 00:21:56
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1317/1500 [0m                     

                       Computation: 110602 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 60.0995
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.1441
                       Mean reward: 664.88
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.4621
    Episode_Reward/rotating_object: 131.1772
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.89s
                      Time elapsed: 00:21:57
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1318/1500 [0m                     

                       Computation: 112402 steps/s (collection: 0.764s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 55.8441
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.1407
                       Mean reward: 681.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4570
    Episode_Reward/rotating_object: 130.8873
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.87s
                      Time elapsed: 00:21:58
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1319/1500 [0m                     

                       Computation: 108658 steps/s (collection: 0.804s, learning 0.101s)
             Mean action noise std: 3.23
          Mean value_function loss: 66.5501
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.1458
                       Mean reward: 664.53
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.4562
    Episode_Reward/rotating_object: 130.8110
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.90s
                      Time elapsed: 00:21:59
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1320/1500 [0m                     

                       Computation: 112105 steps/s (collection: 0.767s, learning 0.110s)
             Mean action noise std: 3.23
          Mean value_function loss: 61.1096
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.1625
                       Mean reward: 652.03
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.4550
    Episode_Reward/rotating_object: 129.0839
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.88s
                      Time elapsed: 00:21:59
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1321/1500 [0m                     

                       Computation: 107903 steps/s (collection: 0.800s, learning 0.111s)
             Mean action noise std: 3.23
          Mean value_function loss: 50.4287
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.1725
                       Mean reward: 665.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4607
    Episode_Reward/rotating_object: 134.8335
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.91s
                      Time elapsed: 00:22:00
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1322/1500 [0m                     

                       Computation: 111092 steps/s (collection: 0.775s, learning 0.110s)
             Mean action noise std: 3.24
          Mean value_function loss: 57.6738
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.1849
                       Mean reward: 665.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4562
    Episode_Reward/rotating_object: 130.9037
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.88s
                      Time elapsed: 00:22:01
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1323/1500 [0m                     

                       Computation: 113994 steps/s (collection: 0.759s, learning 0.104s)
             Mean action noise std: 3.24
          Mean value_function loss: 52.4924
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1867
                       Mean reward: 668.80
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.4557
    Episode_Reward/rotating_object: 131.5205
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.86s
                      Time elapsed: 00:22:02
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1324/1500 [0m                     

                       Computation: 109022 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 3.24
          Mean value_function loss: 54.4650
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.1907
                       Mean reward: 656.84
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.4651
    Episode_Reward/rotating_object: 133.4099
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.90s
                      Time elapsed: 00:22:03
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1325/1500 [0m                     

                       Computation: 111278 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 3.24
          Mean value_function loss: 60.8213
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2004
                       Mean reward: 647.96
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.4566
    Episode_Reward/rotating_object: 132.8963
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.88s
                      Time elapsed: 00:22:04
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1326/1500 [0m                     

                       Computation: 109166 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 3.24
          Mean value_function loss: 54.4879
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2060
                       Mean reward: 682.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4618
    Episode_Reward/rotating_object: 133.6679
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.90s
                      Time elapsed: 00:22:05
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1327/1500 [0m                     

                       Computation: 113155 steps/s (collection: 0.771s, learning 0.098s)
             Mean action noise std: 3.24
          Mean value_function loss: 63.3456
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2085
                       Mean reward: 657.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4551
    Episode_Reward/rotating_object: 131.7360
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.87s
                      Time elapsed: 00:22:06
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1328/1500 [0m                     

                       Computation: 112805 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 3.25
          Mean value_function loss: 57.7154
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2158
                       Mean reward: 650.13
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.4564
    Episode_Reward/rotating_object: 132.1393
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.87s
                      Time elapsed: 00:22:07
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1329/1500 [0m                     

                       Computation: 110891 steps/s (collection: 0.776s, learning 0.111s)
             Mean action noise std: 3.25
          Mean value_function loss: 66.1816
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.2218
                       Mean reward: 679.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4577
    Episode_Reward/rotating_object: 132.1531
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.89s
                      Time elapsed: 00:22:07
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1330/1500 [0m                     

                       Computation: 114369 steps/s (collection: 0.754s, learning 0.105s)
             Mean action noise std: 3.25
          Mean value_function loss: 65.3342
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.2297
                       Mean reward: 662.66
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 132.0738
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.86s
                      Time elapsed: 00:22:08
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1331/1500 [0m                     

                       Computation: 110826 steps/s (collection: 0.780s, learning 0.107s)
             Mean action noise std: 3.25
          Mean value_function loss: 58.9944
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2406
                       Mean reward: 663.11
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.4557
    Episode_Reward/rotating_object: 132.3871
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.89s
                      Time elapsed: 00:22:09
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1332/1500 [0m                     

                       Computation: 113611 steps/s (collection: 0.763s, learning 0.102s)
             Mean action noise std: 3.25
          Mean value_function loss: 58.5934
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.2406
                       Mean reward: 655.69
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.4592
    Episode_Reward/rotating_object: 131.0483
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.87s
                      Time elapsed: 00:22:10
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1333/1500 [0m                     

                       Computation: 47542 steps/s (collection: 1.965s, learning 0.103s)
             Mean action noise std: 3.26
          Mean value_function loss: 57.8708
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.2409
                       Mean reward: 649.70
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.4574
    Episode_Reward/rotating_object: 131.1458
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.07s
                      Time elapsed: 00:22:12
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1334/1500 [0m                     

                       Computation: 33767 steps/s (collection: 2.793s, learning 0.119s)
             Mean action noise std: 3.26
          Mean value_function loss: 64.3028
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2416
                       Mean reward: 694.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4562
    Episode_Reward/rotating_object: 132.9739
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 2.91s
                      Time elapsed: 00:22:15
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1335/1500 [0m                     

                       Computation: 34832 steps/s (collection: 2.700s, learning 0.123s)
             Mean action noise std: 3.26
          Mean value_function loss: 65.9726
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2434
                       Mean reward: 648.60
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.4550
    Episode_Reward/rotating_object: 130.9229
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 2.82s
                      Time elapsed: 00:22:18
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1336/1500 [0m                     

                       Computation: 36686 steps/s (collection: 2.574s, learning 0.106s)
             Mean action noise std: 3.26
          Mean value_function loss: 62.7522
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.2428
                       Mean reward: 645.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4549
    Episode_Reward/rotating_object: 129.7044
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 2.68s
                      Time elapsed: 00:22:21
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1337/1500 [0m                     

                       Computation: 34406 steps/s (collection: 2.734s, learning 0.123s)
             Mean action noise std: 3.26
          Mean value_function loss: 67.1204
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2433
                       Mean reward: 641.68
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.4599
    Episode_Reward/rotating_object: 132.4996
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 2.86s
                      Time elapsed: 00:22:23
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1338/1500 [0m                     

                       Computation: 33411 steps/s (collection: 2.809s, learning 0.133s)
             Mean action noise std: 3.26
          Mean value_function loss: 61.4193
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2428
                       Mean reward: 655.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4525
    Episode_Reward/rotating_object: 125.8317
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 2.94s
                      Time elapsed: 00:22:26
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1339/1500 [0m                     

                       Computation: 32412 steps/s (collection: 2.888s, learning 0.145s)
             Mean action noise std: 3.26
          Mean value_function loss: 60.6772
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.2396
                       Mean reward: 650.24
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.4539
    Episode_Reward/rotating_object: 127.6569
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.03s
                      Time elapsed: 00:22:29
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1340/1500 [0m                     

                       Computation: 33218 steps/s (collection: 2.823s, learning 0.136s)
             Mean action noise std: 3.26
          Mean value_function loss: 63.9036
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2429
                       Mean reward: 667.23
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4572
    Episode_Reward/rotating_object: 129.8358
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 2.96s
                      Time elapsed: 00:22:32
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1341/1500 [0m                     

                       Computation: 23097 steps/s (collection: 4.134s, learning 0.122s)
             Mean action noise std: 3.27
          Mean value_function loss: 58.2850
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.2481
                       Mean reward: 655.63
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.4563
    Episode_Reward/rotating_object: 129.7140
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.26s
                      Time elapsed: 00:22:37
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1342/1500 [0m                     

                       Computation: 111341 steps/s (collection: 0.787s, learning 0.096s)
             Mean action noise std: 3.27
          Mean value_function loss: 60.5815
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.2513
                       Mean reward: 679.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4610
    Episode_Reward/rotating_object: 131.9770
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.88s
                      Time elapsed: 00:22:37
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1343/1500 [0m                     

                       Computation: 117303 steps/s (collection: 0.737s, learning 0.101s)
             Mean action noise std: 3.27
          Mean value_function loss: 63.1392
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2555
                       Mean reward: 651.10
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.4581
    Episode_Reward/rotating_object: 129.9974
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.84s
                      Time elapsed: 00:22:38
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1344/1500 [0m                     

                       Computation: 114943 steps/s (collection: 0.745s, learning 0.111s)
             Mean action noise std: 3.27
          Mean value_function loss: 54.3518
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.2606
                       Mean reward: 646.31
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.4649
    Episode_Reward/rotating_object: 132.0414
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.86s
                      Time elapsed: 00:22:39
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1345/1500 [0m                     

                       Computation: 109098 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 3.28
          Mean value_function loss: 52.2582
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2682
                       Mean reward: 650.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4558
    Episode_Reward/rotating_object: 128.6080
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.90s
                      Time elapsed: 00:22:40
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1346/1500 [0m                     

                       Computation: 107147 steps/s (collection: 0.796s, learning 0.121s)
             Mean action noise std: 3.28
          Mean value_function loss: 47.3421
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.2766
                       Mean reward: 646.85
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.4530
    Episode_Reward/rotating_object: 131.4285
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.92s
                      Time elapsed: 00:22:41
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1347/1500 [0m                     

                       Computation: 105447 steps/s (collection: 0.820s, learning 0.112s)
             Mean action noise std: 3.28
          Mean value_function loss: 60.1019
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2744
                       Mean reward: 663.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4546
    Episode_Reward/rotating_object: 131.8991
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.93s
                      Time elapsed: 00:22:42
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1348/1500 [0m                     

                       Computation: 113465 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 3.28
          Mean value_function loss: 53.8867
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2746
                       Mean reward: 644.56
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.4551
    Episode_Reward/rotating_object: 131.6462
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.87s
                      Time elapsed: 00:22:43
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1349/1500 [0m                     

                       Computation: 113696 steps/s (collection: 0.760s, learning 0.105s)
             Mean action noise std: 3.28
          Mean value_function loss: 67.3828
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.2753
                       Mean reward: 659.51
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4506
    Episode_Reward/rotating_object: 129.9807
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.86s
                      Time elapsed: 00:22:44
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1350/1500 [0m                     

                       Computation: 111484 steps/s (collection: 0.773s, learning 0.109s)
             Mean action noise std: 3.28
          Mean value_function loss: 51.3719
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.2748
                       Mean reward: 644.78
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.4476
    Episode_Reward/rotating_object: 127.5449
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.88s
                      Time elapsed: 00:22:45
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1351/1500 [0m                     

                       Computation: 118309 steps/s (collection: 0.733s, learning 0.098s)
             Mean action noise std: 3.28
          Mean value_function loss: 50.1155
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.2702
                       Mean reward: 649.16
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4553
    Episode_Reward/rotating_object: 131.6433
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.83s
                      Time elapsed: 00:22:45
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1352/1500 [0m                     

                       Computation: 116797 steps/s (collection: 0.741s, learning 0.101s)
             Mean action noise std: 3.28
          Mean value_function loss: 55.2102
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2799
                       Mean reward: 643.78
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.4590
    Episode_Reward/rotating_object: 132.5241
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.84s
                      Time elapsed: 00:22:46
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1353/1500 [0m                     

                       Computation: 113327 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 3.28
          Mean value_function loss: 54.0655
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.2719
                       Mean reward: 643.39
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4576
    Episode_Reward/rotating_object: 130.7106
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.87s
                      Time elapsed: 00:22:47
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1354/1500 [0m                     

                       Computation: 117343 steps/s (collection: 0.738s, learning 0.100s)
             Mean action noise std: 3.29
          Mean value_function loss: 47.3962
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.2645
                       Mean reward: 659.16
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.4589
    Episode_Reward/rotating_object: 134.4256
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.84s
                      Time elapsed: 00:22:48
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1355/1500 [0m                     

                       Computation: 113052 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 3.29
          Mean value_function loss: 55.5779
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.2642
                       Mean reward: 641.80
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.4573
    Episode_Reward/rotating_object: 132.0256
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.87s
                      Time elapsed: 00:22:49
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1356/1500 [0m                     

                       Computation: 117279 steps/s (collection: 0.753s, learning 0.086s)
             Mean action noise std: 3.29
          Mean value_function loss: 57.6722
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2704
                       Mean reward: 677.14
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.4536
    Episode_Reward/rotating_object: 132.3737
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.84s
                      Time elapsed: 00:22:50
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1357/1500 [0m                     

                       Computation: 114321 steps/s (collection: 0.754s, learning 0.106s)
             Mean action noise std: 3.29
          Mean value_function loss: 53.8294
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.2742
                       Mean reward: 650.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4529
    Episode_Reward/rotating_object: 131.2362
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.86s
                      Time elapsed: 00:22:50
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1358/1500 [0m                     

                       Computation: 117197 steps/s (collection: 0.742s, learning 0.097s)
             Mean action noise std: 3.29
          Mean value_function loss: 60.7683
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.2693
                       Mean reward: 689.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4502
    Episode_Reward/rotating_object: 134.3757
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.84s
                      Time elapsed: 00:22:51
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1359/1500 [0m                     

                       Computation: 114667 steps/s (collection: 0.763s, learning 0.094s)
             Mean action noise std: 3.29
          Mean value_function loss: 55.9055
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.2667
                       Mean reward: 658.49
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.4519
    Episode_Reward/rotating_object: 130.0175
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.86s
                      Time elapsed: 00:22:52
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1360/1500 [0m                     

                       Computation: 115194 steps/s (collection: 0.755s, learning 0.099s)
             Mean action noise std: 3.29
          Mean value_function loss: 54.0344
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2652
                       Mean reward: 648.94
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.4551
    Episode_Reward/rotating_object: 131.8393
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.85s
                      Time elapsed: 00:22:53
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1361/1500 [0m                     

                       Computation: 111055 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 3.29
          Mean value_function loss: 62.5094
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2708
                       Mean reward: 649.81
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.4559
    Episode_Reward/rotating_object: 132.1150
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.89s
                      Time elapsed: 00:22:54
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1362/1500 [0m                     

                       Computation: 114526 steps/s (collection: 0.758s, learning 0.100s)
             Mean action noise std: 3.30
          Mean value_function loss: 53.5784
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.2777
                       Mean reward: 658.60
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.4592
    Episode_Reward/rotating_object: 134.4159
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.86s
                      Time elapsed: 00:22:55
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1363/1500 [0m                     

                       Computation: 110481 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 57.7843
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2714
                       Mean reward: 664.38
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.4569
    Episode_Reward/rotating_object: 132.3429
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.89s
                      Time elapsed: 00:22:56
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1364/1500 [0m                     

                       Computation: 115606 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.3319
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2670
                       Mean reward: 649.60
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.4568
    Episode_Reward/rotating_object: 130.1648
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.85s
                      Time elapsed: 00:22:57
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1365/1500 [0m                     

                       Computation: 111348 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 3.30
          Mean value_function loss: 57.1778
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2746
                       Mean reward: 680.57
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.4564
    Episode_Reward/rotating_object: 131.6614
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.88s
                      Time elapsed: 00:22:57
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1366/1500 [0m                     

                       Computation: 118187 steps/s (collection: 0.745s, learning 0.087s)
             Mean action noise std: 3.30
          Mean value_function loss: 55.7920
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.2810
                       Mean reward: 631.46
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4539
    Episode_Reward/rotating_object: 132.9847
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.83s
                      Time elapsed: 00:22:58
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1367/1500 [0m                     

                       Computation: 116865 steps/s (collection: 0.742s, learning 0.100s)
             Mean action noise std: 3.30
          Mean value_function loss: 65.9017
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2773
                       Mean reward: 649.13
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4605
    Episode_Reward/rotating_object: 132.4326
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.84s
                      Time elapsed: 00:22:59
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1368/1500 [0m                     

                       Computation: 116367 steps/s (collection: 0.745s, learning 0.100s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.9680
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 18.2816
                       Mean reward: 669.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4557
    Episode_Reward/rotating_object: 133.3755
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.84s
                      Time elapsed: 00:23:00
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1369/1500 [0m                     

                       Computation: 117124 steps/s (collection: 0.748s, learning 0.091s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.0735
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2874
                       Mean reward: 636.37
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.4623
    Episode_Reward/rotating_object: 132.8571
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.84s
                      Time elapsed: 00:23:01
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1370/1500 [0m                     

                       Computation: 114009 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.6433
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2996
                       Mean reward: 640.85
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.4585
    Episode_Reward/rotating_object: 130.6764
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.86s
                      Time elapsed: 00:23:02
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1371/1500 [0m                     

                       Computation: 114140 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 3.31
          Mean value_function loss: 58.5200
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3003
                       Mean reward: 670.67
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.4555
    Episode_Reward/rotating_object: 130.4597
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.86s
                      Time elapsed: 00:23:02
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1372/1500 [0m                     

                       Computation: 112581 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 3.31
          Mean value_function loss: 55.0609
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3015
                       Mean reward: 635.13
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.4563
    Episode_Reward/rotating_object: 133.1581
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.87s
                      Time elapsed: 00:23:03
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1373/1500 [0m                     

                       Computation: 110694 steps/s (collection: 0.767s, learning 0.121s)
             Mean action noise std: 3.31
          Mean value_function loss: 58.7110
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2982
                       Mean reward: 659.58
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.4534
    Episode_Reward/rotating_object: 129.7036
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.89s
                      Time elapsed: 00:23:04
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1374/1500 [0m                     

                       Computation: 110391 steps/s (collection: 0.769s, learning 0.121s)
             Mean action noise std: 3.31
          Mean value_function loss: 64.4171
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3004
                       Mean reward: 648.61
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.4567
    Episode_Reward/rotating_object: 132.7337
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.89s
                      Time elapsed: 00:23:05
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1375/1500 [0m                     

                       Computation: 116076 steps/s (collection: 0.745s, learning 0.102s)
             Mean action noise std: 3.31
          Mean value_function loss: 48.4250
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3026
                       Mean reward: 640.23
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4476
    Episode_Reward/rotating_object: 128.9824
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.85s
                      Time elapsed: 00:23:06
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1376/1500 [0m                     

                       Computation: 115192 steps/s (collection: 0.748s, learning 0.105s)
             Mean action noise std: 3.31
          Mean value_function loss: 59.2390
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3035
                       Mean reward: 677.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4416
    Episode_Reward/rotating_object: 129.1570
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.85s
                      Time elapsed: 00:23:07
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1377/1500 [0m                     

                       Computation: 116721 steps/s (collection: 0.751s, learning 0.092s)
             Mean action noise std: 3.31
          Mean value_function loss: 62.2053
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2981
                       Mean reward: 673.53
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.4598
    Episode_Reward/rotating_object: 132.4493
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.84s
                      Time elapsed: 00:23:08
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1378/1500 [0m                     

                       Computation: 111074 steps/s (collection: 0.791s, learning 0.095s)
             Mean action noise std: 3.32
          Mean value_function loss: 64.7545
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3008
                       Mean reward: 667.69
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.4524
    Episode_Reward/rotating_object: 128.6602
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.89s
                      Time elapsed: 00:23:09
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1379/1500 [0m                     

                       Computation: 118823 steps/s (collection: 0.735s, learning 0.093s)
             Mean action noise std: 3.32
          Mean value_function loss: 49.7520
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.3042
                       Mean reward: 611.32
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.4523
    Episode_Reward/rotating_object: 129.7472
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.83s
                      Time elapsed: 00:23:09
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1380/1500 [0m                     

                       Computation: 111628 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 3.32
          Mean value_function loss: 60.5526
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3008
                       Mean reward: 647.01
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.4580
    Episode_Reward/rotating_object: 132.8598
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.88s
                      Time elapsed: 00:23:10
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1381/1500 [0m                     

                       Computation: 114764 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 3.32
          Mean value_function loss: 59.1987
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3046
                       Mean reward: 626.67
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.4610
    Episode_Reward/rotating_object: 132.1257
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.86s
                      Time elapsed: 00:23:11
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1382/1500 [0m                     

                       Computation: 115635 steps/s (collection: 0.755s, learning 0.096s)
             Mean action noise std: 3.32
          Mean value_function loss: 58.8347
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3127
                       Mean reward: 623.82
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.4588
    Episode_Reward/rotating_object: 131.3486
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.85s
                      Time elapsed: 00:23:12
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1383/1500 [0m                     

                       Computation: 114016 steps/s (collection: 0.765s, learning 0.097s)
             Mean action noise std: 3.32
          Mean value_function loss: 63.4467
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.3128
                       Mean reward: 668.69
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.4621
    Episode_Reward/rotating_object: 132.6462
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.86s
                      Time elapsed: 00:23:13
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1384/1500 [0m                     

                       Computation: 115340 steps/s (collection: 0.750s, learning 0.102s)
             Mean action noise std: 3.32
          Mean value_function loss: 59.0118
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.3140
                       Mean reward: 655.42
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.4520
    Episode_Reward/rotating_object: 131.9345
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.85s
                      Time elapsed: 00:23:14
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1385/1500 [0m                     

                       Computation: 117758 steps/s (collection: 0.742s, learning 0.093s)
             Mean action noise std: 3.32
          Mean value_function loss: 57.5805
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3143
                       Mean reward: 636.56
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.4614
    Episode_Reward/rotating_object: 133.1017
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.83s
                      Time elapsed: 00:23:15
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1386/1500 [0m                     

                       Computation: 116035 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 3.33
          Mean value_function loss: 57.8617
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3169
                       Mean reward: 679.89
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.4593
    Episode_Reward/rotating_object: 132.4782
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.85s
                      Time elapsed: 00:23:15
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1387/1500 [0m                     

                       Computation: 109855 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 3.33
          Mean value_function loss: 60.5698
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3138
                       Mean reward: 679.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4629
    Episode_Reward/rotating_object: 131.7496
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.89s
                      Time elapsed: 00:23:16
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1388/1500 [0m                     

                       Computation: 113945 steps/s (collection: 0.764s, learning 0.099s)
             Mean action noise std: 3.33
          Mean value_function loss: 54.8834
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.3145
                       Mean reward: 673.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4566
    Episode_Reward/rotating_object: 135.0660
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.86s
                      Time elapsed: 00:23:17
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1389/1500 [0m                     

                       Computation: 115255 steps/s (collection: 0.746s, learning 0.107s)
             Mean action noise std: 3.33
          Mean value_function loss: 57.0654
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3170
                       Mean reward: 652.37
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.4597
    Episode_Reward/rotating_object: 131.4662
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.85s
                      Time elapsed: 00:23:18
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1390/1500 [0m                     

                       Computation: 117987 steps/s (collection: 0.740s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 60.2957
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3198
                       Mean reward: 647.77
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.4557
    Episode_Reward/rotating_object: 129.6562
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.83s
                      Time elapsed: 00:23:19
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1391/1500 [0m                     

                       Computation: 116918 steps/s (collection: 0.751s, learning 0.089s)
             Mean action noise std: 3.33
          Mean value_function loss: 60.8840
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3239
                       Mean reward: 668.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4583
    Episode_Reward/rotating_object: 132.3977
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.84s
                      Time elapsed: 00:23:20
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1392/1500 [0m                     

                       Computation: 119732 steps/s (collection: 0.728s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 54.6851
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3224
                       Mean reward: 665.38
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.4603
    Episode_Reward/rotating_object: 133.3687
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.82s
                      Time elapsed: 00:23:20
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1393/1500 [0m                     

                       Computation: 115525 steps/s (collection: 0.757s, learning 0.094s)
             Mean action noise std: 3.34
          Mean value_function loss: 58.6489
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3169
                       Mean reward: 684.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4580
    Episode_Reward/rotating_object: 131.7858
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.85s
                      Time elapsed: 00:23:21
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1394/1500 [0m                     

                       Computation: 118894 steps/s (collection: 0.735s, learning 0.092s)
             Mean action noise std: 3.34
          Mean value_function loss: 55.0361
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3198
                       Mean reward: 674.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4594
    Episode_Reward/rotating_object: 134.1697
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.83s
                      Time elapsed: 00:23:22
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1395/1500 [0m                     

                       Computation: 117454 steps/s (collection: 0.738s, learning 0.099s)
             Mean action noise std: 3.34
          Mean value_function loss: 49.4868
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.3238
                       Mean reward: 672.95
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.4592
    Episode_Reward/rotating_object: 132.0864
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.84s
                      Time elapsed: 00:23:23
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1396/1500 [0m                     

                       Computation: 114449 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 61.6362
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3340
                       Mean reward: 682.21
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.4546
    Episode_Reward/rotating_object: 133.5601
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.86s
                      Time elapsed: 00:23:24
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1397/1500 [0m                     

                       Computation: 112114 steps/s (collection: 0.781s, learning 0.096s)
             Mean action noise std: 3.34
          Mean value_function loss: 65.4246
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3376
                       Mean reward: 681.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4627
    Episode_Reward/rotating_object: 134.0163
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.88s
                      Time elapsed: 00:23:25
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1398/1500 [0m                     

                       Computation: 111858 steps/s (collection: 0.779s, learning 0.100s)
             Mean action noise std: 3.34
          Mean value_function loss: 60.4221
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.3460
                       Mean reward: 663.03
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4600
    Episode_Reward/rotating_object: 132.3146
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.88s
                      Time elapsed: 00:23:26
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1399/1500 [0m                     

                       Computation: 117118 steps/s (collection: 0.747s, learning 0.092s)
             Mean action noise std: 3.35
          Mean value_function loss: 64.8582
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.3456
                       Mean reward: 677.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4570
    Episode_Reward/rotating_object: 130.9008
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.84s
                      Time elapsed: 00:23:26
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1400/1500 [0m                     

                       Computation: 115815 steps/s (collection: 0.744s, learning 0.105s)
             Mean action noise std: 3.35
          Mean value_function loss: 60.0664
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.3458
                       Mean reward: 651.94
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.4591
    Episode_Reward/rotating_object: 131.8333
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.85s
                      Time elapsed: 00:23:27
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1401/1500 [0m                     

                       Computation: 117737 steps/s (collection: 0.745s, learning 0.090s)
             Mean action noise std: 3.35
          Mean value_function loss: 62.9994
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3433
                       Mean reward: 660.78
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.4554
    Episode_Reward/rotating_object: 133.0816
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.83s
                      Time elapsed: 00:23:28
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1402/1500 [0m                     

                       Computation: 114866 steps/s (collection: 0.752s, learning 0.104s)
             Mean action noise std: 3.35
          Mean value_function loss: 58.6520
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3428
                       Mean reward: 683.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4561
    Episode_Reward/rotating_object: 129.7247
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.86s
                      Time elapsed: 00:23:29
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1403/1500 [0m                     

                       Computation: 116760 steps/s (collection: 0.747s, learning 0.095s)
             Mean action noise std: 3.35
          Mean value_function loss: 56.6983
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3454
                       Mean reward: 634.60
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.4595
    Episode_Reward/rotating_object: 131.5236
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.84s
                      Time elapsed: 00:23:30
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1404/1500 [0m                     

                       Computation: 115667 steps/s (collection: 0.753s, learning 0.097s)
             Mean action noise std: 3.35
          Mean value_function loss: 56.5558
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3485
                       Mean reward: 664.59
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.4632
    Episode_Reward/rotating_object: 133.4042
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.85s
                      Time elapsed: 00:23:31
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1405/1500 [0m                     

                       Computation: 117770 steps/s (collection: 0.743s, learning 0.092s)
             Mean action noise std: 3.35
          Mean value_function loss: 58.4698
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3547
                       Mean reward: 635.31
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4677
    Episode_Reward/rotating_object: 134.2432
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.83s
                      Time elapsed: 00:23:32
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1406/1500 [0m                     

                       Computation: 118102 steps/s (collection: 0.736s, learning 0.096s)
             Mean action noise std: 3.36
          Mean value_function loss: 58.5538
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.3633
                       Mean reward: 622.20
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.4562
    Episode_Reward/rotating_object: 127.9378
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.83s
                      Time elapsed: 00:23:32
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1407/1500 [0m                     

                       Computation: 115269 steps/s (collection: 0.736s, learning 0.117s)
             Mean action noise std: 3.36
          Mean value_function loss: 52.5957
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.3650
                       Mean reward: 644.70
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.4634
    Episode_Reward/rotating_object: 129.4147
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.85s
                      Time elapsed: 00:23:33
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1408/1500 [0m                     

                       Computation: 114448 steps/s (collection: 0.746s, learning 0.113s)
             Mean action noise std: 3.36
          Mean value_function loss: 62.4627
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3607
                       Mean reward: 684.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4610
    Episode_Reward/rotating_object: 135.1548
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.86s
                      Time elapsed: 00:23:34
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1409/1500 [0m                     

                       Computation: 112744 steps/s (collection: 0.759s, learning 0.113s)
             Mean action noise std: 3.36
          Mean value_function loss: 63.3541
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3594
                       Mean reward: 683.45
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.4578
    Episode_Reward/rotating_object: 131.7514
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.87s
                      Time elapsed: 00:23:35
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1410/1500 [0m                     

                       Computation: 117732 steps/s (collection: 0.728s, learning 0.107s)
             Mean action noise std: 3.36
          Mean value_function loss: 53.0359
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3598
                       Mean reward: 676.98
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.4679
    Episode_Reward/rotating_object: 135.6455
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.83s
                      Time elapsed: 00:23:36
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1411/1500 [0m                     

                       Computation: 116682 steps/s (collection: 0.739s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 53.3584
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3591
                       Mean reward: 648.92
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.4693
    Episode_Reward/rotating_object: 133.0551
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.84s
                      Time elapsed: 00:23:37
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1412/1500 [0m                     

                       Computation: 117884 steps/s (collection: 0.739s, learning 0.095s)
             Mean action noise std: 3.36
          Mean value_function loss: 49.9666
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3688
                       Mean reward: 692.93
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.4663
    Episode_Reward/rotating_object: 134.0866
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.83s
                      Time elapsed: 00:23:37
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1413/1500 [0m                     

                       Computation: 113893 steps/s (collection: 0.758s, learning 0.105s)
             Mean action noise std: 3.37
          Mean value_function loss: 60.9354
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3821
                       Mean reward: 627.67
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.4637
    Episode_Reward/rotating_object: 130.5189
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.86s
                      Time elapsed: 00:23:38
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1414/1500 [0m                     

                       Computation: 113447 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 59.9725
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.3849
                       Mean reward: 615.69
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.4617
    Episode_Reward/rotating_object: 130.3297
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.87s
                      Time elapsed: 00:23:39
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1415/1500 [0m                     

                       Computation: 114737 steps/s (collection: 0.759s, learning 0.098s)
             Mean action noise std: 3.37
          Mean value_function loss: 60.3633
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3941
                       Mean reward: 671.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4620
    Episode_Reward/rotating_object: 133.8346
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.86s
                      Time elapsed: 00:23:40
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1416/1500 [0m                     

                       Computation: 118522 steps/s (collection: 0.730s, learning 0.100s)
             Mean action noise std: 3.37
          Mean value_function loss: 52.5879
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.4007
                       Mean reward: 654.31
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.4618
    Episode_Reward/rotating_object: 131.7173
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.83s
                      Time elapsed: 00:23:41
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1417/1500 [0m                     

                       Computation: 114992 steps/s (collection: 0.759s, learning 0.096s)
             Mean action noise std: 3.37
          Mean value_function loss: 51.8760
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4058
                       Mean reward: 672.69
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.4592
    Episode_Reward/rotating_object: 133.6240
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.85s
                      Time elapsed: 00:23:42
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1418/1500 [0m                     

                       Computation: 116758 steps/s (collection: 0.745s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 55.4803
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.4127
                       Mean reward: 689.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4549
    Episode_Reward/rotating_object: 133.7088
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.84s
                      Time elapsed: 00:23:43
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1419/1500 [0m                     

                       Computation: 118961 steps/s (collection: 0.734s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 60.4934
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.4193
                       Mean reward: 656.49
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4607
    Episode_Reward/rotating_object: 132.3213
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.83s
                      Time elapsed: 00:23:43
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1420/1500 [0m                     

                       Computation: 116974 steps/s (collection: 0.750s, learning 0.091s)
             Mean action noise std: 3.38
          Mean value_function loss: 58.4176
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.4244
                       Mean reward: 656.64
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.4553
    Episode_Reward/rotating_object: 133.6707
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.84s
                      Time elapsed: 00:23:44
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1421/1500 [0m                     

                       Computation: 120202 steps/s (collection: 0.725s, learning 0.093s)
             Mean action noise std: 3.38
          Mean value_function loss: 53.9143
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4262
                       Mean reward: 681.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4598
    Episode_Reward/rotating_object: 134.0762
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.82s
                      Time elapsed: 00:23:45
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1422/1500 [0m                     

                       Computation: 118585 steps/s (collection: 0.741s, learning 0.088s)
             Mean action noise std: 3.39
          Mean value_function loss: 53.7689
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.4333
                       Mean reward: 664.32
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.4479
    Episode_Reward/rotating_object: 131.5187
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.83s
                      Time elapsed: 00:23:46
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1423/1500 [0m                     

                       Computation: 112106 steps/s (collection: 0.776s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 51.1367
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4293
                       Mean reward: 654.08
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.4637
    Episode_Reward/rotating_object: 133.4194
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.88s
                      Time elapsed: 00:23:47
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1424/1500 [0m                     

                       Computation: 111842 steps/s (collection: 0.778s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 50.7726
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.4282
                       Mean reward: 679.58
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.4570
    Episode_Reward/rotating_object: 135.2827
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.88s
                      Time elapsed: 00:23:48
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1425/1500 [0m                     

                       Computation: 116842 steps/s (collection: 0.741s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 54.9526
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4242
                       Mean reward: 666.32
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4587
    Episode_Reward/rotating_object: 132.0193
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.84s
                      Time elapsed: 00:23:48
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1426/1500 [0m                     

                       Computation: 117402 steps/s (collection: 0.740s, learning 0.097s)
             Mean action noise std: 3.39
          Mean value_function loss: 55.2439
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4310
                       Mean reward: 677.78
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.4652
    Episode_Reward/rotating_object: 135.9076
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.84s
                      Time elapsed: 00:23:49
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1427/1500 [0m                     

                       Computation: 114730 steps/s (collection: 0.750s, learning 0.107s)
             Mean action noise std: 3.39
          Mean value_function loss: 54.3488
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 18.4472
                       Mean reward: 677.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4543
    Episode_Reward/rotating_object: 130.9307
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.86s
                      Time elapsed: 00:23:50
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1428/1500 [0m                     

                       Computation: 115734 steps/s (collection: 0.749s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 45.7677
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.4569
                       Mean reward: 666.92
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4602
    Episode_Reward/rotating_object: 130.9163
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.85s
                      Time elapsed: 00:23:51
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1429/1500 [0m                     

                       Computation: 116018 steps/s (collection: 0.754s, learning 0.094s)
             Mean action noise std: 3.40
          Mean value_function loss: 50.3039
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4586
                       Mean reward: 669.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4639
    Episode_Reward/rotating_object: 132.6066
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.85s
                      Time elapsed: 00:23:52
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1430/1500 [0m                     

                       Computation: 115147 steps/s (collection: 0.752s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 51.7299
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.4532
                       Mean reward: 658.56
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.4573
    Episode_Reward/rotating_object: 132.5298
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.85s
                      Time elapsed: 00:23:53
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1431/1500 [0m                     

                       Computation: 114838 steps/s (collection: 0.764s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 57.4285
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.4521
                       Mean reward: 688.53
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.4534
    Episode_Reward/rotating_object: 132.5696
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.86s
                      Time elapsed: 00:23:54
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1432/1500 [0m                     

                       Computation: 114837 steps/s (collection: 0.751s, learning 0.105s)
             Mean action noise std: 3.40
          Mean value_function loss: 53.7472
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4614
                       Mean reward: 685.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4675
    Episode_Reward/rotating_object: 135.8041
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.86s
                      Time elapsed: 00:23:54
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1433/1500 [0m                     

                       Computation: 115839 steps/s (collection: 0.747s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 52.0892
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.4586
                       Mean reward: 674.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4663
    Episode_Reward/rotating_object: 135.2585
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.85s
                      Time elapsed: 00:23:55
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1434/1500 [0m                     

                       Computation: 118071 steps/s (collection: 0.741s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 43.3575
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.4605
                       Mean reward: 670.06
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.4607
    Episode_Reward/rotating_object: 134.1083
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.83s
                      Time elapsed: 00:23:56
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1435/1500 [0m                     

                       Computation: 119457 steps/s (collection: 0.729s, learning 0.094s)
             Mean action noise std: 3.40
          Mean value_function loss: 47.5198
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4614
                       Mean reward: 684.77
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.4617
    Episode_Reward/rotating_object: 134.4121
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.82s
                      Time elapsed: 00:23:57
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1436/1500 [0m                     

                       Computation: 105662 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 3.40
          Mean value_function loss: 43.6048
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.4591
                       Mean reward: 658.05
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.4594
    Episode_Reward/rotating_object: 133.1688
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.93s
                      Time elapsed: 00:23:58
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1437/1500 [0m                     

                       Computation: 107413 steps/s (collection: 0.783s, learning 0.132s)
             Mean action noise std: 3.41
          Mean value_function loss: 44.7115
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4622
                       Mean reward: 673.76
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.4649
    Episode_Reward/rotating_object: 134.4969
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.92s
                      Time elapsed: 00:23:59
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1438/1500 [0m                     

                       Computation: 92481 steps/s (collection: 0.910s, learning 0.153s)
             Mean action noise std: 3.41
          Mean value_function loss: 50.8321
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4655
                       Mean reward: 682.89
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.4616
    Episode_Reward/rotating_object: 133.1928
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.06s
                      Time elapsed: 00:24:00
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1439/1500 [0m                     

                       Computation: 96521 steps/s (collection: 0.912s, learning 0.106s)
             Mean action noise std: 3.41
          Mean value_function loss: 55.8997
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4673
                       Mean reward: 680.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4582
    Episode_Reward/rotating_object: 133.4818
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.02s
                      Time elapsed: 00:24:01
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1440/1500 [0m                     

                       Computation: 87409 steps/s (collection: 0.915s, learning 0.209s)
             Mean action noise std: 3.41
          Mean value_function loss: 49.2565
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4702
                       Mean reward: 683.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4607
    Episode_Reward/rotating_object: 134.4476
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.12s
                      Time elapsed: 00:24:02
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1441/1500 [0m                     

                       Computation: 89749 steps/s (collection: 0.951s, learning 0.144s)
             Mean action noise std: 3.41
          Mean value_function loss: 60.6736
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4784
                       Mean reward: 690.11
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.4633
    Episode_Reward/rotating_object: 134.9557
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.10s
                      Time elapsed: 00:24:03
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1442/1500 [0m                     

                       Computation: 81841 steps/s (collection: 0.990s, learning 0.211s)
             Mean action noise std: 3.42
          Mean value_function loss: 51.1538
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.4848
                       Mean reward: 703.07
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.4636
    Episode_Reward/rotating_object: 134.7166
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.20s
                      Time elapsed: 00:24:04
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1443/1500 [0m                     

                       Computation: 96705 steps/s (collection: 0.821s, learning 0.196s)
             Mean action noise std: 3.42
          Mean value_function loss: 49.9806
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4899
                       Mean reward: 693.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4693
    Episode_Reward/rotating_object: 138.9942
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.02s
                      Time elapsed: 00:24:05
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1444/1500 [0m                     

                       Computation: 84720 steps/s (collection: 1.002s, learning 0.159s)
             Mean action noise std: 3.42
          Mean value_function loss: 60.2920
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4950
                       Mean reward: 688.84
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.4630
    Episode_Reward/rotating_object: 135.2274
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.16s
                      Time elapsed: 00:24:06
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1445/1500 [0m                     

                       Computation: 84013 steps/s (collection: 0.983s, learning 0.187s)
             Mean action noise std: 3.42
          Mean value_function loss: 49.2679
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5034
                       Mean reward: 687.89
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.4573
    Episode_Reward/rotating_object: 133.3093
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.17s
                      Time elapsed: 00:24:08
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1446/1500 [0m                     

                       Computation: 88139 steps/s (collection: 0.995s, learning 0.121s)
             Mean action noise std: 3.42
          Mean value_function loss: 53.3613
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5045
                       Mean reward: 681.08
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.4679
    Episode_Reward/rotating_object: 136.0179
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.12s
                      Time elapsed: 00:24:09
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1447/1500 [0m                     

                       Computation: 112605 steps/s (collection: 0.783s, learning 0.090s)
             Mean action noise std: 3.42
          Mean value_function loss: 61.3742
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.5016
                       Mean reward: 660.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4609
    Episode_Reward/rotating_object: 132.0650
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.87s
                      Time elapsed: 00:24:10
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1448/1500 [0m                     

                       Computation: 110730 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 3.42
          Mean value_function loss: 54.5404
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5038
                       Mean reward: 676.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4698
    Episode_Reward/rotating_object: 134.6846
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.89s
                      Time elapsed: 00:24:11
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1449/1500 [0m                     

                       Computation: 102192 steps/s (collection: 0.840s, learning 0.122s)
             Mean action noise std: 3.42
          Mean value_function loss: 55.0632
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.5059
                       Mean reward: 684.20
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.4679
    Episode_Reward/rotating_object: 137.3837
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.96s
                      Time elapsed: 00:24:11
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1450/1500 [0m                     

                       Computation: 113648 steps/s (collection: 0.772s, learning 0.093s)
             Mean action noise std: 3.42
          Mean value_function loss: 53.0256
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5066
                       Mean reward: 656.13
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.4633
    Episode_Reward/rotating_object: 133.7702
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.86s
                      Time elapsed: 00:24:12
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1451/1500 [0m                     

                       Computation: 106749 steps/s (collection: 0.816s, learning 0.105s)
             Mean action noise std: 3.43
          Mean value_function loss: 48.0671
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5100
                       Mean reward: 664.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4661
    Episode_Reward/rotating_object: 132.4347
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.92s
                      Time elapsed: 00:24:13
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1452/1500 [0m                     

                       Computation: 117560 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 50.6547
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.5106
                       Mean reward: 682.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4675
    Episode_Reward/rotating_object: 135.8850
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.84s
                      Time elapsed: 00:24:14
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1453/1500 [0m                     

                       Computation: 103924 steps/s (collection: 0.781s, learning 0.165s)
             Mean action noise std: 3.43
          Mean value_function loss: 51.9793
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.5122
                       Mean reward: 677.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4732
    Episode_Reward/rotating_object: 136.6760
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.95s
                      Time elapsed: 00:24:15
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1454/1500 [0m                     

                       Computation: 110342 steps/s (collection: 0.750s, learning 0.141s)
             Mean action noise std: 3.43
          Mean value_function loss: 47.4001
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5269
                       Mean reward: 663.89
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.4658
    Episode_Reward/rotating_object: 133.0240
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.89s
                      Time elapsed: 00:24:16
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1455/1500 [0m                     

                       Computation: 118725 steps/s (collection: 0.735s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 54.3548
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 18.5324
                       Mean reward: 689.26
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.4716
    Episode_Reward/rotating_object: 136.4004
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.83s
                      Time elapsed: 00:24:17
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1456/1500 [0m                     

                       Computation: 96930 steps/s (collection: 0.874s, learning 0.140s)
             Mean action noise std: 3.43
          Mean value_function loss: 54.0622
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.5324
                       Mean reward: 679.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4665
    Episode_Reward/rotating_object: 134.9198
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.01s
                      Time elapsed: 00:24:18
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1457/1500 [0m                     

                       Computation: 101115 steps/s (collection: 0.869s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 53.6666
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.5324
                       Mean reward: 704.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4739
    Episode_Reward/rotating_object: 136.4611
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.97s
                      Time elapsed: 00:24:19
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1458/1500 [0m                     

                       Computation: 118149 steps/s (collection: 0.739s, learning 0.093s)
             Mean action noise std: 3.44
          Mean value_function loss: 55.1459
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5417
                       Mean reward: 667.50
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.4726
    Episode_Reward/rotating_object: 133.0210
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.83s
                      Time elapsed: 00:24:20
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1459/1500 [0m                     

                       Computation: 108653 steps/s (collection: 0.802s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 49.7007
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.5453
                       Mean reward: 674.36
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.4700
    Episode_Reward/rotating_object: 134.9659
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.90s
                      Time elapsed: 00:24:20
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1460/1500 [0m                     

                       Computation: 114201 steps/s (collection: 0.767s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 55.3488
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5427
                       Mean reward: 702.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4755
    Episode_Reward/rotating_object: 139.3163
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.86s
                      Time elapsed: 00:24:21
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1461/1500 [0m                     

                       Computation: 109749 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 55.0216
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.5451
                       Mean reward: 674.01
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.4668
    Episode_Reward/rotating_object: 133.7590
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.90s
                      Time elapsed: 00:24:22
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1462/1500 [0m                     

                       Computation: 114573 steps/s (collection: 0.763s, learning 0.095s)
             Mean action noise std: 3.44
          Mean value_function loss: 47.0807
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.5480
                       Mean reward: 681.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4703
    Episode_Reward/rotating_object: 133.7457
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.86s
                      Time elapsed: 00:24:23
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1463/1500 [0m                     

                       Computation: 111066 steps/s (collection: 0.750s, learning 0.135s)
             Mean action noise std: 3.45
          Mean value_function loss: 52.7010
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5431
                       Mean reward: 650.53
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.4709
    Episode_Reward/rotating_object: 133.1976
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.89s
                      Time elapsed: 00:24:24
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1464/1500 [0m                     

                       Computation: 109744 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 3.45
          Mean value_function loss: 55.8666
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5493
                       Mean reward: 677.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4700
    Episode_Reward/rotating_object: 135.2108
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.90s
                      Time elapsed: 00:24:25
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1465/1500 [0m                     

                       Computation: 117822 steps/s (collection: 0.748s, learning 0.087s)
             Mean action noise std: 3.45
          Mean value_function loss: 53.4513
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5507
                       Mean reward: 692.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4703
    Episode_Reward/rotating_object: 133.5859
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.83s
                      Time elapsed: 00:24:26
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1466/1500 [0m                     

                       Computation: 107740 steps/s (collection: 0.779s, learning 0.134s)
             Mean action noise std: 3.45
          Mean value_function loss: 57.3886
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.5471
                       Mean reward: 701.60
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.4702
    Episode_Reward/rotating_object: 135.6130
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.91s
                      Time elapsed: 00:24:27
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1467/1500 [0m                     

                       Computation: 112272 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 3.45
          Mean value_function loss: 60.7337
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5451
                       Mean reward: 694.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4724
    Episode_Reward/rotating_object: 135.9048
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.88s
                      Time elapsed: 00:24:27
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1468/1500 [0m                     

                       Computation: 104696 steps/s (collection: 0.815s, learning 0.124s)
             Mean action noise std: 3.45
          Mean value_function loss: 54.9349
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.5487
                       Mean reward: 669.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4711
    Episode_Reward/rotating_object: 135.8729
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.94s
                      Time elapsed: 00:24:28
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1469/1500 [0m                     

                       Computation: 111507 steps/s (collection: 0.781s, learning 0.100s)
             Mean action noise std: 3.45
          Mean value_function loss: 50.1742
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5597
                       Mean reward: 686.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4635
    Episode_Reward/rotating_object: 134.3020
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.88s
                      Time elapsed: 00:24:29
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1470/1500 [0m                     

                       Computation: 109341 steps/s (collection: 0.750s, learning 0.149s)
             Mean action noise std: 3.46
          Mean value_function loss: 50.2946
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5685
                       Mean reward: 690.08
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4641
    Episode_Reward/rotating_object: 133.4749
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.90s
                      Time elapsed: 00:24:30
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1471/1500 [0m                     

                       Computation: 116519 steps/s (collection: 0.751s, learning 0.092s)
             Mean action noise std: 3.46
          Mean value_function loss: 59.6873
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5705
                       Mean reward: 679.86
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.4675
    Episode_Reward/rotating_object: 136.6336
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.84s
                      Time elapsed: 00:24:31
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1472/1500 [0m                     

                       Computation: 109661 steps/s (collection: 0.763s, learning 0.134s)
             Mean action noise std: 3.46
          Mean value_function loss: 52.9123
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.5710
                       Mean reward: 677.79
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.4691
    Episode_Reward/rotating_object: 136.2817
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.90s
                      Time elapsed: 00:24:32
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1473/1500 [0m                     

                       Computation: 113410 steps/s (collection: 0.754s, learning 0.113s)
             Mean action noise std: 3.46
          Mean value_function loss: 50.6022
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5709
                       Mean reward: 645.56
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.4625
    Episode_Reward/rotating_object: 130.9798
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.87s
                      Time elapsed: 00:24:33
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1474/1500 [0m                     

                       Computation: 113773 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 47.8360
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5758
                       Mean reward: 670.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4658
    Episode_Reward/rotating_object: 134.7031
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.86s
                      Time elapsed: 00:24:34
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1475/1500 [0m                     

                       Computation: 115590 steps/s (collection: 0.764s, learning 0.087s)
             Mean action noise std: 3.46
          Mean value_function loss: 44.9426
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5814
                       Mean reward: 662.28
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.4597
    Episode_Reward/rotating_object: 134.1356
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.85s
                      Time elapsed: 00:24:35
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1476/1500 [0m                     

                       Computation: 111875 steps/s (collection: 0.778s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 58.3069
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 18.5920
                       Mean reward: 685.06
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.4632
    Episode_Reward/rotating_object: 132.5103
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.88s
                      Time elapsed: 00:24:35
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1477/1500 [0m                     

                       Computation: 114483 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 3.47
          Mean value_function loss: 56.3503
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.6001
                       Mean reward: 667.80
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.4664
    Episode_Reward/rotating_object: 135.5932
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.86s
                      Time elapsed: 00:24:36
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1478/1500 [0m                     

                       Computation: 110741 steps/s (collection: 0.771s, learning 0.116s)
             Mean action noise std: 3.47
          Mean value_function loss: 52.6368
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6058
                       Mean reward: 689.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4628
    Episode_Reward/rotating_object: 133.8241
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.89s
                      Time elapsed: 00:24:37
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1479/1500 [0m                     

                       Computation: 112031 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 3.47
          Mean value_function loss: 47.8791
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6176
                       Mean reward: 658.82
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.4622
    Episode_Reward/rotating_object: 133.2682
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.88s
                      Time elapsed: 00:24:38
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1480/1500 [0m                     

                       Computation: 114098 steps/s (collection: 0.741s, learning 0.120s)
             Mean action noise std: 3.48
          Mean value_function loss: 42.3567
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.6144
                       Mean reward: 650.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4675
    Episode_Reward/rotating_object: 133.9368
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.86s
                      Time elapsed: 00:24:39
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1481/1500 [0m                     

                       Computation: 114638 steps/s (collection: 0.761s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 46.4077
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.6152
                       Mean reward: 694.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4696
    Episode_Reward/rotating_object: 135.8956
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.86s
                      Time elapsed: 00:24:40
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1482/1500 [0m                     

                       Computation: 111346 steps/s (collection: 0.777s, learning 0.106s)
             Mean action noise std: 3.48
          Mean value_function loss: 45.8663
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6217
                       Mean reward: 690.32
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4688
    Episode_Reward/rotating_object: 134.6062
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.88s
                      Time elapsed: 00:24:41
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1483/1500 [0m                     

                       Computation: 110711 steps/s (collection: 0.792s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 53.9215
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6282
                       Mean reward: 660.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4699
    Episode_Reward/rotating_object: 135.4815
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.89s
                      Time elapsed: 00:24:42
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1484/1500 [0m                     

                       Computation: 115263 steps/s (collection: 0.744s, learning 0.109s)
             Mean action noise std: 3.48
          Mean value_function loss: 49.2468
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.6294
                       Mean reward: 638.36
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.4639
    Episode_Reward/rotating_object: 131.8015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.85s
                      Time elapsed: 00:24:42
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1485/1500 [0m                     

                       Computation: 111586 steps/s (collection: 0.764s, learning 0.117s)
             Mean action noise std: 3.48
          Mean value_function loss: 53.5646
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.6270
                       Mean reward: 651.54
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.4714
    Episode_Reward/rotating_object: 134.0748
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.88s
                      Time elapsed: 00:24:43
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1486/1500 [0m                     

                       Computation: 106981 steps/s (collection: 0.780s, learning 0.139s)
             Mean action noise std: 3.49
          Mean value_function loss: 54.4973
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.6280
                       Mean reward: 684.93
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4656
    Episode_Reward/rotating_object: 133.9188
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.92s
                      Time elapsed: 00:24:44
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1487/1500 [0m                     

                       Computation: 117391 steps/s (collection: 0.744s, learning 0.094s)
             Mean action noise std: 3.49
          Mean value_function loss: 59.7831
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.6298
                       Mean reward: 688.96
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.4723
    Episode_Reward/rotating_object: 136.3563
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.84s
                      Time elapsed: 00:24:45
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1488/1500 [0m                     

                       Computation: 113976 steps/s (collection: 0.746s, learning 0.116s)
             Mean action noise std: 3.49
          Mean value_function loss: 51.5643
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6375
                       Mean reward: 701.42
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.4747
    Episode_Reward/rotating_object: 137.3004
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.86s
                      Time elapsed: 00:24:46
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1489/1500 [0m                     

                       Computation: 111555 steps/s (collection: 0.776s, learning 0.106s)
             Mean action noise std: 3.49
          Mean value_function loss: 57.2970
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6435
                       Mean reward: 672.66
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.4687
    Episode_Reward/rotating_object: 134.4349
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.88s
                      Time elapsed: 00:24:47
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1490/1500 [0m                     

                       Computation: 111945 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 3.49
          Mean value_function loss: 66.5842
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.6437
                       Mean reward: 655.55
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.4669
    Episode_Reward/rotating_object: 134.8575
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.88s
                      Time elapsed: 00:24:48
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1491/1500 [0m                     

                       Computation: 113388 steps/s (collection: 0.764s, learning 0.103s)
             Mean action noise std: 3.50
          Mean value_function loss: 53.9394
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6487
                       Mean reward: 684.83
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.4707
    Episode_Reward/rotating_object: 136.1810
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.87s
                      Time elapsed: 00:24:49
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1492/1500 [0m                     

                       Computation: 110558 steps/s (collection: 0.762s, learning 0.128s)
             Mean action noise std: 3.50
          Mean value_function loss: 61.9528
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6627
                       Mean reward: 666.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4690
    Episode_Reward/rotating_object: 134.6009
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.89s
                      Time elapsed: 00:24:49
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1493/1500 [0m                     

                       Computation: 114314 steps/s (collection: 0.763s, learning 0.097s)
             Mean action noise std: 3.50
          Mean value_function loss: 67.1723
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.6746
                       Mean reward: 660.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4714
    Episode_Reward/rotating_object: 134.7975
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.86s
                      Time elapsed: 00:24:50
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1494/1500 [0m                     

                       Computation: 113340 steps/s (collection: 0.759s, learning 0.108s)
             Mean action noise std: 3.50
          Mean value_function loss: 66.1452
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6858
                       Mean reward: 673.32
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.4712
    Episode_Reward/rotating_object: 134.2940
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.87s
                      Time elapsed: 00:24:51
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1495/1500 [0m                     

                       Computation: 116147 steps/s (collection: 0.737s, learning 0.109s)
             Mean action noise std: 3.50
          Mean value_function loss: 60.3677
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6871
                       Mean reward: 668.40
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.4679
    Episode_Reward/rotating_object: 133.7813
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.85s
                      Time elapsed: 00:24:52
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1496/1500 [0m                     

                       Computation: 118807 steps/s (collection: 0.739s, learning 0.089s)
             Mean action noise std: 3.50
          Mean value_function loss: 60.6384
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.6915
                       Mean reward: 675.01
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4717
    Episode_Reward/rotating_object: 133.8506
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.83s
                      Time elapsed: 00:24:53
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1497/1500 [0m                     

                       Computation: 114000 steps/s (collection: 0.758s, learning 0.104s)
             Mean action noise std: 3.51
          Mean value_function loss: 57.3865
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.6947
                       Mean reward: 660.87
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.4734
    Episode_Reward/rotating_object: 134.4982
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.86s
                      Time elapsed: 00:24:54
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1498/1500 [0m                     

                       Computation: 115476 steps/s (collection: 0.758s, learning 0.094s)
             Mean action noise std: 3.51
          Mean value_function loss: 57.9483
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.7034
                       Mean reward: 675.09
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4661
    Episode_Reward/rotating_object: 132.3988
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.85s
                      Time elapsed: 00:24:55
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1499/1500 [0m                     

                       Computation: 112375 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 3.51
          Mean value_function loss: 54.0857
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.7047
                       Mean reward: 661.58
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.4699
    Episode_Reward/rotating_object: 132.6111
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.87s
                      Time elapsed: 00:24:55
                               ETA: 00:00:00

