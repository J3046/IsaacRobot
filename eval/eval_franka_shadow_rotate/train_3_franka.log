################################################################################
                      [1m Learning iteration 0/1500 [0m                       

                       Computation: 18071 steps/s (collection: 5.146s, learning 0.294s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0026
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 11.3555
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0002
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.44s
                      Time elapsed: 00:00:05
                               ETA: 02:15:59

################################################################################
                      [1m Learning iteration 1/1500 [0m                       

                       Computation: 34789 steps/s (collection: 2.664s, learning 0.162s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.3888
                       Mean reward: 0.00
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0009
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.83s
                      Time elapsed: 00:00:08
                               ETA: 01:43:14

################################################################################
                      [1m Learning iteration 2/1500 [0m                       

                       Computation: 36873 steps/s (collection: 2.528s, learning 0.138s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 11.4043
                       Mean reward: 0.00
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0018
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.67s
                      Time elapsed: 00:00:10
                               ETA: 01:30:58

################################################################################
                      [1m Learning iteration 3/1500 [0m                       

                       Computation: 34837 steps/s (collection: 2.678s, learning 0.144s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 11.4387
                       Mean reward: 0.01
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0026
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.82s
                      Time elapsed: 00:00:13
                               ETA: 01:25:47

################################################################################
                      [1m Learning iteration 4/1500 [0m                       

                       Computation: 33029 steps/s (collection: 2.834s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.4461
                       Mean reward: 0.01
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0034
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.98s
                      Time elapsed: 00:00:16
                               ETA: 01:23:25

################################################################################
                      [1m Learning iteration 5/1500 [0m                       

                       Computation: 35787 steps/s (collection: 2.595s, learning 0.152s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 11.4376
                       Mean reward: 0.01
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0047
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.75s
                      Time elapsed: 00:00:19
                               ETA: 01:20:52

################################################################################
                      [1m Learning iteration 6/1500 [0m                       

                       Computation: 34677 steps/s (collection: 2.685s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 11.4359
                       Mean reward: 0.02
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0057
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.83s
                      Time elapsed: 00:00:22
                               ETA: 01:19:21

################################################################################
                      [1m Learning iteration 7/1500 [0m                       

                       Computation: 34467 steps/s (collection: 2.704s, learning 0.148s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 11.4301
                       Mean reward: 0.02
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0068
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.85s
                      Time elapsed: 00:00:25
                               ETA: 01:18:16

################################################################################
                      [1m Learning iteration 8/1500 [0m                       

                       Computation: 27586 steps/s (collection: 3.440s, learning 0.124s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 11.4397
                       Mean reward: 0.03
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0093
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.56s
                      Time elapsed: 00:00:28
                               ETA: 01:19:22

################################################################################
                      [1m Learning iteration 9/1500 [0m                       

                       Computation: 127526 steps/s (collection: 0.644s, learning 0.127s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 11.4549
                       Mean reward: 0.05
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0106
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.77s
                      Time elapsed: 00:00:29
                               ETA: 01:13:18

################################################################################
                      [1m Learning iteration 10/1500 [0m                      

                       Computation: 135943 steps/s (collection: 0.634s, learning 0.089s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 11.4363
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0147
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.72s
                      Time elapsed: 00:00:30
                               ETA: 01:08:13

################################################################################
                      [1m Learning iteration 11/1500 [0m                      

                       Computation: 139282 steps/s (collection: 0.605s, learning 0.101s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 11.4322
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0168
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.71s
                      Time elapsed: 00:00:30
                               ETA: 01:03:57

################################################################################
                      [1m Learning iteration 12/1500 [0m                      

                       Computation: 130458 steps/s (collection: 0.648s, learning 0.106s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 11.4204
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0209
    Episode_Reward/rotating_object: 0.0003
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.75s
                      Time elapsed: 00:00:31
                               ETA: 01:00:26

################################################################################
                      [1m Learning iteration 13/1500 [0m                      

                       Computation: 127235 steps/s (collection: 0.654s, learning 0.119s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 11.3995
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0263
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.77s
                      Time elapsed: 00:00:32
                               ETA: 00:57:26

################################################################################
                      [1m Learning iteration 14/1500 [0m                      

                       Computation: 136085 steps/s (collection: 0.636s, learning 0.086s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 11.3855
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0345
    Episode_Reward/rotating_object: 0.0001
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.72s
                      Time elapsed: 00:00:33
                               ETA: 00:54:46

################################################################################
                      [1m Learning iteration 15/1500 [0m                      

                       Computation: 132242 steps/s (collection: 0.653s, learning 0.091s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 11.3875
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0453
    Episode_Reward/rotating_object: 0.0012
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.74s
                      Time elapsed: 00:00:33
                               ETA: 00:52:28

################################################################################
                      [1m Learning iteration 16/1500 [0m                      

                       Computation: 123431 steps/s (collection: 0.708s, learning 0.089s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 11.4201
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0612
    Episode_Reward/rotating_object: 0.0049
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.80s
                      Time elapsed: 00:00:34
                               ETA: 00:50:30

################################################################################
                      [1m Learning iteration 17/1500 [0m                      

                       Computation: 125224 steps/s (collection: 0.700s, learning 0.085s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.1099
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 11.4464
                       Mean reward: 0.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0801
    Episode_Reward/rotating_object: 0.0175
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.79s
                      Time elapsed: 00:00:35
                               ETA: 00:48:44

################################################################################
                      [1m Learning iteration 18/1500 [0m                      

                       Computation: 121850 steps/s (collection: 0.685s, learning 0.122s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1793
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.5045
                       Mean reward: 0.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0933
    Episode_Reward/rotating_object: 0.0320
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.81s
                      Time elapsed: 00:00:36
                               ETA: 00:47:11

################################################################################
                      [1m Learning iteration 19/1500 [0m                      

                       Computation: 123861 steps/s (collection: 0.707s, learning 0.087s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.3182
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.5748
                       Mean reward: 0.95
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.1123
    Episode_Reward/rotating_object: 0.0519
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.79s
                      Time elapsed: 00:00:37
                               ETA: 00:45:47

################################################################################
                      [1m Learning iteration 20/1500 [0m                      

                       Computation: 123515 steps/s (collection: 0.706s, learning 0.090s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.3697
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 11.6411
                       Mean reward: 2.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1307
    Episode_Reward/rotating_object: 0.1952
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.80s
                      Time elapsed: 00:00:37
                               ETA: 00:44:30

################################################################################
                      [1m Learning iteration 21/1500 [0m                      

                       Computation: 120761 steps/s (collection: 0.728s, learning 0.086s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.2652
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.7419
                       Mean reward: 2.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1514
    Episode_Reward/rotating_object: 0.1994
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.81s
                      Time elapsed: 00:00:38
                               ETA: 00:43:22

################################################################################
                      [1m Learning iteration 22/1500 [0m                      

                       Computation: 123276 steps/s (collection: 0.706s, learning 0.091s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.2880
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 11.7754
                       Mean reward: 1.94
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.1595
    Episode_Reward/rotating_object: 0.1755
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.80s
                      Time elapsed: 00:00:39
                               ETA: 00:42:18

################################################################################
                      [1m Learning iteration 23/1500 [0m                      

                       Computation: 108849 steps/s (collection: 0.746s, learning 0.157s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3210
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.8493
                       Mean reward: 1.81
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.1799
    Episode_Reward/rotating_object: 0.3011
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.90s
                      Time elapsed: 00:00:40
                               ETA: 00:41:26

################################################################################
                      [1m Learning iteration 24/1500 [0m                      

                       Computation: 114840 steps/s (collection: 0.704s, learning 0.152s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1520
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 11.9240
                       Mean reward: 3.00
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.1938
    Episode_Reward/rotating_object: 0.3350
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                      Time elapsed: 00:00:41
                               ETA: 00:40:36

################################################################################
                      [1m Learning iteration 25/1500 [0m                      

                       Computation: 120800 steps/s (collection: 0.718s, learning 0.096s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2181
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 11.9710
                       Mean reward: 3.26
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.1993
    Episode_Reward/rotating_object: 0.4372
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.81s
                      Time elapsed: 00:00:42
                               ETA: 00:39:47

################################################################################
                      [1m Learning iteration 26/1500 [0m                      

                       Computation: 115480 steps/s (collection: 0.742s, learning 0.109s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1931
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 12.0200
                       Mean reward: 2.45
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.2026
    Episode_Reward/rotating_object: 0.3215
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.85s
                      Time elapsed: 00:00:42
                               ETA: 00:39:03

################################################################################
                      [1m Learning iteration 27/1500 [0m                      

                       Computation: 110475 steps/s (collection: 0.786s, learning 0.104s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.3074
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 12.0893
                       Mean reward: 2.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2066
    Episode_Reward/rotating_object: 0.3410
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.89s
                      Time elapsed: 00:00:43
                               ETA: 00:38:25

################################################################################
                      [1m Learning iteration 28/1500 [0m                      

                       Computation: 121135 steps/s (collection: 0.725s, learning 0.087s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.3661
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.1891
                       Mean reward: 4.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2134
    Episode_Reward/rotating_object: 0.4672
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.81s
                      Time elapsed: 00:00:44
                               ETA: 00:37:45

################################################################################
                      [1m Learning iteration 29/1500 [0m                      

                       Computation: 97591 steps/s (collection: 0.889s, learning 0.119s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.2571
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 12.2458
                       Mean reward: 4.64
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.2151
    Episode_Reward/rotating_object: 0.4128
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.01s
                      Time elapsed: 00:00:45
                               ETA: 00:37:17

################################################################################
                      [1m Learning iteration 30/1500 [0m                      

                       Computation: 110359 steps/s (collection: 0.794s, learning 0.097s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.3596
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.2948
                       Mean reward: 2.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2190
    Episode_Reward/rotating_object: 0.3563
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.89s
                      Time elapsed: 00:00:46
                               ETA: 00:36:46

################################################################################
                      [1m Learning iteration 31/1500 [0m                      

                       Computation: 117759 steps/s (collection: 0.713s, learning 0.122s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.5105
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 12.3673
                       Mean reward: 4.31
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.2233
    Episode_Reward/rotating_object: 0.4164
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.83s
                      Time elapsed: 00:00:47
                               ETA: 00:36:14

################################################################################
                      [1m Learning iteration 32/1500 [0m                      

                       Computation: 119578 steps/s (collection: 0.711s, learning 0.112s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.3435
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.4403
                       Mean reward: 5.65
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.2208
    Episode_Reward/rotating_object: 0.6933
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.82s
                      Time elapsed: 00:00:48
                               ETA: 00:35:43

################################################################################
                      [1m Learning iteration 33/1500 [0m                      

                       Computation: 114582 steps/s (collection: 0.764s, learning 0.094s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2043
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 12.5155
                       Mean reward: 2.97
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.2341
    Episode_Reward/rotating_object: 0.4744
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.86s
                      Time elapsed: 00:00:49
                               ETA: 00:35:16

################################################################################
                      [1m Learning iteration 34/1500 [0m                      

                       Computation: 113191 steps/s (collection: 0.782s, learning 0.086s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.3711
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.5805
                       Mean reward: 4.27
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.2354
    Episode_Reward/rotating_object: 0.5785
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.87s
                      Time elapsed: 00:00:49
                               ETA: 00:34:50

################################################################################
                      [1m Learning iteration 35/1500 [0m                      

                       Computation: 117031 steps/s (collection: 0.755s, learning 0.085s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1596
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 12.6573
                       Mean reward: 2.96
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.2366
    Episode_Reward/rotating_object: 0.4137
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.84s
                      Time elapsed: 00:00:50
                               ETA: 00:34:25

################################################################################
                      [1m Learning iteration 36/1500 [0m                      

                       Computation: 111961 steps/s (collection: 0.764s, learning 0.114s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.2011
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 12.6921
                       Mean reward: 3.66
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.2424
    Episode_Reward/rotating_object: 0.4669
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.88s
                      Time elapsed: 00:00:51
                               ETA: 00:34:02

################################################################################
                      [1m Learning iteration 37/1500 [0m                      

                       Computation: 114852 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.2589
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 12.7223
                       Mean reward: 5.13
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.2481
    Episode_Reward/rotating_object: 0.7075
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.86s
                      Time elapsed: 00:00:52
                               ETA: 00:33:40

################################################################################
                      [1m Learning iteration 38/1500 [0m                      

                       Computation: 117807 steps/s (collection: 0.720s, learning 0.115s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2446
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 12.7703
                       Mean reward: 3.71
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.2547
    Episode_Reward/rotating_object: 0.4919
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.83s
                      Time elapsed: 00:00:53
                               ETA: 00:33:18

################################################################################
                      [1m Learning iteration 39/1500 [0m                      

                       Computation: 111881 steps/s (collection: 0.745s, learning 0.134s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2349
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 12.8018
                       Mean reward: 4.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2598
    Episode_Reward/rotating_object: 0.4657
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.88s
                      Time elapsed: 00:00:54
                               ETA: 00:32:59

################################################################################
                      [1m Learning iteration 40/1500 [0m                      

                       Computation: 121015 steps/s (collection: 0.712s, learning 0.100s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3344
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 12.8305
                       Mean reward: 3.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2687
    Episode_Reward/rotating_object: 0.6451
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.81s
                      Time elapsed: 00:00:55
                               ETA: 00:32:39

################################################################################
                      [1m Learning iteration 41/1500 [0m                      

                       Computation: 115882 steps/s (collection: 0.750s, learning 0.098s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3763
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 12.8585
                       Mean reward: 5.74
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.2772
    Episode_Reward/rotating_object: 0.7139
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.85s
                      Time elapsed: 00:00:55
                               ETA: 00:32:20

################################################################################
                      [1m Learning iteration 42/1500 [0m                      

                       Computation: 109570 steps/s (collection: 0.800s, learning 0.097s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5636
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.9191
                       Mean reward: 4.67
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.2869
    Episode_Reward/rotating_object: 0.6527
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.90s
                      Time elapsed: 00:00:56
                               ETA: 00:32:04

################################################################################
                      [1m Learning iteration 43/1500 [0m                      

                       Computation: 114460 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.5963
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.9506
                       Mean reward: 7.11
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.2921
    Episode_Reward/rotating_object: 0.9566
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.86s
                      Time elapsed: 00:00:57
                               ETA: 00:31:47

################################################################################
                      [1m Learning iteration 44/1500 [0m                      

                       Computation: 106811 steps/s (collection: 0.807s, learning 0.114s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.3153
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.9859
                       Mean reward: 6.02
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.2938
    Episode_Reward/rotating_object: 1.0012
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.92s
                      Time elapsed: 00:00:58
                               ETA: 00:31:34

################################################################################
                      [1m Learning iteration 45/1500 [0m                      

                       Computation: 112451 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.6722
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.0352
                       Mean reward: 6.95
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 0.2973
    Episode_Reward/rotating_object: 0.9176
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.87s
                      Time elapsed: 00:00:59
                               ETA: 00:31:19

################################################################################
                      [1m Learning iteration 46/1500 [0m                      

                       Computation: 95699 steps/s (collection: 0.809s, learning 0.218s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.9357
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.0873
                       Mean reward: 5.72
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.3057
    Episode_Reward/rotating_object: 1.0194
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.03s
                      Time elapsed: 00:01:00
                               ETA: 00:31:09

################################################################################
                      [1m Learning iteration 47/1500 [0m                      

                       Computation: 114533 steps/s (collection: 0.761s, learning 0.098s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.9993
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 13.1231
                       Mean reward: 7.21
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 1.1059
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.86s
                      Time elapsed: 00:01:01
                               ETA: 00:30:55

################################################################################
                      [1m Learning iteration 48/1500 [0m                      

                       Computation: 116561 steps/s (collection: 0.728s, learning 0.116s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.8620
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 13.1574
                       Mean reward: 9.67
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 1.2213
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.84s
                      Time elapsed: 00:01:02
                               ETA: 00:30:41

################################################################################
                      [1m Learning iteration 49/1500 [0m                      

                       Computation: 114465 steps/s (collection: 0.767s, learning 0.092s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.6629
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.1827
                       Mean reward: 8.29
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 1.2784
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.86s
                      Time elapsed: 00:01:03
                               ETA: 00:30:28

################################################################################
                      [1m Learning iteration 50/1500 [0m                      

                       Computation: 115773 steps/s (collection: 0.759s, learning 0.090s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.7026
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 13.1938
                       Mean reward: 9.28
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 1.5388
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.85s
                      Time elapsed: 00:01:03
                               ETA: 00:30:15

################################################################################
                      [1m Learning iteration 51/1500 [0m                      

                       Computation: 114936 steps/s (collection: 0.754s, learning 0.101s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.7055
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.2094
                       Mean reward: 7.18
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 1.5868
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.86s
                      Time elapsed: 00:01:04
                               ETA: 00:30:03

################################################################################
                      [1m Learning iteration 52/1500 [0m                      

                       Computation: 111518 steps/s (collection: 0.794s, learning 0.088s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.7017
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 13.2261
                       Mean reward: 8.51
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 1.5455
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.88s
                      Time elapsed: 00:01:05
                               ETA: 00:29:51

################################################################################
                      [1m Learning iteration 53/1500 [0m                      

                       Computation: 108782 steps/s (collection: 0.780s, learning 0.124s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.9131
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.2490
                       Mean reward: 7.55
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3273
    Episode_Reward/rotating_object: 1.2546
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.90s
                      Time elapsed: 00:01:06
                               ETA: 00:29:41

################################################################################
                      [1m Learning iteration 54/1500 [0m                      

                       Computation: 105485 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.1569
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.2566
                       Mean reward: 8.52
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3289
    Episode_Reward/rotating_object: 1.3747
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.93s
                      Time elapsed: 00:01:07
                               ETA: 00:29:32

################################################################################
                      [1m Learning iteration 55/1500 [0m                      

                       Computation: 109201 steps/s (collection: 0.771s, learning 0.130s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7488
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.2860
                       Mean reward: 7.38
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3303
    Episode_Reward/rotating_object: 1.6293
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.90s
                      Time elapsed: 00:01:08
                               ETA: 00:29:22

################################################################################
                      [1m Learning iteration 56/1500 [0m                      

                       Computation: 116708 steps/s (collection: 0.754s, learning 0.088s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.1582
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 13.2871
                       Mean reward: 10.48
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 1.6168
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.84s
                      Time elapsed: 00:01:09
                               ETA: 00:29:12

################################################################################
                      [1m Learning iteration 57/1500 [0m                      

                       Computation: 110779 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.4827
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 13.2961
                       Mean reward: 8.55
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.3314
    Episode_Reward/rotating_object: 1.5705
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.89s
                      Time elapsed: 00:01:10
                               ETA: 00:29:02

################################################################################
                      [1m Learning iteration 58/1500 [0m                      

                       Computation: 108962 steps/s (collection: 0.795s, learning 0.108s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.3303
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 13.3039
                       Mean reward: 8.89
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.3311
    Episode_Reward/rotating_object: 1.5142
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.90s
                      Time elapsed: 00:01:10
                               ETA: 00:28:54

################################################################################
                      [1m Learning iteration 59/1500 [0m                      

                       Computation: 108310 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.5110
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 13.3185
                       Mean reward: 12.60
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.3333
    Episode_Reward/rotating_object: 1.6822
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.91s
                      Time elapsed: 00:01:11
                               ETA: 00:28:45

################################################################################
                      [1m Learning iteration 60/1500 [0m                      

                       Computation: 104888 steps/s (collection: 0.817s, learning 0.120s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.0743
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.3337
                       Mean reward: 12.54
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 1.8776
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.94s
                      Time elapsed: 00:01:12
                               ETA: 00:28:38

################################################################################
                      [1m Learning iteration 61/1500 [0m                      

                       Computation: 102419 steps/s (collection: 0.847s, learning 0.113s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.0478
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.3719
                       Mean reward: 10.15
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3347
    Episode_Reward/rotating_object: 1.9831
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.96s
                      Time elapsed: 00:01:13
                               ETA: 00:28:31

################################################################################
                      [1m Learning iteration 62/1500 [0m                      

                       Computation: 110517 steps/s (collection: 0.794s, learning 0.096s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.0818
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.4149
                       Mean reward: 9.60
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.3373
    Episode_Reward/rotating_object: 1.8623
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.89s
                      Time elapsed: 00:01:14
                               ETA: 00:28:23

################################################################################
                      [1m Learning iteration 63/1500 [0m                      

                       Computation: 104787 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.3446
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 13.4303
                       Mean reward: 8.55
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.3313
    Episode_Reward/rotating_object: 1.8457
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.94s
                      Time elapsed: 00:01:15
                               ETA: 00:28:17

################################################################################
                      [1m Learning iteration 64/1500 [0m                      

                       Computation: 104136 steps/s (collection: 0.817s, learning 0.127s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.4153
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.4532
                       Mean reward: 11.41
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.3325
    Episode_Reward/rotating_object: 1.7947
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.94s
                      Time elapsed: 00:01:16
                               ETA: 00:28:10

################################################################################
                      [1m Learning iteration 65/1500 [0m                      

                       Computation: 105893 steps/s (collection: 0.835s, learning 0.093s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.4885
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.4832
                       Mean reward: 6.46
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 0.3314
    Episode_Reward/rotating_object: 1.8045
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.93s
                      Time elapsed: 00:01:17
                               ETA: 00:28:04

################################################################################
                      [1m Learning iteration 66/1500 [0m                      

                       Computation: 110983 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.4997
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.5030
                       Mean reward: 12.60
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 1.8774
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.89s
                      Time elapsed: 00:01:18
                               ETA: 00:27:56

################################################################################
                      [1m Learning iteration 67/1500 [0m                      

                       Computation: 102242 steps/s (collection: 0.799s, learning 0.163s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.3572
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 13.5335
                       Mean reward: 9.87
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.3326
    Episode_Reward/rotating_object: 1.5827
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.96s
                      Time elapsed: 00:01:19
                               ETA: 00:27:51

################################################################################
                      [1m Learning iteration 68/1500 [0m                      

                       Computation: 111937 steps/s (collection: 0.788s, learning 0.090s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.3541
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.5497
                       Mean reward: 13.69
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.3305
    Episode_Reward/rotating_object: 2.0519
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.88s
                      Time elapsed: 00:01:20
                               ETA: 00:27:44

################################################################################
                      [1m Learning iteration 69/1500 [0m                      

                       Computation: 110221 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.3335
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 13.5845
                       Mean reward: 7.45
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.3300
    Episode_Reward/rotating_object: 1.5905
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.89s
                      Time elapsed: 00:01:21
                               ETA: 00:27:37

################################################################################
                      [1m Learning iteration 70/1500 [0m                      

                       Computation: 102186 steps/s (collection: 0.845s, learning 0.117s)
             Mean action noise std: 1.34
          Mean value_function loss: 2.0701
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 13.6421
                       Mean reward: 12.10
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.3317
    Episode_Reward/rotating_object: 1.8817
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.96s
                      Time elapsed: 00:01:22
                               ETA: 00:27:32

################################################################################
                      [1m Learning iteration 71/1500 [0m                      

                       Computation: 102240 steps/s (collection: 0.872s, learning 0.089s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.7481
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.6788
                       Mean reward: 12.90
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 2.2522
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.96s
                      Time elapsed: 00:01:22
                               ETA: 00:27:27

################################################################################
                      [1m Learning iteration 72/1500 [0m                      

                       Computation: 114535 steps/s (collection: 0.763s, learning 0.095s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.9353
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 13.7045
                       Mean reward: 15.20
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.3284
    Episode_Reward/rotating_object: 2.4718
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.86s
                      Time elapsed: 00:01:23
                               ETA: 00:27:20

################################################################################
                      [1m Learning iteration 73/1500 [0m                      

                       Computation: 102577 steps/s (collection: 0.856s, learning 0.103s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.8758
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 13.7195
                       Mean reward: 11.00
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.3311
    Episode_Reward/rotating_object: 2.0976
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.96s
                      Time elapsed: 00:01:24
                               ETA: 00:27:15

################################################################################
                      [1m Learning iteration 74/1500 [0m                      

                       Computation: 111871 steps/s (collection: 0.777s, learning 0.102s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.0998
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 13.7522
                       Mean reward: 9.15
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3315
    Episode_Reward/rotating_object: 1.7157
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.88s
                      Time elapsed: 00:01:25
                               ETA: 00:27:09

################################################################################
                      [1m Learning iteration 75/1500 [0m                      

                       Computation: 102366 steps/s (collection: 0.857s, learning 0.103s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.9132
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 13.7783
                       Mean reward: 11.83
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.3286
    Episode_Reward/rotating_object: 2.1800
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.96s
                      Time elapsed: 00:01:26
                               ETA: 00:27:04

################################################################################
                      [1m Learning iteration 76/1500 [0m                      

                       Computation: 106210 steps/s (collection: 0.815s, learning 0.111s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.0473
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 13.8232
                       Mean reward: 11.03
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 0.3361
    Episode_Reward/rotating_object: 2.0551
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.93s
                      Time elapsed: 00:01:27
                               ETA: 00:26:59

################################################################################
                      [1m Learning iteration 77/1500 [0m                      

                       Computation: 101681 steps/s (collection: 0.873s, learning 0.093s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.1874
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 13.8565
                       Mean reward: 13.29
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.3358
    Episode_Reward/rotating_object: 2.2656
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.97s
                      Time elapsed: 00:01:28
                               ETA: 00:26:55

################################################################################
                      [1m Learning iteration 78/1500 [0m                      

                       Computation: 104077 steps/s (collection: 0.855s, learning 0.090s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.9245
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.8825
                       Mean reward: 11.34
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.3316
    Episode_Reward/rotating_object: 2.7205
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.94s
                      Time elapsed: 00:01:29
                               ETA: 00:26:50

################################################################################
                      [1m Learning iteration 79/1500 [0m                      

                       Computation: 104404 steps/s (collection: 0.834s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.1086
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 13.9526
                       Mean reward: 12.30
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.3393
    Episode_Reward/rotating_object: 2.3435
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.94s
                      Time elapsed: 00:01:30
                               ETA: 00:26:46

################################################################################
                      [1m Learning iteration 80/1500 [0m                      

                       Computation: 99636 steps/s (collection: 0.896s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.9721
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 13.9916
                       Mean reward: 17.98
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.3410
    Episode_Reward/rotating_object: 2.4579
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.99s
                      Time elapsed: 00:01:31
                               ETA: 00:26:42

################################################################################
                      [1m Learning iteration 81/1500 [0m                      

                       Computation: 108640 steps/s (collection: 0.791s, learning 0.114s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.3138
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.0051
                       Mean reward: 14.07
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.3364
    Episode_Reward/rotating_object: 2.3934
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.90s
                      Time elapsed: 00:01:32
                               ETA: 00:26:37

################################################################################
                      [1m Learning iteration 82/1500 [0m                      

                       Computation: 99078 steps/s (collection: 0.819s, learning 0.173s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.6853
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.0198
                       Mean reward: 12.02
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.3337
    Episode_Reward/rotating_object: 1.9618
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.99s
                      Time elapsed: 00:01:33
                               ETA: 00:26:34

################################################################################
                      [1m Learning iteration 83/1500 [0m                      

                       Computation: 112297 steps/s (collection: 0.781s, learning 0.094s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.5325
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.0557
                       Mean reward: 14.63
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.3347
    Episode_Reward/rotating_object: 2.2712
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.88s
                      Time elapsed: 00:01:34
                               ETA: 00:26:28

################################################################################
                      [1m Learning iteration 84/1500 [0m                      

                       Computation: 109276 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.9938
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.0883
                       Mean reward: 12.67
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.3341
    Episode_Reward/rotating_object: 2.3747
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.90s
                      Time elapsed: 00:01:35
                               ETA: 00:26:24

################################################################################
                      [1m Learning iteration 85/1500 [0m                      

                       Computation: 103996 steps/s (collection: 0.861s, learning 0.084s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.0967
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.1093
                       Mean reward: 12.53
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 0.3287
    Episode_Reward/rotating_object: 2.3999
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.95s
                      Time elapsed: 00:01:36
                               ETA: 00:26:20

################################################################################
                      [1m Learning iteration 86/1500 [0m                      

                       Computation: 109581 steps/s (collection: 0.806s, learning 0.091s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.6180
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.1390
                       Mean reward: 13.66
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.3317
    Episode_Reward/rotating_object: 2.5860
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.90s
                      Time elapsed: 00:01:36
                               ETA: 00:26:15

################################################################################
                      [1m Learning iteration 87/1500 [0m                      

                       Computation: 105517 steps/s (collection: 0.832s, learning 0.100s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.7456
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.1466
                       Mean reward: 10.85
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.3235
    Episode_Reward/rotating_object: 1.9081
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.93s
                      Time elapsed: 00:01:37
                               ETA: 00:26:11

################################################################################
                      [1m Learning iteration 88/1500 [0m                      

                       Computation: 109247 steps/s (collection: 0.814s, learning 0.086s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.1853
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.1624
                       Mean reward: 16.83
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.3301
    Episode_Reward/rotating_object: 2.4030
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.90s
                      Time elapsed: 00:01:38
                               ETA: 00:26:06

################################################################################
                      [1m Learning iteration 89/1500 [0m                      

                       Computation: 109672 steps/s (collection: 0.794s, learning 0.103s)
             Mean action noise std: 1.44
          Mean value_function loss: 3.0925
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.1875
                       Mean reward: 9.62
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 2.1461
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.90s
                      Time elapsed: 00:01:39
                               ETA: 00:26:02

################################################################################
                      [1m Learning iteration 90/1500 [0m                      

                       Computation: 112964 steps/s (collection: 0.770s, learning 0.100s)
             Mean action noise std: 1.45
          Mean value_function loss: 2.7102
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.2034
                       Mean reward: 17.96
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.3306
    Episode_Reward/rotating_object: 3.0707
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.87s
                      Time elapsed: 00:01:40
                               ETA: 00:25:57

################################################################################
                      [1m Learning iteration 91/1500 [0m                      

                       Computation: 107047 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.3174
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.2311
                       Mean reward: 14.04
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 2.2744
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.92s
                      Time elapsed: 00:01:41
                               ETA: 00:25:53

################################################################################
                      [1m Learning iteration 92/1500 [0m                      

                       Computation: 107303 steps/s (collection: 0.781s, learning 0.136s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.2165
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.2507
                       Mean reward: 11.39
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 2.2559
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.92s
                      Time elapsed: 00:01:42
                               ETA: 00:25:49

################################################################################
                      [1m Learning iteration 93/1500 [0m                      

                       Computation: 107367 steps/s (collection: 0.769s, learning 0.146s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.4149
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.2691
                       Mean reward: 16.38
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 2.8317
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.92s
                      Time elapsed: 00:01:43
                               ETA: 00:25:45

################################################################################
                      [1m Learning iteration 94/1500 [0m                      

                       Computation: 107721 steps/s (collection: 0.764s, learning 0.148s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.5656
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.2968
                       Mean reward: 14.86
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 2.5721
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.91s
                      Time elapsed: 00:01:44
                               ETA: 00:25:42

################################################################################
                      [1m Learning iteration 95/1500 [0m                      

                       Computation: 103924 steps/s (collection: 0.806s, learning 0.140s)
             Mean action noise std: 1.47
          Mean value_function loss: 3.4654
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3319
                       Mean reward: 14.37
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 2.4451
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.95s
                      Time elapsed: 00:01:45
                               ETA: 00:25:38

################################################################################
                      [1m Learning iteration 96/1500 [0m                      

                       Computation: 111333 steps/s (collection: 0.787s, learning 0.096s)
             Mean action noise std: 1.47
          Mean value_function loss: 3.5129
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.3648
                       Mean reward: 21.06
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.3236
    Episode_Reward/rotating_object: 3.0027
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.88s
                      Time elapsed: 00:01:46
                               ETA: 00:25:34

################################################################################
                      [1m Learning iteration 97/1500 [0m                      

                       Computation: 110718 steps/s (collection: 0.777s, learning 0.111s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.6195
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.3792
                       Mean reward: 15.03
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 2.5992
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.89s
                      Time elapsed: 00:01:46
                               ETA: 00:25:30

################################################################################
                      [1m Learning iteration 98/1500 [0m                      

                       Computation: 109978 steps/s (collection: 0.787s, learning 0.107s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.0845
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.3987
                       Mean reward: 13.64
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3294
    Episode_Reward/rotating_object: 2.7030
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.89s
                      Time elapsed: 00:01:47
                               ETA: 00:25:26

################################################################################
                      [1m Learning iteration 99/1500 [0m                      

                       Computation: 97953 steps/s (collection: 0.850s, learning 0.154s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.1297
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.4109
                       Mean reward: 15.85
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 2.6570
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.00s
                      Time elapsed: 00:01:48
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 100/1500 [0m                      

                       Computation: 97480 steps/s (collection: 0.876s, learning 0.132s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.3755
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.4344
                       Mean reward: 14.36
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 2.5543
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.01s
                      Time elapsed: 00:01:49
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 101/1500 [0m                      

                       Computation: 93464 steps/s (collection: 0.918s, learning 0.134s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.9322
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.4602
                       Mean reward: 15.79
               Mean episode length: 240.12
    Episode_Reward/reaching_object: 0.3270
    Episode_Reward/rotating_object: 2.9214
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.05s
                      Time elapsed: 00:01:50
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 102/1500 [0m                      

                       Computation: 109364 steps/s (collection: 0.809s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 3.5658
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.4823
                       Mean reward: 16.72
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.3284
    Episode_Reward/rotating_object: 3.5229
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.90s
                      Time elapsed: 00:01:51
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 103/1500 [0m                      

                       Computation: 104206 steps/s (collection: 0.849s, learning 0.095s)
             Mean action noise std: 1.50
          Mean value_function loss: 4.1504
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.4947
                       Mean reward: 17.10
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 2.6301
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.94s
                      Time elapsed: 00:01:52
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 104/1500 [0m                      

                       Computation: 97815 steps/s (collection: 0.907s, learning 0.098s)
             Mean action noise std: 1.51
          Mean value_function loss: 5.2718
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.5148
                       Mean reward: 14.00
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 3.2329
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.00s
                      Time elapsed: 00:01:53
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 105/1500 [0m                      

                       Computation: 101094 steps/s (collection: 0.847s, learning 0.126s)
             Mean action noise std: 1.51
          Mean value_function loss: 4.9081
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.5449
                       Mean reward: 14.36
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 2.4838
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.97s
                      Time elapsed: 00:01:54
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 106/1500 [0m                      

                       Computation: 101990 steps/s (collection: 0.858s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 4.6589
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.6007
                       Mean reward: 14.30
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 2.8122
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.96s
                      Time elapsed: 00:01:55
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 107/1500 [0m                      

                       Computation: 103396 steps/s (collection: 0.842s, learning 0.109s)
             Mean action noise std: 1.52
          Mean value_function loss: 4.9550
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.6170
                       Mean reward: 18.66
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 3.1386
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.95s
                      Time elapsed: 00:01:56
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 108/1500 [0m                      

                       Computation: 105325 steps/s (collection: 0.828s, learning 0.106s)
             Mean action noise std: 1.53
          Mean value_function loss: 4.7528
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.6386
                       Mean reward: 21.00
               Mean episode length: 228.88
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 4.1872
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.93s
                      Time elapsed: 00:01:57
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 109/1500 [0m                      

                       Computation: 110046 steps/s (collection: 0.801s, learning 0.093s)
             Mean action noise std: 1.54
          Mean value_function loss: 4.7210
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.6720
                       Mean reward: 16.49
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 3.3424
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.89s
                      Time elapsed: 00:01:58
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 110/1500 [0m                      

                       Computation: 108652 steps/s (collection: 0.797s, learning 0.108s)
             Mean action noise std: 1.54
          Mean value_function loss: 5.0546
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.6887
                       Mean reward: 21.84
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 3.2738
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.90s
                      Time elapsed: 00:01:59
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 111/1500 [0m                      

                       Computation: 108291 steps/s (collection: 0.806s, learning 0.102s)
             Mean action noise std: 1.54
          Mean value_function loss: 5.3511
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.7038
                       Mean reward: 16.83
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 0.3122
    Episode_Reward/rotating_object: 3.3047
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.91s
                      Time elapsed: 00:02:00
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 112/1500 [0m                      

                       Computation: 111050 steps/s (collection: 0.793s, learning 0.093s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.1813
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7169
                       Mean reward: 18.95
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 3.0229
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.89s
                      Time elapsed: 00:02:01
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 113/1500 [0m                      

                       Computation: 104886 steps/s (collection: 0.833s, learning 0.104s)
             Mean action noise std: 1.55
          Mean value_function loss: 6.0194
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.7368
                       Mean reward: 15.46
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 3.5107
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.94s
                      Time elapsed: 00:02:02
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 114/1500 [0m                      

                       Computation: 112573 steps/s (collection: 0.785s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 5.7168
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.7489
                       Mean reward: 14.21
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 2.9295
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.87s
                      Time elapsed: 00:02:02
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 115/1500 [0m                      

                       Computation: 110848 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.5890
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7592
                       Mean reward: 18.91
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 3.0996
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.89s
                      Time elapsed: 00:02:03
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 116/1500 [0m                      

                       Computation: 114243 steps/s (collection: 0.766s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 6.5553
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7908
                       Mean reward: 18.08
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 3.1301
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.86s
                      Time elapsed: 00:02:04
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 117/1500 [0m                      

                       Computation: 114862 steps/s (collection: 0.764s, learning 0.092s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.2286
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8109
                       Mean reward: 18.56
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 3.4189
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.86s
                      Time elapsed: 00:02:05
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 118/1500 [0m                      

                       Computation: 114374 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.9261
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8166
                       Mean reward: 25.41
               Mean episode length: 225.76
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 3.6535
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.86s
                      Time elapsed: 00:02:06
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 119/1500 [0m                      

                       Computation: 109856 steps/s (collection: 0.800s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.2327
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8220
                       Mean reward: 17.18
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 0.3064
    Episode_Reward/rotating_object: 3.3743
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.89s
                      Time elapsed: 00:02:07
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 120/1500 [0m                      

                       Computation: 114107 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 5.7474
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 14.8369
                       Mean reward: 19.18
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 0.3020
    Episode_Reward/rotating_object: 3.0108
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.86s
                      Time elapsed: 00:02:08
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 121/1500 [0m                      

                       Computation: 113532 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.3112
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.8494
                       Mean reward: 22.37
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.3033
    Episode_Reward/rotating_object: 3.9489
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.87s
                      Time elapsed: 00:02:09
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 122/1500 [0m                      

                       Computation: 113204 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.4473
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8637
                       Mean reward: 13.08
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 0.3024
    Episode_Reward/rotating_object: 3.0845
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.87s
                      Time elapsed: 00:02:09
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 123/1500 [0m                      

                       Computation: 112745 steps/s (collection: 0.759s, learning 0.113s)
             Mean action noise std: 1.58
          Mean value_function loss: 7.1142
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8786
                       Mean reward: 27.06
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 0.2953
    Episode_Reward/rotating_object: 4.2986
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.87s
                      Time elapsed: 00:02:10
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 124/1500 [0m                      

                       Computation: 110293 steps/s (collection: 0.788s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 7.2561
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8937
                       Mean reward: 17.57
               Mean episode length: 225.25
    Episode_Reward/reaching_object: 0.3018
    Episode_Reward/rotating_object: 4.0631
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.89s
                      Time elapsed: 00:02:11
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 125/1500 [0m                      

                       Computation: 104972 steps/s (collection: 0.815s, learning 0.121s)
             Mean action noise std: 1.58
          Mean value_function loss: 5.5248
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9103
                       Mean reward: 26.22
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 0.2892
    Episode_Reward/rotating_object: 4.3133
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.94s
                      Time elapsed: 00:02:12
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 126/1500 [0m                      

                       Computation: 109247 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 1.59
          Mean value_function loss: 6.3521
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9292
                       Mean reward: 16.39
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 0.2986
    Episode_Reward/rotating_object: 3.4998
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.90s
                      Time elapsed: 00:02:13
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 127/1500 [0m                      

                       Computation: 108168 steps/s (collection: 0.804s, learning 0.105s)
             Mean action noise std: 1.59
          Mean value_function loss: 6.6388
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9528
                       Mean reward: 21.15
               Mean episode length: 231.27
    Episode_Reward/reaching_object: 0.2994
    Episode_Reward/rotating_object: 3.4484
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.91s
                      Time elapsed: 00:02:14
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 128/1500 [0m                      

                       Computation: 106391 steps/s (collection: 0.802s, learning 0.122s)
             Mean action noise std: 1.60
          Mean value_function loss: 7.3286
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.9648
                       Mean reward: 28.87
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 0.3016
    Episode_Reward/rotating_object: 4.0246
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.92s
                      Time elapsed: 00:02:15
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 129/1500 [0m                      

                       Computation: 105602 steps/s (collection: 0.826s, learning 0.105s)
             Mean action noise std: 1.60
          Mean value_function loss: 7.1072
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.9823
                       Mean reward: 20.79
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 0.2938
    Episode_Reward/rotating_object: 3.5330
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.93s
                      Time elapsed: 00:02:16
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 130/1500 [0m                      

                       Computation: 108277 steps/s (collection: 0.802s, learning 0.106s)
             Mean action noise std: 1.60
          Mean value_function loss: 7.9233
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.0012
                       Mean reward: 18.73
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 0.2872
    Episode_Reward/rotating_object: 3.7410
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.91s
                      Time elapsed: 00:02:17
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 131/1500 [0m                      

                       Computation: 111978 steps/s (collection: 0.778s, learning 0.100s)
             Mean action noise std: 1.61
          Mean value_function loss: 8.9884
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.0253
                       Mean reward: 23.40
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 0.2872
    Episode_Reward/rotating_object: 3.9650
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.88s
                      Time elapsed: 00:02:18
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 132/1500 [0m                      

                       Computation: 111270 steps/s (collection: 0.768s, learning 0.116s)
             Mean action noise std: 1.61
          Mean value_function loss: 9.2136
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.0293
                       Mean reward: 20.63
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 0.2921
    Episode_Reward/rotating_object: 3.9493
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.88s
                      Time elapsed: 00:02:18
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 133/1500 [0m                      

                       Computation: 109424 steps/s (collection: 0.788s, learning 0.111s)
             Mean action noise std: 1.61
          Mean value_function loss: 10.0260
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.0370
                       Mean reward: 17.42
               Mean episode length: 226.47
    Episode_Reward/reaching_object: 0.2823
    Episode_Reward/rotating_object: 4.0415
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.90s
                      Time elapsed: 00:02:19
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 134/1500 [0m                      

                       Computation: 110915 steps/s (collection: 0.773s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 9.6675
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.0555
                       Mean reward: 32.55
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 0.2951
    Episode_Reward/rotating_object: 5.1088
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.89s
                      Time elapsed: 00:02:20
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 135/1500 [0m                      

                       Computation: 113059 steps/s (collection: 0.763s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 9.5718
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.0777
                       Mean reward: 20.85
               Mean episode length: 220.76
    Episode_Reward/reaching_object: 0.2839
    Episode_Reward/rotating_object: 4.4620
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.87s
                      Time elapsed: 00:02:21
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 136/1500 [0m                      

                       Computation: 112565 steps/s (collection: 0.782s, learning 0.091s)
             Mean action noise std: 1.63
          Mean value_function loss: 9.7135
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1037
                       Mean reward: 22.87
               Mean episode length: 227.84
    Episode_Reward/reaching_object: 0.2917
    Episode_Reward/rotating_object: 4.7694
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.87s
                      Time elapsed: 00:02:22
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 137/1500 [0m                      

                       Computation: 113922 steps/s (collection: 0.756s, learning 0.107s)
             Mean action noise std: 1.63
          Mean value_function loss: 10.1440
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.1328
                       Mean reward: 28.59
               Mean episode length: 222.54
    Episode_Reward/reaching_object: 0.2857
    Episode_Reward/rotating_object: 4.7767
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.86s
                      Time elapsed: 00:02:23
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 138/1500 [0m                      

                       Computation: 115011 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 1.64
          Mean value_function loss: 10.9584
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.1574
                       Mean reward: 20.62
               Mean episode length: 222.53
    Episode_Reward/reaching_object: 0.2858
    Episode_Reward/rotating_object: 4.0174
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.85s
                      Time elapsed: 00:02:24
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 139/1500 [0m                      

                       Computation: 111750 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 1.64
          Mean value_function loss: 10.8331
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.1665
                       Mean reward: 24.00
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 0.2891
    Episode_Reward/rotating_object: 5.0503
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.88s
                      Time elapsed: 00:02:25
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 140/1500 [0m                      

                       Computation: 110748 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.1993
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.1740
                       Mean reward: 23.37
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.2871
    Episode_Reward/rotating_object: 4.4488
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.89s
                      Time elapsed: 00:02:25
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 141/1500 [0m                      

                       Computation: 112108 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 12.2336
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.2033
                       Mean reward: 18.04
               Mean episode length: 216.28
    Episode_Reward/reaching_object: 0.2886
    Episode_Reward/rotating_object: 4.8674
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.88s
                      Time elapsed: 00:02:26
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 142/1500 [0m                      

                       Computation: 110164 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 1.65
          Mean value_function loss: 11.7917
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.2071
                       Mean reward: 24.31
               Mean episode length: 205.23
    Episode_Reward/reaching_object: 0.2776
    Episode_Reward/rotating_object: 4.9467
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.89s
                      Time elapsed: 00:02:27
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 143/1500 [0m                      

                       Computation: 116296 steps/s (collection: 0.755s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 11.5199
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.2141
                       Mean reward: 28.81
               Mean episode length: 215.93
    Episode_Reward/reaching_object: 0.2861
    Episode_Reward/rotating_object: 5.5644
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.85s
                      Time elapsed: 00:02:28
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 144/1500 [0m                      

                       Computation: 110678 steps/s (collection: 0.784s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 12.3539
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.2288
                       Mean reward: 22.83
               Mean episode length: 214.54
    Episode_Reward/reaching_object: 0.2881
    Episode_Reward/rotating_object: 4.4035
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.89s
                      Time elapsed: 00:02:29
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 145/1500 [0m                      

                       Computation: 111794 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 12.4842
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.2452
                       Mean reward: 25.68
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 0.2780
    Episode_Reward/rotating_object: 4.5195
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.88s
                      Time elapsed: 00:02:30
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 146/1500 [0m                      

                       Computation: 113411 steps/s (collection: 0.764s, learning 0.103s)
             Mean action noise std: 1.66
          Mean value_function loss: 13.3287
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.2477
                       Mean reward: 37.41
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 0.2909
    Episode_Reward/rotating_object: 5.8586
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.87s
                      Time elapsed: 00:02:31
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 147/1500 [0m                      

                       Computation: 105550 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 1.66
          Mean value_function loss: 14.1649
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.2569
                       Mean reward: 27.78
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 0.2850
    Episode_Reward/rotating_object: 6.3148
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.93s
                      Time elapsed: 00:02:32
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 148/1500 [0m                      

                       Computation: 104256 steps/s (collection: 0.840s, learning 0.103s)
             Mean action noise std: 1.67
          Mean value_function loss: 14.4209
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.2639
                       Mean reward: 33.22
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 0.2871
    Episode_Reward/rotating_object: 5.9232
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.94s
                      Time elapsed: 00:02:33
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 149/1500 [0m                      

                       Computation: 112721 steps/s (collection: 0.785s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 13.2286
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2713
                       Mean reward: 36.19
               Mean episode length: 216.76
    Episode_Reward/reaching_object: 0.2822
    Episode_Reward/rotating_object: 5.8770
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.87s
                      Time elapsed: 00:02:33
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 150/1500 [0m                      

                       Computation: 111583 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 12.8104
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2801
                       Mean reward: 26.28
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 0.2876
    Episode_Reward/rotating_object: 6.0737
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.88s
                      Time elapsed: 00:02:34
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 151/1500 [0m                      

                       Computation: 114537 steps/s (collection: 0.767s, learning 0.091s)
             Mean action noise std: 1.67
          Mean value_function loss: 15.2882
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.3028
                       Mean reward: 22.99
               Mean episode length: 220.75
    Episode_Reward/reaching_object: 0.2792
    Episode_Reward/rotating_object: 5.3832
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.86s
                      Time elapsed: 00:02:35
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 152/1500 [0m                      

                       Computation: 108435 steps/s (collection: 0.809s, learning 0.098s)
             Mean action noise std: 1.68
          Mean value_function loss: 14.5251
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.3221
                       Mean reward: 34.83
               Mean episode length: 212.47
    Episode_Reward/reaching_object: 0.2820
    Episode_Reward/rotating_object: 6.3408
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.91s
                      Time elapsed: 00:02:36
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 153/1500 [0m                      

                       Computation: 113707 steps/s (collection: 0.778s, learning 0.086s)
             Mean action noise std: 1.68
          Mean value_function loss: 14.3966
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.3422
                       Mean reward: 26.21
               Mean episode length: 216.80
    Episode_Reward/reaching_object: 0.2806
    Episode_Reward/rotating_object: 6.4365
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.86s
                      Time elapsed: 00:02:37
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 154/1500 [0m                      

                       Computation: 112444 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 13.0526
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.3457
                       Mean reward: 28.94
               Mean episode length: 222.93
    Episode_Reward/reaching_object: 0.2753
    Episode_Reward/rotating_object: 6.7245
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.87s
                      Time elapsed: 00:02:38
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 155/1500 [0m                      

                       Computation: 114567 steps/s (collection: 0.759s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 16.4422
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.3598
                       Mean reward: 25.18
               Mean episode length: 211.32
    Episode_Reward/reaching_object: 0.2773
    Episode_Reward/rotating_object: 5.8440
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.86s
                      Time elapsed: 00:02:39
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 156/1500 [0m                      

                       Computation: 113253 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 16.6972
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.3753
                       Mean reward: 43.47
               Mean episode length: 218.99
    Episode_Reward/reaching_object: 0.2783
    Episode_Reward/rotating_object: 6.6640
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.87s
                      Time elapsed: 00:02:40
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 157/1500 [0m                      

                       Computation: 111622 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 17.7449
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.3902
                       Mean reward: 45.23
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 6.9963
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.88s
                      Time elapsed: 00:02:40
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 158/1500 [0m                      

                       Computation: 109657 steps/s (collection: 0.807s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 18.7642
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.3991
                       Mean reward: 32.83
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 0.2766
    Episode_Reward/rotating_object: 6.3660
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.90s
                      Time elapsed: 00:02:41
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 159/1500 [0m                      

                       Computation: 111363 steps/s (collection: 0.792s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 17.4472
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.4118
                       Mean reward: 29.62
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 0.2808
    Episode_Reward/rotating_object: 7.1956
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.88s
                      Time elapsed: 00:02:42
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 160/1500 [0m                      

                       Computation: 111751 steps/s (collection: 0.792s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 18.8423
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4431
                       Mean reward: 33.27
               Mean episode length: 223.89
    Episode_Reward/reaching_object: 0.2844
    Episode_Reward/rotating_object: 6.9684
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.88s
                      Time elapsed: 00:02:43
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 161/1500 [0m                      

                       Computation: 113179 steps/s (collection: 0.775s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 18.6640
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4524
                       Mean reward: 43.84
               Mean episode length: 210.97
    Episode_Reward/reaching_object: 0.2878
    Episode_Reward/rotating_object: 8.8428
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.87s
                      Time elapsed: 00:02:44
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 162/1500 [0m                      

                       Computation: 113556 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 17.6045
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4604
                       Mean reward: 38.10
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.2838
    Episode_Reward/rotating_object: 6.5616
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.87s
                      Time elapsed: 00:02:45
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 163/1500 [0m                      

                       Computation: 114365 steps/s (collection: 0.759s, learning 0.101s)
             Mean action noise std: 1.71
          Mean value_function loss: 16.8363
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.4713
                       Mean reward: 41.83
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 0.2742
    Episode_Reward/rotating_object: 7.0497
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.86s
                      Time elapsed: 00:02:46
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 164/1500 [0m                      

                       Computation: 112124 steps/s (collection: 0.788s, learning 0.089s)
             Mean action noise std: 1.71
          Mean value_function loss: 15.0004
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.4748
                       Mean reward: 38.68
               Mean episode length: 215.35
    Episode_Reward/reaching_object: 0.2850
    Episode_Reward/rotating_object: 7.9203
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.88s
                      Time elapsed: 00:02:47
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 165/1500 [0m                      

                       Computation: 110229 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 16.3560
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.4806
                       Mean reward: 42.78
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 0.2889
    Episode_Reward/rotating_object: 8.3425
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.89s
                      Time elapsed: 00:02:47
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 166/1500 [0m                      

                       Computation: 107626 steps/s (collection: 0.802s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 17.8883
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.4921
                       Mean reward: 46.18
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.2949
    Episode_Reward/rotating_object: 7.7593
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.91s
                      Time elapsed: 00:02:48
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 167/1500 [0m                      

                       Computation: 112726 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 20.1393
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5099
                       Mean reward: 38.71
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 0.2896
    Episode_Reward/rotating_object: 8.5967
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.87s
                      Time elapsed: 00:02:49
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 168/1500 [0m                      

                       Computation: 112555 steps/s (collection: 0.774s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 21.0368
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5199
                       Mean reward: 40.91
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 0.2905
    Episode_Reward/rotating_object: 8.6253
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.87s
                      Time elapsed: 00:02:50
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 169/1500 [0m                      

                       Computation: 112287 steps/s (collection: 0.775s, learning 0.100s)
             Mean action noise std: 1.72
          Mean value_function loss: 19.4590
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5222
                       Mean reward: 39.11
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 0.2930
    Episode_Reward/rotating_object: 9.1497
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.88s
                      Time elapsed: 00:02:51
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 170/1500 [0m                      

                       Computation: 112637 steps/s (collection: 0.764s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 18.8134
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5216
                       Mean reward: 42.35
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 0.2852
    Episode_Reward/rotating_object: 7.4288
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.87s
                      Time elapsed: 00:02:52
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 171/1500 [0m                      

                       Computation: 109240 steps/s (collection: 0.774s, learning 0.126s)
             Mean action noise std: 1.73
          Mean value_function loss: 18.1054
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5288
                       Mean reward: 36.41
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 0.3014
    Episode_Reward/rotating_object: 9.1607
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.90s
                      Time elapsed: 00:02:53
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 172/1500 [0m                      

                       Computation: 104199 steps/s (collection: 0.839s, learning 0.105s)
             Mean action noise std: 1.73
          Mean value_function loss: 20.0476
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.5274
                       Mean reward: 55.38
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.2938
    Episode_Reward/rotating_object: 9.4809
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.94s
                      Time elapsed: 00:02:54
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 173/1500 [0m                      

                       Computation: 111301 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 20.3022
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.5308
                       Mean reward: 47.95
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.2976
    Episode_Reward/rotating_object: 9.5847
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.88s
                      Time elapsed: 00:02:55
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 174/1500 [0m                      

                       Computation: 110833 steps/s (collection: 0.794s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 19.5876
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.5366
                       Mean reward: 56.63
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.2955
    Episode_Reward/rotating_object: 10.5331
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.89s
                      Time elapsed: 00:02:55
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 175/1500 [0m                      

                       Computation: 112174 steps/s (collection: 0.777s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 19.8670
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 15.5381
                       Mean reward: 53.00
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 0.2992
    Episode_Reward/rotating_object: 9.1718
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.88s
                      Time elapsed: 00:02:56
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 176/1500 [0m                      

                       Computation: 112288 steps/s (collection: 0.788s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 21.7764
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 15.5382
                       Mean reward: 49.11
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 0.3003
    Episode_Reward/rotating_object: 10.0806
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.88s
                      Time elapsed: 00:02:57
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 177/1500 [0m                      

                       Computation: 104731 steps/s (collection: 0.789s, learning 0.149s)
             Mean action noise std: 1.73
          Mean value_function loss: 16.1737
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.5385
                       Mean reward: 44.87
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 0.2911
    Episode_Reward/rotating_object: 9.3134
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.94s
                      Time elapsed: 00:02:58
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 178/1500 [0m                      

                       Computation: 93945 steps/s (collection: 0.866s, learning 0.181s)
             Mean action noise std: 1.73
          Mean value_function loss: 17.2107
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5424
                       Mean reward: 69.52
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 0.2989
    Episode_Reward/rotating_object: 12.2006
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.05s
                      Time elapsed: 00:02:59
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 179/1500 [0m                      

                       Computation: 88575 steps/s (collection: 0.969s, learning 0.141s)
             Mean action noise std: 1.73
          Mean value_function loss: 17.2860
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 15.5453
                       Mean reward: 43.65
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.2945
    Episode_Reward/rotating_object: 12.0008
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.11s
                      Time elapsed: 00:03:00
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 180/1500 [0m                      

                       Computation: 92081 steps/s (collection: 0.966s, learning 0.102s)
             Mean action noise std: 1.73
          Mean value_function loss: 19.6761
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5446
                       Mean reward: 63.36
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.2938
    Episode_Reward/rotating_object: 12.0911
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.07s
                      Time elapsed: 00:03:01
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 181/1500 [0m                      

                       Computation: 101206 steps/s (collection: 0.868s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 22.1532
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5456
                       Mean reward: 62.33
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.3018
    Episode_Reward/rotating_object: 12.7859
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.97s
                      Time elapsed: 00:03:02
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 182/1500 [0m                      

                       Computation: 105313 steps/s (collection: 0.835s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 24.3779
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.5488
                       Mean reward: 57.06
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 0.2901
    Episode_Reward/rotating_object: 13.3634
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.93s
                      Time elapsed: 00:03:03
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 183/1500 [0m                      

                       Computation: 90157 steps/s (collection: 0.993s, learning 0.098s)
             Mean action noise std: 1.74
          Mean value_function loss: 24.3142
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5498
                       Mean reward: 50.85
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 0.2864
    Episode_Reward/rotating_object: 11.5256
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.09s
                      Time elapsed: 00:03:04
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 184/1500 [0m                      

                       Computation: 99518 steps/s (collection: 0.887s, learning 0.101s)
             Mean action noise std: 1.74
          Mean value_function loss: 26.0061
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.5489
                       Mean reward: 43.13
               Mean episode length: 238.47
    Episode_Reward/reaching_object: 0.2952
    Episode_Reward/rotating_object: 9.8759
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.99s
                      Time elapsed: 00:03:05
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 185/1500 [0m                      

                       Computation: 103083 steps/s (collection: 0.839s, learning 0.115s)
             Mean action noise std: 1.74
          Mean value_function loss: 25.9795
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.5498
                       Mean reward: 67.23
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.2881
    Episode_Reward/rotating_object: 11.1352
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.95s
                      Time elapsed: 00:03:06
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 186/1500 [0m                      

                       Computation: 109977 steps/s (collection: 0.805s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 25.8696
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5562
                       Mean reward: 68.60
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 0.2842
    Episode_Reward/rotating_object: 12.4322
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.89s
                      Time elapsed: 00:03:07
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 187/1500 [0m                      

                       Computation: 108033 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 24.5685
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5635
                       Mean reward: 54.86
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.2851
    Episode_Reward/rotating_object: 11.9692
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.91s
                      Time elapsed: 00:03:08
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 188/1500 [0m                      

                       Computation: 108614 steps/s (collection: 0.814s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 24.7009
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.5692
                       Mean reward: 66.47
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.2882
    Episode_Reward/rotating_object: 13.6653
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.91s
                      Time elapsed: 00:03:09
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 189/1500 [0m                      

                       Computation: 109974 steps/s (collection: 0.764s, learning 0.130s)
             Mean action noise std: 1.74
          Mean value_function loss: 29.6511
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.5688
                       Mean reward: 54.64
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.2822
    Episode_Reward/rotating_object: 13.4890
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.89s
                      Time elapsed: 00:03:10
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 190/1500 [0m                      

                       Computation: 107520 steps/s (collection: 0.781s, learning 0.134s)
             Mean action noise std: 1.74
          Mean value_function loss: 27.6974
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5702
                       Mean reward: 83.69
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.2876
    Episode_Reward/rotating_object: 13.6619
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.91s
                      Time elapsed: 00:03:11
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 191/1500 [0m                      

                       Computation: 108652 steps/s (collection: 0.759s, learning 0.146s)
             Mean action noise std: 1.74
          Mean value_function loss: 29.3672
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.5676
                       Mean reward: 64.63
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 0.2902
    Episode_Reward/rotating_object: 13.9679
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.90s
                      Time elapsed: 00:03:12
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 192/1500 [0m                      

                       Computation: 106695 steps/s (collection: 0.785s, learning 0.136s)
             Mean action noise std: 1.75
          Mean value_function loss: 30.5642
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.5702
                       Mean reward: 68.83
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 0.2924
    Episode_Reward/rotating_object: 13.4144
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.92s
                      Time elapsed: 00:03:13
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 193/1500 [0m                      

                       Computation: 110529 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 29.2760
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5763
                       Mean reward: 68.57
               Mean episode length: 229.99
    Episode_Reward/reaching_object: 0.2904
    Episode_Reward/rotating_object: 13.6608
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.89s
                      Time elapsed: 00:03:14
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 194/1500 [0m                      

                       Computation: 101987 steps/s (collection: 0.838s, learning 0.126s)
             Mean action noise std: 1.75
          Mean value_function loss: 27.3162
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.5816
                       Mean reward: 83.34
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.2977
    Episode_Reward/rotating_object: 16.5615
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.96s
                      Time elapsed: 00:03:15
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 195/1500 [0m                      

                       Computation: 95019 steps/s (collection: 0.912s, learning 0.123s)
             Mean action noise std: 1.75
          Mean value_function loss: 32.1717
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5865
                       Mean reward: 69.63
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.2893
    Episode_Reward/rotating_object: 13.8675
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.03s
                      Time elapsed: 00:03:16
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 196/1500 [0m                      

                       Computation: 114105 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 1.75
          Mean value_function loss: 29.0798
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5882
                       Mean reward: 59.25
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 0.2916
    Episode_Reward/rotating_object: 14.2749
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.86s
                      Time elapsed: 00:03:16
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 197/1500 [0m                      

                       Computation: 107527 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 30.2939
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5943
                       Mean reward: 83.97
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 0.2903
    Episode_Reward/rotating_object: 16.5566
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.91s
                      Time elapsed: 00:03:17
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 198/1500 [0m                      

                       Computation: 108574 steps/s (collection: 0.809s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 31.8513
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5999
                       Mean reward: 65.06
               Mean episode length: 231.52
    Episode_Reward/reaching_object: 0.2952
    Episode_Reward/rotating_object: 13.2183
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.91s
                      Time elapsed: 00:03:18
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 199/1500 [0m                      

                       Computation: 108573 steps/s (collection: 0.813s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.4406
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6043
                       Mean reward: 62.70
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.2915
    Episode_Reward/rotating_object: 14.1541
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.91s
                      Time elapsed: 00:03:19
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 200/1500 [0m                      

                       Computation: 110353 steps/s (collection: 0.801s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.4522
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6056
                       Mean reward: 63.58
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 0.2899
    Episode_Reward/rotating_object: 13.5685
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.89s
                      Time elapsed: 00:03:20
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 201/1500 [0m                      

                       Computation: 107479 steps/s (collection: 0.809s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 32.6598
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.6060
                       Mean reward: 60.58
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 0.2976
    Episode_Reward/rotating_object: 13.2216
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.91s
                      Time elapsed: 00:03:21
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 202/1500 [0m                      

                       Computation: 114536 steps/s (collection: 0.765s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 31.4783
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.6036
                       Mean reward: 72.19
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 0.2945
    Episode_Reward/rotating_object: 14.4502
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.86s
                      Time elapsed: 00:03:22
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 203/1500 [0m                      

                       Computation: 111801 steps/s (collection: 0.741s, learning 0.138s)
             Mean action noise std: 1.76
          Mean value_function loss: 29.9834
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5998
                       Mean reward: 73.73
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 0.3005
    Episode_Reward/rotating_object: 17.0965
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.88s
                      Time elapsed: 00:03:23
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 204/1500 [0m                      

                       Computation: 111013 steps/s (collection: 0.792s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 29.0938
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 15.6026
                       Mean reward: 95.38
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.3044
    Episode_Reward/rotating_object: 16.3797
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.89s
                      Time elapsed: 00:03:24
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 205/1500 [0m                      

                       Computation: 97393 steps/s (collection: 0.864s, learning 0.145s)
             Mean action noise std: 1.76
          Mean value_function loss: 31.2314
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5993
                       Mean reward: 81.58
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 0.3035
    Episode_Reward/rotating_object: 17.9612
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.01s
                      Time elapsed: 00:03:25
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 206/1500 [0m                      

                       Computation: 107147 steps/s (collection: 0.786s, learning 0.131s)
             Mean action noise std: 1.76
          Mean value_function loss: 29.3467
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5907
                       Mean reward: 93.92
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 0.3052
    Episode_Reward/rotating_object: 18.2794
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.92s
                      Time elapsed: 00:03:25
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 207/1500 [0m                      

                       Computation: 107749 steps/s (collection: 0.817s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 30.2190
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.6027
                       Mean reward: 59.91
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 0.2991
    Episode_Reward/rotating_object: 17.5763
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.91s
                      Time elapsed: 00:03:26
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 208/1500 [0m                      

                       Computation: 113616 steps/s (collection: 0.767s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 31.5012
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.6166
                       Mean reward: 83.22
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 18.1171
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.87s
                      Time elapsed: 00:03:27
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 209/1500 [0m                      

                       Computation: 112932 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 37.4944
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6240
                       Mean reward: 83.94
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 0.2956
    Episode_Reward/rotating_object: 18.7876
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.87s
                      Time elapsed: 00:03:28
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 210/1500 [0m                      

                       Computation: 109271 steps/s (collection: 0.792s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 31.1387
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.6213
                       Mean reward: 122.38
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.3047
    Episode_Reward/rotating_object: 22.5741
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.90s
                      Time elapsed: 00:03:29
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 211/1500 [0m                      

                       Computation: 111424 steps/s (collection: 0.772s, learning 0.111s)
             Mean action noise std: 1.76
          Mean value_function loss: 30.6516
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.6203
                       Mean reward: 101.35
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 0.2949
    Episode_Reward/rotating_object: 16.7589
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.88s
                      Time elapsed: 00:03:30
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 212/1500 [0m                      

                       Computation: 102873 steps/s (collection: 0.793s, learning 0.163s)
             Mean action noise std: 1.76
          Mean value_function loss: 29.1622
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6125
                       Mean reward: 98.03
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.3051
    Episode_Reward/rotating_object: 21.5585
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.96s
                      Time elapsed: 00:03:31
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 213/1500 [0m                      

                       Computation: 105779 steps/s (collection: 0.780s, learning 0.149s)
             Mean action noise std: 1.76
          Mean value_function loss: 30.8756
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6085
                       Mean reward: 117.38
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 0.2987
    Episode_Reward/rotating_object: 19.8227
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.93s
                      Time elapsed: 00:03:32
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 214/1500 [0m                      

                       Computation: 113895 steps/s (collection: 0.734s, learning 0.130s)
             Mean action noise std: 1.76
          Mean value_function loss: 32.9401
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6135
                       Mean reward: 100.14
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 20.9824
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.86s
                      Time elapsed: 00:03:33
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 215/1500 [0m                      

                       Computation: 111192 steps/s (collection: 0.778s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 31.7285
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.6067
                       Mean reward: 101.27
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.3013
    Episode_Reward/rotating_object: 20.2800
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.88s
                      Time elapsed: 00:03:34
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 216/1500 [0m                      

                       Computation: 109209 steps/s (collection: 0.802s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 33.9336
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 15.6003
                       Mean reward: 127.33
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 0.2994
    Episode_Reward/rotating_object: 19.4026
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.90s
                      Time elapsed: 00:03:34
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 217/1500 [0m                      

                       Computation: 114900 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.5577
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.6020
                       Mean reward: 135.66
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 0.3022
    Episode_Reward/rotating_object: 24.9737
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.86s
                      Time elapsed: 00:03:35
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 218/1500 [0m                      

                       Computation: 115730 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.9615
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.6015
                       Mean reward: 97.89
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 0.3034
    Episode_Reward/rotating_object: 23.5232
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.85s
                      Time elapsed: 00:03:36
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 219/1500 [0m                      

                       Computation: 104835 steps/s (collection: 0.843s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 36.9052
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 15.6041
                       Mean reward: 104.90
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.3062
    Episode_Reward/rotating_object: 21.6936
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.94s
                      Time elapsed: 00:03:37
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 220/1500 [0m                      

                       Computation: 109242 steps/s (collection: 0.780s, learning 0.120s)
             Mean action noise std: 1.76
          Mean value_function loss: 37.9429
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 15.6076
                       Mean reward: 122.02
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 23.2009
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.90s
                      Time elapsed: 00:03:38
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 221/1500 [0m                      

                       Computation: 105650 steps/s (collection: 0.803s, learning 0.127s)
             Mean action noise std: 1.77
          Mean value_function loss: 39.6948
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 15.6082
                       Mean reward: 109.99
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 22.1760
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.93s
                      Time elapsed: 00:03:39
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 222/1500 [0m                      

                       Computation: 115880 steps/s (collection: 0.750s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 38.7306
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6069
                       Mean reward: 119.00
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 22.6917
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.85s
                      Time elapsed: 00:03:40
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 223/1500 [0m                      

                       Computation: 104653 steps/s (collection: 0.774s, learning 0.165s)
             Mean action noise std: 1.77
          Mean value_function loss: 41.9068
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.6055
                       Mean reward: 132.76
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 0.3100
    Episode_Reward/rotating_object: 24.1873
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.94s
                      Time elapsed: 00:03:41
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 224/1500 [0m                      

                       Computation: 111795 steps/s (collection: 0.781s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 48.4026
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6057
                       Mean reward: 140.98
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 0.3032
    Episode_Reward/rotating_object: 24.7876
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.88s
                      Time elapsed: 00:03:42
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 225/1500 [0m                      

                       Computation: 112866 steps/s (collection: 0.785s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 51.5343
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.6099
                       Mean reward: 113.88
               Mean episode length: 222.52
    Episode_Reward/reaching_object: 0.2972
    Episode_Reward/rotating_object: 23.7511
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.87s
                      Time elapsed: 00:03:42
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 226/1500 [0m                      

                       Computation: 109554 steps/s (collection: 0.802s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 37.3335
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 15.6125
                       Mean reward: 114.07
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 25.4718
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.90s
                      Time elapsed: 00:03:43
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 227/1500 [0m                      

                       Computation: 106340 steps/s (collection: 0.829s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 38.8930
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 15.6131
                       Mean reward: 155.14
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 27.3111
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.92s
                      Time elapsed: 00:03:44
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 228/1500 [0m                      

                       Computation: 109164 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 39.2467
               Mean surrogate loss: 0.0174
                 Mean entropy loss: 15.6130
                       Mean reward: 114.32
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 27.5384
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.90s
                      Time elapsed: 00:03:45
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 229/1500 [0m                      

                       Computation: 106401 steps/s (collection: 0.795s, learning 0.129s)
             Mean action noise std: 1.77
          Mean value_function loss: 26.5526
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6132
                       Mean reward: 138.51
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 25.3019
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.92s
                      Time elapsed: 00:03:46
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 230/1500 [0m                      

                       Computation: 100495 steps/s (collection: 0.879s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 26.3953
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6148
                       Mean reward: 118.71
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 25.6738
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.98s
                      Time elapsed: 00:03:47
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 231/1500 [0m                      

                       Computation: 105278 steps/s (collection: 0.814s, learning 0.120s)
             Mean action noise std: 1.77
          Mean value_function loss: 30.6080
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.6182
                       Mean reward: 137.55
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 25.9569
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.93s
                      Time elapsed: 00:03:48
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 232/1500 [0m                      

                       Computation: 102971 steps/s (collection: 0.832s, learning 0.123s)
             Mean action noise std: 1.77
          Mean value_function loss: 33.7598
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.6200
                       Mean reward: 161.15
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 26.5293
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.95s
                      Time elapsed: 00:03:49
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 233/1500 [0m                      

                       Computation: 100705 steps/s (collection: 0.868s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 37.6679
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 15.6202
                       Mean reward: 150.42
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 26.0168
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.98s
                      Time elapsed: 00:03:50
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 234/1500 [0m                      

                       Computation: 111331 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 57.4325
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6225
                       Mean reward: 136.67
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 27.3494
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.88s
                      Time elapsed: 00:03:51
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 235/1500 [0m                      

                       Computation: 100748 steps/s (collection: 0.770s, learning 0.206s)
             Mean action noise std: 1.77
          Mean value_function loss: 57.8649
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6297
                       Mean reward: 128.54
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 0.3019
    Episode_Reward/rotating_object: 23.8587
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.98s
                      Time elapsed: 00:03:52
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 236/1500 [0m                      

                       Computation: 100966 steps/s (collection: 0.852s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 70.0636
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.6335
                       Mean reward: 106.49
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 22.1171
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.97s
                      Time elapsed: 00:03:53
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 237/1500 [0m                      

                       Computation: 104365 steps/s (collection: 0.778s, learning 0.164s)
             Mean action noise std: 1.77
          Mean value_function loss: 66.8384
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.6330
                       Mean reward: 109.45
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 23.5569
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.94s
                      Time elapsed: 00:03:54
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 238/1500 [0m                      

                       Computation: 97207 steps/s (collection: 0.866s, learning 0.146s)
             Mean action noise std: 1.78
          Mean value_function loss: 73.9342
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.6341
                       Mean reward: 103.38
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 24.0132
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.01s
                      Time elapsed: 00:03:55
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 239/1500 [0m                      

                       Computation: 96722 steps/s (collection: 0.832s, learning 0.184s)
             Mean action noise std: 1.78
          Mean value_function loss: 59.0942
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6423
                       Mean reward: 111.66
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 28.2824
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.02s
                      Time elapsed: 00:03:56
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 240/1500 [0m                      

                       Computation: 100162 steps/s (collection: 0.853s, learning 0.129s)
             Mean action noise std: 1.78
          Mean value_function loss: 61.4060
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6485
                       Mean reward: 115.14
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 27.4625
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.98s
                      Time elapsed: 00:03:57
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 241/1500 [0m                      

                       Computation: 102298 steps/s (collection: 0.819s, learning 0.142s)
             Mean action noise std: 1.78
          Mean value_function loss: 69.6347
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.6484
                       Mean reward: 154.69
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.3256
    Episode_Reward/rotating_object: 31.5229
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.96s
                      Time elapsed: 00:03:58
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 242/1500 [0m                      

                       Computation: 91763 steps/s (collection: 0.950s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 61.3187
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6492
                       Mean reward: 127.60
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 25.0280
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.07s
                      Time elapsed: 00:03:59
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 243/1500 [0m                      

                       Computation: 111518 steps/s (collection: 0.789s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 63.1320
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.6556
                       Mean reward: 138.12
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 29.0911
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.88s
                      Time elapsed: 00:04:00
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 244/1500 [0m                      

                       Computation: 110967 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 55.3268
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6603
                       Mean reward: 145.59
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 28.8105
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.89s
                      Time elapsed: 00:04:01
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 245/1500 [0m                      

                       Computation: 103345 steps/s (collection: 0.833s, learning 0.118s)
             Mean action noise std: 1.78
          Mean value_function loss: 66.2378
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6672
                       Mean reward: 163.35
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 27.6251
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.95s
                      Time elapsed: 00:04:01
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 246/1500 [0m                      

                       Computation: 105499 steps/s (collection: 0.803s, learning 0.129s)
             Mean action noise std: 1.79
          Mean value_function loss: 67.7743
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6684
                       Mean reward: 180.81
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 0.3295
    Episode_Reward/rotating_object: 29.4453
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.93s
                      Time elapsed: 00:04:02
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 247/1500 [0m                      

                       Computation: 103298 steps/s (collection: 0.835s, learning 0.117s)
             Mean action noise std: 1.79
          Mean value_function loss: 67.2049
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.6702
                       Mean reward: 140.30
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 29.5457
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.95s
                      Time elapsed: 00:04:03
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 248/1500 [0m                      

                       Computation: 102813 steps/s (collection: 0.828s, learning 0.129s)
             Mean action noise std: 1.79
          Mean value_function loss: 67.0589
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.6745
                       Mean reward: 166.24
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 31.2731
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.96s
                      Time elapsed: 00:04:04
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 249/1500 [0m                      

                       Computation: 104725 steps/s (collection: 0.836s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 73.5693
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6779
                       Mean reward: 160.66
               Mean episode length: 222.33
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 28.3426
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.94s
                      Time elapsed: 00:04:05
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 250/1500 [0m                      

                       Computation: 111475 steps/s (collection: 0.765s, learning 0.117s)
             Mean action noise std: 1.79
          Mean value_function loss: 71.8309
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6836
                       Mean reward: 170.25
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 0.3284
    Episode_Reward/rotating_object: 31.0518
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.88s
                      Time elapsed: 00:04:06
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 251/1500 [0m                      

                       Computation: 108531 steps/s (collection: 0.794s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 69.6554
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.6882
                       Mean reward: 135.11
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 28.1720
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.91s
                      Time elapsed: 00:04:07
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 252/1500 [0m                      

                       Computation: 104620 steps/s (collection: 0.805s, learning 0.134s)
             Mean action noise std: 1.79
          Mean value_function loss: 59.0269
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.6897
                       Mean reward: 160.63
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.3266
    Episode_Reward/rotating_object: 31.3528
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.94s
                      Time elapsed: 00:04:08
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 253/1500 [0m                      

                       Computation: 104375 steps/s (collection: 0.815s, learning 0.127s)
             Mean action noise std: 1.79
          Mean value_function loss: 65.6374
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 15.6915
                       Mean reward: 175.07
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 32.5975
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.94s
                      Time elapsed: 00:04:09
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 254/1500 [0m                      

                       Computation: 109004 steps/s (collection: 0.810s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 84.6851
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.6920
                       Mean reward: 141.97
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 29.4196
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.90s
                      Time elapsed: 00:04:10
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 255/1500 [0m                      

                       Computation: 102871 steps/s (collection: 0.794s, learning 0.162s)
             Mean action noise std: 1.79
          Mean value_function loss: 83.8130
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.6918
                       Mean reward: 160.88
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 32.3299
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.96s
                      Time elapsed: 00:04:11
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 256/1500 [0m                      

                       Computation: 100927 steps/s (collection: 0.820s, learning 0.154s)
             Mean action noise std: 1.79
          Mean value_function loss: 74.2730
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.6933
                       Mean reward: 174.41
               Mean episode length: 221.48
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 31.9621
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.97s
                      Time elapsed: 00:04:12
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 257/1500 [0m                      

                       Computation: 106344 steps/s (collection: 0.815s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 69.2592
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.6962
                       Mean reward: 153.15
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 28.5709
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.92s
                      Time elapsed: 00:04:13
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 258/1500 [0m                      

                       Computation: 91172 steps/s (collection: 0.871s, learning 0.207s)
             Mean action noise std: 1.79
          Mean value_function loss: 56.1726
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7001
                       Mean reward: 151.46
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.3120
    Episode_Reward/rotating_object: 27.5144
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.08s
                      Time elapsed: 00:04:14
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 259/1500 [0m                      

                       Computation: 104651 steps/s (collection: 0.819s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 60.1847
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 15.7056
                       Mean reward: 148.26
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 28.0983
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.94s
                      Time elapsed: 00:04:15
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 260/1500 [0m                      

                       Computation: 89332 steps/s (collection: 0.892s, learning 0.209s)
             Mean action noise std: 1.80
          Mean value_function loss: 54.1168
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.7123
                       Mean reward: 157.64
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 31.1076
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.10s
                      Time elapsed: 00:04:16
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 261/1500 [0m                      

                       Computation: 99763 steps/s (collection: 0.848s, learning 0.138s)
             Mean action noise std: 1.80
          Mean value_function loss: 64.9689
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7168
                       Mean reward: 169.94
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 29.2870
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.99s
                      Time elapsed: 00:04:17
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 262/1500 [0m                      

                       Computation: 83269 steps/s (collection: 0.991s, learning 0.189s)
             Mean action noise std: 1.80
          Mean value_function loss: 68.3589
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.7248
                       Mean reward: 148.14
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 28.0551
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 1.18s
                      Time elapsed: 00:04:18
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 263/1500 [0m                      

                       Computation: 103956 steps/s (collection: 0.830s, learning 0.116s)
             Mean action noise std: 1.80
          Mean value_function loss: 61.7644
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.7262
                       Mean reward: 167.12
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 0.3056
    Episode_Reward/rotating_object: 29.5014
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.95s
                      Time elapsed: 00:04:19
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 264/1500 [0m                      

                       Computation: 98977 steps/s (collection: 0.867s, learning 0.126s)
             Mean action noise std: 1.80
          Mean value_function loss: 60.5612
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.7260
                       Mean reward: 162.24
               Mean episode length: 223.23
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 31.6989
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.99s
                      Time elapsed: 00:04:20
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 265/1500 [0m                      

                       Computation: 105448 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 56.4574
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.7266
                       Mean reward: 157.52
               Mean episode length: 218.95
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 32.4103
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.93s
                      Time elapsed: 00:04:21
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 266/1500 [0m                      

                       Computation: 96290 steps/s (collection: 0.864s, learning 0.157s)
             Mean action noise std: 1.80
          Mean value_function loss: 56.7426
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7287
                       Mean reward: 143.86
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 33.3521
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.02s
                      Time elapsed: 00:04:22
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 267/1500 [0m                      

                       Computation: 95527 steps/s (collection: 0.848s, learning 0.182s)
             Mean action noise std: 1.80
          Mean value_function loss: 54.6836
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 15.7328
                       Mean reward: 190.94
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 34.4304
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.03s
                      Time elapsed: 00:04:23
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 268/1500 [0m                      

                       Computation: 101497 steps/s (collection: 0.837s, learning 0.132s)
             Mean action noise std: 1.80
          Mean value_function loss: 59.6723
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7362
                       Mean reward: 124.81
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 27.4965
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.97s
                      Time elapsed: 00:04:24
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 269/1500 [0m                      

                       Computation: 94501 steps/s (collection: 0.921s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 92.4396
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 15.7390
                       Mean reward: 154.92
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 0.3043
    Episode_Reward/rotating_object: 30.4567
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.04s
                      Time elapsed: 00:04:25
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 270/1500 [0m                      

                       Computation: 112583 steps/s (collection: 0.782s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 55.3288
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7402
                       Mean reward: 156.89
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 31.7744
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.87s
                      Time elapsed: 00:04:26
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 271/1500 [0m                      

                       Computation: 86234 steps/s (collection: 0.976s, learning 0.164s)
             Mean action noise std: 1.80
          Mean value_function loss: 64.9367
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7408
                       Mean reward: 130.62
               Mean episode length: 218.47
    Episode_Reward/reaching_object: 0.3017
    Episode_Reward/rotating_object: 32.4516
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.14s
                      Time elapsed: 00:04:27
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 272/1500 [0m                      

                       Computation: 85696 steps/s (collection: 0.970s, learning 0.177s)
             Mean action noise std: 1.81
          Mean value_function loss: 61.5921
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.7428
                       Mean reward: 200.47
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 33.3705
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.15s
                      Time elapsed: 00:04:28
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 273/1500 [0m                      

                       Computation: 74194 steps/s (collection: 1.114s, learning 0.211s)
             Mean action noise std: 1.81
          Mean value_function loss: 58.3133
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.7530
                       Mean reward: 143.34
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 0.3044
    Episode_Reward/rotating_object: 28.2924
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.32s
                      Time elapsed: 00:04:29
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 274/1500 [0m                      

                       Computation: 92314 steps/s (collection: 0.902s, learning 0.163s)
             Mean action noise std: 1.81
          Mean value_function loss: 58.2759
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.7664
                       Mean reward: 138.98
               Mean episode length: 219.92
    Episode_Reward/reaching_object: 0.2990
    Episode_Reward/rotating_object: 29.3040
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.06s
                      Time elapsed: 00:04:30
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 275/1500 [0m                      

                       Computation: 90205 steps/s (collection: 0.961s, learning 0.129s)
             Mean action noise std: 1.81
          Mean value_function loss: 63.5197
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7693
                       Mean reward: 134.51
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 29.1029
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.09s
                      Time elapsed: 00:04:32
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 276/1500 [0m                      

                       Computation: 100289 steps/s (collection: 0.858s, learning 0.122s)
             Mean action noise std: 1.81
          Mean value_function loss: 72.4268
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7752
                       Mean reward: 99.54
               Mean episode length: 228.85
    Episode_Reward/reaching_object: 0.2983
    Episode_Reward/rotating_object: 22.8172
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.98s
                      Time elapsed: 00:04:33
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 277/1500 [0m                      

                       Computation: 86700 steps/s (collection: 0.954s, learning 0.180s)
             Mean action noise std: 1.81
          Mean value_function loss: 76.5919
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.7821
                       Mean reward: 126.84
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 0.3022
    Episode_Reward/rotating_object: 27.3601
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.13s
                      Time elapsed: 00:04:34
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 278/1500 [0m                      

                       Computation: 87834 steps/s (collection: 0.905s, learning 0.214s)
             Mean action noise std: 1.81
          Mean value_function loss: 75.4502
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.7806
                       Mean reward: 155.76
               Mean episode length: 228.33
    Episode_Reward/reaching_object: 0.3076
    Episode_Reward/rotating_object: 28.9965
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.12s
                      Time elapsed: 00:04:35
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 279/1500 [0m                      

                       Computation: 90687 steps/s (collection: 0.838s, learning 0.246s)
             Mean action noise std: 1.82
          Mean value_function loss: 84.9998
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7813
                       Mean reward: 135.66
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 30.1138
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.08s
                      Time elapsed: 00:04:36
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 280/1500 [0m                      

                       Computation: 102106 steps/s (collection: 0.814s, learning 0.149s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.9358
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 15.7859
                       Mean reward: 162.19
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 0.3085
    Episode_Reward/rotating_object: 28.8355
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.96s
                      Time elapsed: 00:04:37
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 281/1500 [0m                      

                       Computation: 89002 steps/s (collection: 0.932s, learning 0.172s)
             Mean action noise std: 1.82
          Mean value_function loss: 80.6476
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 15.7882
                       Mean reward: 142.60
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 30.0425
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.10s
                      Time elapsed: 00:04:38
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 282/1500 [0m                      

                       Computation: 83364 steps/s (collection: 1.007s, learning 0.173s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.9462
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7904
                       Mean reward: 192.20
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 33.5531
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 1.18s
                      Time elapsed: 00:04:39
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 283/1500 [0m                      

                       Computation: 96625 steps/s (collection: 0.856s, learning 0.161s)
             Mean action noise std: 1.82
          Mean value_function loss: 74.7628
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.7917
                       Mean reward: 141.37
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 31.5141
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.02s
                      Time elapsed: 00:04:40
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 284/1500 [0m                      

                       Computation: 96422 steps/s (collection: 0.865s, learning 0.155s)
             Mean action noise std: 1.82
          Mean value_function loss: 68.9901
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7926
                       Mean reward: 172.02
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 31.7901
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.02s
                      Time elapsed: 00:04:41
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 285/1500 [0m                      

                       Computation: 99794 steps/s (collection: 0.845s, learning 0.141s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.7142
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7937
                       Mean reward: 179.56
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 34.3867
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.99s
                      Time elapsed: 00:04:42
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 286/1500 [0m                      

                       Computation: 94137 steps/s (collection: 0.864s, learning 0.181s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.5406
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.7985
                       Mean reward: 183.67
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 36.9618
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.04s
                      Time elapsed: 00:04:43
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 287/1500 [0m                      

                       Computation: 94268 steps/s (collection: 0.889s, learning 0.154s)
             Mean action noise std: 1.82
          Mean value_function loss: 80.9538
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 15.8006
                       Mean reward: 153.40
               Mean episode length: 227.55
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 33.6948
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.04s
                      Time elapsed: 00:04:44
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 288/1500 [0m                      

                       Computation: 94154 steps/s (collection: 0.907s, learning 0.137s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.4042
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 15.8014
                       Mean reward: 185.70
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 34.9885
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 1.04s
                      Time elapsed: 00:04:45
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 289/1500 [0m                      

                       Computation: 90378 steps/s (collection: 0.934s, learning 0.154s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.8040
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 15.8022
                       Mean reward: 237.36
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 39.3535
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.09s
                      Time elapsed: 00:04:46
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 290/1500 [0m                      

                       Computation: 90482 steps/s (collection: 0.896s, learning 0.191s)
             Mean action noise std: 1.82
          Mean value_function loss: 91.4379
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 15.8028
                       Mean reward: 188.66
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 37.4498
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.09s
                      Time elapsed: 00:04:47
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 291/1500 [0m                      

                       Computation: 87052 steps/s (collection: 0.982s, learning 0.148s)
             Mean action noise std: 1.82
          Mean value_function loss: 104.8982
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 15.8030
                       Mean reward: 155.50
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 35.0068
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.13s
                      Time elapsed: 00:04:49
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 292/1500 [0m                      

                       Computation: 86958 steps/s (collection: 0.939s, learning 0.191s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.0287
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.8034
                       Mean reward: 170.29
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 33.8558
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 1.13s
                      Time elapsed: 00:04:50
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 293/1500 [0m                      

                       Computation: 95029 steps/s (collection: 0.848s, learning 0.187s)
             Mean action noise std: 1.82
          Mean value_function loss: 66.9375
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8056
                       Mean reward: 180.69
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 0.3235
    Episode_Reward/rotating_object: 34.5854
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.03s
                      Time elapsed: 00:04:51
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 294/1500 [0m                      

                       Computation: 95558 steps/s (collection: 0.908s, learning 0.121s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.7850
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.8123
                       Mean reward: 212.82
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.3262
    Episode_Reward/rotating_object: 39.3594
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.03s
                      Time elapsed: 00:04:52
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 295/1500 [0m                      

                       Computation: 93160 steps/s (collection: 0.892s, learning 0.163s)
             Mean action noise std: 1.82
          Mean value_function loss: 69.8107
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.8165
                       Mean reward: 183.51
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 0.3327
    Episode_Reward/rotating_object: 40.4235
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.06s
                      Time elapsed: 00:04:53
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 296/1500 [0m                      

                       Computation: 108379 steps/s (collection: 0.806s, learning 0.101s)
             Mean action noise std: 1.82
          Mean value_function loss: 61.9854
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8135
                       Mean reward: 187.83
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 0.3279
    Episode_Reward/rotating_object: 36.9963
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.91s
                      Time elapsed: 00:04:54
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 297/1500 [0m                      

                       Computation: 95385 steps/s (collection: 0.825s, learning 0.206s)
             Mean action noise std: 1.82
          Mean value_function loss: 73.2668
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8127
                       Mean reward: 185.77
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 36.9706
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.03s
                      Time elapsed: 00:04:55
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 298/1500 [0m                      

                       Computation: 84708 steps/s (collection: 0.931s, learning 0.229s)
             Mean action noise std: 1.83
          Mean value_function loss: 80.8628
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.8172
                       Mean reward: 209.87
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 35.9328
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.16s
                      Time elapsed: 00:04:56
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 299/1500 [0m                      

                       Computation: 97286 steps/s (collection: 0.910s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 74.6442
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8181
                       Mean reward: 192.85
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.3246
    Episode_Reward/rotating_object: 38.3695
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 1.01s
                      Time elapsed: 00:04:57
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 300/1500 [0m                      

                       Computation: 90253 steps/s (collection: 0.949s, learning 0.140s)
             Mean action noise std: 1.83
          Mean value_function loss: 87.0448
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.8174
                       Mean reward: 192.51
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.3304
    Episode_Reward/rotating_object: 42.1044
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.09s
                      Time elapsed: 00:04:58
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 301/1500 [0m                      

                       Computation: 104827 steps/s (collection: 0.843s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 93.6956
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.8195
                       Mean reward: 165.92
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 36.2358
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.94s
                      Time elapsed: 00:04:59
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 302/1500 [0m                      

                       Computation: 95380 steps/s (collection: 0.883s, learning 0.148s)
             Mean action noise std: 1.83
          Mean value_function loss: 92.8587
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 15.8203
                       Mean reward: 180.44
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 0.3312
    Episode_Reward/rotating_object: 40.9503
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.03s
                      Time elapsed: 00:05:00
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 303/1500 [0m                      

                       Computation: 109072 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 99.4959
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 15.8224
                       Mean reward: 203.56
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 38.8589
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.90s
                      Time elapsed: 00:05:01
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 304/1500 [0m                      

                       Computation: 100993 steps/s (collection: 0.854s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 101.5077
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.8253
                       Mean reward: 203.88
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 37.8183
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.97s
                      Time elapsed: 00:05:02
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 305/1500 [0m                      

                       Computation: 107435 steps/s (collection: 0.792s, learning 0.123s)
             Mean action noise std: 1.83
          Mean value_function loss: 98.7738
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8260
                       Mean reward: 223.30
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.3272
    Episode_Reward/rotating_object: 42.4271
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.92s
                      Time elapsed: 00:05:03
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 306/1500 [0m                      

                       Computation: 94141 steps/s (collection: 0.829s, learning 0.215s)
             Mean action noise std: 1.83
          Mean value_function loss: 102.1738
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8267
                       Mean reward: 191.14
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 0.3316
    Episode_Reward/rotating_object: 42.7862
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 1.04s
                      Time elapsed: 00:05:04
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 307/1500 [0m                      

                       Computation: 87877 steps/s (collection: 0.966s, learning 0.153s)
             Mean action noise std: 1.83
          Mean value_function loss: 107.1415
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8286
                       Mean reward: 232.47
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.3353
    Episode_Reward/rotating_object: 43.9250
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 1.12s
                      Time elapsed: 00:05:05
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 308/1500 [0m                      

                       Computation: 86271 steps/s (collection: 1.026s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 101.3468
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8288
                       Mean reward: 262.93
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3274
    Episode_Reward/rotating_object: 42.3560
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 1.14s
                      Time elapsed: 00:05:06
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 309/1500 [0m                      

                       Computation: 86008 steps/s (collection: 1.008s, learning 0.135s)
             Mean action noise std: 1.83
          Mean value_function loss: 114.7314
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 15.8332
                       Mean reward: 208.05
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.3343
    Episode_Reward/rotating_object: 40.6741
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.14s
                      Time elapsed: 00:05:07
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 310/1500 [0m                      

                       Computation: 82177 steps/s (collection: 1.020s, learning 0.176s)
             Mean action noise std: 1.83
          Mean value_function loss: 120.5323
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.8337
                       Mean reward: 229.51
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 0.3303
    Episode_Reward/rotating_object: 42.2406
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.20s
                      Time elapsed: 00:05:08
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 311/1500 [0m                      

                       Computation: 100157 steps/s (collection: 0.838s, learning 0.144s)
             Mean action noise std: 1.83
          Mean value_function loss: 152.4206
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 15.8346
                       Mean reward: 177.33
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 0.3314
    Episode_Reward/rotating_object: 44.4148
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.98s
                      Time elapsed: 00:05:09
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 312/1500 [0m                      

                       Computation: 93862 steps/s (collection: 0.872s, learning 0.175s)
             Mean action noise std: 1.83
          Mean value_function loss: 137.6969
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.8338
                       Mean reward: 229.03
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 0.3402
    Episode_Reward/rotating_object: 45.4425
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 1.05s
                      Time elapsed: 00:05:10
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 313/1500 [0m                      

                       Computation: 96624 steps/s (collection: 0.871s, learning 0.147s)
             Mean action noise std: 1.83
          Mean value_function loss: 126.5777
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8306
                       Mean reward: 201.71
               Mean episode length: 227.67
    Episode_Reward/reaching_object: 0.3312
    Episode_Reward/rotating_object: 41.1583
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 1.02s
                      Time elapsed: 00:05:11
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 314/1500 [0m                      

                       Computation: 113902 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 122.3847
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8300
                       Mean reward: 213.53
               Mean episode length: 224.35
    Episode_Reward/reaching_object: 0.3308
    Episode_Reward/rotating_object: 39.4789
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.86s
                      Time elapsed: 00:05:12
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 315/1500 [0m                      

                       Computation: 111847 steps/s (collection: 0.783s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 130.9768
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.8344
                       Mean reward: 207.60
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.3325
    Episode_Reward/rotating_object: 43.8228
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.88s
                      Time elapsed: 00:05:13
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 316/1500 [0m                      

                       Computation: 112389 steps/s (collection: 0.780s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 130.8127
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8388
                       Mean reward: 179.03
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 38.9406
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.87s
                      Time elapsed: 00:05:14
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 317/1500 [0m                      

                       Computation: 113395 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 131.7035
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.8406
                       Mean reward: 213.02
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 36.8819
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.87s
                      Time elapsed: 00:05:15
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 318/1500 [0m                      

                       Computation: 115941 steps/s (collection: 0.760s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 106.9766
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8433
                       Mean reward: 211.26
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.3266
    Episode_Reward/rotating_object: 37.7370
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.85s
                      Time elapsed: 00:05:16
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 319/1500 [0m                      

                       Computation: 115234 steps/s (collection: 0.757s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 110.8095
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8488
                       Mean reward: 231.57
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.3398
    Episode_Reward/rotating_object: 41.5360
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.85s
                      Time elapsed: 00:05:17
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 320/1500 [0m                      

                       Computation: 113671 steps/s (collection: 0.763s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 109.1151
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.8545
                       Mean reward: 184.54
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 0.3333
    Episode_Reward/rotating_object: 39.8282
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.86s
                      Time elapsed: 00:05:18
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 321/1500 [0m                      

                       Computation: 117430 steps/s (collection: 0.750s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 100.5700
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 15.8571
                       Mean reward: 209.61
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 0.3372
    Episode_Reward/rotating_object: 39.7780
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.84s
                      Time elapsed: 00:05:18
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 322/1500 [0m                      

                       Computation: 115656 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 83.1019
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.8617
                       Mean reward: 221.33
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 41.6583
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.85s
                      Time elapsed: 00:05:19
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 323/1500 [0m                      

                       Computation: 116349 steps/s (collection: 0.756s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 81.5474
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.8645
                       Mean reward: 212.85
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.3374
    Episode_Reward/rotating_object: 40.6465
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.84s
                      Time elapsed: 00:05:20
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 324/1500 [0m                      

                       Computation: 115569 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 91.6989
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8674
                       Mean reward: 191.89
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 39.3539
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.85s
                      Time elapsed: 00:05:21
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 325/1500 [0m                      

                       Computation: 113317 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 1.85
          Mean value_function loss: 91.3716
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8757
                       Mean reward: 213.46
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 0.3335
    Episode_Reward/rotating_object: 42.0704
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.87s
                      Time elapsed: 00:05:22
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 326/1500 [0m                      

                       Computation: 114187 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 96.2133
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8811
                       Mean reward: 218.08
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3286
    Episode_Reward/rotating_object: 45.0410
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.86s
                      Time elapsed: 00:05:23
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 327/1500 [0m                      

                       Computation: 114474 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 90.9482
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.8901
                       Mean reward: 189.23
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 36.6331
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.86s
                      Time elapsed: 00:05:23
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 328/1500 [0m                      

                       Computation: 113531 steps/s (collection: 0.778s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.4279
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8922
                       Mean reward: 202.71
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 0.3295
    Episode_Reward/rotating_object: 39.8136
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.87s
                      Time elapsed: 00:05:24
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 329/1500 [0m                      

                       Computation: 113432 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.0317
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8912
                       Mean reward: 192.51
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 41.9625
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.87s
                      Time elapsed: 00:05:25
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 330/1500 [0m                      

                       Computation: 114443 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.6568
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8900
                       Mean reward: 227.09
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 41.8009
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.86s
                      Time elapsed: 00:05:26
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 331/1500 [0m                      

                       Computation: 112869 steps/s (collection: 0.766s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 80.3823
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8914
                       Mean reward: 191.97
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 40.0332
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.87s
                      Time elapsed: 00:05:27
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 332/1500 [0m                      

                       Computation: 112665 steps/s (collection: 0.763s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 82.1345
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.8944
                       Mean reward: 261.95
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 43.7366
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.87s
                      Time elapsed: 00:05:28
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 333/1500 [0m                      

                       Computation: 48557 steps/s (collection: 1.928s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.4083
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 15.8956
                       Mean reward: 214.50
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 42.4619
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.02s
                      Time elapsed: 00:05:30
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 334/1500 [0m                      

                       Computation: 37346 steps/s (collection: 2.526s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.3839
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 15.8958
                       Mean reward: 216.01
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 41.6754
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.63s
                      Time elapsed: 00:05:32
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 335/1500 [0m                      

                       Computation: 35545 steps/s (collection: 2.651s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.9894
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 15.8963
                       Mean reward: 181.28
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 37.8917
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 2.77s
                      Time elapsed: 00:05:35
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 336/1500 [0m                      

                       Computation: 35029 steps/s (collection: 2.675s, learning 0.131s)
             Mean action noise std: 1.85
          Mean value_function loss: 79.6702
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.8965
                       Mean reward: 200.46
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 43.2584
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 2.81s
                      Time elapsed: 00:05:38
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 337/1500 [0m                      

                       Computation: 36177 steps/s (collection: 2.592s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 80.5068
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8979
                       Mean reward: 184.87
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 39.8540
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 2.72s
                      Time elapsed: 00:05:41
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 338/1500 [0m                      

                       Computation: 32942 steps/s (collection: 2.854s, learning 0.130s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.2579
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.8998
                       Mean reward: 217.95
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 44.1703
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 2.98s
                      Time elapsed: 00:05:44
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 339/1500 [0m                      

                       Computation: 35344 steps/s (collection: 2.656s, learning 0.126s)
             Mean action noise std: 1.85
          Mean value_function loss: 88.3816
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8960
                       Mean reward: 215.05
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 44.4367
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 2.78s
                      Time elapsed: 00:05:47
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 340/1500 [0m                      

                       Computation: 35680 steps/s (collection: 2.621s, learning 0.134s)
             Mean action noise std: 1.85
          Mean value_function loss: 90.0288
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.8912
                       Mean reward: 223.87
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3237
    Episode_Reward/rotating_object: 41.1951
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 2.76s
                      Time elapsed: 00:05:49
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 341/1500 [0m                      

                       Computation: 24238 steps/s (collection: 3.929s, learning 0.127s)
             Mean action noise std: 1.85
          Mean value_function loss: 106.4039
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8931
                       Mean reward: 214.33
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 44.3621
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.06s
                      Time elapsed: 00:05:53
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 342/1500 [0m                      

                       Computation: 111206 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 126.6060
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8950
                       Mean reward: 224.69
               Mean episode length: 232.97
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 43.8982
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.88s
                      Time elapsed: 00:05:54
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 343/1500 [0m                      

                       Computation: 117858 steps/s (collection: 0.749s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 110.5476
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8988
                       Mean reward: 206.38
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 42.5433
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.83s
                      Time elapsed: 00:05:55
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 344/1500 [0m                      

                       Computation: 113988 steps/s (collection: 0.761s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 108.2084
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9018
                       Mean reward: 232.37
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 49.4340
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.86s
                      Time elapsed: 00:05:56
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 345/1500 [0m                      

                       Computation: 116495 steps/s (collection: 0.752s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 106.9225
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9055
                       Mean reward: 269.20
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 0.3301
    Episode_Reward/rotating_object: 49.9855
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.84s
                      Time elapsed: 00:05:57
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 346/1500 [0m                      

                       Computation: 115118 steps/s (collection: 0.751s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 109.7465
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.9072
                       Mean reward: 203.28
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 47.2192
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.85s
                      Time elapsed: 00:05:58
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 347/1500 [0m                      

                       Computation: 115550 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 119.3632
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9093
                       Mean reward: 231.04
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 41.2326
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.85s
                      Time elapsed: 00:05:58
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 348/1500 [0m                      

                       Computation: 117825 steps/s (collection: 0.749s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 107.6339
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.9126
                       Mean reward: 236.74
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.3288
    Episode_Reward/rotating_object: 46.9836
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.83s
                      Time elapsed: 00:05:59
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 349/1500 [0m                      

                       Computation: 117163 steps/s (collection: 0.747s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 103.5263
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.9166
                       Mean reward: 247.31
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 0.3307
    Episode_Reward/rotating_object: 49.4440
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.84s
                      Time elapsed: 00:06:00
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 350/1500 [0m                      

                       Computation: 113893 steps/s (collection: 0.751s, learning 0.113s)
             Mean action noise std: 1.86
          Mean value_function loss: 115.9333
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.9208
                       Mean reward: 253.13
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 0.3318
    Episode_Reward/rotating_object: 48.2696
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.86s
                      Time elapsed: 00:06:01
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 351/1500 [0m                      

                       Computation: 113772 steps/s (collection: 0.759s, learning 0.105s)
             Mean action noise std: 1.87
          Mean value_function loss: 115.7256
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9244
                       Mean reward: 200.98
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 0.3335
    Episode_Reward/rotating_object: 49.3874
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.86s
                      Time elapsed: 00:06:02
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 352/1500 [0m                      

                       Computation: 118735 steps/s (collection: 0.731s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 100.5840
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 15.9263
                       Mean reward: 238.84
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.3303
    Episode_Reward/rotating_object: 46.5886
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.83s
                      Time elapsed: 00:06:03
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 353/1500 [0m                      

                       Computation: 113477 steps/s (collection: 0.755s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 102.5592
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9273
                       Mean reward: 207.94
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 0.3286
    Episode_Reward/rotating_object: 46.2375
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.87s
                      Time elapsed: 00:06:04
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 354/1500 [0m                      

                       Computation: 112135 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 1.87
          Mean value_function loss: 102.8666
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9262
                       Mean reward: 240.95
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 47.8121
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.88s
                      Time elapsed: 00:06:04
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 355/1500 [0m                      

                       Computation: 116458 steps/s (collection: 0.740s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 121.1004
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9253
                       Mean reward: 257.09
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 48.3216
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.84s
                      Time elapsed: 00:06:05
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 356/1500 [0m                      

                       Computation: 114099 steps/s (collection: 0.755s, learning 0.107s)
             Mean action noise std: 1.87
          Mean value_function loss: 115.3260
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.9271
                       Mean reward: 251.99
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 48.6768
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.86s
                      Time elapsed: 00:06:06
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 357/1500 [0m                      

                       Computation: 111935 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 1.87
          Mean value_function loss: 119.7298
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9318
                       Mean reward: 238.95
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 0.3378
    Episode_Reward/rotating_object: 50.5167
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.88s
                      Time elapsed: 00:06:07
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 358/1500 [0m                      

                       Computation: 117126 steps/s (collection: 0.751s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 119.3983
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9309
                       Mean reward: 262.13
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 0.3327
    Episode_Reward/rotating_object: 48.0142
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.84s
                      Time elapsed: 00:06:08
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 359/1500 [0m                      

                       Computation: 117604 steps/s (collection: 0.739s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 125.5992
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.9328
                       Mean reward: 243.41
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 46.0778
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.84s
                      Time elapsed: 00:06:09
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 360/1500 [0m                      

                       Computation: 118709 steps/s (collection: 0.743s, learning 0.086s)
             Mean action noise std: 1.87
          Mean value_function loss: 133.7328
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9397
                       Mean reward: 224.60
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 47.6836
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.83s
                      Time elapsed: 00:06:10
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 361/1500 [0m                      

                       Computation: 113311 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 156.5116
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9466
                       Mean reward: 205.97
               Mean episode length: 228.95
    Episode_Reward/reaching_object: 0.3353
    Episode_Reward/rotating_object: 48.5754
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.87s
                      Time elapsed: 00:06:10
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 362/1500 [0m                      

                       Computation: 117644 steps/s (collection: 0.751s, learning 0.085s)
             Mean action noise std: 1.87
          Mean value_function loss: 129.6291
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.9514
                       Mean reward: 239.65
               Mean episode length: 225.36
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 44.1331
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.84s
                      Time elapsed: 00:06:11
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 363/1500 [0m                      

                       Computation: 116019 steps/s (collection: 0.756s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.2044
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 15.9530
                       Mean reward: 218.99
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 0.3239
    Episode_Reward/rotating_object: 42.9287
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.85s
                      Time elapsed: 00:06:12
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 364/1500 [0m                      

                       Computation: 116433 steps/s (collection: 0.753s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 152.3503
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.9549
                       Mean reward: 241.23
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 46.5812
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.84s
                      Time elapsed: 00:06:13
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 365/1500 [0m                      

                       Computation: 112883 steps/s (collection: 0.767s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 148.9395
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9573
                       Mean reward: 246.57
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 46.1082
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.87s
                      Time elapsed: 00:06:14
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 366/1500 [0m                      

                       Computation: 117759 steps/s (collection: 0.748s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 122.4826
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.9597
                       Mean reward: 223.31
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 0.3227
    Episode_Reward/rotating_object: 46.5511
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.83s
                      Time elapsed: 00:06:15
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 367/1500 [0m                      

                       Computation: 114021 steps/s (collection: 0.776s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 119.3375
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9665
                       Mean reward: 193.78
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 45.4950
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.86s
                      Time elapsed: 00:06:15
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 368/1500 [0m                      

                       Computation: 114001 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 1.88
          Mean value_function loss: 113.5833
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9775
                       Mean reward: 232.54
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 0.3252
    Episode_Reward/rotating_object: 47.0091
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.86s
                      Time elapsed: 00:06:16
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 369/1500 [0m                      

                       Computation: 116195 steps/s (collection: 0.758s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 112.8762
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9796
                       Mean reward: 233.33
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 48.6765
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.85s
                      Time elapsed: 00:06:17
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 370/1500 [0m                      

                       Computation: 115882 steps/s (collection: 0.760s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 103.9045
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9826
                       Mean reward: 230.53
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 0.3221
    Episode_Reward/rotating_object: 46.0227
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.85s
                      Time elapsed: 00:06:18
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 371/1500 [0m                      

                       Computation: 117554 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 96.4039
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9860
                       Mean reward: 217.95
               Mean episode length: 232.37
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 46.9642
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.84s
                      Time elapsed: 00:06:19
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 372/1500 [0m                      

                       Computation: 118393 steps/s (collection: 0.736s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 105.7247
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.9892
                       Mean reward: 208.64
               Mean episode length: 222.25
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 46.3682
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.83s
                      Time elapsed: 00:06:20
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 373/1500 [0m                      

                       Computation: 114858 steps/s (collection: 0.770s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 104.9752
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9982
                       Mean reward: 257.34
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 48.6277
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.86s
                      Time elapsed: 00:06:21
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 374/1500 [0m                      

                       Computation: 113053 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 104.8526
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.0081
                       Mean reward: 270.32
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 50.0655
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.87s
                      Time elapsed: 00:06:21
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 375/1500 [0m                      

                       Computation: 114993 steps/s (collection: 0.769s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 109.1553
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.0133
                       Mean reward: 266.31
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 51.9148
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.85s
                      Time elapsed: 00:06:22
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 376/1500 [0m                      

                       Computation: 114507 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 111.5149
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.0176
                       Mean reward: 275.33
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 49.8264
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.86s
                      Time elapsed: 00:06:23
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 377/1500 [0m                      

                       Computation: 118744 steps/s (collection: 0.740s, learning 0.088s)
             Mean action noise std: 1.89
          Mean value_function loss: 114.3073
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.0226
                       Mean reward: 272.79
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 50.8195
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.83s
                      Time elapsed: 00:06:24
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 378/1500 [0m                      

                       Computation: 115931 steps/s (collection: 0.748s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 119.6129
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.0278
                       Mean reward: 269.21
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 49.3860
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.85s
                      Time elapsed: 00:06:25
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 379/1500 [0m                      

                       Computation: 117720 steps/s (collection: 0.744s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 116.4250
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.0356
                       Mean reward: 297.87
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.3184
    Episode_Reward/rotating_object: 51.9590
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.84s
                      Time elapsed: 00:06:26
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 380/1500 [0m                      

                       Computation: 116092 steps/s (collection: 0.754s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 125.2375
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.0351
                       Mean reward: 215.60
               Mean episode length: 216.81
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 47.6380
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.85s
                      Time elapsed: 00:06:26
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 381/1500 [0m                      

                       Computation: 111122 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 1.90
          Mean value_function loss: 125.1402
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0364
                       Mean reward: 270.31
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 53.1458
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.88s
                      Time elapsed: 00:06:27
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 382/1500 [0m                      

                       Computation: 111611 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 1.90
          Mean value_function loss: 105.7147
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 16.0380
                       Mean reward: 278.54
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 53.0338
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.88s
                      Time elapsed: 00:06:28
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 383/1500 [0m                      

                       Computation: 115152 steps/s (collection: 0.763s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 106.9289
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0392
                       Mean reward: 306.74
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 54.2081
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.85s
                      Time elapsed: 00:06:29
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 384/1500 [0m                      

                       Computation: 115928 steps/s (collection: 0.753s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 112.2272
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.0403
                       Mean reward: 233.49
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 43.7847
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.85s
                      Time elapsed: 00:06:30
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 385/1500 [0m                      

                       Computation: 117201 steps/s (collection: 0.741s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 109.0166
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.0383
                       Mean reward: 233.57
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 50.9176
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.84s
                      Time elapsed: 00:06:31
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 386/1500 [0m                      

                       Computation: 114734 steps/s (collection: 0.761s, learning 0.096s)
             Mean action noise std: 1.90
          Mean value_function loss: 114.2984
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0414
                       Mean reward: 301.51
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 56.5256
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.86s
                      Time elapsed: 00:06:32
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 387/1500 [0m                      

                       Computation: 112617 steps/s (collection: 0.760s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 123.6757
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0444
                       Mean reward: 213.51
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 0.3220
    Episode_Reward/rotating_object: 51.3391
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.87s
                      Time elapsed: 00:06:33
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 388/1500 [0m                      

                       Computation: 111852 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 117.5167
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.0439
                       Mean reward: 311.96
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 0.3380
    Episode_Reward/rotating_object: 60.9012
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.88s
                      Time elapsed: 00:06:33
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 389/1500 [0m                      

                       Computation: 112705 steps/s (collection: 0.754s, learning 0.118s)
             Mean action noise std: 1.90
          Mean value_function loss: 122.1185
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.0448
                       Mean reward: 287.56
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 55.1447
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.87s
                      Time elapsed: 00:06:34
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 390/1500 [0m                      

                       Computation: 113358 steps/s (collection: 0.753s, learning 0.115s)
             Mean action noise std: 1.90
          Mean value_function loss: 122.5489
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0500
                       Mean reward: 264.76
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 49.4822
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.87s
                      Time elapsed: 00:06:35
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 391/1500 [0m                      

                       Computation: 111850 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 1.90
          Mean value_function loss: 115.8362
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.0543
                       Mean reward: 293.16
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 51.7945
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.88s
                      Time elapsed: 00:06:36
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 392/1500 [0m                      

                       Computation: 113528 steps/s (collection: 0.763s, learning 0.103s)
             Mean action noise std: 1.90
          Mean value_function loss: 125.4994
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0486
                       Mean reward: 278.36
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 0.3310
    Episode_Reward/rotating_object: 57.3596
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.87s
                      Time elapsed: 00:06:37
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 393/1500 [0m                      

                       Computation: 113720 steps/s (collection: 0.757s, learning 0.108s)
             Mean action noise std: 1.90
          Mean value_function loss: 126.4686
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0500
                       Mean reward: 286.74
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 0.3315
    Episode_Reward/rotating_object: 53.5190
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.86s
                      Time elapsed: 00:06:38
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 394/1500 [0m                      

                       Computation: 115442 steps/s (collection: 0.763s, learning 0.088s)
             Mean action noise std: 1.90
          Mean value_function loss: 128.3910
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0559
                       Mean reward: 303.70
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 51.4944
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.85s
                      Time elapsed: 00:06:39
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 395/1500 [0m                      

                       Computation: 114783 steps/s (collection: 0.770s, learning 0.087s)
             Mean action noise std: 1.90
          Mean value_function loss: 121.5408
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.0568
                       Mean reward: 287.51
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 0.3329
    Episode_Reward/rotating_object: 58.0214
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.86s
                      Time elapsed: 00:06:39
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 396/1500 [0m                      

                       Computation: 111676 steps/s (collection: 0.792s, learning 0.088s)
             Mean action noise std: 1.91
          Mean value_function loss: 123.7243
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0579
                       Mean reward: 297.42
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.3370
    Episode_Reward/rotating_object: 55.7169
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.88s
                      Time elapsed: 00:06:40
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 397/1500 [0m                      

                       Computation: 114624 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 114.2640
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0630
                       Mean reward: 294.14
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.3349
    Episode_Reward/rotating_object: 55.5599
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.86s
                      Time elapsed: 00:06:41
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 398/1500 [0m                      

                       Computation: 114455 steps/s (collection: 0.772s, learning 0.087s)
             Mean action noise std: 1.91
          Mean value_function loss: 116.6109
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0664
                       Mean reward: 255.45
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.3310
    Episode_Reward/rotating_object: 51.5659
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.86s
                      Time elapsed: 00:06:42
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 399/1500 [0m                      

                       Computation: 111816 steps/s (collection: 0.776s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 111.0531
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.0731
                       Mean reward: 261.98
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 0.3429
    Episode_Reward/rotating_object: 56.5819
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.88s
                      Time elapsed: 00:06:43
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 400/1500 [0m                      

                       Computation: 116824 steps/s (collection: 0.739s, learning 0.102s)
             Mean action noise std: 1.91
          Mean value_function loss: 112.2233
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.0801
                       Mean reward: 270.20
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 0.3343
    Episode_Reward/rotating_object: 54.9363
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.84s
                      Time elapsed: 00:06:44
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 401/1500 [0m                      

                       Computation: 114971 steps/s (collection: 0.764s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 131.9231
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0834
                       Mean reward: 287.42
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 58.6177
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.86s
                      Time elapsed: 00:06:45
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 402/1500 [0m                      

                       Computation: 114352 steps/s (collection: 0.757s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 114.6349
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0868
                       Mean reward: 290.10
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 54.8873
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.86s
                      Time elapsed: 00:06:46
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 403/1500 [0m                      

                       Computation: 110105 steps/s (collection: 0.786s, learning 0.107s)
             Mean action noise std: 1.91
          Mean value_function loss: 112.6941
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0937
                       Mean reward: 267.41
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.3404
    Episode_Reward/rotating_object: 59.9470
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.89s
                      Time elapsed: 00:06:46
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 404/1500 [0m                      

                       Computation: 113931 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 110.6198
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0989
                       Mean reward: 250.51
               Mean episode length: 231.07
    Episode_Reward/reaching_object: 0.3404
    Episode_Reward/rotating_object: 60.4749
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.86s
                      Time elapsed: 00:06:47
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 405/1500 [0m                      

                       Computation: 111789 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 1.92
          Mean value_function loss: 106.6687
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1031
                       Mean reward: 265.52
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 54.1785
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.88s
                      Time elapsed: 00:06:48
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 406/1500 [0m                      

                       Computation: 118387 steps/s (collection: 0.741s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 108.6587
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1110
                       Mean reward: 255.83
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 0.3308
    Episode_Reward/rotating_object: 58.4431
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.83s
                      Time elapsed: 00:06:49
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 407/1500 [0m                      

                       Computation: 114273 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 105.3520
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.1107
                       Mean reward: 321.20
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 0.3358
    Episode_Reward/rotating_object: 55.6570
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.86s
                      Time elapsed: 00:06:50
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 408/1500 [0m                      

                       Computation: 111324 steps/s (collection: 0.773s, learning 0.110s)
             Mean action noise std: 1.92
          Mean value_function loss: 106.7974
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1112
                       Mean reward: 293.62
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 57.3716
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.88s
                      Time elapsed: 00:06:51
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 409/1500 [0m                      

                       Computation: 114163 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 107.8537
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1148
                       Mean reward: 319.81
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.3347
    Episode_Reward/rotating_object: 55.5167
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.86s
                      Time elapsed: 00:06:52
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 410/1500 [0m                      

                       Computation: 114394 steps/s (collection: 0.757s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 108.7952
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.1202
                       Mean reward: 274.84
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 55.2852
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.86s
                      Time elapsed: 00:06:52
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 411/1500 [0m                      

                       Computation: 114050 steps/s (collection: 0.775s, learning 0.087s)
             Mean action noise std: 1.92
          Mean value_function loss: 113.9621
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1205
                       Mean reward: 285.59
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.3366
    Episode_Reward/rotating_object: 58.3924
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.86s
                      Time elapsed: 00:06:53
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 412/1500 [0m                      

                       Computation: 111567 steps/s (collection: 0.786s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 127.9077
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1184
                       Mean reward: 291.59
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 50.6989
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.88s
                      Time elapsed: 00:06:54
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 413/1500 [0m                      

                       Computation: 112194 steps/s (collection: 0.781s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 123.8435
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.1174
                       Mean reward: 313.32
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.3309
    Episode_Reward/rotating_object: 58.3417
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.88s
                      Time elapsed: 00:06:55
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 414/1500 [0m                      

                       Computation: 115527 steps/s (collection: 0.758s, learning 0.093s)
             Mean action noise std: 1.92
          Mean value_function loss: 104.1222
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.1203
                       Mean reward: 323.86
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 54.5550
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.85s
                      Time elapsed: 00:06:56
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 415/1500 [0m                      

                       Computation: 113653 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 1.92
          Mean value_function loss: 106.7276
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.1236
                       Mean reward: 270.99
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.3294
    Episode_Reward/rotating_object: 55.6268
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.86s
                      Time elapsed: 00:06:57
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 416/1500 [0m                      

                       Computation: 113815 steps/s (collection: 0.769s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 106.3499
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1250
                       Mean reward: 267.14
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 49.0502
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.86s
                      Time elapsed: 00:06:58
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 417/1500 [0m                      

                       Computation: 118908 steps/s (collection: 0.739s, learning 0.088s)
             Mean action noise std: 1.93
          Mean value_function loss: 123.8114
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1261
                       Mean reward: 295.73
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 0.3325
    Episode_Reward/rotating_object: 56.9528
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.83s
                      Time elapsed: 00:06:58
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 418/1500 [0m                      

                       Computation: 114740 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 1.93
          Mean value_function loss: 124.2228
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1284
                       Mean reward: 273.42
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 0.3323
    Episode_Reward/rotating_object: 57.4949
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.86s
                      Time elapsed: 00:06:59
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 419/1500 [0m                      

                       Computation: 116795 steps/s (collection: 0.751s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 109.5547
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1296
                       Mean reward: 284.33
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 0.3328
    Episode_Reward/rotating_object: 56.9334
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.84s
                      Time elapsed: 00:07:00
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 420/1500 [0m                      

                       Computation: 115820 steps/s (collection: 0.761s, learning 0.088s)
             Mean action noise std: 1.93
          Mean value_function loss: 99.2400
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1312
                       Mean reward: 281.48
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 58.1235
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.85s
                      Time elapsed: 00:07:01
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 421/1500 [0m                      

                       Computation: 113425 steps/s (collection: 0.774s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 99.3780
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1306
                       Mean reward: 303.16
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 0.3405
    Episode_Reward/rotating_object: 64.5677
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.87s
                      Time elapsed: 00:07:02
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 422/1500 [0m                      

                       Computation: 117469 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 109.0296
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.1301
                       Mean reward: 342.13
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.3427
    Episode_Reward/rotating_object: 67.1234
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.84s
                      Time elapsed: 00:07:03
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 423/1500 [0m                      

                       Computation: 116028 steps/s (collection: 0.744s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 113.0746
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1306
                       Mean reward: 293.82
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 0.3387
    Episode_Reward/rotating_object: 59.8428
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.85s
                      Time elapsed: 00:07:04
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 424/1500 [0m                      

                       Computation: 114325 steps/s (collection: 0.752s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 98.2392
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1324
                       Mean reward: 284.80
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3341
    Episode_Reward/rotating_object: 59.0968
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.86s
                      Time elapsed: 00:07:04
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 425/1500 [0m                      

                       Computation: 112091 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 1.93
          Mean value_function loss: 108.2098
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.1353
                       Mean reward: 319.10
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.3404
    Episode_Reward/rotating_object: 60.6044
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.88s
                      Time elapsed: 00:07:05
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 426/1500 [0m                      

                       Computation: 113344 steps/s (collection: 0.770s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 106.3315
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1404
                       Mean reward: 318.40
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.3396
    Episode_Reward/rotating_object: 64.3344
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.87s
                      Time elapsed: 00:07:06
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 427/1500 [0m                      

                       Computation: 114127 steps/s (collection: 0.755s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 107.4326
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1450
                       Mean reward: 308.00
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.3424
    Episode_Reward/rotating_object: 57.8893
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.86s
                      Time elapsed: 00:07:07
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 428/1500 [0m                      

                       Computation: 115732 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 119.1728
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1474
                       Mean reward: 306.39
               Mean episode length: 230.70
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 59.2285
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.85s
                      Time elapsed: 00:07:08
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 429/1500 [0m                      

                       Computation: 115081 steps/s (collection: 0.746s, learning 0.108s)
             Mean action noise std: 1.94
          Mean value_function loss: 118.6080
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1531
                       Mean reward: 277.47
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.3378
    Episode_Reward/rotating_object: 58.8867
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.85s
                      Time elapsed: 00:07:09
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 430/1500 [0m                      

                       Computation: 118657 steps/s (collection: 0.743s, learning 0.086s)
             Mean action noise std: 1.94
          Mean value_function loss: 125.0856
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1547
                       Mean reward: 323.97
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 0.3432
    Episode_Reward/rotating_object: 61.1907
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.83s
                      Time elapsed: 00:07:10
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 431/1500 [0m                      

                       Computation: 115144 steps/s (collection: 0.767s, learning 0.087s)
             Mean action noise std: 1.94
          Mean value_function loss: 115.6651
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1572
                       Mean reward: 314.05
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.3349
    Episode_Reward/rotating_object: 61.2470
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.85s
                      Time elapsed: 00:07:10
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 432/1500 [0m                      

                       Computation: 113727 steps/s (collection: 0.775s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 127.7447
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1503
                       Mean reward: 254.32
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 59.6204
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.86s
                      Time elapsed: 00:07:11
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 433/1500 [0m                      

                       Computation: 114871 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 121.7536
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1461
                       Mean reward: 285.16
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 0.3335
    Episode_Reward/rotating_object: 60.9782
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.86s
                      Time elapsed: 00:07:12
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 434/1500 [0m                      

                       Computation: 112981 steps/s (collection: 0.772s, learning 0.098s)
             Mean action noise std: 1.94
          Mean value_function loss: 118.8028
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1412
                       Mean reward: 309.93
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.3409
    Episode_Reward/rotating_object: 62.5484
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.87s
                      Time elapsed: 00:07:13
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 435/1500 [0m                      

                       Computation: 114600 steps/s (collection: 0.768s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 112.3841
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.1391
                       Mean reward: 282.23
               Mean episode length: 226.73
    Episode_Reward/reaching_object: 0.3404
    Episode_Reward/rotating_object: 63.7099
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.86s
                      Time elapsed: 00:07:14
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 436/1500 [0m                      

                       Computation: 112930 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 1.94
          Mean value_function loss: 108.3876
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 16.1409
                       Mean reward: 281.50
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.3380
    Episode_Reward/rotating_object: 59.4202
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.87s
                      Time elapsed: 00:07:15
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 437/1500 [0m                      

                       Computation: 114261 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 1.94
          Mean value_function loss: 121.4615
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1451
                       Mean reward: 268.29
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 58.5212
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.86s
                      Time elapsed: 00:07:16
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 438/1500 [0m                      

                       Computation: 118751 steps/s (collection: 0.739s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 108.0314
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1538
                       Mean reward: 310.64
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3427
    Episode_Reward/rotating_object: 59.4186
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.83s
                      Time elapsed: 00:07:16
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 439/1500 [0m                      

                       Computation: 114976 steps/s (collection: 0.761s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 112.1341
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 16.1595
                       Mean reward: 311.23
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 0.3425
    Episode_Reward/rotating_object: 63.7064
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.85s
                      Time elapsed: 00:07:17
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 440/1500 [0m                      

                       Computation: 114526 steps/s (collection: 0.763s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 111.1203
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1602
                       Mean reward: 289.84
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.3446
    Episode_Reward/rotating_object: 64.8227
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.86s
                      Time elapsed: 00:07:18
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 441/1500 [0m                      

                       Computation: 117656 steps/s (collection: 0.741s, learning 0.095s)
             Mean action noise std: 1.94
          Mean value_function loss: 123.0389
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1635
                       Mean reward: 307.69
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 65.3174
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.84s
                      Time elapsed: 00:07:19
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 442/1500 [0m                      

                       Computation: 115980 steps/s (collection: 0.745s, learning 0.103s)
             Mean action noise std: 1.95
          Mean value_function loss: 117.7530
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.1643
                       Mean reward: 336.83
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 0.3307
    Episode_Reward/rotating_object: 57.9845
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.85s
                      Time elapsed: 00:07:20
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 443/1500 [0m                      

                       Computation: 116443 steps/s (collection: 0.749s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 113.9557
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1657
                       Mean reward: 261.30
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.3451
    Episode_Reward/rotating_object: 61.7679
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.84s
                      Time elapsed: 00:07:21
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 444/1500 [0m                      

                       Computation: 115637 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 104.6641
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1695
                       Mean reward: 346.23
               Mean episode length: 238.45
    Episode_Reward/reaching_object: 0.3440
    Episode_Reward/rotating_object: 64.6553
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.85s
                      Time elapsed: 00:07:22
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 445/1500 [0m                      

                       Computation: 114297 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 114.8875
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1694
                       Mean reward: 312.55
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.3333
    Episode_Reward/rotating_object: 59.3396
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.86s
                      Time elapsed: 00:07:22
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 446/1500 [0m                      

                       Computation: 113986 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 109.3737
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1705
                       Mean reward: 321.21
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 58.7658
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.86s
                      Time elapsed: 00:07:23
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 447/1500 [0m                      

                       Computation: 117002 steps/s (collection: 0.750s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 118.5579
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.1729
                       Mean reward: 345.92
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 60.0162
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.84s
                      Time elapsed: 00:07:24
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 448/1500 [0m                      

                       Computation: 116108 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 127.2454
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.1709
                       Mean reward: 277.87
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 0.3315
    Episode_Reward/rotating_object: 60.2011
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.85s
                      Time elapsed: 00:07:25
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 449/1500 [0m                      

                       Computation: 115800 steps/s (collection: 0.755s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 119.5269
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.1726
                       Mean reward: 339.40
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 0.3321
    Episode_Reward/rotating_object: 62.0469
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.85s
                      Time elapsed: 00:07:26
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 450/1500 [0m                      

                       Computation: 117332 steps/s (collection: 0.752s, learning 0.086s)
             Mean action noise std: 1.95
          Mean value_function loss: 121.0046
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1713
                       Mean reward: 243.97
               Mean episode length: 228.49
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 61.0698
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.84s
                      Time elapsed: 00:07:27
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 451/1500 [0m                      

                       Computation: 117914 steps/s (collection: 0.745s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 124.9532
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1709
                       Mean reward: 232.14
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.3390
    Episode_Reward/rotating_object: 58.7021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.83s
                      Time elapsed: 00:07:27
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 452/1500 [0m                      

                       Computation: 115255 steps/s (collection: 0.763s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 153.6828
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1719
                       Mean reward: 300.50
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 0.3328
    Episode_Reward/rotating_object: 57.2647
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.85s
                      Time elapsed: 00:07:28
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 453/1500 [0m                      

                       Computation: 117463 steps/s (collection: 0.744s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 130.3194
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1807
                       Mean reward: 297.18
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 0.3335
    Episode_Reward/rotating_object: 57.8351
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.84s
                      Time elapsed: 00:07:29
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 454/1500 [0m                      

                       Computation: 115610 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 132.2415
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1870
                       Mean reward: 276.73
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 59.3230
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.85s
                      Time elapsed: 00:07:30
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 455/1500 [0m                      

                       Computation: 112766 steps/s (collection: 0.779s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 124.0656
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.1920
                       Mean reward: 309.74
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 0.3310
    Episode_Reward/rotating_object: 60.8737
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.87s
                      Time elapsed: 00:07:31
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 456/1500 [0m                      

                       Computation: 113195 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 1.96
          Mean value_function loss: 134.6923
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1967
                       Mean reward: 288.47
               Mean episode length: 227.14
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 60.2785
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.87s
                      Time elapsed: 00:07:32
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 457/1500 [0m                      

                       Computation: 112100 steps/s (collection: 0.787s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 129.4877
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.1995
                       Mean reward: 296.13
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.3323
    Episode_Reward/rotating_object: 59.5112
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.88s
                      Time elapsed: 00:07:33
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 458/1500 [0m                      

                       Computation: 110226 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 119.5563
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2001
                       Mean reward: 321.38
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 62.7703
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.89s
                      Time elapsed: 00:07:33
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 459/1500 [0m                      

                       Computation: 111054 steps/s (collection: 0.781s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 124.4323
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2034
                       Mean reward: 331.36
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 0.3412
    Episode_Reward/rotating_object: 64.4322
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.89s
                      Time elapsed: 00:07:34
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 460/1500 [0m                      

                       Computation: 110231 steps/s (collection: 0.774s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 117.7948
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.2076
                       Mean reward: 365.46
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.3433
    Episode_Reward/rotating_object: 67.7056
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.89s
                      Time elapsed: 00:07:35
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 461/1500 [0m                      

                       Computation: 110602 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 1.96
          Mean value_function loss: 125.2683
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.2082
                       Mean reward: 361.59
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.3420
    Episode_Reward/rotating_object: 69.6303
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.89s
                      Time elapsed: 00:07:36
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 462/1500 [0m                      

                       Computation: 109737 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 114.3315
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 16.2072
                       Mean reward: 334.32
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.3418
    Episode_Reward/rotating_object: 65.4142
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.90s
                      Time elapsed: 00:07:37
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 463/1500 [0m                      

                       Computation: 110732 steps/s (collection: 0.783s, learning 0.105s)
             Mean action noise std: 1.97
          Mean value_function loss: 127.6578
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2173
                       Mean reward: 303.43
               Mean episode length: 237.22
    Episode_Reward/reaching_object: 0.3411
    Episode_Reward/rotating_object: 66.0951
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.89s
                      Time elapsed: 00:07:38
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 464/1500 [0m                      

                       Computation: 110276 steps/s (collection: 0.782s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 118.3964
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 16.2262
                       Mean reward: 279.01
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 59.8390
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.89s
                      Time elapsed: 00:07:39
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 465/1500 [0m                      

                       Computation: 110611 steps/s (collection: 0.785s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 116.8609
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.2273
                       Mean reward: 349.04
               Mean episode length: 231.53
    Episode_Reward/reaching_object: 0.3419
    Episode_Reward/rotating_object: 66.1750
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.89s
                      Time elapsed: 00:07:40
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 466/1500 [0m                      

                       Computation: 112271 steps/s (collection: 0.781s, learning 0.095s)
             Mean action noise std: 1.97
          Mean value_function loss: 113.5309
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2291
                       Mean reward: 335.95
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 0.3347
    Episode_Reward/rotating_object: 64.4973
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.88s
                      Time elapsed: 00:07:41
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 467/1500 [0m                      

                       Computation: 110145 steps/s (collection: 0.800s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 106.2349
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2310
                       Mean reward: 347.28
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.3349
    Episode_Reward/rotating_object: 64.1801
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.89s
                      Time elapsed: 00:07:41
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 468/1500 [0m                      

                       Computation: 111623 steps/s (collection: 0.764s, learning 0.116s)
             Mean action noise std: 1.97
          Mean value_function loss: 109.2337
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.2362
                       Mean reward: 309.61
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.3353
    Episode_Reward/rotating_object: 63.2165
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.88s
                      Time elapsed: 00:07:42
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 469/1500 [0m                      

                       Computation: 107924 steps/s (collection: 0.803s, learning 0.108s)
             Mean action noise std: 1.97
          Mean value_function loss: 107.0637
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2394
                       Mean reward: 319.23
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.3462
    Episode_Reward/rotating_object: 66.4839
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.91s
                      Time elapsed: 00:07:43
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 470/1500 [0m                      

                       Computation: 110110 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 1.98
          Mean value_function loss: 114.4140
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2461
                       Mean reward: 307.69
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 0.3403
    Episode_Reward/rotating_object: 63.6137
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.89s
                      Time elapsed: 00:07:44
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 471/1500 [0m                      

                       Computation: 110029 steps/s (collection: 0.794s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 122.8917
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.2523
                       Mean reward: 334.96
               Mean episode length: 235.21
    Episode_Reward/reaching_object: 0.3385
    Episode_Reward/rotating_object: 66.8522
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.89s
                      Time elapsed: 00:07:45
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 472/1500 [0m                      

                       Computation: 113314 steps/s (collection: 0.771s, learning 0.097s)
             Mean action noise std: 1.98
          Mean value_function loss: 118.5000
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2529
                       Mean reward: 319.61
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 0.3408
    Episode_Reward/rotating_object: 68.9407
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.87s
                      Time elapsed: 00:07:46
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 473/1500 [0m                      

                       Computation: 112906 steps/s (collection: 0.763s, learning 0.108s)
             Mean action noise std: 1.98
          Mean value_function loss: 100.8812
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2523
                       Mean reward: 330.62
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.3431
    Episode_Reward/rotating_object: 68.6327
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.87s
                      Time elapsed: 00:07:47
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 474/1500 [0m                      

                       Computation: 112323 steps/s (collection: 0.757s, learning 0.119s)
             Mean action noise std: 1.98
          Mean value_function loss: 116.9189
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2520
                       Mean reward: 382.32
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.3369
    Episode_Reward/rotating_object: 67.9769
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.88s
                      Time elapsed: 00:07:48
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 475/1500 [0m                      

                       Computation: 113194 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 114.7325
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2492
                       Mean reward: 325.99
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.3407
    Episode_Reward/rotating_object: 69.1063
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.87s
                      Time elapsed: 00:07:49
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 476/1500 [0m                      

                       Computation: 114781 steps/s (collection: 0.768s, learning 0.089s)
             Mean action noise std: 1.98
          Mean value_function loss: 115.7041
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.2505
                       Mean reward: 353.87
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.3437
    Episode_Reward/rotating_object: 67.7572
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.86s
                      Time elapsed: 00:07:49
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 477/1500 [0m                      

                       Computation: 114526 steps/s (collection: 0.766s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 118.7049
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.2574
                       Mean reward: 336.89
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 0.3450
    Episode_Reward/rotating_object: 68.7309
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.86s
                      Time elapsed: 00:07:50
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 478/1500 [0m                      

                       Computation: 112343 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 132.6898
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2661
                       Mean reward: 366.30
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 0.3430
    Episode_Reward/rotating_object: 69.5616
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.88s
                      Time elapsed: 00:07:51
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 479/1500 [0m                      

                       Computation: 112697 steps/s (collection: 0.782s, learning 0.090s)
             Mean action noise std: 1.99
          Mean value_function loss: 103.9869
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.2704
                       Mean reward: 374.10
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 0.3436
    Episode_Reward/rotating_object: 69.8628
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.87s
                      Time elapsed: 00:07:52
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 480/1500 [0m                      

                       Computation: 113209 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 108.6203
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.2736
                       Mean reward: 366.45
               Mean episode length: 238.40
    Episode_Reward/reaching_object: 0.3446
    Episode_Reward/rotating_object: 70.5861
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.87s
                      Time elapsed: 00:07:53
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 481/1500 [0m                      

                       Computation: 113732 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 119.0498
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.2744
                       Mean reward: 326.15
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.3458
    Episode_Reward/rotating_object: 69.9751
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.86s
                      Time elapsed: 00:07:54
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 482/1500 [0m                      

                       Computation: 114126 steps/s (collection: 0.764s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 117.2719
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.2725
                       Mean reward: 317.30
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.3396
    Episode_Reward/rotating_object: 65.7357
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.86s
                      Time elapsed: 00:07:55
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 483/1500 [0m                      

                       Computation: 112309 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 108.4966
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 16.2736
                       Mean reward: 328.77
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 0.3408
    Episode_Reward/rotating_object: 70.8929
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.88s
                      Time elapsed: 00:07:55
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 484/1500 [0m                      

                       Computation: 109914 steps/s (collection: 0.801s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 116.1415
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2760
                       Mean reward: 356.63
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 72.8134
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.89s
                      Time elapsed: 00:07:56
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 485/1500 [0m                      

                       Computation: 114464 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 1.99
          Mean value_function loss: 113.8386
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.2783
                       Mean reward: 346.91
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.3428
    Episode_Reward/rotating_object: 70.4615
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.86s
                      Time elapsed: 00:07:57
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 486/1500 [0m                      

                       Computation: 110347 steps/s (collection: 0.799s, learning 0.092s)
             Mean action noise std: 1.99
          Mean value_function loss: 124.2477
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2787
                       Mean reward: 403.61
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.3503
    Episode_Reward/rotating_object: 72.9100
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.89s
                      Time elapsed: 00:07:58
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 487/1500 [0m                      

                       Computation: 110590 steps/s (collection: 0.796s, learning 0.093s)
             Mean action noise std: 1.99
          Mean value_function loss: 111.5156
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2835
                       Mean reward: 332.38
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3453
    Episode_Reward/rotating_object: 66.9918
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.89s
                      Time elapsed: 00:07:59
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 488/1500 [0m                      

                       Computation: 109667 steps/s (collection: 0.791s, learning 0.106s)
             Mean action noise std: 1.99
          Mean value_function loss: 115.8265
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2877
                       Mean reward: 347.39
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 66.8773
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.90s
                      Time elapsed: 00:08:00
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 489/1500 [0m                      

                       Computation: 114257 steps/s (collection: 0.754s, learning 0.107s)
             Mean action noise std: 2.00
          Mean value_function loss: 109.1548
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.2978
                       Mean reward: 316.77
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 0.3466
    Episode_Reward/rotating_object: 71.7820
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.86s
                      Time elapsed: 00:08:01
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 490/1500 [0m                      

                       Computation: 115205 steps/s (collection: 0.753s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 112.4606
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.3018
                       Mean reward: 318.20
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 0.3453
    Episode_Reward/rotating_object: 71.8361
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.85s
                      Time elapsed: 00:08:02
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 491/1500 [0m                      

                       Computation: 113218 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 2.00
          Mean value_function loss: 111.6979
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.2992
                       Mean reward: 361.31
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.3477
    Episode_Reward/rotating_object: 71.6926
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.87s
                      Time elapsed: 00:08:02
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 492/1500 [0m                      

                       Computation: 113843 steps/s (collection: 0.770s, learning 0.093s)
             Mean action noise std: 2.00
          Mean value_function loss: 111.0963
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2965
                       Mean reward: 350.45
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3567
    Episode_Reward/rotating_object: 72.9020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.86s
                      Time elapsed: 00:08:03
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 493/1500 [0m                      

                       Computation: 114659 steps/s (collection: 0.763s, learning 0.095s)
             Mean action noise std: 2.00
          Mean value_function loss: 117.0598
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.2938
                       Mean reward: 343.74
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.3501
    Episode_Reward/rotating_object: 67.2918
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.86s
                      Time elapsed: 00:08:04
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 494/1500 [0m                      

                       Computation: 109847 steps/s (collection: 0.789s, learning 0.106s)
             Mean action noise std: 2.00
          Mean value_function loss: 115.0335
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.2973
                       Mean reward: 332.58
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.3409
    Episode_Reward/rotating_object: 67.6197
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.89s
                      Time elapsed: 00:08:05
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 495/1500 [0m                      

                       Computation: 112422 steps/s (collection: 0.759s, learning 0.116s)
             Mean action noise std: 2.00
          Mean value_function loss: 115.5618
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3044
                       Mean reward: 311.19
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 0.3460
    Episode_Reward/rotating_object: 68.0766
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.87s
                      Time elapsed: 00:08:06
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 496/1500 [0m                      

                       Computation: 111653 steps/s (collection: 0.760s, learning 0.120s)
             Mean action noise std: 2.00
          Mean value_function loss: 110.7430
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.3071
                       Mean reward: 367.02
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 0.3432
    Episode_Reward/rotating_object: 72.1788
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.88s
                      Time elapsed: 00:08:07
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 497/1500 [0m                      

                       Computation: 110964 steps/s (collection: 0.775s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 120.2438
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3134
                       Mean reward: 357.53
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3453
    Episode_Reward/rotating_object: 69.5449
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.89s
                      Time elapsed: 00:08:08
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 498/1500 [0m                      

                       Computation: 111919 steps/s (collection: 0.759s, learning 0.119s)
             Mean action noise std: 2.01
          Mean value_function loss: 113.6677
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.3251
                       Mean reward: 362.40
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 0.3431
    Episode_Reward/rotating_object: 73.2615
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.88s
                      Time elapsed: 00:08:09
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 499/1500 [0m                      

                       Computation: 109850 steps/s (collection: 0.782s, learning 0.113s)
             Mean action noise std: 2.01
          Mean value_function loss: 121.5309
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3313
                       Mean reward: 378.96
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 70.9993
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.89s
                      Time elapsed: 00:08:10
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 500/1500 [0m                      

                       Computation: 109674 steps/s (collection: 0.782s, learning 0.115s)
             Mean action noise std: 2.01
          Mean value_function loss: 119.6754
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3393
                       Mean reward: 324.60
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 0.3433
    Episode_Reward/rotating_object: 67.2950
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.90s
                      Time elapsed: 00:08:10
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 501/1500 [0m                      

                       Computation: 110591 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 2.01
          Mean value_function loss: 117.3516
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.3489
                       Mean reward: 367.82
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.3547
    Episode_Reward/rotating_object: 70.8841
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.89s
                      Time elapsed: 00:08:11
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 502/1500 [0m                      

                       Computation: 107920 steps/s (collection: 0.819s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 119.9399
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3541
                       Mean reward: 331.10
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 0.3483
    Episode_Reward/rotating_object: 67.8412
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.91s
                      Time elapsed: 00:08:12
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 503/1500 [0m                      

                       Computation: 107975 steps/s (collection: 0.815s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 113.2619
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 16.3550
                       Mean reward: 293.48
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 0.3389
    Episode_Reward/rotating_object: 68.0699
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.91s
                      Time elapsed: 00:08:13
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 504/1500 [0m                      

                       Computation: 108327 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 2.01
          Mean value_function loss: 122.8053
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3551
                       Mean reward: 348.89
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.3424
    Episode_Reward/rotating_object: 68.1618
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.91s
                      Time elapsed: 00:08:14
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 505/1500 [0m                      

                       Computation: 114128 steps/s (collection: 0.766s, learning 0.095s)
             Mean action noise std: 2.01
          Mean value_function loss: 118.7686
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.3527
                       Mean reward: 428.13
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.3438
    Episode_Reward/rotating_object: 71.8500
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.86s
                      Time elapsed: 00:08:15
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 506/1500 [0m                      

                       Computation: 112699 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 122.6621
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3547
                       Mean reward: 302.28
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.3391
    Episode_Reward/rotating_object: 70.2575
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.87s
                      Time elapsed: 00:08:16
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 507/1500 [0m                      

                       Computation: 111539 steps/s (collection: 0.789s, learning 0.093s)
             Mean action noise std: 2.01
          Mean value_function loss: 132.9393
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3559
                       Mean reward: 409.80
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 0.3460
    Episode_Reward/rotating_object: 70.9301
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.88s
                      Time elapsed: 00:08:17
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 508/1500 [0m                      

                       Computation: 111377 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 129.1620
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.3552
                       Mean reward: 368.95
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.3522
    Episode_Reward/rotating_object: 71.9287
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.88s
                      Time elapsed: 00:08:18
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 509/1500 [0m                      

                       Computation: 110270 steps/s (collection: 0.797s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 118.2099
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3545
                       Mean reward: 357.00
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 0.3340
    Episode_Reward/rotating_object: 67.7276
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.89s
                      Time elapsed: 00:08:18
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 510/1500 [0m                      

                       Computation: 109512 steps/s (collection: 0.797s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 126.7584
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3574
                       Mean reward: 311.89
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 0.3505
    Episode_Reward/rotating_object: 72.5049
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.90s
                      Time elapsed: 00:08:19
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 511/1500 [0m                      

                       Computation: 110210 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 2.02
          Mean value_function loss: 117.9473
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3607
                       Mean reward: 307.18
               Mean episode length: 225.72
    Episode_Reward/reaching_object: 0.3362
    Episode_Reward/rotating_object: 67.4290
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.89s
                      Time elapsed: 00:08:20
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 512/1500 [0m                      

                       Computation: 111144 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 124.2147
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3597
                       Mean reward: 360.30
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 0.3462
    Episode_Reward/rotating_object: 72.3976
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.88s
                      Time elapsed: 00:08:21
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 513/1500 [0m                      

                       Computation: 112497 steps/s (collection: 0.783s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 118.3906
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.3583
                       Mean reward: 365.58
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 75.8501
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.87s
                      Time elapsed: 00:08:22
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 514/1500 [0m                      

                       Computation: 111027 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 2.02
          Mean value_function loss: 115.6553
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3547
                       Mean reward: 423.83
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.3385
    Episode_Reward/rotating_object: 69.1666
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.89s
                      Time elapsed: 00:08:23
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 515/1500 [0m                      

                       Computation: 111581 steps/s (collection: 0.774s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 110.2823
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3572
                       Mean reward: 341.93
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 0.3435
    Episode_Reward/rotating_object: 74.2710
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.88s
                      Time elapsed: 00:08:24
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 516/1500 [0m                      

                       Computation: 115064 steps/s (collection: 0.760s, learning 0.095s)
             Mean action noise std: 2.02
          Mean value_function loss: 112.9596
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3622
                       Mean reward: 342.46
               Mean episode length: 235.07
    Episode_Reward/reaching_object: 0.3458
    Episode_Reward/rotating_object: 68.8261
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.85s
                      Time elapsed: 00:08:25
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 517/1500 [0m                      

                       Computation: 113384 steps/s (collection: 0.768s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 126.6313
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3648
                       Mean reward: 350.98
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 73.9371
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.87s
                      Time elapsed: 00:08:25
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 518/1500 [0m                      

                       Computation: 110924 steps/s (collection: 0.780s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 122.1591
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.3634
                       Mean reward: 365.79
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.3429
    Episode_Reward/rotating_object: 69.9354
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.89s
                      Time elapsed: 00:08:26
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 519/1500 [0m                      

                       Computation: 110107 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 2.02
          Mean value_function loss: 117.3850
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.3639
                       Mean reward: 340.39
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 0.3453
    Episode_Reward/rotating_object: 70.2045
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.89s
                      Time elapsed: 00:08:27
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 520/1500 [0m                      

                       Computation: 108858 steps/s (collection: 0.804s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 114.1135
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.3666
                       Mean reward: 371.34
               Mean episode length: 240.16
    Episode_Reward/reaching_object: 0.3438
    Episode_Reward/rotating_object: 68.3683
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.90s
                      Time elapsed: 00:08:28
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 521/1500 [0m                      

                       Computation: 109189 steps/s (collection: 0.807s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 117.3592
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3700
                       Mean reward: 340.70
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 73.0329
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.90s
                      Time elapsed: 00:08:29
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 522/1500 [0m                      

                       Computation: 111392 steps/s (collection: 0.777s, learning 0.106s)
             Mean action noise std: 2.03
          Mean value_function loss: 123.4477
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3721
                       Mean reward: 348.99
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 0.3405
    Episode_Reward/rotating_object: 71.4717
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.88s
                      Time elapsed: 00:08:30
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 523/1500 [0m                      

                       Computation: 109909 steps/s (collection: 0.785s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 109.4222
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.3753
                       Mean reward: 335.76
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 0.3366
    Episode_Reward/rotating_object: 68.8098
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.89s
                      Time elapsed: 00:08:31
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 524/1500 [0m                      

                       Computation: 111869 steps/s (collection: 0.771s, learning 0.108s)
             Mean action noise std: 2.03
          Mean value_function loss: 111.0140
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3754
                       Mean reward: 365.94
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.3499
    Episode_Reward/rotating_object: 72.1554
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.88s
                      Time elapsed: 00:08:32
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 525/1500 [0m                      

                       Computation: 115601 steps/s (collection: 0.759s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 123.6270
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3859
                       Mean reward: 399.02
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 0.3483
    Episode_Reward/rotating_object: 71.8596
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.85s
                      Time elapsed: 00:08:33
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 526/1500 [0m                      

                       Computation: 107550 steps/s (collection: 0.800s, learning 0.114s)
             Mean action noise std: 2.04
          Mean value_function loss: 114.1220
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3967
                       Mean reward: 367.77
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.3481
    Episode_Reward/rotating_object: 73.6724
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.91s
                      Time elapsed: 00:08:33
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 527/1500 [0m                      

                       Computation: 113418 steps/s (collection: 0.768s, learning 0.099s)
             Mean action noise std: 2.04
          Mean value_function loss: 124.0552
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.3995
                       Mean reward: 381.56
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 0.3397
    Episode_Reward/rotating_object: 74.3260
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.87s
                      Time elapsed: 00:08:34
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 528/1500 [0m                      

                       Computation: 111146 steps/s (collection: 0.787s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 118.6093
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3953
                       Mean reward: 364.59
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 0.3479
    Episode_Reward/rotating_object: 75.9795
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.88s
                      Time elapsed: 00:08:35
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 529/1500 [0m                      

                       Computation: 107338 steps/s (collection: 0.803s, learning 0.113s)
             Mean action noise std: 2.04
          Mean value_function loss: 124.4944
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.3966
                       Mean reward: 404.75
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.3433
    Episode_Reward/rotating_object: 74.2399
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.92s
                      Time elapsed: 00:08:36
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 530/1500 [0m                      

                       Computation: 107982 steps/s (collection: 0.800s, learning 0.111s)
             Mean action noise std: 2.04
          Mean value_function loss: 128.0514
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.4001
                       Mean reward: 383.60
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.3427
    Episode_Reward/rotating_object: 72.3075
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.91s
                      Time elapsed: 00:08:37
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 531/1500 [0m                      

                       Computation: 106676 steps/s (collection: 0.818s, learning 0.103s)
             Mean action noise std: 2.04
          Mean value_function loss: 127.5192
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4107
                       Mean reward: 358.44
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 0.3461
    Episode_Reward/rotating_object: 71.0636
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.92s
                      Time elapsed: 00:08:38
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 532/1500 [0m                      

                       Computation: 108062 steps/s (collection: 0.789s, learning 0.121s)
             Mean action noise std: 2.04
          Mean value_function loss: 125.3532
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4181
                       Mean reward: 396.99
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3422
    Episode_Reward/rotating_object: 73.3385
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.91s
                      Time elapsed: 00:08:39
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 533/1500 [0m                      

                       Computation: 113127 steps/s (collection: 0.751s, learning 0.118s)
             Mean action noise std: 2.04
          Mean value_function loss: 125.9329
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4169
                       Mean reward: 349.43
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 0.3506
    Episode_Reward/rotating_object: 74.4664
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.87s
                      Time elapsed: 00:08:40
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 534/1500 [0m                      

                       Computation: 108632 steps/s (collection: 0.779s, learning 0.126s)
             Mean action noise std: 2.04
          Mean value_function loss: 112.8035
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.4156
                       Mean reward: 395.50
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 0.3544
    Episode_Reward/rotating_object: 78.5037
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.90s
                      Time elapsed: 00:08:41
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 535/1500 [0m                      

                       Computation: 108894 steps/s (collection: 0.799s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 107.3100
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4211
                       Mean reward: 345.43
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 0.3496
    Episode_Reward/rotating_object: 73.7607
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.90s
                      Time elapsed: 00:08:42
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 536/1500 [0m                      

                       Computation: 109376 steps/s (collection: 0.803s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 113.9785
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4286
                       Mean reward: 371.98
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 0.3473
    Episode_Reward/rotating_object: 73.0133
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.90s
                      Time elapsed: 00:08:42
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 537/1500 [0m                      

                       Computation: 109960 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 2.05
          Mean value_function loss: 107.2930
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.4325
                       Mean reward: 343.72
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.3495
    Episode_Reward/rotating_object: 71.9439
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.89s
                      Time elapsed: 00:08:43
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 538/1500 [0m                      

                       Computation: 109861 steps/s (collection: 0.794s, learning 0.101s)
             Mean action noise std: 2.05
          Mean value_function loss: 107.7104
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4347
                       Mean reward: 388.68
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 0.3491
    Episode_Reward/rotating_object: 77.1010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.89s
                      Time elapsed: 00:08:44
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 539/1500 [0m                      

                       Computation: 113230 steps/s (collection: 0.775s, learning 0.094s)
             Mean action noise std: 2.05
          Mean value_function loss: 116.6940
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.4431
                       Mean reward: 343.79
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.3483
    Episode_Reward/rotating_object: 75.7187
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.87s
                      Time elapsed: 00:08:45
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 540/1500 [0m                      

                       Computation: 111725 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 2.06
          Mean value_function loss: 125.0833
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.4486
                       Mean reward: 358.33
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.3442
    Episode_Reward/rotating_object: 73.8575
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.88s
                      Time elapsed: 00:08:46
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 541/1500 [0m                      

                       Computation: 112753 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 2.06
          Mean value_function loss: 129.2047
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.4560
                       Mean reward: 399.85
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.3520
    Episode_Reward/rotating_object: 76.9166
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.87s
                      Time elapsed: 00:08:47
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 542/1500 [0m                      

                       Computation: 113417 steps/s (collection: 0.767s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 123.5634
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.4579
                       Mean reward: 347.98
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.3567
    Episode_Reward/rotating_object: 75.0645
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.87s
                      Time elapsed: 00:08:48
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 543/1500 [0m                      

                       Computation: 109977 steps/s (collection: 0.796s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 124.1524
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4612
                       Mean reward: 316.07
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.3493
    Episode_Reward/rotating_object: 74.4176
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.89s
                      Time elapsed: 00:08:49
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 544/1500 [0m                      

                       Computation: 113519 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 2.06
          Mean value_function loss: 126.4468
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4584
                       Mean reward: 309.07
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 0.3421
    Episode_Reward/rotating_object: 70.6542
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.87s
                      Time elapsed: 00:08:49
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 545/1500 [0m                      

                       Computation: 113627 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 119.5523
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.4586
                       Mean reward: 364.77
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 0.3499
    Episode_Reward/rotating_object: 74.8966
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.87s
                      Time elapsed: 00:08:50
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 546/1500 [0m                      

                       Computation: 112858 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 128.0719
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.4637
                       Mean reward: 381.81
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 0.3515
    Episode_Reward/rotating_object: 76.0468
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.87s
                      Time elapsed: 00:08:51
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 547/1500 [0m                      

                       Computation: 113056 steps/s (collection: 0.765s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 127.2526
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4647
                       Mean reward: 384.87
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.3531
    Episode_Reward/rotating_object: 76.8746
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.87s
                      Time elapsed: 00:08:52
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 548/1500 [0m                      

                       Computation: 110994 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 2.06
          Mean value_function loss: 115.9687
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.4641
                       Mean reward: 363.13
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.3561
    Episode_Reward/rotating_object: 74.0362
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.89s
                      Time elapsed: 00:08:53
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 549/1500 [0m                      

                       Computation: 108814 steps/s (collection: 0.779s, learning 0.125s)
             Mean action noise std: 2.07
          Mean value_function loss: 116.5571
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.4728
                       Mean reward: 432.05
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3543
    Episode_Reward/rotating_object: 74.8838
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.90s
                      Time elapsed: 00:08:54
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 550/1500 [0m                      

                       Computation: 111666 steps/s (collection: 0.772s, learning 0.108s)
             Mean action noise std: 2.07
          Mean value_function loss: 119.0220
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4851
                       Mean reward: 374.00
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.3561
    Episode_Reward/rotating_object: 77.9918
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.88s
                      Time elapsed: 00:08:55
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 551/1500 [0m                      

                       Computation: 108397 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 2.07
          Mean value_function loss: 114.6541
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4931
                       Mean reward: 409.75
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3530
    Episode_Reward/rotating_object: 78.7789
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.91s
                      Time elapsed: 00:08:56
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 552/1500 [0m                      

                       Computation: 109340 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 2.07
          Mean value_function loss: 116.8610
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.4980
                       Mean reward: 374.94
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.3520
    Episode_Reward/rotating_object: 77.5681
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.90s
                      Time elapsed: 00:08:57
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 553/1500 [0m                      

                       Computation: 109965 steps/s (collection: 0.793s, learning 0.101s)
             Mean action noise std: 2.07
          Mean value_function loss: 113.3467
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4940
                       Mean reward: 386.41
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 0.3598
    Episode_Reward/rotating_object: 77.3609
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.89s
                      Time elapsed: 00:08:57
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 554/1500 [0m                      

                       Computation: 109968 steps/s (collection: 0.785s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 129.2611
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4965
                       Mean reward: 424.44
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.3595
    Episode_Reward/rotating_object: 77.9623
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.89s
                      Time elapsed: 00:08:58
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 555/1500 [0m                      

                       Computation: 109179 steps/s (collection: 0.801s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 118.4015
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 16.4967
                       Mean reward: 365.44
               Mean episode length: 236.70
    Episode_Reward/reaching_object: 0.3552
    Episode_Reward/rotating_object: 77.8001
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.90s
                      Time elapsed: 00:08:59
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 556/1500 [0m                      

                       Computation: 110345 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 109.6958
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.5006
                       Mean reward: 418.73
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.3596
    Episode_Reward/rotating_object: 80.2522
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.89s
                      Time elapsed: 00:09:00
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 557/1500 [0m                      

                       Computation: 112618 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 2.08
          Mean value_function loss: 96.2785
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.5116
                       Mean reward: 368.97
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.3601
    Episode_Reward/rotating_object: 77.2338
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.87s
                      Time elapsed: 00:09:01
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 558/1500 [0m                      

                       Computation: 113604 steps/s (collection: 0.773s, learning 0.093s)
             Mean action noise std: 2.09
          Mean value_function loss: 116.2170
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5218
                       Mean reward: 426.71
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3559
    Episode_Reward/rotating_object: 77.3824
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.87s
                      Time elapsed: 00:09:02
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 559/1500 [0m                      

                       Computation: 110633 steps/s (collection: 0.789s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 124.1539
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.5283
                       Mean reward: 396.76
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 0.3485
    Episode_Reward/rotating_object: 74.8820
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.89s
                      Time elapsed: 00:09:03
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 560/1500 [0m                      

                       Computation: 112617 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 116.2842
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5370
                       Mean reward: 381.13
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.3543
    Episode_Reward/rotating_object: 78.0827
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.87s
                      Time elapsed: 00:09:04
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 561/1500 [0m                      

                       Computation: 112456 steps/s (collection: 0.781s, learning 0.094s)
             Mean action noise std: 2.09
          Mean value_function loss: 118.9578
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.5401
                       Mean reward: 385.39
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3603
    Episode_Reward/rotating_object: 82.7599
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.87s
                      Time elapsed: 00:09:05
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 562/1500 [0m                      

                       Computation: 108250 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 2.09
          Mean value_function loss: 113.0719
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.5402
                       Mean reward: 375.33
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.3593
    Episode_Reward/rotating_object: 76.8394
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.91s
                      Time elapsed: 00:09:05
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 563/1500 [0m                      

                       Computation: 108452 steps/s (collection: 0.799s, learning 0.107s)
             Mean action noise std: 2.09
          Mean value_function loss: 125.9302
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5421
                       Mean reward: 392.91
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.3606
    Episode_Reward/rotating_object: 77.9090
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.91s
                      Time elapsed: 00:09:06
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 564/1500 [0m                      

                       Computation: 111216 steps/s (collection: 0.777s, learning 0.107s)
             Mean action noise std: 2.10
          Mean value_function loss: 140.8038
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.5550
                       Mean reward: 427.99
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.3622
    Episode_Reward/rotating_object: 82.3492
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.88s
                      Time elapsed: 00:09:07
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 565/1500 [0m                      

                       Computation: 110928 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 2.10
          Mean value_function loss: 120.0637
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5664
                       Mean reward: 420.92
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 0.3473
    Episode_Reward/rotating_object: 79.0123
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.89s
                      Time elapsed: 00:09:08
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 566/1500 [0m                      

                       Computation: 107215 steps/s (collection: 0.808s, learning 0.109s)
             Mean action noise std: 2.10
          Mean value_function loss: 121.8020
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5788
                       Mean reward: 367.12
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 0.3601
    Episode_Reward/rotating_object: 79.2874
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.92s
                      Time elapsed: 00:09:09
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 567/1500 [0m                      

                       Computation: 107579 steps/s (collection: 0.792s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 113.9369
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.5899
                       Mean reward: 431.27
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 0.3536
    Episode_Reward/rotating_object: 78.9808
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.91s
                      Time elapsed: 00:09:10
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 568/1500 [0m                      

                       Computation: 110055 steps/s (collection: 0.776s, learning 0.118s)
             Mean action noise std: 2.11
          Mean value_function loss: 132.9431
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.5994
                       Mean reward: 366.79
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 0.3511
    Episode_Reward/rotating_object: 76.7533
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.89s
                      Time elapsed: 00:09:11
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 569/1500 [0m                      

                       Computation: 107103 steps/s (collection: 0.798s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 131.9602
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.6070
                       Mean reward: 445.71
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 0.3559
    Episode_Reward/rotating_object: 82.8379
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.92s
                      Time elapsed: 00:09:12
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 570/1500 [0m                      

                       Computation: 106639 steps/s (collection: 0.803s, learning 0.119s)
             Mean action noise std: 2.11
          Mean value_function loss: 127.3825
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.6145
                       Mean reward: 400.29
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.3614
    Episode_Reward/rotating_object: 81.3910
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.92s
                      Time elapsed: 00:09:13
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 571/1500 [0m                      

                       Computation: 108887 steps/s (collection: 0.802s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 131.8222
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6220
                       Mean reward: 380.17
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 0.3506
    Episode_Reward/rotating_object: 76.2818
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.90s
                      Time elapsed: 00:09:14
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 572/1500 [0m                      

                       Computation: 109860 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 141.3492
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6263
                       Mean reward: 358.49
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 0.3510
    Episode_Reward/rotating_object: 76.4597
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.89s
                      Time elapsed: 00:09:14
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 573/1500 [0m                      

                       Computation: 110678 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 2.12
          Mean value_function loss: 132.3573
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.6323
                       Mean reward: 365.56
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 0.3565
    Episode_Reward/rotating_object: 79.5909
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.89s
                      Time elapsed: 00:09:15
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 574/1500 [0m                      

                       Computation: 110744 steps/s (collection: 0.794s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 150.8857
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6337
                       Mean reward: 368.32
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 0.3506
    Episode_Reward/rotating_object: 77.2270
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.89s
                      Time elapsed: 00:09:16
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 575/1500 [0m                      

                       Computation: 109114 steps/s (collection: 0.795s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 142.1123
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6418
                       Mean reward: 401.48
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 0.3478
    Episode_Reward/rotating_object: 74.9806
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.90s
                      Time elapsed: 00:09:17
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 576/1500 [0m                      

                       Computation: 108029 steps/s (collection: 0.813s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 131.5633
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.6442
                       Mean reward: 368.70
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 0.3450
    Episode_Reward/rotating_object: 78.4818
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.91s
                      Time elapsed: 00:09:18
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 577/1500 [0m                      

                       Computation: 111194 steps/s (collection: 0.788s, learning 0.096s)
             Mean action noise std: 2.12
          Mean value_function loss: 135.8392
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6353
                       Mean reward: 394.45
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 0.3545
    Episode_Reward/rotating_object: 79.4689
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.88s
                      Time elapsed: 00:09:19
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 578/1500 [0m                      

                       Computation: 106660 steps/s (collection: 0.817s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 127.2525
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.6371
                       Mean reward: 408.63
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 0.3501
    Episode_Reward/rotating_object: 79.4662
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.92s
                      Time elapsed: 00:09:20
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 579/1500 [0m                      

                       Computation: 111487 steps/s (collection: 0.770s, learning 0.112s)
             Mean action noise std: 2.13
          Mean value_function loss: 126.3013
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6447
                       Mean reward: 420.91
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 0.3424
    Episode_Reward/rotating_object: 69.6659
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.88s
                      Time elapsed: 00:09:21
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 580/1500 [0m                      

                       Computation: 109975 steps/s (collection: 0.790s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 113.6694
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6418
                       Mean reward: 389.96
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 0.3462
    Episode_Reward/rotating_object: 76.5696
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.89s
                      Time elapsed: 00:09:22
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 581/1500 [0m                      

                       Computation: 112933 steps/s (collection: 0.767s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 120.4929
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.6422
                       Mean reward: 370.63
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 0.3537
    Episode_Reward/rotating_object: 79.6190
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.87s
                      Time elapsed: 00:09:22
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 582/1500 [0m                      

                       Computation: 106686 steps/s (collection: 0.814s, learning 0.108s)
             Mean action noise std: 2.13
          Mean value_function loss: 124.2877
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.6482
                       Mean reward: 406.96
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 0.3612
    Episode_Reward/rotating_object: 83.4439
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.92s
                      Time elapsed: 00:09:23
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 583/1500 [0m                      

                       Computation: 112539 steps/s (collection: 0.766s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 126.7492
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6512
                       Mean reward: 412.61
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.3502
    Episode_Reward/rotating_object: 76.4233
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.87s
                      Time elapsed: 00:09:24
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 584/1500 [0m                      

                       Computation: 111092 steps/s (collection: 0.788s, learning 0.097s)
             Mean action noise std: 2.13
          Mean value_function loss: 117.5154
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.6471
                       Mean reward: 396.34
               Mean episode length: 227.77
    Episode_Reward/reaching_object: 0.3531
    Episode_Reward/rotating_object: 80.2791
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.88s
                      Time elapsed: 00:09:25
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 585/1500 [0m                      

                       Computation: 99300 steps/s (collection: 0.804s, learning 0.186s)
             Mean action noise std: 2.13
          Mean value_function loss: 120.8374
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.6493
                       Mean reward: 453.93
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 0.3544
    Episode_Reward/rotating_object: 80.5276
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.99s
                      Time elapsed: 00:09:26
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 586/1500 [0m                      

                       Computation: 94542 steps/s (collection: 0.930s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 109.7522
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6572
                       Mean reward: 423.91
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.3601
    Episode_Reward/rotating_object: 83.2288
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.04s
                      Time elapsed: 00:09:27
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 587/1500 [0m                      

                       Computation: 101779 steps/s (collection: 0.829s, learning 0.136s)
             Mean action noise std: 2.14
          Mean value_function loss: 107.7329
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6683
                       Mean reward: 371.69
               Mean episode length: 231.17
    Episode_Reward/reaching_object: 0.3558
    Episode_Reward/rotating_object: 83.0699
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.97s
                      Time elapsed: 00:09:28
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 588/1500 [0m                      

                       Computation: 103465 steps/s (collection: 0.833s, learning 0.117s)
             Mean action noise std: 2.14
          Mean value_function loss: 116.7299
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.6777
                       Mean reward: 402.58
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 0.3632
    Episode_Reward/rotating_object: 84.0178
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.95s
                      Time elapsed: 00:09:29
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 589/1500 [0m                      

                       Computation: 102173 steps/s (collection: 0.852s, learning 0.111s)
             Mean action noise std: 2.14
          Mean value_function loss: 108.1842
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6904
                       Mean reward: 416.91
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.3585
    Episode_Reward/rotating_object: 84.5760
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.96s
                      Time elapsed: 00:09:30
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 590/1500 [0m                      

                       Computation: 112730 steps/s (collection: 0.777s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 121.7079
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.6966
                       Mean reward: 370.57
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 0.3590
    Episode_Reward/rotating_object: 82.7440
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.87s
                      Time elapsed: 00:09:31
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 591/1500 [0m                      

                       Computation: 107267 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 131.3497
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6921
                       Mean reward: 429.06
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.3608
    Episode_Reward/rotating_object: 82.4191
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.92s
                      Time elapsed: 00:09:32
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 592/1500 [0m                      

                       Computation: 100191 steps/s (collection: 0.829s, learning 0.152s)
             Mean action noise std: 2.15
          Mean value_function loss: 129.5590
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.6964
                       Mean reward: 424.71
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.3570
    Episode_Reward/rotating_object: 81.6047
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.98s
                      Time elapsed: 00:09:33
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 593/1500 [0m                      

                       Computation: 99466 steps/s (collection: 0.830s, learning 0.158s)
             Mean action noise std: 2.15
          Mean value_function loss: 127.3748
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.7052
                       Mean reward: 433.37
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.3587
    Episode_Reward/rotating_object: 84.4714
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.99s
                      Time elapsed: 00:09:34
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 594/1500 [0m                      

                       Computation: 111311 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 2.15
          Mean value_function loss: 134.4180
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.7050
                       Mean reward: 418.93
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 0.3569
    Episode_Reward/rotating_object: 78.9915
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.88s
                      Time elapsed: 00:09:35
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 595/1500 [0m                      

                       Computation: 97210 steps/s (collection: 0.803s, learning 0.209s)
             Mean action noise std: 2.15
          Mean value_function loss: 122.2491
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.7023
                       Mean reward: 409.61
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 0.3512
    Episode_Reward/rotating_object: 82.7262
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 1.01s
                      Time elapsed: 00:09:36
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 596/1500 [0m                      

                       Computation: 108474 steps/s (collection: 0.806s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 137.7013
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.7035
                       Mean reward: 366.56
               Mean episode length: 225.89
    Episode_Reward/reaching_object: 0.3508
    Episode_Reward/rotating_object: 75.0742
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.91s
                      Time elapsed: 00:09:37
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 597/1500 [0m                      

                       Computation: 106087 steps/s (collection: 0.803s, learning 0.124s)
             Mean action noise std: 2.16
          Mean value_function loss: 124.1378
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7106
                       Mean reward: 425.27
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.3609
    Episode_Reward/rotating_object: 79.9505
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.93s
                      Time elapsed: 00:09:38
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 598/1500 [0m                      

                       Computation: 104893 steps/s (collection: 0.836s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 121.4153
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.7188
                       Mean reward: 410.15
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 0.3575
    Episode_Reward/rotating_object: 82.1551
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.94s
                      Time elapsed: 00:09:39
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 599/1500 [0m                      

                       Computation: 108880 steps/s (collection: 0.794s, learning 0.109s)
             Mean action noise std: 2.16
          Mean value_function loss: 130.7737
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.7243
                       Mean reward: 399.45
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 0.3557
    Episode_Reward/rotating_object: 79.2188
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.90s
                      Time elapsed: 00:09:39
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 600/1500 [0m                      

                       Computation: 103323 steps/s (collection: 0.831s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 128.7422
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7196
                       Mean reward: 423.99
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 0.3567
    Episode_Reward/rotating_object: 82.8020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.95s
                      Time elapsed: 00:09:40
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 601/1500 [0m                      

                       Computation: 100276 steps/s (collection: 0.860s, learning 0.121s)
             Mean action noise std: 2.16
          Mean value_function loss: 132.8167
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7155
                       Mean reward: 441.19
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.3647
    Episode_Reward/rotating_object: 87.7585
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.98s
                      Time elapsed: 00:09:41
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 602/1500 [0m                      

                       Computation: 99855 steps/s (collection: 0.819s, learning 0.165s)
             Mean action noise std: 2.16
          Mean value_function loss: 118.4345
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.7165
                       Mean reward: 404.24
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.3640
    Episode_Reward/rotating_object: 80.3536
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.98s
                      Time elapsed: 00:09:42
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 603/1500 [0m                      

                       Computation: 98283 steps/s (collection: 0.850s, learning 0.151s)
             Mean action noise std: 2.16
          Mean value_function loss: 122.7727
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.7131
                       Mean reward: 401.77
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 0.3641
    Episode_Reward/rotating_object: 82.6484
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 1.00s
                      Time elapsed: 00:09:43
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 604/1500 [0m                      

                       Computation: 102201 steps/s (collection: 0.869s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 113.2158
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.7129
                       Mean reward: 466.10
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.3631
    Episode_Reward/rotating_object: 84.1711
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.96s
                      Time elapsed: 00:09:44
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 605/1500 [0m                      

                       Computation: 98387 steps/s (collection: 0.849s, learning 0.150s)
             Mean action noise std: 2.16
          Mean value_function loss: 123.2571
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7160
                       Mean reward: 423.44
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 0.3663
    Episode_Reward/rotating_object: 83.3137
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 1.00s
                      Time elapsed: 00:09:45
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 606/1500 [0m                      

                       Computation: 107450 steps/s (collection: 0.804s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 117.2302
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7192
                       Mean reward: 369.52
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 0.3637
    Episode_Reward/rotating_object: 84.3375
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.91s
                      Time elapsed: 00:09:46
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 607/1500 [0m                      

                       Computation: 106845 steps/s (collection: 0.822s, learning 0.098s)
             Mean action noise std: 2.17
          Mean value_function loss: 122.1268
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7185
                       Mean reward: 417.98
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.3627
    Episode_Reward/rotating_object: 81.3215
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.92s
                      Time elapsed: 00:09:47
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 608/1500 [0m                      

                       Computation: 102055 steps/s (collection: 0.836s, learning 0.127s)
             Mean action noise std: 2.17
          Mean value_function loss: 120.3420
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7151
                       Mean reward: 465.13
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3649
    Episode_Reward/rotating_object: 80.3829
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.96s
                      Time elapsed: 00:09:48
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 609/1500 [0m                      

                       Computation: 105443 steps/s (collection: 0.837s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 137.0223
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.7223
                       Mean reward: 380.70
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.3521
    Episode_Reward/rotating_object: 78.2212
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.93s
                      Time elapsed: 00:09:49
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 610/1500 [0m                      

                       Computation: 105386 steps/s (collection: 0.838s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 120.8235
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 16.7292
                       Mean reward: 416.74
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3635
    Episode_Reward/rotating_object: 78.6361
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.93s
                      Time elapsed: 00:09:50
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 611/1500 [0m                      

                       Computation: 107355 steps/s (collection: 0.812s, learning 0.104s)
             Mean action noise std: 2.17
          Mean value_function loss: 118.6778
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.7274
                       Mean reward: 409.43
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.3593
    Episode_Reward/rotating_object: 82.2941
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.92s
                      Time elapsed: 00:09:51
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 612/1500 [0m                      

                       Computation: 101631 steps/s (collection: 0.784s, learning 0.183s)
             Mean action noise std: 2.18
          Mean value_function loss: 112.5783
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.7306
                       Mean reward: 375.93
               Mean episode length: 236.70
    Episode_Reward/reaching_object: 0.3660
    Episode_Reward/rotating_object: 80.5893
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.97s
                      Time elapsed: 00:09:52
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 613/1500 [0m                      

                       Computation: 107347 steps/s (collection: 0.806s, learning 0.110s)
             Mean action noise std: 2.18
          Mean value_function loss: 111.7234
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.7299
                       Mean reward: 370.65
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 0.3665
    Episode_Reward/rotating_object: 75.0702
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.92s
                      Time elapsed: 00:09:53
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 614/1500 [0m                      

                       Computation: 102255 steps/s (collection: 0.795s, learning 0.167s)
             Mean action noise std: 2.18
          Mean value_function loss: 128.3957
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7373
                       Mean reward: 421.60
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.3632
    Episode_Reward/rotating_object: 80.8811
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.96s
                      Time elapsed: 00:09:54
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 615/1500 [0m                      

                       Computation: 105859 steps/s (collection: 0.835s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 121.2431
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7485
                       Mean reward: 471.02
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 0.3715
    Episode_Reward/rotating_object: 85.2067
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.93s
                      Time elapsed: 00:09:55
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 616/1500 [0m                      

                       Computation: 98165 steps/s (collection: 0.840s, learning 0.162s)
             Mean action noise std: 2.19
          Mean value_function loss: 128.2178
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.7493
                       Mean reward: 447.58
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3710
    Episode_Reward/rotating_object: 84.0125
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 1.00s
                      Time elapsed: 00:09:56
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 617/1500 [0m                      

                       Computation: 102922 steps/s (collection: 0.856s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 129.1806
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7520
                       Mean reward: 363.52
               Mean episode length: 219.20
    Episode_Reward/reaching_object: 0.3641
    Episode_Reward/rotating_object: 81.2295
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.96s
                      Time elapsed: 00:09:57
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 618/1500 [0m                      

                       Computation: 108528 steps/s (collection: 0.812s, learning 0.094s)
             Mean action noise std: 2.19
          Mean value_function loss: 130.8587
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7496
                       Mean reward: 420.83
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 0.3686
    Episode_Reward/rotating_object: 78.8670
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.91s
                      Time elapsed: 00:09:57
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 619/1500 [0m                      

                       Computation: 105342 steps/s (collection: 0.816s, learning 0.118s)
             Mean action noise std: 2.19
          Mean value_function loss: 128.0974
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7532
                       Mean reward: 447.46
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 0.3686
    Episode_Reward/rotating_object: 79.6351
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.93s
                      Time elapsed: 00:09:58
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 620/1500 [0m                      

                       Computation: 102339 steps/s (collection: 0.867s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 132.0198
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7554
                       Mean reward: 411.87
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.3673
    Episode_Reward/rotating_object: 79.6311
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.96s
                      Time elapsed: 00:09:59
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 621/1500 [0m                      

                       Computation: 108488 steps/s (collection: 0.812s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 129.1053
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7650
                       Mean reward: 418.98
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.3607
    Episode_Reward/rotating_object: 77.7867
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.91s
                      Time elapsed: 00:10:00
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 622/1500 [0m                      

                       Computation: 102342 steps/s (collection: 0.862s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 114.7937
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.7784
                       Mean reward: 413.72
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 0.3633
    Episode_Reward/rotating_object: 83.3537
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.96s
                      Time elapsed: 00:10:01
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 623/1500 [0m                      

                       Computation: 105914 steps/s (collection: 0.825s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 114.3268
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.7764
                       Mean reward: 442.33
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 82.6134
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.93s
                      Time elapsed: 00:10:02
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 624/1500 [0m                      

                       Computation: 108397 steps/s (collection: 0.790s, learning 0.117s)
             Mean action noise std: 2.20
          Mean value_function loss: 95.8563
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.7702
                       Mean reward: 456.22
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.3755
    Episode_Reward/rotating_object: 87.8340
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.91s
                      Time elapsed: 00:10:03
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 625/1500 [0m                      

                       Computation: 109712 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 2.20
          Mean value_function loss: 110.0454
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.7648
                       Mean reward: 410.69
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.3706
    Episode_Reward/rotating_object: 83.7626
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.90s
                      Time elapsed: 00:10:04
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 626/1500 [0m                      

                       Computation: 107002 steps/s (collection: 0.796s, learning 0.123s)
             Mean action noise std: 2.20
          Mean value_function loss: 101.8490
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7782
                       Mean reward: 434.91
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 87.6564
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.92s
                      Time elapsed: 00:10:05
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 627/1500 [0m                      

                       Computation: 110255 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 124.3950
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.7911
                       Mean reward: 459.40
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.3757
    Episode_Reward/rotating_object: 92.8272
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.89s
                      Time elapsed: 00:10:06
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 628/1500 [0m                      

                       Computation: 97884 steps/s (collection: 0.843s, learning 0.161s)
             Mean action noise std: 2.21
          Mean value_function loss: 117.0107
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7980
                       Mean reward: 413.32
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 0.3678
    Episode_Reward/rotating_object: 85.5750
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 1.00s
                      Time elapsed: 00:10:07
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 629/1500 [0m                      

                       Computation: 106845 steps/s (collection: 0.815s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 107.8486
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.8067
                       Mean reward: 463.17
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 0.3664
    Episode_Reward/rotating_object: 84.2945
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.92s
                      Time elapsed: 00:10:08
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 630/1500 [0m                      

                       Computation: 104291 steps/s (collection: 0.797s, learning 0.145s)
             Mean action noise std: 2.22
          Mean value_function loss: 144.2627
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8166
                       Mean reward: 395.82
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.3626
    Episode_Reward/rotating_object: 81.8619
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.94s
                      Time elapsed: 00:10:09
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 631/1500 [0m                      

                       Computation: 103342 steps/s (collection: 0.808s, learning 0.143s)
             Mean action noise std: 2.22
          Mean value_function loss: 119.3568
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.8186
                       Mean reward: 427.64
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.3701
    Episode_Reward/rotating_object: 84.3699
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.95s
                      Time elapsed: 00:10:10
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 632/1500 [0m                      

                       Computation: 102670 steps/s (collection: 0.818s, learning 0.140s)
             Mean action noise std: 2.22
          Mean value_function loss: 114.8949
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8120
                       Mean reward: 420.19
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3677
    Episode_Reward/rotating_object: 82.3905
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.96s
                      Time elapsed: 00:10:11
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 633/1500 [0m                      

                       Computation: 94810 steps/s (collection: 0.851s, learning 0.186s)
             Mean action noise std: 2.22
          Mean value_function loss: 105.6054
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.8109
                       Mean reward: 449.84
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.3712
    Episode_Reward/rotating_object: 82.5529
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 1.04s
                      Time elapsed: 00:10:12
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 634/1500 [0m                      

                       Computation: 95685 steps/s (collection: 0.852s, learning 0.176s)
             Mean action noise std: 2.22
          Mean value_function loss: 118.7136
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.8069
                       Mean reward: 419.53
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 88.4858
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 1.03s
                      Time elapsed: 00:10:13
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 635/1500 [0m                      

                       Computation: 100388 steps/s (collection: 0.875s, learning 0.105s)
             Mean action noise std: 2.22
          Mean value_function loss: 125.1271
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8118
                       Mean reward: 403.04
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 0.3642
    Episode_Reward/rotating_object: 80.9004
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.98s
                      Time elapsed: 00:10:14
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 636/1500 [0m                      

                       Computation: 106232 steps/s (collection: 0.804s, learning 0.121s)
             Mean action noise std: 2.22
          Mean value_function loss: 121.4603
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8162
                       Mean reward: 476.19
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.3718
    Episode_Reward/rotating_object: 89.5415
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.93s
                      Time elapsed: 00:10:15
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 637/1500 [0m                      

                       Computation: 105257 steps/s (collection: 0.826s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 126.6311
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.8275
                       Mean reward: 409.89
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 0.3638
    Episode_Reward/rotating_object: 82.9774
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.93s
                      Time elapsed: 00:10:15
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 638/1500 [0m                      

                       Computation: 106031 steps/s (collection: 0.829s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 120.7865
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8407
                       Mean reward: 470.77
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 0.3746
    Episode_Reward/rotating_object: 85.8328
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.93s
                      Time elapsed: 00:10:16
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 639/1500 [0m                      

                       Computation: 104786 steps/s (collection: 0.847s, learning 0.092s)
             Mean action noise std: 2.23
          Mean value_function loss: 110.1454
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.8524
                       Mean reward: 435.73
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 85.9287
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.94s
                      Time elapsed: 00:10:17
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 640/1500 [0m                      

                       Computation: 105646 steps/s (collection: 0.816s, learning 0.115s)
             Mean action noise std: 2.23
          Mean value_function loss: 112.4720
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8617
                       Mean reward: 440.76
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 88.4508
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.93s
                      Time elapsed: 00:10:18
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 641/1500 [0m                      

                       Computation: 108465 steps/s (collection: 0.804s, learning 0.103s)
             Mean action noise std: 2.23
          Mean value_function loss: 98.2679
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8693
                       Mean reward: 460.65
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.3780
    Episode_Reward/rotating_object: 90.5813
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.91s
                      Time elapsed: 00:10:19
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 642/1500 [0m                      

                       Computation: 108105 steps/s (collection: 0.807s, learning 0.102s)
             Mean action noise std: 2.24
          Mean value_function loss: 106.7347
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.8718
                       Mean reward: 456.98
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3725
    Episode_Reward/rotating_object: 86.5825
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.91s
                      Time elapsed: 00:10:20
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 643/1500 [0m                      

                       Computation: 106095 steps/s (collection: 0.824s, learning 0.103s)
             Mean action noise std: 2.24
          Mean value_function loss: 105.7651
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.8780
                       Mean reward: 459.73
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.3670
    Episode_Reward/rotating_object: 85.5111
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.93s
                      Time elapsed: 00:10:21
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 644/1500 [0m                      

                       Computation: 110896 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 112.6324
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.8815
                       Mean reward: 478.33
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 86.5372
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.89s
                      Time elapsed: 00:10:22
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 645/1500 [0m                      

                       Computation: 106133 steps/s (collection: 0.811s, learning 0.115s)
             Mean action noise std: 2.24
          Mean value_function loss: 118.3343
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.8822
                       Mean reward: 474.79
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.3652
    Episode_Reward/rotating_object: 89.6701
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.93s
                      Time elapsed: 00:10:23
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 646/1500 [0m                      

                       Computation: 105140 steps/s (collection: 0.788s, learning 0.147s)
             Mean action noise std: 2.24
          Mean value_function loss: 120.4493
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.8882
                       Mean reward: 413.51
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.3635
    Episode_Reward/rotating_object: 82.6093
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.93s
                      Time elapsed: 00:10:24
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 647/1500 [0m                      

                       Computation: 112674 steps/s (collection: 0.769s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 120.9656
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8909
                       Mean reward: 381.10
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 0.3666
    Episode_Reward/rotating_object: 83.1030
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.87s
                      Time elapsed: 00:10:25
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 648/1500 [0m                      

                       Computation: 103225 steps/s (collection: 0.787s, learning 0.166s)
             Mean action noise std: 2.24
          Mean value_function loss: 127.9973
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.8900
                       Mean reward: 426.98
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 86.9996
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.95s
                      Time elapsed: 00:10:26
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 649/1500 [0m                      

                       Computation: 108152 steps/s (collection: 0.778s, learning 0.131s)
             Mean action noise std: 2.25
          Mean value_function loss: 134.0288
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.8952
                       Mean reward: 397.51
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.3590
    Episode_Reward/rotating_object: 80.3665
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.91s
                      Time elapsed: 00:10:26
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 650/1500 [0m                      

                       Computation: 108133 steps/s (collection: 0.804s, learning 0.105s)
             Mean action noise std: 2.25
          Mean value_function loss: 128.0534
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.8965
                       Mean reward: 426.59
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.3661
    Episode_Reward/rotating_object: 83.6586
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.91s
                      Time elapsed: 00:10:27
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 651/1500 [0m                      

                       Computation: 110335 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 121.9519
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.8959
                       Mean reward: 443.66
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3696
    Episode_Reward/rotating_object: 87.3070
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.89s
                      Time elapsed: 00:10:28
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 652/1500 [0m                      

                       Computation: 107887 steps/s (collection: 0.819s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 120.9448
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.8978
                       Mean reward: 451.81
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 88.0826
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.91s
                      Time elapsed: 00:10:29
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 653/1500 [0m                      

                       Computation: 105420 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 130.0637
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.9091
                       Mean reward: 397.40
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 87.8142
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.93s
                      Time elapsed: 00:10:30
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 654/1500 [0m                      

                       Computation: 107817 steps/s (collection: 0.806s, learning 0.106s)
             Mean action noise std: 2.26
          Mean value_function loss: 120.1316
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.9224
                       Mean reward: 409.12
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 88.8761
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.91s
                      Time elapsed: 00:10:31
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 655/1500 [0m                      

                       Computation: 112968 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 2.26
          Mean value_function loss: 119.7057
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.9300
                       Mean reward: 421.63
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 0.3654
    Episode_Reward/rotating_object: 87.3434
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.87s
                      Time elapsed: 00:10:32
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 656/1500 [0m                      

                       Computation: 107864 steps/s (collection: 0.805s, learning 0.106s)
             Mean action noise std: 2.26
          Mean value_function loss: 118.1233
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9354
                       Mean reward: 426.77
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.3665
    Episode_Reward/rotating_object: 85.3568
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.91s
                      Time elapsed: 00:10:33
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 657/1500 [0m                      

                       Computation: 103977 steps/s (collection: 0.811s, learning 0.135s)
             Mean action noise std: 2.27
          Mean value_function loss: 116.5230
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.9477
                       Mean reward: 423.00
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 0.3689
    Episode_Reward/rotating_object: 87.3285
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.95s
                      Time elapsed: 00:10:34
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 658/1500 [0m                      

                       Computation: 104952 steps/s (collection: 0.786s, learning 0.151s)
             Mean action noise std: 2.27
          Mean value_function loss: 107.5475
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9491
                       Mean reward: 434.23
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 0.3664
    Episode_Reward/rotating_object: 85.7507
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.94s
                      Time elapsed: 00:10:35
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 659/1500 [0m                      

                       Computation: 106167 steps/s (collection: 0.779s, learning 0.147s)
             Mean action noise std: 2.27
          Mean value_function loss: 111.1782
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.9549
                       Mean reward: 432.93
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3736
    Episode_Reward/rotating_object: 90.2931
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.93s
                      Time elapsed: 00:10:36
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 660/1500 [0m                      

                       Computation: 108515 steps/s (collection: 0.771s, learning 0.135s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.0215
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.9610
                       Mean reward: 401.24
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 0.3663
    Episode_Reward/rotating_object: 86.5037
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.91s
                      Time elapsed: 00:10:37
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 661/1500 [0m                      

                       Computation: 111093 steps/s (collection: 0.791s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 106.7695
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.9649
                       Mean reward: 428.21
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 0.3713
    Episode_Reward/rotating_object: 84.6643
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.88s
                      Time elapsed: 00:10:37
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 662/1500 [0m                      

                       Computation: 110772 steps/s (collection: 0.792s, learning 0.095s)
             Mean action noise std: 2.27
          Mean value_function loss: 120.8287
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.9709
                       Mean reward: 447.88
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.3732
    Episode_Reward/rotating_object: 90.1828
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.89s
                      Time elapsed: 00:10:38
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 663/1500 [0m                      

                       Computation: 108289 steps/s (collection: 0.804s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 117.0095
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.9798
                       Mean reward: 407.90
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.3613
    Episode_Reward/rotating_object: 84.9277
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.91s
                      Time elapsed: 00:10:39
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 664/1500 [0m                      

                       Computation: 107286 steps/s (collection: 0.819s, learning 0.097s)
             Mean action noise std: 2.28
          Mean value_function loss: 126.4786
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.9933
                       Mean reward: 437.13
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 0.3648
    Episode_Reward/rotating_object: 87.3109
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.92s
                      Time elapsed: 00:10:40
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 665/1500 [0m                      

                       Computation: 108636 steps/s (collection: 0.799s, learning 0.106s)
             Mean action noise std: 2.29
          Mean value_function loss: 123.1664
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.0068
                       Mean reward: 444.35
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.3637
    Episode_Reward/rotating_object: 84.0226
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.90s
                      Time elapsed: 00:10:41
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 666/1500 [0m                      

                       Computation: 68123 steps/s (collection: 1.348s, learning 0.095s)
             Mean action noise std: 2.29
          Mean value_function loss: 134.3315
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0106
                       Mean reward: 465.89
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.3673
    Episode_Reward/rotating_object: 84.4199
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.44s
                      Time elapsed: 00:10:42
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 667/1500 [0m                      

                       Computation: 36928 steps/s (collection: 2.558s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 124.2968
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.0150
                       Mean reward: 439.27
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.3725
    Episode_Reward/rotating_object: 89.7683
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 2.66s
                      Time elapsed: 00:10:45
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 668/1500 [0m                      

                       Computation: 34031 steps/s (collection: 2.762s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 133.0848
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0133
                       Mean reward: 434.89
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.3716
    Episode_Reward/rotating_object: 87.1484
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 2.89s
                      Time elapsed: 00:10:48
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 669/1500 [0m                      

                       Computation: 35477 steps/s (collection: 2.644s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 117.9479
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.0179
                       Mean reward: 485.48
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 92.4354
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 2.77s
                      Time elapsed: 00:10:51
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 670/1500 [0m                      

                       Computation: 32796 steps/s (collection: 2.873s, learning 0.125s)
             Mean action noise std: 2.29
          Mean value_function loss: 123.0391
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.0145
                       Mean reward: 472.39
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.3684
    Episode_Reward/rotating_object: 87.7790
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.00s
                      Time elapsed: 00:10:54
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 671/1500 [0m                      

                       Computation: 34539 steps/s (collection: 2.721s, learning 0.126s)
             Mean action noise std: 2.29
          Mean value_function loss: 121.3820
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.0048
                       Mean reward: 417.47
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 85.1909
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 2.85s
                      Time elapsed: 00:10:57
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 672/1500 [0m                      

                       Computation: 33549 steps/s (collection: 2.809s, learning 0.122s)
             Mean action noise std: 2.29
          Mean value_function loss: 138.2932
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.9986
                       Mean reward: 460.51
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.3688
    Episode_Reward/rotating_object: 86.1521
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 2.93s
                      Time elapsed: 00:11:00
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 673/1500 [0m                      

                       Computation: 31620 steps/s (collection: 2.945s, learning 0.164s)
             Mean action noise std: 2.29
          Mean value_function loss: 129.7044
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.9982
                       Mean reward: 479.58
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 88.4473
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.11s
                      Time elapsed: 00:11:03
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 674/1500 [0m                      

                       Computation: 30638 steps/s (collection: 3.050s, learning 0.159s)
             Mean action noise std: 2.30
          Mean value_function loss: 135.5839
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.0024
                       Mean reward: 421.06
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 86.1495
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.21s
                      Time elapsed: 00:11:06
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 675/1500 [0m                      

                       Computation: 31207 steps/s (collection: 3.032s, learning 0.118s)
             Mean action noise std: 2.30
          Mean value_function loss: 114.1547
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0090
                       Mean reward: 510.28
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 91.4206
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.15s
                      Time elapsed: 00:11:09
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 676/1500 [0m                      

                       Computation: 103546 steps/s (collection: 0.845s, learning 0.105s)
             Mean action noise std: 2.30
          Mean value_function loss: 122.4127
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.0109
                       Mean reward: 460.09
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.3724
    Episode_Reward/rotating_object: 87.7401
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.95s
                      Time elapsed: 00:11:10
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 677/1500 [0m                      

                       Computation: 105735 steps/s (collection: 0.783s, learning 0.147s)
             Mean action noise std: 2.30
          Mean value_function loss: 125.2459
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.0198
                       Mean reward: 421.10
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 90.1910
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.93s
                      Time elapsed: 00:11:11
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 678/1500 [0m                      

                       Computation: 112162 steps/s (collection: 0.772s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 122.6812
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.0295
                       Mean reward: 446.36
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 85.6599
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.88s
                      Time elapsed: 00:11:12
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 679/1500 [0m                      

                       Computation: 108862 steps/s (collection: 0.777s, learning 0.126s)
             Mean action noise std: 2.31
          Mean value_function loss: 125.4909
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.0270
                       Mean reward: 420.71
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 0.3746
    Episode_Reward/rotating_object: 86.4813
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.90s
                      Time elapsed: 00:11:13
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 680/1500 [0m                      

                       Computation: 106354 steps/s (collection: 0.801s, learning 0.124s)
             Mean action noise std: 2.31
          Mean value_function loss: 125.6633
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.0250
                       Mean reward: 397.66
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 0.3725
    Episode_Reward/rotating_object: 85.2595
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.92s
                      Time elapsed: 00:11:14
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 681/1500 [0m                      

                       Computation: 110915 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 2.31
          Mean value_function loss: 120.0112
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.0290
                       Mean reward: 384.18
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.3718
    Episode_Reward/rotating_object: 82.4301
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.89s
                      Time elapsed: 00:11:15
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 682/1500 [0m                      

                       Computation: 106117 steps/s (collection: 0.825s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 138.0201
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.0305
                       Mean reward: 450.52
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.3757
    Episode_Reward/rotating_object: 91.3671
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.93s
                      Time elapsed: 00:11:15
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 683/1500 [0m                      

                       Computation: 109457 steps/s (collection: 0.794s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 132.9132
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0340
                       Mean reward: 415.79
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 88.7936
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.90s
                      Time elapsed: 00:11:16
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 684/1500 [0m                      

                       Computation: 105549 steps/s (collection: 0.818s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 134.1286
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0357
                       Mean reward: 409.24
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 0.3653
    Episode_Reward/rotating_object: 86.0001
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.93s
                      Time elapsed: 00:11:17
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 685/1500 [0m                      

                       Computation: 108849 steps/s (collection: 0.809s, learning 0.094s)
             Mean action noise std: 2.31
          Mean value_function loss: 114.5959
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0362
                       Mean reward: 446.70
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.3673
    Episode_Reward/rotating_object: 86.8919
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.90s
                      Time elapsed: 00:11:18
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 686/1500 [0m                      

                       Computation: 106083 steps/s (collection: 0.831s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 116.5187
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0415
                       Mean reward: 424.01
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 0.3750
    Episode_Reward/rotating_object: 88.2300
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.93s
                      Time elapsed: 00:11:19
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 687/1500 [0m                      

                       Computation: 110905 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 2.32
          Mean value_function loss: 112.4972
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.0535
                       Mean reward: 423.04
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 0.3681
    Episode_Reward/rotating_object: 86.3638
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.89s
                      Time elapsed: 00:11:20
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 688/1500 [0m                      

                       Computation: 108206 steps/s (collection: 0.790s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 111.0670
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.0590
                       Mean reward: 485.81
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 93.7301
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.91s
                      Time elapsed: 00:11:21
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 689/1500 [0m                      

                       Computation: 105315 steps/s (collection: 0.783s, learning 0.150s)
             Mean action noise std: 2.32
          Mean value_function loss: 113.4829
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.0659
                       Mean reward: 471.58
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3705
    Episode_Reward/rotating_object: 88.6362
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.93s
                      Time elapsed: 00:11:22
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 690/1500 [0m                      

                       Computation: 109050 steps/s (collection: 0.776s, learning 0.125s)
             Mean action noise std: 2.32
          Mean value_function loss: 108.2876
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0658
                       Mean reward: 457.40
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.3690
    Episode_Reward/rotating_object: 88.0975
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.90s
                      Time elapsed: 00:11:23
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 691/1500 [0m                      

                       Computation: 114800 steps/s (collection: 0.760s, learning 0.097s)
             Mean action noise std: 2.33
          Mean value_function loss: 103.0128
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.0686
                       Mean reward: 439.77
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 91.5383
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.86s
                      Time elapsed: 00:11:24
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 692/1500 [0m                      

                       Computation: 108090 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 2.33
          Mean value_function loss: 111.5211
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.0691
                       Mean reward: 466.12
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.3695
    Episode_Reward/rotating_object: 89.3001
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.91s
                      Time elapsed: 00:11:25
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 693/1500 [0m                      

                       Computation: 109981 steps/s (collection: 0.803s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 105.1758
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.0741
                       Mean reward: 438.46
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 91.5074
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.89s
                      Time elapsed: 00:11:25
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 694/1500 [0m                      

                       Computation: 109585 steps/s (collection: 0.808s, learning 0.089s)
             Mean action noise std: 2.33
          Mean value_function loss: 117.6459
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0724
                       Mean reward: 462.26
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3706
    Episode_Reward/rotating_object: 89.6599
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.90s
                      Time elapsed: 00:11:26
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 695/1500 [0m                      

                       Computation: 112958 steps/s (collection: 0.772s, learning 0.099s)
             Mean action noise std: 2.33
          Mean value_function loss: 121.6424
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.0714
                       Mean reward: 425.59
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.3691
    Episode_Reward/rotating_object: 87.4945
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.87s
                      Time elapsed: 00:11:27
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 696/1500 [0m                      

                       Computation: 113436 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 116.7380
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0750
                       Mean reward: 431.45
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 87.3365
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.87s
                      Time elapsed: 00:11:28
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 697/1500 [0m                      

                       Computation: 111842 steps/s (collection: 0.764s, learning 0.115s)
             Mean action noise std: 2.33
          Mean value_function loss: 122.5254
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.0811
                       Mean reward: 436.17
               Mean episode length: 238.45
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 92.2018
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.88s
                      Time elapsed: 00:11:29
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 698/1500 [0m                      

                       Computation: 106106 steps/s (collection: 0.784s, learning 0.143s)
             Mean action noise std: 2.34
          Mean value_function loss: 110.4349
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.0882
                       Mean reward: 440.80
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3768
    Episode_Reward/rotating_object: 91.9832
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.93s
                      Time elapsed: 00:11:30
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 699/1500 [0m                      

                       Computation: 107347 steps/s (collection: 0.788s, learning 0.128s)
             Mean action noise std: 2.34
          Mean value_function loss: 130.0309
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0898
                       Mean reward: 473.97
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.3816
    Episode_Reward/rotating_object: 93.5424
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.92s
                      Time elapsed: 00:11:31
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 700/1500 [0m                      

                       Computation: 109891 steps/s (collection: 0.769s, learning 0.125s)
             Mean action noise std: 2.34
          Mean value_function loss: 121.6945
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0931
                       Mean reward: 417.18
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 0.3809
    Episode_Reward/rotating_object: 90.1281
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.89s
                      Time elapsed: 00:11:32
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 701/1500 [0m                      

                       Computation: 111082 steps/s (collection: 0.786s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 113.1739
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0971
                       Mean reward: 505.46
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.3795
    Episode_Reward/rotating_object: 91.8832
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.88s
                      Time elapsed: 00:11:33
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 702/1500 [0m                      

                       Computation: 107077 steps/s (collection: 0.809s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 116.3744
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.0956
                       Mean reward: 495.13
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.3765
    Episode_Reward/rotating_object: 91.8535
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.92s
                      Time elapsed: 00:11:33
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 703/1500 [0m                      

                       Computation: 106884 steps/s (collection: 0.816s, learning 0.104s)
             Mean action noise std: 2.34
          Mean value_function loss: 119.0697
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.0936
                       Mean reward: 447.14
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.3790
    Episode_Reward/rotating_object: 92.8663
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.92s
                      Time elapsed: 00:11:34
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 704/1500 [0m                      

                       Computation: 109913 steps/s (collection: 0.801s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 117.2673
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.0980
                       Mean reward: 448.86
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.3802
    Episode_Reward/rotating_object: 92.4369
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.89s
                      Time elapsed: 00:11:35
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 705/1500 [0m                      

                       Computation: 114064 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 113.0263
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1074
                       Mean reward: 440.83
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.3754
    Episode_Reward/rotating_object: 84.9173
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.86s
                      Time elapsed: 00:11:36
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 706/1500 [0m                      

                       Computation: 110127 steps/s (collection: 0.783s, learning 0.110s)
             Mean action noise std: 2.35
          Mean value_function loss: 111.6118
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.1186
                       Mean reward: 504.87
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.3737
    Episode_Reward/rotating_object: 93.5997
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.89s
                      Time elapsed: 00:11:37
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 707/1500 [0m                      

                       Computation: 116300 steps/s (collection: 0.747s, learning 0.099s)
             Mean action noise std: 2.36
          Mean value_function loss: 108.8318
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.1287
                       Mean reward: 450.34
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 0.3705
    Episode_Reward/rotating_object: 86.4592
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.85s
                      Time elapsed: 00:11:38
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 708/1500 [0m                      

                       Computation: 112772 steps/s (collection: 0.748s, learning 0.124s)
             Mean action noise std: 2.36
          Mean value_function loss: 104.1033
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.1350
                       Mean reward: 463.84
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.3750
    Episode_Reward/rotating_object: 87.8876
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.87s
                      Time elapsed: 00:11:39
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 709/1500 [0m                      

                       Computation: 111780 steps/s (collection: 0.789s, learning 0.090s)
             Mean action noise std: 2.36
          Mean value_function loss: 101.1381
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.1440
                       Mean reward: 466.80
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3773
    Episode_Reward/rotating_object: 93.8203
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.88s
                      Time elapsed: 00:11:40
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 710/1500 [0m                      

                       Computation: 117494 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 2.36
          Mean value_function loss: 107.3356
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1543
                       Mean reward: 453.58
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.3822
    Episode_Reward/rotating_object: 92.7049
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.84s
                      Time elapsed: 00:11:40
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 711/1500 [0m                      

                       Computation: 110139 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 2.37
          Mean value_function loss: 99.2557
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1644
                       Mean reward: 470.89
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 94.1680
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.89s
                      Time elapsed: 00:11:41
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 712/1500 [0m                      

                       Computation: 116877 steps/s (collection: 0.748s, learning 0.093s)
             Mean action noise std: 2.37
          Mean value_function loss: 99.5091
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1674
                       Mean reward: 488.67
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.3809
    Episode_Reward/rotating_object: 96.0054
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.84s
                      Time elapsed: 00:11:42
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 713/1500 [0m                      

                       Computation: 112365 steps/s (collection: 0.771s, learning 0.104s)
             Mean action noise std: 2.37
          Mean value_function loss: 103.9412
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.1678
                       Mean reward: 442.95
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.3789
    Episode_Reward/rotating_object: 93.0852
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.87s
                      Time elapsed: 00:11:43
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 714/1500 [0m                      

                       Computation: 116587 steps/s (collection: 0.745s, learning 0.098s)
             Mean action noise std: 2.37
          Mean value_function loss: 106.6263
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.1702
                       Mean reward: 477.30
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3783
    Episode_Reward/rotating_object: 91.2657
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.84s
                      Time elapsed: 00:11:44
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 715/1500 [0m                      

                       Computation: 112798 steps/s (collection: 0.762s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 102.7206
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1890
                       Mean reward: 415.64
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.3708
    Episode_Reward/rotating_object: 90.9211
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.87s
                      Time elapsed: 00:11:45
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 716/1500 [0m                      

                       Computation: 99159 steps/s (collection: 0.777s, learning 0.214s)
             Mean action noise std: 2.38
          Mean value_function loss: 110.2447
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.2021
                       Mean reward: 492.29
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.3746
    Episode_Reward/rotating_object: 91.7220
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.99s
                      Time elapsed: 00:11:46
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 717/1500 [0m                      

                       Computation: 89474 steps/s (collection: 0.881s, learning 0.217s)
             Mean action noise std: 2.38
          Mean value_function loss: 99.3970
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2045
                       Mean reward: 453.53
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.3671
    Episode_Reward/rotating_object: 93.5197
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.10s
                      Time elapsed: 00:11:47
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 718/1500 [0m                      

                       Computation: 111843 steps/s (collection: 0.776s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 94.6048
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2145
                       Mean reward: 398.79
               Mean episode length: 227.92
    Episode_Reward/reaching_object: 0.3721
    Episode_Reward/rotating_object: 90.1693
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.88s
                      Time elapsed: 00:11:48
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 719/1500 [0m                      

                       Computation: 115659 steps/s (collection: 0.760s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 101.5952
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2235
                       Mean reward: 465.04
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.3793
    Episode_Reward/rotating_object: 92.9846
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.85s
                      Time elapsed: 00:11:49
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 720/1500 [0m                      

                       Computation: 114316 steps/s (collection: 0.750s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 104.9297
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.2285
                       Mean reward: 474.18
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 86.1196
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.86s
                      Time elapsed: 00:11:49
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 721/1500 [0m                      

                       Computation: 112036 steps/s (collection: 0.773s, learning 0.104s)
             Mean action noise std: 2.40
          Mean value_function loss: 109.2990
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.2400
                       Mean reward: 427.13
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.3777
    Episode_Reward/rotating_object: 91.3738
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.88s
                      Time elapsed: 00:11:50
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 722/1500 [0m                      

                       Computation: 118018 steps/s (collection: 0.741s, learning 0.092s)
             Mean action noise std: 2.40
          Mean value_function loss: 97.9716
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.2458
                       Mean reward: 475.82
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 0.3726
    Episode_Reward/rotating_object: 91.1633
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.83s
                      Time elapsed: 00:11:51
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 723/1500 [0m                      

                       Computation: 112850 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 2.40
          Mean value_function loss: 110.0642
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.2487
                       Mean reward: 458.39
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3750
    Episode_Reward/rotating_object: 90.3925
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.87s
                      Time elapsed: 00:11:52
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 724/1500 [0m                      

                       Computation: 115374 steps/s (collection: 0.766s, learning 0.086s)
             Mean action noise std: 2.40
          Mean value_function loss: 116.8226
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2505
                       Mean reward: 482.97
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3789
    Episode_Reward/rotating_object: 93.7302
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.85s
                      Time elapsed: 00:11:53
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 725/1500 [0m                      

                       Computation: 107987 steps/s (collection: 0.788s, learning 0.123s)
             Mean action noise std: 2.40
          Mean value_function loss: 103.1448
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2554
                       Mean reward: 454.81
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.3771
    Episode_Reward/rotating_object: 87.9016
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.91s
                      Time elapsed: 00:11:54
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 726/1500 [0m                      

                       Computation: 101375 steps/s (collection: 0.802s, learning 0.168s)
             Mean action noise std: 2.40
          Mean value_function loss: 107.7972
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2664
                       Mean reward: 477.26
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.3775
    Episode_Reward/rotating_object: 95.8536
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.97s
                      Time elapsed: 00:11:55
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 727/1500 [0m                      

                       Computation: 111723 steps/s (collection: 0.756s, learning 0.124s)
             Mean action noise std: 2.40
          Mean value_function loss: 109.3013
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.2665
                       Mean reward: 452.37
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 90.3190
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.88s
                      Time elapsed: 00:11:56
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 728/1500 [0m                      

                       Computation: 108886 steps/s (collection: 0.792s, learning 0.111s)
             Mean action noise std: 2.41
          Mean value_function loss: 113.2080
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.2646
                       Mean reward: 473.77
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3757
    Episode_Reward/rotating_object: 92.7969
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.90s
                      Time elapsed: 00:11:57
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 729/1500 [0m                      

                       Computation: 115529 steps/s (collection: 0.742s, learning 0.109s)
             Mean action noise std: 2.41
          Mean value_function loss: 104.1363
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 17.2661
                       Mean reward: 476.07
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 0.3757
    Episode_Reward/rotating_object: 88.7924
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.85s
                      Time elapsed: 00:11:57
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 730/1500 [0m                      

                       Computation: 114587 steps/s (collection: 0.753s, learning 0.105s)
             Mean action noise std: 2.41
          Mean value_function loss: 124.3012
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.2698
                       Mean reward: 449.19
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.3738
    Episode_Reward/rotating_object: 87.1013
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.86s
                      Time elapsed: 00:11:58
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 731/1500 [0m                      

                       Computation: 108737 steps/s (collection: 0.813s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 128.6813
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.2647
                       Mean reward: 447.32
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 0.3750
    Episode_Reward/rotating_object: 94.0612
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.90s
                      Time elapsed: 00:11:59
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 732/1500 [0m                      

                       Computation: 116857 steps/s (collection: 0.754s, learning 0.087s)
             Mean action noise std: 2.41
          Mean value_function loss: 134.2280
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.2602
                       Mean reward: 447.43
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 0.3784
    Episode_Reward/rotating_object: 90.6909
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.84s
                      Time elapsed: 00:12:00
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 733/1500 [0m                      

                       Computation: 110358 steps/s (collection: 0.779s, learning 0.112s)
             Mean action noise std: 2.41
          Mean value_function loss: 132.5584
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.2594
                       Mean reward: 456.13
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 0.3759
    Episode_Reward/rotating_object: 91.3072
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.89s
                      Time elapsed: 00:12:01
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 734/1500 [0m                      

                       Computation: 118090 steps/s (collection: 0.737s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 127.3534
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.2561
                       Mean reward: 474.25
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 0.3699
    Episode_Reward/rotating_object: 89.6586
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.83s
                      Time elapsed: 00:12:02
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 735/1500 [0m                      

                       Computation: 110934 steps/s (collection: 0.740s, learning 0.147s)
             Mean action noise std: 2.41
          Mean value_function loss: 138.4185
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.2486
                       Mean reward: 409.25
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 0.3756
    Episode_Reward/rotating_object: 91.8298
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.89s
                      Time elapsed: 00:12:03
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 736/1500 [0m                      

                       Computation: 112940 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 121.8183
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2499
                       Mean reward: 443.14
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 0.3714
    Episode_Reward/rotating_object: 90.7869
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.87s
                      Time elapsed: 00:12:03
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 737/1500 [0m                      

                       Computation: 117390 steps/s (collection: 0.743s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 123.4012
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.2551
                       Mean reward: 371.49
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 0.3716
    Episode_Reward/rotating_object: 89.4272
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.84s
                      Time elapsed: 00:12:04
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 738/1500 [0m                      

                       Computation: 113898 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 125.2909
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.2587
                       Mean reward: 447.15
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 0.3722
    Episode_Reward/rotating_object: 87.5266
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.86s
                      Time elapsed: 00:12:05
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 739/1500 [0m                      

                       Computation: 111782 steps/s (collection: 0.770s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 121.2962
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.2647
                       Mean reward: 439.45
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 0.3748
    Episode_Reward/rotating_object: 91.6654
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.88s
                      Time elapsed: 00:12:06
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 740/1500 [0m                      

                       Computation: 112369 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 117.9122
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.2743
                       Mean reward: 484.89
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.3795
    Episode_Reward/rotating_object: 92.5289
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.87s
                      Time elapsed: 00:12:07
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 741/1500 [0m                      

                       Computation: 116656 steps/s (collection: 0.756s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 134.7779
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.2716
                       Mean reward: 434.80
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 0.3758
    Episode_Reward/rotating_object: 90.9264
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.84s
                      Time elapsed: 00:12:08
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 742/1500 [0m                      

                       Computation: 116613 steps/s (collection: 0.745s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 118.8498
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2606
                       Mean reward: 434.64
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.3747
    Episode_Reward/rotating_object: 87.3836
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.84s
                      Time elapsed: 00:12:09
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 743/1500 [0m                      

                       Computation: 115872 steps/s (collection: 0.747s, learning 0.101s)
             Mean action noise std: 2.42
          Mean value_function loss: 136.9424
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.2556
                       Mean reward: 492.35
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 93.2764
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.85s
                      Time elapsed: 00:12:09
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 744/1500 [0m                      

                       Computation: 118925 steps/s (collection: 0.740s, learning 0.086s)
             Mean action noise std: 2.42
          Mean value_function loss: 127.5947
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.2548
                       Mean reward: 456.81
               Mean episode length: 232.44
    Episode_Reward/reaching_object: 0.3812
    Episode_Reward/rotating_object: 90.8021
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.83s
                      Time elapsed: 00:12:10
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 745/1500 [0m                      

                       Computation: 113355 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 132.7019
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2451
                       Mean reward: 423.61
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.3689
    Episode_Reward/rotating_object: 87.0670
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.87s
                      Time elapsed: 00:12:11
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 746/1500 [0m                      

                       Computation: 112220 steps/s (collection: 0.772s, learning 0.104s)
             Mean action noise std: 2.42
          Mean value_function loss: 121.5095
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.2423
                       Mean reward: 478.39
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 89.8575
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.88s
                      Time elapsed: 00:12:12
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 747/1500 [0m                      

                       Computation: 113999 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 116.7782
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.2435
                       Mean reward: 478.58
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.3823
    Episode_Reward/rotating_object: 93.8785
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.86s
                      Time elapsed: 00:12:13
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 748/1500 [0m                      

                       Computation: 112262 steps/s (collection: 0.747s, learning 0.129s)
             Mean action noise std: 2.42
          Mean value_function loss: 106.3442
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2514
                       Mean reward: 451.94
               Mean episode length: 234.16
    Episode_Reward/reaching_object: 0.3825
    Episode_Reward/rotating_object: 97.3031
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.88s
                      Time elapsed: 00:12:14
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 749/1500 [0m                      

                       Computation: 113684 steps/s (collection: 0.738s, learning 0.127s)
             Mean action noise std: 2.42
          Mean value_function loss: 114.1980
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2504
                       Mean reward: 460.94
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 0.3776
    Episode_Reward/rotating_object: 92.2184
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.86s
                      Time elapsed: 00:12:15
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 750/1500 [0m                      

                       Computation: 109208 steps/s (collection: 0.805s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 115.3814
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.2536
                       Mean reward: 464.97
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.3759
    Episode_Reward/rotating_object: 90.2720
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.90s
                      Time elapsed: 00:12:16
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 751/1500 [0m                      

                       Computation: 115040 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 115.4871
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.2572
                       Mean reward: 469.25
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 0.3761
    Episode_Reward/rotating_object: 92.9866
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.85s
                      Time elapsed: 00:12:16
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 752/1500 [0m                      

                       Computation: 116260 steps/s (collection: 0.747s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 110.2942
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2579
                       Mean reward: 445.76
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.3825
    Episode_Reward/rotating_object: 95.6718
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.85s
                      Time elapsed: 00:12:17
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 753/1500 [0m                      

                       Computation: 109002 steps/s (collection: 0.787s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 118.0268
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2587
                       Mean reward: 471.75
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.3754
    Episode_Reward/rotating_object: 96.2883
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.90s
                      Time elapsed: 00:12:18
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 754/1500 [0m                      

                       Computation: 114099 steps/s (collection: 0.762s, learning 0.100s)
             Mean action noise std: 2.43
          Mean value_function loss: 118.6998
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2648
                       Mean reward: 497.56
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 96.9424
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.86s
                      Time elapsed: 00:12:19
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 755/1500 [0m                      

                       Computation: 112043 steps/s (collection: 0.776s, learning 0.101s)
             Mean action noise std: 2.43
          Mean value_function loss: 130.6646
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.2735
                       Mean reward: 506.15
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 93.1487
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.88s
                      Time elapsed: 00:12:20
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 756/1500 [0m                      

                       Computation: 116027 steps/s (collection: 0.746s, learning 0.101s)
             Mean action noise std: 2.43
          Mean value_function loss: 129.1253
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.2653
                       Mean reward: 501.39
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.3768
    Episode_Reward/rotating_object: 93.8540
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.85s
                      Time elapsed: 00:12:21
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 757/1500 [0m                      

                       Computation: 111859 steps/s (collection: 0.721s, learning 0.158s)
             Mean action noise std: 2.43
          Mean value_function loss: 131.2695
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2633
                       Mean reward: 470.03
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.3720
    Episode_Reward/rotating_object: 91.5425
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.88s
                      Time elapsed: 00:12:22
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 758/1500 [0m                      

                       Computation: 117898 steps/s (collection: 0.746s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 126.4298
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.2613
                       Mean reward: 426.14
               Mean episode length: 226.70
    Episode_Reward/reaching_object: 0.3791
    Episode_Reward/rotating_object: 94.4736
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.83s
                      Time elapsed: 00:12:22
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 759/1500 [0m                      

                       Computation: 112009 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 2.43
          Mean value_function loss: 121.9394
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.2560
                       Mean reward: 476.35
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 0.3803
    Episode_Reward/rotating_object: 98.0468
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.88s
                      Time elapsed: 00:12:23
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 760/1500 [0m                      

                       Computation: 113429 steps/s (collection: 0.756s, learning 0.111s)
             Mean action noise std: 2.43
          Mean value_function loss: 107.4876
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2524
                       Mean reward: 473.96
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 0.3813
    Episode_Reward/rotating_object: 91.1129
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.87s
                      Time elapsed: 00:12:24
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 761/1500 [0m                      

                       Computation: 115013 steps/s (collection: 0.760s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 122.5513
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2490
                       Mean reward: 438.99
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 0.3811
    Episode_Reward/rotating_object: 93.9333
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.85s
                      Time elapsed: 00:12:25
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 762/1500 [0m                      

                       Computation: 112383 steps/s (collection: 0.768s, learning 0.107s)
             Mean action noise std: 2.44
          Mean value_function loss: 120.8374
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.2515
                       Mean reward: 475.37
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 93.8755
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.87s
                      Time elapsed: 00:12:26
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 763/1500 [0m                      

                       Computation: 111749 steps/s (collection: 0.731s, learning 0.149s)
             Mean action noise std: 2.44
          Mean value_function loss: 116.0715
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.2509
                       Mean reward: 509.56
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.3757
    Episode_Reward/rotating_object: 91.0729
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.88s
                      Time elapsed: 00:12:27
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 764/1500 [0m                      

                       Computation: 118519 steps/s (collection: 0.729s, learning 0.101s)
             Mean action noise std: 2.44
          Mean value_function loss: 117.4252
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 17.2535
                       Mean reward: 473.03
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 0.3786
    Episode_Reward/rotating_object: 93.9639
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.83s
                      Time elapsed: 00:12:28
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 765/1500 [0m                      

                       Computation: 117363 steps/s (collection: 0.743s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 116.6814
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.2552
                       Mean reward: 435.08
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 0.3793
    Episode_Reward/rotating_object: 92.2294
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.84s
                      Time elapsed: 00:12:28
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 766/1500 [0m                      

                       Computation: 111729 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 105.1750
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2612
                       Mean reward: 430.13
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 0.3772
    Episode_Reward/rotating_object: 90.9794
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.88s
                      Time elapsed: 00:12:29
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 767/1500 [0m                      

                       Computation: 119470 steps/s (collection: 0.727s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 117.7971
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.2629
                       Mean reward: 492.40
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.3800
    Episode_Reward/rotating_object: 95.2208
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.82s
                      Time elapsed: 00:12:30
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 768/1500 [0m                      

                       Computation: 113946 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 110.0898
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.2687
                       Mean reward: 493.56
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 0.3743
    Episode_Reward/rotating_object: 97.2195
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.86s
                      Time elapsed: 00:12:31
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 769/1500 [0m                      

                       Computation: 117530 steps/s (collection: 0.751s, learning 0.086s)
             Mean action noise std: 2.45
          Mean value_function loss: 122.1081
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2689
                       Mean reward: 490.17
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.3801
    Episode_Reward/rotating_object: 97.5816
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.84s
                      Time elapsed: 00:12:32
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 770/1500 [0m                      

                       Computation: 112268 steps/s (collection: 0.737s, learning 0.139s)
             Mean action noise std: 2.45
          Mean value_function loss: 116.1516
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.2732
                       Mean reward: 507.70
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.3856
    Episode_Reward/rotating_object: 97.6496
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.88s
                      Time elapsed: 00:12:33
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 771/1500 [0m                      

                       Computation: 113895 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 111.6432
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.2780
                       Mean reward: 466.12
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.3779
    Episode_Reward/rotating_object: 100.7360
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.86s
                      Time elapsed: 00:12:34
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 772/1500 [0m                      

                       Computation: 118423 steps/s (collection: 0.738s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 122.4597
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2831
                       Mean reward: 502.27
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 0.3881
    Episode_Reward/rotating_object: 99.4162
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.83s
                      Time elapsed: 00:12:34
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 773/1500 [0m                      

                       Computation: 117220 steps/s (collection: 0.749s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 114.2991
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.2901
                       Mean reward: 502.88
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.3846
    Episode_Reward/rotating_object: 100.0287
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.84s
                      Time elapsed: 00:12:35
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 774/1500 [0m                      

                       Computation: 114403 steps/s (collection: 0.755s, learning 0.104s)
             Mean action noise std: 2.46
          Mean value_function loss: 121.2891
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.3077
                       Mean reward: 502.02
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 0.3816
    Episode_Reward/rotating_object: 95.4611
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.86s
                      Time elapsed: 00:12:36
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 775/1500 [0m                      

                       Computation: 114601 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 125.9936
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.3114
                       Mean reward: 483.26
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 93.3999
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.86s
                      Time elapsed: 00:12:37
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 776/1500 [0m                      

                       Computation: 114320 steps/s (collection: 0.766s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 126.4713
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3104
                       Mean reward: 400.29
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 0.3656
    Episode_Reward/rotating_object: 90.5051
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.86s
                      Time elapsed: 00:12:38
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 777/1500 [0m                      

                       Computation: 110347 steps/s (collection: 0.745s, learning 0.146s)
             Mean action noise std: 2.47
          Mean value_function loss: 137.3207
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.3197
                       Mean reward: 463.41
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 92.1485
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.89s
                      Time elapsed: 00:12:39
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 778/1500 [0m                      

                       Computation: 114876 steps/s (collection: 0.745s, learning 0.111s)
             Mean action noise std: 2.47
          Mean value_function loss: 128.7741
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.3246
                       Mean reward: 486.50
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.3799
    Episode_Reward/rotating_object: 97.9929
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.86s
                      Time elapsed: 00:12:40
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 779/1500 [0m                      

                       Computation: 113832 steps/s (collection: 0.772s, learning 0.091s)
             Mean action noise std: 2.47
          Mean value_function loss: 116.5731
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3297
                       Mean reward: 472.74
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 93.4976
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.86s
                      Time elapsed: 00:12:40
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 780/1500 [0m                      

                       Computation: 118380 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 2.48
          Mean value_function loss: 128.6693
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3388
                       Mean reward: 489.25
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 0.3804
    Episode_Reward/rotating_object: 97.1607
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.83s
                      Time elapsed: 00:12:41
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 781/1500 [0m                      

                       Computation: 111460 steps/s (collection: 0.770s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 126.6208
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.3485
                       Mean reward: 438.16
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.3804
    Episode_Reward/rotating_object: 94.7682
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.88s
                      Time elapsed: 00:12:42
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 782/1500 [0m                      

                       Computation: 120197 steps/s (collection: 0.730s, learning 0.088s)
             Mean action noise std: 2.48
          Mean value_function loss: 123.5755
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.3516
                       Mean reward: 422.54
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 0.3811
    Episode_Reward/rotating_object: 95.3538
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.82s
                      Time elapsed: 00:12:43
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 783/1500 [0m                      

                       Computation: 113163 steps/s (collection: 0.754s, learning 0.115s)
             Mean action noise std: 2.48
          Mean value_function loss: 139.2268
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.3500
                       Mean reward: 421.74
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 0.3774
    Episode_Reward/rotating_object: 91.4579
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.87s
                      Time elapsed: 00:12:44
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 784/1500 [0m                      

                       Computation: 108988 steps/s (collection: 0.760s, learning 0.142s)
             Mean action noise std: 2.48
          Mean value_function loss: 114.1236
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3522
                       Mean reward: 476.25
               Mean episode length: 231.42
    Episode_Reward/reaching_object: 0.3782
    Episode_Reward/rotating_object: 93.4532
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.90s
                      Time elapsed: 00:12:45
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 785/1500 [0m                      

                       Computation: 116638 steps/s (collection: 0.741s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 116.7147
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3519
                       Mean reward: 506.38
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.3782
    Episode_Reward/rotating_object: 98.3159
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.84s
                      Time elapsed: 00:12:46
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 786/1500 [0m                      

                       Computation: 116785 steps/s (collection: 0.742s, learning 0.100s)
             Mean action noise std: 2.49
          Mean value_function loss: 113.7960
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.3519
                       Mean reward: 485.75
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.3716
    Episode_Reward/rotating_object: 94.1265
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.84s
                      Time elapsed: 00:12:46
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 787/1500 [0m                      

                       Computation: 115626 steps/s (collection: 0.762s, learning 0.089s)
             Mean action noise std: 2.49
          Mean value_function loss: 119.3675
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 17.3513
                       Mean reward: 416.05
               Mean episode length: 229.89
    Episode_Reward/reaching_object: 0.3790
    Episode_Reward/rotating_object: 92.2972
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.85s
                      Time elapsed: 00:12:47
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 788/1500 [0m                      

                       Computation: 116205 steps/s (collection: 0.760s, learning 0.086s)
             Mean action noise std: 2.49
          Mean value_function loss: 126.0624
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.3507
                       Mean reward: 505.31
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.3764
    Episode_Reward/rotating_object: 96.3219
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.85s
                      Time elapsed: 00:12:48
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 789/1500 [0m                      

                       Computation: 113535 steps/s (collection: 0.775s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 115.5713
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3506
                       Mean reward: 496.25
               Mean episode length: 237.14
    Episode_Reward/reaching_object: 0.3747
    Episode_Reward/rotating_object: 95.6914
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.87s
                      Time elapsed: 00:12:49
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 790/1500 [0m                      

                       Computation: 113170 steps/s (collection: 0.760s, learning 0.109s)
             Mean action noise std: 2.49
          Mean value_function loss: 125.2826
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.3521
                       Mean reward: 519.93
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.3828
    Episode_Reward/rotating_object: 98.4045
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.87s
                      Time elapsed: 00:12:50
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 791/1500 [0m                      

                       Computation: 113639 steps/s (collection: 0.758s, learning 0.107s)
             Mean action noise std: 2.49
          Mean value_function loss: 121.6017
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.3578
                       Mean reward: 455.61
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.3833
    Episode_Reward/rotating_object: 99.0410
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.87s
                      Time elapsed: 00:12:51
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 792/1500 [0m                      

                       Computation: 117580 steps/s (collection: 0.741s, learning 0.095s)
             Mean action noise std: 2.49
          Mean value_function loss: 126.2697
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.3618
                       Mean reward: 468.19
               Mean episode length: 232.42
    Episode_Reward/reaching_object: 0.3784
    Episode_Reward/rotating_object: 96.0708
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.84s
                      Time elapsed: 00:12:52
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 793/1500 [0m                      

                       Computation: 115198 steps/s (collection: 0.748s, learning 0.105s)
             Mean action noise std: 2.49
          Mean value_function loss: 135.2800
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3573
                       Mean reward: 471.85
               Mean episode length: 229.55
    Episode_Reward/reaching_object: 0.3850
    Episode_Reward/rotating_object: 97.9895
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.85s
                      Time elapsed: 00:12:52
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 794/1500 [0m                      

                       Computation: 117297 steps/s (collection: 0.751s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 124.0547
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3551
                       Mean reward: 508.51
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.3804
    Episode_Reward/rotating_object: 96.2325
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.84s
                      Time elapsed: 00:12:53
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 795/1500 [0m                      

                       Computation: 106984 steps/s (collection: 0.789s, learning 0.130s)
             Mean action noise std: 2.50
          Mean value_function loss: 115.6417
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.3677
                       Mean reward: 484.93
               Mean episode length: 238.66
    Episode_Reward/reaching_object: 0.3794
    Episode_Reward/rotating_object: 93.6230
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.92s
                      Time elapsed: 00:12:54
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 796/1500 [0m                      

                       Computation: 111203 steps/s (collection: 0.794s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 109.7031
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.3711
                       Mean reward: 506.35
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.3845
    Episode_Reward/rotating_object: 97.4554
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.88s
                      Time elapsed: 00:12:55
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 797/1500 [0m                      

                       Computation: 116934 steps/s (collection: 0.751s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 115.0136
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3739
                       Mean reward: 465.61
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.3851
    Episode_Reward/rotating_object: 97.9679
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.84s
                      Time elapsed: 00:12:56
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 798/1500 [0m                      

                       Computation: 113726 steps/s (collection: 0.728s, learning 0.136s)
             Mean action noise std: 2.50
          Mean value_function loss: 99.9914
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.3783
                       Mean reward: 494.94
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 0.3875
    Episode_Reward/rotating_object: 101.5836
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.86s
                      Time elapsed: 00:12:57
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 799/1500 [0m                      

                       Computation: 113647 steps/s (collection: 0.732s, learning 0.133s)
             Mean action noise std: 2.50
          Mean value_function loss: 111.0866
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.3791
                       Mean reward: 456.09
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 95.6746
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.86s
                      Time elapsed: 00:12:58
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 800/1500 [0m                      

                       Computation: 120188 steps/s (collection: 0.732s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 107.6183
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3810
                       Mean reward: 493.86
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 101.8385
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.82s
                      Time elapsed: 00:12:58
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 801/1500 [0m                      

                       Computation: 116788 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 2.51
          Mean value_function loss: 114.7051
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3765
                       Mean reward: 539.21
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 103.1279
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.84s
                      Time elapsed: 00:12:59
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 802/1500 [0m                      

                       Computation: 118316 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 2.51
          Mean value_function loss: 119.4062
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.3743
                       Mean reward: 463.79
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 0.3833
    Episode_Reward/rotating_object: 92.9392
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.83s
                      Time elapsed: 00:13:00
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 803/1500 [0m                      

                       Computation: 110656 steps/s (collection: 0.792s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 117.9111
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.3799
                       Mean reward: 496.47
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.3896
    Episode_Reward/rotating_object: 97.7019
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.89s
                      Time elapsed: 00:13:01
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 804/1500 [0m                      

                       Computation: 112493 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 127.2678
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.3820
                       Mean reward: 469.88
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.3902
    Episode_Reward/rotating_object: 101.9213
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.87s
                      Time elapsed: 00:13:02
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 805/1500 [0m                      

                       Computation: 118757 steps/s (collection: 0.740s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 128.4907
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3807
                       Mean reward: 494.97
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 0.3900
    Episode_Reward/rotating_object: 97.9288
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.83s
                      Time elapsed: 00:13:03
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 806/1500 [0m                      

                       Computation: 114138 steps/s (collection: 0.731s, learning 0.131s)
             Mean action noise std: 2.52
          Mean value_function loss: 127.0149
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3818
                       Mean reward: 449.07
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 92.2746
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.86s
                      Time elapsed: 00:13:04
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 807/1500 [0m                      

                       Computation: 115343 steps/s (collection: 0.758s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 116.1132
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.3798
                       Mean reward: 505.45
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.3929
    Episode_Reward/rotating_object: 96.5430
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.85s
                      Time elapsed: 00:13:04
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 808/1500 [0m                      

                       Computation: 115604 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 2.52
          Mean value_function loss: 126.3722
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3757
                       Mean reward: 522.87
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.3931
    Episode_Reward/rotating_object: 102.3241
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.85s
                      Time elapsed: 00:13:05
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 809/1500 [0m                      

                       Computation: 115291 steps/s (collection: 0.756s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 121.1847
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.3780
                       Mean reward: 492.19
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3877
    Episode_Reward/rotating_object: 95.7108
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.85s
                      Time elapsed: 00:13:06
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 810/1500 [0m                      

                       Computation: 116740 steps/s (collection: 0.754s, learning 0.088s)
             Mean action noise std: 2.53
          Mean value_function loss: 118.7485
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3900
                       Mean reward: 441.30
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 0.3841
    Episode_Reward/rotating_object: 92.7442
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.84s
                      Time elapsed: 00:13:07
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 811/1500 [0m                      

                       Computation: 114938 steps/s (collection: 0.745s, learning 0.111s)
             Mean action noise std: 2.53
          Mean value_function loss: 123.1694
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3976
                       Mean reward: 486.68
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 100.2273
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.86s
                      Time elapsed: 00:13:08
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 812/1500 [0m                      

                       Computation: 113302 steps/s (collection: 0.754s, learning 0.114s)
             Mean action noise std: 2.53
          Mean value_function loss: 108.7767
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.4029
                       Mean reward: 483.73
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 0.3990
    Episode_Reward/rotating_object: 100.0613
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.87s
                      Time elapsed: 00:13:09
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 813/1500 [0m                      

                       Computation: 120536 steps/s (collection: 0.724s, learning 0.092s)
             Mean action noise std: 2.53
          Mean value_function loss: 120.9943
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4049
                       Mean reward: 488.06
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 0.3853
    Episode_Reward/rotating_object: 94.6894
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.82s
                      Time elapsed: 00:13:10
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 814/1500 [0m                      

                       Computation: 112779 steps/s (collection: 0.786s, learning 0.086s)
             Mean action noise std: 2.53
          Mean value_function loss: 110.2157
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.4130
                       Mean reward: 464.66
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 0.3843
    Episode_Reward/rotating_object: 95.3544
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.87s
                      Time elapsed: 00:13:10
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 815/1500 [0m                      

                       Computation: 111922 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 2.53
          Mean value_function loss: 120.3032
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4136
                       Mean reward: 531.69
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.3915
    Episode_Reward/rotating_object: 99.4375
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.88s
                      Time elapsed: 00:13:11
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 816/1500 [0m                      

                       Computation: 113982 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 123.2396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4192
                       Mean reward: 491.89
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 96.7717
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.86s
                      Time elapsed: 00:13:12
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 817/1500 [0m                      

                       Computation: 115067 steps/s (collection: 0.767s, learning 0.087s)
             Mean action noise std: 2.54
          Mean value_function loss: 124.2660
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.4205
                       Mean reward: 514.48
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.3894
    Episode_Reward/rotating_object: 99.9540
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.85s
                      Time elapsed: 00:13:13
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 818/1500 [0m                      

                       Computation: 118036 steps/s (collection: 0.744s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 117.6113
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4208
                       Mean reward: 505.37
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 100.3352
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.83s
                      Time elapsed: 00:13:14
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 819/1500 [0m                      

                       Computation: 111241 steps/s (collection: 0.741s, learning 0.143s)
             Mean action noise std: 2.54
          Mean value_function loss: 115.3888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.4238
                       Mean reward: 479.97
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.3915
    Episode_Reward/rotating_object: 97.0197
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.88s
                      Time elapsed: 00:13:15
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 820/1500 [0m                      

                       Computation: 110869 steps/s (collection: 0.740s, learning 0.147s)
             Mean action noise std: 2.54
          Mean value_function loss: 120.7415
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.4306
                       Mean reward: 533.15
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 99.2322
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.89s
                      Time elapsed: 00:13:16
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 821/1500 [0m                      

                       Computation: 114381 steps/s (collection: 0.758s, learning 0.101s)
             Mean action noise std: 2.55
          Mean value_function loss: 120.1604
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.4358
                       Mean reward: 489.78
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.3935
    Episode_Reward/rotating_object: 101.5191
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.86s
                      Time elapsed: 00:13:16
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 822/1500 [0m                      

                       Computation: 116285 steps/s (collection: 0.755s, learning 0.090s)
             Mean action noise std: 2.55
          Mean value_function loss: 115.8324
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.4458
                       Mean reward: 539.51
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3900
    Episode_Reward/rotating_object: 100.2365
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.85s
                      Time elapsed: 00:13:17
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 823/1500 [0m                      

                       Computation: 117225 steps/s (collection: 0.752s, learning 0.087s)
             Mean action noise std: 2.55
          Mean value_function loss: 114.9320
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.4419
                       Mean reward: 529.02
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 102.2057
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.84s
                      Time elapsed: 00:13:18
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 824/1500 [0m                      

                       Computation: 110414 steps/s (collection: 0.779s, learning 0.112s)
             Mean action noise std: 2.55
          Mean value_function loss: 107.8630
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.4393
                       Mean reward: 452.65
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 98.7685
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.89s
                      Time elapsed: 00:13:19
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 825/1500 [0m                      

                       Computation: 118261 steps/s (collection: 0.733s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 110.7705
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.4407
                       Mean reward: 522.15
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.3994
    Episode_Reward/rotating_object: 104.6713
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.83s
                      Time elapsed: 00:13:20
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 826/1500 [0m                      

                       Computation: 112263 steps/s (collection: 0.744s, learning 0.132s)
             Mean action noise std: 2.55
          Mean value_function loss: 116.1450
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.4423
                       Mean reward: 501.84
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 0.3890
    Episode_Reward/rotating_object: 98.7307
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.88s
                      Time elapsed: 00:13:21
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 827/1500 [0m                      

                       Computation: 107552 steps/s (collection: 0.772s, learning 0.142s)
             Mean action noise std: 2.56
          Mean value_function loss: 106.9803
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4487
                       Mean reward: 454.90
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.3876
    Episode_Reward/rotating_object: 97.0413
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.91s
                      Time elapsed: 00:13:22
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 828/1500 [0m                      

                       Computation: 116285 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 116.7641
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.4651
                       Mean reward: 465.63
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.3836
    Episode_Reward/rotating_object: 99.4569
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.85s
                      Time elapsed: 00:13:22
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 829/1500 [0m                      

                       Computation: 110291 steps/s (collection: 0.795s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 103.3213
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 17.4729
                       Mean reward: 455.50
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 0.3910
    Episode_Reward/rotating_object: 100.4003
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.89s
                      Time elapsed: 00:13:23
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 830/1500 [0m                      

                       Computation: 103902 steps/s (collection: 0.828s, learning 0.118s)
             Mean action noise std: 2.57
          Mean value_function loss: 91.8259
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4800
                       Mean reward: 515.36
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 0.4037
    Episode_Reward/rotating_object: 105.4427
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.95s
                      Time elapsed: 00:13:24
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 831/1500 [0m                      

                       Computation: 113119 steps/s (collection: 0.757s, learning 0.112s)
             Mean action noise std: 2.57
          Mean value_function loss: 94.2699
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.4901
                       Mean reward: 518.31
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.3998
    Episode_Reward/rotating_object: 106.0227
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.87s
                      Time elapsed: 00:13:25
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 832/1500 [0m                      

                       Computation: 113333 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 2.57
          Mean value_function loss: 97.8800
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 17.4971
                       Mean reward: 522.64
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 100.7342
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.87s
                      Time elapsed: 00:13:26
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 833/1500 [0m                      

                       Computation: 109979 steps/s (collection: 0.790s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 86.6673
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4998
                       Mean reward: 470.70
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.3976
    Episode_Reward/rotating_object: 102.7401
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.89s
                      Time elapsed: 00:13:27
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 834/1500 [0m                      

                       Computation: 111028 steps/s (collection: 0.762s, learning 0.124s)
             Mean action noise std: 2.58
          Mean value_function loss: 95.1925
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.5030
                       Mean reward: 490.63
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3995
    Episode_Reward/rotating_object: 105.3622
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.89s
                      Time elapsed: 00:13:28
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 835/1500 [0m                      

                       Computation: 115939 steps/s (collection: 0.746s, learning 0.102s)
             Mean action noise std: 2.58
          Mean value_function loss: 99.8825
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.5165
                       Mean reward: 537.71
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3923
    Episode_Reward/rotating_object: 106.1194
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.85s
                      Time elapsed: 00:13:29
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 836/1500 [0m                      

                       Computation: 108401 steps/s (collection: 0.744s, learning 0.163s)
             Mean action noise std: 2.58
          Mean value_function loss: 99.5386
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.5316
                       Mean reward: 509.98
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 99.2818
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.91s
                      Time elapsed: 00:13:30
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 837/1500 [0m                      

                       Computation: 115632 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 2.59
          Mean value_function loss: 108.4917
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.5360
                       Mean reward: 498.96
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.3925
    Episode_Reward/rotating_object: 102.3857
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.85s
                      Time elapsed: 00:13:30
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 838/1500 [0m                      

                       Computation: 117044 steps/s (collection: 0.750s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 112.7552
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.5371
                       Mean reward: 498.66
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.3987
    Episode_Reward/rotating_object: 100.9882
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.84s
                      Time elapsed: 00:13:31
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 839/1500 [0m                      

                       Computation: 112956 steps/s (collection: 0.772s, learning 0.098s)
             Mean action noise std: 2.59
          Mean value_function loss: 108.8719
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.5419
                       Mean reward: 549.19
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 102.8191
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.87s
                      Time elapsed: 00:13:32
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 840/1500 [0m                      

                       Computation: 115059 steps/s (collection: 0.751s, learning 0.103s)
             Mean action noise std: 2.59
          Mean value_function loss: 111.5776
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.5445
                       Mean reward: 499.15
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.3940
    Episode_Reward/rotating_object: 95.0808
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.85s
                      Time elapsed: 00:13:33
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 841/1500 [0m                      

                       Computation: 107278 steps/s (collection: 0.797s, learning 0.119s)
             Mean action noise std: 2.59
          Mean value_function loss: 105.2627
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.5442
                       Mean reward: 483.39
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 0.3986
    Episode_Reward/rotating_object: 101.7294
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.92s
                      Time elapsed: 00:13:34
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 842/1500 [0m                      

                       Computation: 111166 steps/s (collection: 0.775s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 113.9411
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.5535
                       Mean reward: 507.23
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3954
    Episode_Reward/rotating_object: 100.1873
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.88s
                      Time elapsed: 00:13:35
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 843/1500 [0m                      

                       Computation: 111566 steps/s (collection: 0.740s, learning 0.141s)
             Mean action noise std: 2.60
          Mean value_function loss: 114.8117
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.5600
                       Mean reward: 511.38
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.4018
    Episode_Reward/rotating_object: 103.2843
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.88s
                      Time elapsed: 00:13:36
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 844/1500 [0m                      

                       Computation: 113852 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 2.60
          Mean value_function loss: 114.3399
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5649
                       Mean reward: 515.16
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3934
    Episode_Reward/rotating_object: 101.1835
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.86s
                      Time elapsed: 00:13:37
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 845/1500 [0m                      

                       Computation: 114377 steps/s (collection: 0.761s, learning 0.099s)
             Mean action noise std: 2.60
          Mean value_function loss: 112.3126
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.5739
                       Mean reward: 483.20
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.3880
    Episode_Reward/rotating_object: 94.9581
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.86s
                      Time elapsed: 00:13:37
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 846/1500 [0m                      

                       Computation: 108644 steps/s (collection: 0.792s, learning 0.113s)
             Mean action noise std: 2.61
          Mean value_function loss: 108.5836
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.5820
                       Mean reward: 501.76
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 96.5519
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.90s
                      Time elapsed: 00:13:38
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 847/1500 [0m                      

                       Computation: 109282 steps/s (collection: 0.803s, learning 0.097s)
             Mean action noise std: 2.61
          Mean value_function loss: 105.4035
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.5922
                       Mean reward: 521.19
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 100.9576
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.90s
                      Time elapsed: 00:13:39
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 848/1500 [0m                      

                       Computation: 114088 steps/s (collection: 0.748s, learning 0.114s)
             Mean action noise std: 2.61
          Mean value_function loss: 105.7709
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.6032
                       Mean reward: 492.98
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 97.7752
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.86s
                      Time elapsed: 00:13:40
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 849/1500 [0m                      

                       Computation: 109450 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 2.62
          Mean value_function loss: 125.7225
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6109
                       Mean reward: 488.67
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.3951
    Episode_Reward/rotating_object: 96.1049
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.90s
                      Time elapsed: 00:13:41
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 850/1500 [0m                      

                       Computation: 108894 steps/s (collection: 0.793s, learning 0.110s)
             Mean action noise std: 2.62
          Mean value_function loss: 111.9476
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.6162
                       Mean reward: 453.37
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.3915
    Episode_Reward/rotating_object: 98.0934
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.90s
                      Time elapsed: 00:13:42
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 851/1500 [0m                      

                       Computation: 111500 steps/s (collection: 0.764s, learning 0.118s)
             Mean action noise std: 2.62
          Mean value_function loss: 111.3057
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.6212
                       Mean reward: 540.64
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 103.3462
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.88s
                      Time elapsed: 00:13:43
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 852/1500 [0m                      

                       Computation: 113381 steps/s (collection: 0.757s, learning 0.110s)
             Mean action noise std: 2.62
          Mean value_function loss: 112.2492
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.6195
                       Mean reward: 475.20
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3905
    Episode_Reward/rotating_object: 98.8823
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.87s
                      Time elapsed: 00:13:44
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 853/1500 [0m                      

                       Computation: 111982 steps/s (collection: 0.741s, learning 0.137s)
             Mean action noise std: 2.62
          Mean value_function loss: 108.6034
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6124
                       Mean reward: 518.89
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 0.3870
    Episode_Reward/rotating_object: 98.8937
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.88s
                      Time elapsed: 00:13:45
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 854/1500 [0m                      

                       Computation: 109693 steps/s (collection: 0.775s, learning 0.121s)
             Mean action noise std: 2.63
          Mean value_function loss: 102.7280
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6084
                       Mean reward: 509.98
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.3960
    Episode_Reward/rotating_object: 102.4889
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.90s
                      Time elapsed: 00:13:45
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 855/1500 [0m                      

                       Computation: 113613 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 2.63
          Mean value_function loss: 107.4311
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.6148
                       Mean reward: 524.90
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.3929
    Episode_Reward/rotating_object: 99.8876
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.87s
                      Time elapsed: 00:13:46
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 856/1500 [0m                      

                       Computation: 115467 steps/s (collection: 0.762s, learning 0.090s)
             Mean action noise std: 2.63
          Mean value_function loss: 107.9283
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6181
                       Mean reward: 548.92
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3938
    Episode_Reward/rotating_object: 102.4770
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.85s
                      Time elapsed: 00:13:47
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 857/1500 [0m                      

                       Computation: 111717 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 108.3060
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6147
                       Mean reward: 510.51
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 97.3801
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.88s
                      Time elapsed: 00:13:48
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 858/1500 [0m                      

                       Computation: 115864 steps/s (collection: 0.754s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 109.0671
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 17.6114
                       Mean reward: 540.93
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.3973
    Episode_Reward/rotating_object: 104.1561
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.85s
                      Time elapsed: 00:13:49
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 859/1500 [0m                      

                       Computation: 110709 steps/s (collection: 0.753s, learning 0.135s)
             Mean action noise std: 2.63
          Mean value_function loss: 97.3110
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6018
                       Mean reward: 489.77
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 100.3688
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.89s
                      Time elapsed: 00:13:50
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 860/1500 [0m                      

                       Computation: 118097 steps/s (collection: 0.729s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 103.0217
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.5946
                       Mean reward: 526.25
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.3876
    Episode_Reward/rotating_object: 98.1866
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.83s
                      Time elapsed: 00:13:51
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 861/1500 [0m                      

                       Computation: 117882 steps/s (collection: 0.736s, learning 0.098s)
             Mean action noise std: 2.63
          Mean value_function loss: 112.6323
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.5982
                       Mean reward: 523.10
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.3922
    Episode_Reward/rotating_object: 100.7894
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.83s
                      Time elapsed: 00:13:51
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 862/1500 [0m                      

                       Computation: 113987 steps/s (collection: 0.768s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 88.8570
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.6005
                       Mean reward: 501.45
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 95.0953
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.86s
                      Time elapsed: 00:13:52
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 863/1500 [0m                      

                       Computation: 109301 steps/s (collection: 0.760s, learning 0.140s)
             Mean action noise std: 2.64
          Mean value_function loss: 94.0243
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.6033
                       Mean reward: 556.03
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.3910
    Episode_Reward/rotating_object: 101.2808
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.90s
                      Time elapsed: 00:13:53
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 864/1500 [0m                      

                       Computation: 104538 steps/s (collection: 0.837s, learning 0.104s)
             Mean action noise std: 2.64
          Mean value_function loss: 100.9831
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.6042
                       Mean reward: 504.66
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 0.3935
    Episode_Reward/rotating_object: 101.1434
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.94s
                      Time elapsed: 00:13:54
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 865/1500 [0m                      

                       Computation: 115734 steps/s (collection: 0.754s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 99.4854
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.6028
                       Mean reward: 494.23
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.3885
    Episode_Reward/rotating_object: 99.8716
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.85s
                      Time elapsed: 00:13:55
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 866/1500 [0m                      

                       Computation: 112167 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 100.5386
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.6051
                       Mean reward: 482.51
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.3899
    Episode_Reward/rotating_object: 102.3453
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.88s
                      Time elapsed: 00:13:56
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 867/1500 [0m                      

                       Computation: 118170 steps/s (collection: 0.742s, learning 0.090s)
             Mean action noise std: 2.64
          Mean value_function loss: 105.7135
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6041
                       Mean reward: 516.80
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3905
    Episode_Reward/rotating_object: 101.7046
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.83s
                      Time elapsed: 00:13:57
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 868/1500 [0m                      

                       Computation: 116145 steps/s (collection: 0.745s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 102.2948
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6038
                       Mean reward: 520.32
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.3927
    Episode_Reward/rotating_object: 101.7827
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.85s
                      Time elapsed: 00:13:58
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 869/1500 [0m                      

                       Computation: 114050 steps/s (collection: 0.743s, learning 0.119s)
             Mean action noise std: 2.65
          Mean value_function loss: 104.8411
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6021
                       Mean reward: 521.89
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 100.0932
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.86s
                      Time elapsed: 00:13:58
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 870/1500 [0m                      

                       Computation: 112614 steps/s (collection: 0.739s, learning 0.134s)
             Mean action noise std: 2.65
          Mean value_function loss: 101.6933
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6094
                       Mean reward: 479.23
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.3900
    Episode_Reward/rotating_object: 99.2488
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.87s
                      Time elapsed: 00:13:59
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 871/1500 [0m                      

                       Computation: 113072 steps/s (collection: 0.749s, learning 0.120s)
             Mean action noise std: 2.65
          Mean value_function loss: 94.3143
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.6186
                       Mean reward: 501.24
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.3975
    Episode_Reward/rotating_object: 106.5808
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.87s
                      Time elapsed: 00:14:00
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 872/1500 [0m                      

                       Computation: 115478 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 2.66
          Mean value_function loss: 97.7152
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6224
                       Mean reward: 516.43
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 101.9363
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.85s
                      Time elapsed: 00:14:01
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 873/1500 [0m                      

                       Computation: 114856 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 96.2850
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6200
                       Mean reward: 531.97
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.3950
    Episode_Reward/rotating_object: 103.5568
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.86s
                      Time elapsed: 00:14:02
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 874/1500 [0m                      

                       Computation: 110589 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 2.66
          Mean value_function loss: 100.5505
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.6237
                       Mean reward: 529.98
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.3936
    Episode_Reward/rotating_object: 103.1065
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.89s
                      Time elapsed: 00:14:03
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 875/1500 [0m                      

                       Computation: 116756 steps/s (collection: 0.736s, learning 0.106s)
             Mean action noise std: 2.66
          Mean value_function loss: 105.8011
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.6276
                       Mean reward: 533.65
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.3950
    Episode_Reward/rotating_object: 103.0821
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.84s
                      Time elapsed: 00:14:04
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 876/1500 [0m                      

                       Computation: 116640 steps/s (collection: 0.755s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 103.4048
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6287
                       Mean reward: 493.25
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 0.3846
    Episode_Reward/rotating_object: 101.1038
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.84s
                      Time elapsed: 00:14:04
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 877/1500 [0m                      

                       Computation: 110150 steps/s (collection: 0.747s, learning 0.146s)
             Mean action noise std: 2.66
          Mean value_function loss: 107.5211
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.6196
                       Mean reward: 504.63
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.3903
    Episode_Reward/rotating_object: 100.2887
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.89s
                      Time elapsed: 00:14:05
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 878/1500 [0m                      

                       Computation: 117667 steps/s (collection: 0.744s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 105.4427
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.6282
                       Mean reward: 512.23
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.3869
    Episode_Reward/rotating_object: 100.4452
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.84s
                      Time elapsed: 00:14:06
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 879/1500 [0m                      

                       Computation: 117187 steps/s (collection: 0.749s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 104.0116
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.6270
                       Mean reward: 542.45
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3977
    Episode_Reward/rotating_object: 107.9973
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.84s
                      Time elapsed: 00:14:07
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 880/1500 [0m                      

                       Computation: 103642 steps/s (collection: 0.858s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 89.7201
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.6300
                       Mean reward: 506.33
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 0.3966
    Episode_Reward/rotating_object: 102.3434
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.95s
                      Time elapsed: 00:14:08
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 881/1500 [0m                      

                       Computation: 118453 steps/s (collection: 0.739s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 101.5268
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6315
                       Mean reward: 519.91
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 108.0809
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.83s
                      Time elapsed: 00:14:09
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 882/1500 [0m                      

                       Computation: 117037 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 2.68
          Mean value_function loss: 99.9126
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.6390
                       Mean reward: 535.24
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.3983
    Episode_Reward/rotating_object: 103.9754
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.84s
                      Time elapsed: 00:14:10
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 883/1500 [0m                      

                       Computation: 115668 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 2.68
          Mean value_function loss: 99.2317
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.6472
                       Mean reward: 534.16
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 107.0071
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.85s
                      Time elapsed: 00:14:10
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 884/1500 [0m                      

                       Computation: 111778 steps/s (collection: 0.751s, learning 0.129s)
             Mean action noise std: 2.68
          Mean value_function loss: 114.0857
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6478
                       Mean reward: 526.86
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.3951
    Episode_Reward/rotating_object: 106.1741
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.88s
                      Time elapsed: 00:14:11
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 885/1500 [0m                      

                       Computation: 113655 steps/s (collection: 0.756s, learning 0.109s)
             Mean action noise std: 2.68
          Mean value_function loss: 117.1983
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6472
                       Mean reward: 492.08
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.3929
    Episode_Reward/rotating_object: 100.1842
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.86s
                      Time elapsed: 00:14:12
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 886/1500 [0m                      

                       Computation: 115750 steps/s (collection: 0.763s, learning 0.086s)
             Mean action noise std: 2.68
          Mean value_function loss: 118.7964
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.6472
                       Mean reward: 539.40
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 107.6977
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.85s
                      Time elapsed: 00:14:13
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 887/1500 [0m                      

                       Computation: 111909 steps/s (collection: 0.776s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 98.4535
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6518
                       Mean reward: 534.55
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 106.1572
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.88s
                      Time elapsed: 00:14:14
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 888/1500 [0m                      

                       Computation: 113659 steps/s (collection: 0.752s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 110.2466
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.6595
                       Mean reward: 538.11
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 0.4005
    Episode_Reward/rotating_object: 107.2400
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.86s
                      Time elapsed: 00:14:15
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 889/1500 [0m                      

                       Computation: 113871 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 2.69
          Mean value_function loss: 94.2852
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.6627
                       Mean reward: 548.47
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 106.5912
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.86s
                      Time elapsed: 00:14:16
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 890/1500 [0m                      

                       Computation: 117537 steps/s (collection: 0.748s, learning 0.088s)
             Mean action noise std: 2.69
          Mean value_function loss: 92.6629
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.6615
                       Mean reward: 524.88
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 0.4019
    Episode_Reward/rotating_object: 106.3306
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.84s
                      Time elapsed: 00:14:16
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 891/1500 [0m                      

                       Computation: 111580 steps/s (collection: 0.765s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 109.1241
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6581
                       Mean reward: 467.85
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.3928
    Episode_Reward/rotating_object: 102.3536
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.88s
                      Time elapsed: 00:14:17
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 892/1500 [0m                      

                       Computation: 113898 steps/s (collection: 0.761s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 104.7820
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6546
                       Mean reward: 508.97
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.4036
    Episode_Reward/rotating_object: 106.3312
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.86s
                      Time elapsed: 00:14:18
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 893/1500 [0m                      

                       Computation: 114951 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 2.69
          Mean value_function loss: 109.7757
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6631
                       Mean reward: 490.89
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 101.7244
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.86s
                      Time elapsed: 00:14:19
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 894/1500 [0m                      

                       Computation: 118176 steps/s (collection: 0.746s, learning 0.086s)
             Mean action noise std: 2.69
          Mean value_function loss: 113.0401
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6675
                       Mean reward: 511.10
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 104.7314
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.83s
                      Time elapsed: 00:14:20
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 895/1500 [0m                      

                       Computation: 109068 steps/s (collection: 0.790s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 107.2040
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.6722
                       Mean reward: 552.01
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 109.0077
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.90s
                      Time elapsed: 00:14:21
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 896/1500 [0m                      

                       Computation: 116761 steps/s (collection: 0.746s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 111.5396
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.6795
                       Mean reward: 555.52
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 105.6616
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.84s
                      Time elapsed: 00:14:22
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 897/1500 [0m                      

                       Computation: 110670 steps/s (collection: 0.775s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 108.6097
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.6863
                       Mean reward: 551.57
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 104.5613
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.89s
                      Time elapsed: 00:14:23
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 898/1500 [0m                      

                       Computation: 111980 steps/s (collection: 0.785s, learning 0.093s)
             Mean action noise std: 2.70
          Mean value_function loss: 101.7710
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6850
                       Mean reward: 491.45
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 102.6218
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.88s
                      Time elapsed: 00:14:23
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 899/1500 [0m                      

                       Computation: 107448 steps/s (collection: 0.777s, learning 0.138s)
             Mean action noise std: 2.71
          Mean value_function loss: 111.0186
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.6761
                       Mean reward: 517.67
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.3990
    Episode_Reward/rotating_object: 105.9883
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.91s
                      Time elapsed: 00:14:24
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 900/1500 [0m                      

                       Computation: 111688 steps/s (collection: 0.769s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 124.6216
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6760
                       Mean reward: 494.86
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 99.5032
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.88s
                      Time elapsed: 00:14:25
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 901/1500 [0m                      

                       Computation: 113065 steps/s (collection: 0.764s, learning 0.106s)
             Mean action noise std: 2.71
          Mean value_function loss: 110.1044
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.6729
                       Mean reward: 526.72
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 101.8752
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.87s
                      Time elapsed: 00:14:26
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 902/1500 [0m                      

                       Computation: 112230 steps/s (collection: 0.768s, learning 0.108s)
             Mean action noise std: 2.71
          Mean value_function loss: 126.8128
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6715
                       Mean reward: 500.13
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 102.4537
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.88s
                      Time elapsed: 00:14:27
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 903/1500 [0m                      

                       Computation: 116706 steps/s (collection: 0.755s, learning 0.087s)
             Mean action noise std: 2.71
          Mean value_function loss: 107.6282
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.6770
                       Mean reward: 518.23
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.3972
    Episode_Reward/rotating_object: 98.3926
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.84s
                      Time elapsed: 00:14:28
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 904/1500 [0m                      

                       Computation: 111599 steps/s (collection: 0.786s, learning 0.095s)
             Mean action noise std: 2.71
          Mean value_function loss: 115.9127
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 17.6854
                       Mean reward: 528.15
               Mean episode length: 242.68
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 104.8391
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.88s
                      Time elapsed: 00:14:29
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 905/1500 [0m                      

                       Computation: 102569 steps/s (collection: 0.805s, learning 0.154s)
             Mean action noise std: 2.71
          Mean value_function loss: 134.8764
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.6854
                       Mean reward: 518.93
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.3982
    Episode_Reward/rotating_object: 100.6377
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.96s
                      Time elapsed: 00:14:30
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 906/1500 [0m                      

                       Computation: 107194 steps/s (collection: 0.823s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 118.0474
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6813
                       Mean reward: 561.86
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 102.3530
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.92s
                      Time elapsed: 00:14:31
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 907/1500 [0m                      

                       Computation: 110548 steps/s (collection: 0.801s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 123.8874
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6809
                       Mean reward: 514.03
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 0.3992
    Episode_Reward/rotating_object: 102.4661
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.89s
                      Time elapsed: 00:14:31
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 908/1500 [0m                      

                       Computation: 117518 steps/s (collection: 0.750s, learning 0.086s)
             Mean action noise std: 2.72
          Mean value_function loss: 130.5285
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6835
                       Mean reward: 463.67
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.3915
    Episode_Reward/rotating_object: 97.0816
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.84s
                      Time elapsed: 00:14:32
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 909/1500 [0m                      

                       Computation: 110913 steps/s (collection: 0.763s, learning 0.123s)
             Mean action noise std: 2.72
          Mean value_function loss: 122.7136
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.6862
                       Mean reward: 482.67
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 99.5232
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.89s
                      Time elapsed: 00:14:33
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 910/1500 [0m                      

                       Computation: 115178 steps/s (collection: 0.760s, learning 0.094s)
             Mean action noise std: 2.72
          Mean value_function loss: 128.3461
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6899
                       Mean reward: 504.67
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 0.3955
    Episode_Reward/rotating_object: 98.5255
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.85s
                      Time elapsed: 00:14:34
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 911/1500 [0m                      

                       Computation: 118313 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 2.73
          Mean value_function loss: 118.5038
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.6929
                       Mean reward: 504.83
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.3973
    Episode_Reward/rotating_object: 98.7731
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.83s
                      Time elapsed: 00:14:35
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 912/1500 [0m                      

                       Computation: 110944 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 2.73
          Mean value_function loss: 112.2636
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.6939
                       Mean reward: 522.72
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 100.9845
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.89s
                      Time elapsed: 00:14:36
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 913/1500 [0m                      

                       Computation: 112610 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 2.73
          Mean value_function loss: 109.1115
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.6898
                       Mean reward: 530.27
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.3977
    Episode_Reward/rotating_object: 104.3632
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.87s
                      Time elapsed: 00:14:37
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 914/1500 [0m                      

                       Computation: 113218 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 2.73
          Mean value_function loss: 115.4789
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.6935
                       Mean reward: 528.31
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 101.0950
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.87s
                      Time elapsed: 00:14:37
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 915/1500 [0m                      

                       Computation: 109736 steps/s (collection: 0.776s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 116.6887
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.6944
                       Mean reward: 528.26
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.3927
    Episode_Reward/rotating_object: 100.2937
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.90s
                      Time elapsed: 00:14:38
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 916/1500 [0m                      

                       Computation: 111438 steps/s (collection: 0.743s, learning 0.140s)
             Mean action noise std: 2.74
          Mean value_function loss: 120.4552
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7014
                       Mean reward: 511.76
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 103.0489
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.88s
                      Time elapsed: 00:14:39
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 917/1500 [0m                      

                       Computation: 101631 steps/s (collection: 0.793s, learning 0.175s)
             Mean action noise std: 2.74
          Mean value_function loss: 122.6939
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.7042
                       Mean reward: 538.91
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 103.1951
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.97s
                      Time elapsed: 00:14:40
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 918/1500 [0m                      

                       Computation: 110984 steps/s (collection: 0.767s, learning 0.119s)
             Mean action noise std: 2.74
          Mean value_function loss: 113.1849
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7081
                       Mean reward: 492.19
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.3938
    Episode_Reward/rotating_object: 98.0130
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.89s
                      Time elapsed: 00:14:41
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 919/1500 [0m                      

                       Computation: 113306 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 2.74
          Mean value_function loss: 123.3116
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7078
                       Mean reward: 511.70
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 0.4016
    Episode_Reward/rotating_object: 105.1564
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.87s
                      Time elapsed: 00:14:42
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 920/1500 [0m                      

                       Computation: 111041 steps/s (collection: 0.797s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 120.1375
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7017
                       Mean reward: 514.61
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 0.3951
    Episode_Reward/rotating_object: 102.6665
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.89s
                      Time elapsed: 00:14:43
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 921/1500 [0m                      

                       Computation: 111669 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 126.4425
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7065
                       Mean reward: 528.23
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 105.1115
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.88s
                      Time elapsed: 00:14:44
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 922/1500 [0m                      

                       Computation: 114710 steps/s (collection: 0.748s, learning 0.109s)
             Mean action noise std: 2.74
          Mean value_function loss: 122.1364
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.7095
                       Mean reward: 511.68
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.4034
    Episode_Reward/rotating_object: 103.0791
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.86s
                      Time elapsed: 00:14:45
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 923/1500 [0m                      

                       Computation: 112720 steps/s (collection: 0.787s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 117.2892
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.7159
                       Mean reward: 520.38
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 101.7809
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.87s
                      Time elapsed: 00:14:45
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 924/1500 [0m                      

                       Computation: 108564 steps/s (collection: 0.775s, learning 0.131s)
             Mean action noise std: 2.75
          Mean value_function loss: 111.3823
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7150
                       Mean reward: 520.95
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 105.6602
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.91s
                      Time elapsed: 00:14:46
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 925/1500 [0m                      

                       Computation: 106859 steps/s (collection: 0.762s, learning 0.158s)
             Mean action noise std: 2.75
          Mean value_function loss: 114.9721
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7152
                       Mean reward: 505.78
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.3916
    Episode_Reward/rotating_object: 101.6661
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.92s
                      Time elapsed: 00:14:47
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 926/1500 [0m                      

                       Computation: 111379 steps/s (collection: 0.762s, learning 0.121s)
             Mean action noise std: 2.75
          Mean value_function loss: 103.5158
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7149
                       Mean reward: 545.32
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 0.3992
    Episode_Reward/rotating_object: 103.5858
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.88s
                      Time elapsed: 00:14:48
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 927/1500 [0m                      

                       Computation: 112435 steps/s (collection: 0.764s, learning 0.111s)
             Mean action noise std: 2.75
          Mean value_function loss: 117.7149
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.7137
                       Mean reward: 515.11
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 0.4048
    Episode_Reward/rotating_object: 106.3218
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.87s
                      Time elapsed: 00:14:49
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 928/1500 [0m                      

                       Computation: 117187 steps/s (collection: 0.749s, learning 0.090s)
             Mean action noise std: 2.75
          Mean value_function loss: 111.9616
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7178
                       Mean reward: 516.23
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 0.3967
    Episode_Reward/rotating_object: 103.6547
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.84s
                      Time elapsed: 00:14:50
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 929/1500 [0m                      

                       Computation: 107456 steps/s (collection: 0.802s, learning 0.113s)
             Mean action noise std: 2.75
          Mean value_function loss: 127.5274
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7181
                       Mean reward: 523.69
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 99.4701
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.91s
                      Time elapsed: 00:14:51
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 930/1500 [0m                      

                       Computation: 112376 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 105.2746
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7274
                       Mean reward: 531.13
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.4009
    Episode_Reward/rotating_object: 101.5023
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.87s
                      Time elapsed: 00:14:52
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 931/1500 [0m                      

                       Computation: 109591 steps/s (collection: 0.792s, learning 0.105s)
             Mean action noise std: 2.76
          Mean value_function loss: 94.1420
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.7348
                       Mean reward: 488.94
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.3939
    Episode_Reward/rotating_object: 101.3531
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.90s
                      Time elapsed: 00:14:53
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 932/1500 [0m                      

                       Computation: 99955 steps/s (collection: 0.876s, learning 0.108s)
             Mean action noise std: 2.76
          Mean value_function loss: 115.0294
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7364
                       Mean reward: 497.03
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.4005
    Episode_Reward/rotating_object: 105.6324
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.98s
                      Time elapsed: 00:14:54
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 933/1500 [0m                      

                       Computation: 114792 steps/s (collection: 0.755s, learning 0.101s)
             Mean action noise std: 2.76
          Mean value_function loss: 106.7636
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.7356
                       Mean reward: 510.54
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 0.3989
    Episode_Reward/rotating_object: 108.7375
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.86s
                      Time elapsed: 00:14:54
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 934/1500 [0m                      

                       Computation: 116611 steps/s (collection: 0.736s, learning 0.107s)
             Mean action noise std: 2.76
          Mean value_function loss: 119.4562
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.7425
                       Mean reward: 497.65
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 100.7416
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.84s
                      Time elapsed: 00:14:55
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 935/1500 [0m                      

                       Computation: 112023 steps/s (collection: 0.744s, learning 0.134s)
             Mean action noise std: 2.77
          Mean value_function loss: 114.9673
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7512
                       Mean reward: 523.33
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.3935
    Episode_Reward/rotating_object: 106.0778
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.88s
                      Time elapsed: 00:14:56
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 936/1500 [0m                      

                       Computation: 113841 steps/s (collection: 0.760s, learning 0.104s)
             Mean action noise std: 2.77
          Mean value_function loss: 106.7070
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.7597
                       Mean reward: 504.50
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3952
    Episode_Reward/rotating_object: 104.3863
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.86s
                      Time elapsed: 00:14:57
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 937/1500 [0m                      

                       Computation: 112022 steps/s (collection: 0.771s, learning 0.106s)
             Mean action noise std: 2.77
          Mean value_function loss: 104.6495
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 17.7671
                       Mean reward: 538.41
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 107.2321
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.88s
                      Time elapsed: 00:14:58
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 938/1500 [0m                      

                       Computation: 113387 steps/s (collection: 0.772s, learning 0.095s)
             Mean action noise std: 2.77
          Mean value_function loss: 111.7072
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 17.7660
                       Mean reward: 525.58
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3956
    Episode_Reward/rotating_object: 101.2972
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.87s
                      Time elapsed: 00:14:59
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 939/1500 [0m                      

                       Computation: 111528 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 93.6093
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7634
                       Mean reward: 497.36
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.3983
    Episode_Reward/rotating_object: 102.7643
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.88s
                      Time elapsed: 00:15:00
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 940/1500 [0m                      

                       Computation: 113614 steps/s (collection: 0.761s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 103.0746
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.7647
                       Mean reward: 497.67
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 0.3986
    Episode_Reward/rotating_object: 105.9818
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.87s
                      Time elapsed: 00:15:00
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 941/1500 [0m                      

                       Computation: 112240 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 121.8822
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7732
                       Mean reward: 560.66
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 106.3708
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.88s
                      Time elapsed: 00:15:01
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 942/1500 [0m                      

                       Computation: 111226 steps/s (collection: 0.771s, learning 0.113s)
             Mean action noise std: 2.78
          Mean value_function loss: 111.0082
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7728
                       Mean reward: 559.06
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.3969
    Episode_Reward/rotating_object: 107.6357
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.88s
                      Time elapsed: 00:15:02
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 943/1500 [0m                      

                       Computation: 108166 steps/s (collection: 0.776s, learning 0.133s)
             Mean action noise std: 2.78
          Mean value_function loss: 105.8174
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7770
                       Mean reward: 544.78
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.4007
    Episode_Reward/rotating_object: 105.8576
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.91s
                      Time elapsed: 00:15:03
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 944/1500 [0m                      

                       Computation: 108677 steps/s (collection: 0.790s, learning 0.115s)
             Mean action noise std: 2.79
          Mean value_function loss: 114.4080
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7821
                       Mean reward: 533.06
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 103.6162
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.90s
                      Time elapsed: 00:15:04
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 945/1500 [0m                      

                       Computation: 116378 steps/s (collection: 0.752s, learning 0.092s)
             Mean action noise std: 2.79
          Mean value_function loss: 103.3829
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 17.7956
                       Mean reward: 510.43
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 103.2197
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.84s
                      Time elapsed: 00:15:05
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 946/1500 [0m                      

                       Computation: 113811 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 2.79
          Mean value_function loss: 96.5700
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.7970
                       Mean reward: 545.67
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.4027
    Episode_Reward/rotating_object: 109.3909
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.86s
                      Time elapsed: 00:15:06
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 947/1500 [0m                      

                       Computation: 112438 steps/s (collection: 0.774s, learning 0.101s)
             Mean action noise std: 2.79
          Mean value_function loss: 100.9205
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7924
                       Mean reward: 527.57
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.4032
    Episode_Reward/rotating_object: 109.2277
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.87s
                      Time elapsed: 00:15:07
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 948/1500 [0m                      

                       Computation: 118395 steps/s (collection: 0.743s, learning 0.088s)
             Mean action noise std: 2.79
          Mean value_function loss: 89.4599
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.7985
                       Mean reward: 524.51
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 107.6016
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.83s
                      Time elapsed: 00:15:07
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 949/1500 [0m                      

                       Computation: 113164 steps/s (collection: 0.766s, learning 0.103s)
             Mean action noise std: 2.80
          Mean value_function loss: 92.9748
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8026
                       Mean reward: 559.48
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.4052
    Episode_Reward/rotating_object: 106.8055
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.87s
                      Time elapsed: 00:15:08
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 950/1500 [0m                      

                       Computation: 120105 steps/s (collection: 0.730s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 90.1666
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.7976
                       Mean reward: 510.34
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 103.7573
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.82s
                      Time elapsed: 00:15:09
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 951/1500 [0m                      

                       Computation: 112002 steps/s (collection: 0.731s, learning 0.147s)
             Mean action noise std: 2.80
          Mean value_function loss: 91.3947
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.8059
                       Mean reward: 528.04
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 106.1388
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.88s
                      Time elapsed: 00:15:10
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 952/1500 [0m                      

                       Computation: 113805 steps/s (collection: 0.765s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 101.4757
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.8103
                       Mean reward: 556.93
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 108.0244
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.86s
                      Time elapsed: 00:15:11
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 953/1500 [0m                      

                       Computation: 113166 steps/s (collection: 0.764s, learning 0.105s)
             Mean action noise std: 2.80
          Mean value_function loss: 100.3476
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.8105
                       Mean reward: 548.70
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 107.9924
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.87s
                      Time elapsed: 00:15:12
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 954/1500 [0m                      

                       Computation: 108238 steps/s (collection: 0.790s, learning 0.118s)
             Mean action noise std: 2.81
          Mean value_function loss: 95.4551
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8167
                       Mean reward: 527.68
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 105.1900
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.91s
                      Time elapsed: 00:15:13
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 955/1500 [0m                      

                       Computation: 114221 steps/s (collection: 0.760s, learning 0.101s)
             Mean action noise std: 2.81
          Mean value_function loss: 100.9639
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.8186
                       Mean reward: 561.50
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.4071
    Episode_Reward/rotating_object: 109.0089
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.86s
                      Time elapsed: 00:15:14
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 956/1500 [0m                      

                       Computation: 107572 steps/s (collection: 0.788s, learning 0.126s)
             Mean action noise std: 2.81
          Mean value_function loss: 100.7406
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.8207
                       Mean reward: 550.65
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 109.6463
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.91s
                      Time elapsed: 00:15:14
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 957/1500 [0m                      

                       Computation: 107073 steps/s (collection: 0.789s, learning 0.129s)
             Mean action noise std: 2.81
          Mean value_function loss: 110.7065
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.8208
                       Mean reward: 534.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4004
    Episode_Reward/rotating_object: 103.6907
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.92s
                      Time elapsed: 00:15:15
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 958/1500 [0m                      

                       Computation: 103522 steps/s (collection: 0.780s, learning 0.170s)
             Mean action noise std: 2.81
          Mean value_function loss: 93.7255
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8183
                       Mean reward: 463.64
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 0.4000
    Episode_Reward/rotating_object: 103.7739
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.95s
                      Time elapsed: 00:15:16
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 959/1500 [0m                      

                       Computation: 112577 steps/s (collection: 0.760s, learning 0.114s)
             Mean action noise std: 2.82
          Mean value_function loss: 96.2397
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8271
                       Mean reward: 546.61
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 108.8221
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.87s
                      Time elapsed: 00:15:17
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 960/1500 [0m                      

                       Computation: 111150 steps/s (collection: 0.759s, learning 0.125s)
             Mean action noise std: 2.82
          Mean value_function loss: 95.8925
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.8328
                       Mean reward: 572.76
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.4014
    Episode_Reward/rotating_object: 105.7607
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.88s
                      Time elapsed: 00:15:18
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 961/1500 [0m                      

                       Computation: 110240 steps/s (collection: 0.773s, learning 0.118s)
             Mean action noise std: 2.82
          Mean value_function loss: 91.1134
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8363
                       Mean reward: 583.49
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.4046
    Episode_Reward/rotating_object: 111.9552
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.89s
                      Time elapsed: 00:15:19
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 962/1500 [0m                      

                       Computation: 116906 steps/s (collection: 0.741s, learning 0.100s)
             Mean action noise std: 2.82
          Mean value_function loss: 86.9258
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.8456
                       Mean reward: 559.14
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.4042
    Episode_Reward/rotating_object: 105.1082
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.84s
                      Time elapsed: 00:15:20
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 963/1500 [0m                      

                       Computation: 114640 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 2.82
          Mean value_function loss: 102.3612
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.8517
                       Mean reward: 560.86
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.4022
    Episode_Reward/rotating_object: 111.6955
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.86s
                      Time elapsed: 00:15:21
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 964/1500 [0m                      

                       Computation: 107635 steps/s (collection: 0.804s, learning 0.110s)
             Mean action noise std: 2.83
          Mean value_function loss: 91.3773
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.8607
                       Mean reward: 606.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4058
    Episode_Reward/rotating_object: 113.3685
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.91s
                      Time elapsed: 00:15:22
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 965/1500 [0m                      

                       Computation: 112929 steps/s (collection: 0.756s, learning 0.115s)
             Mean action noise std: 2.83
          Mean value_function loss: 92.0001
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8708
                       Mean reward: 547.02
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 109.6365
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.87s
                      Time elapsed: 00:15:22
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 966/1500 [0m                      

                       Computation: 109671 steps/s (collection: 0.770s, learning 0.127s)
             Mean action noise std: 2.83
          Mean value_function loss: 90.0626
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8763
                       Mean reward: 473.07
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 102.6461
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.90s
                      Time elapsed: 00:15:23
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 967/1500 [0m                      

                       Computation: 116589 steps/s (collection: 0.757s, learning 0.087s)
             Mean action noise std: 2.83
          Mean value_function loss: 98.1052
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.8786
                       Mean reward: 540.31
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 110.3997
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.84s
                      Time elapsed: 00:15:24
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 968/1500 [0m                      

                       Computation: 111712 steps/s (collection: 0.731s, learning 0.149s)
             Mean action noise std: 2.83
          Mean value_function loss: 89.5285
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8742
                       Mean reward: 545.03
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 108.8673
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.88s
                      Time elapsed: 00:15:25
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 969/1500 [0m                      

                       Computation: 110243 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 2.84
          Mean value_function loss: 97.0256
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8755
                       Mean reward: 543.09
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 0.3986
    Episode_Reward/rotating_object: 109.6348
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.89s
                      Time elapsed: 00:15:26
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 970/1500 [0m                      

                       Computation: 115730 steps/s (collection: 0.751s, learning 0.099s)
             Mean action noise std: 2.84
          Mean value_function loss: 104.6123
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8795
                       Mean reward: 524.72
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.4047
    Episode_Reward/rotating_object: 112.2613
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.85s
                      Time elapsed: 00:15:27
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 971/1500 [0m                      

                       Computation: 107954 steps/s (collection: 0.805s, learning 0.106s)
             Mean action noise std: 2.84
          Mean value_function loss: 111.5051
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.8759
                       Mean reward: 526.29
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.4002
    Episode_Reward/rotating_object: 104.2524
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.91s
                      Time elapsed: 00:15:28
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 972/1500 [0m                      

                       Computation: 109288 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 102.3746
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8746
                       Mean reward: 581.11
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.4052
    Episode_Reward/rotating_object: 108.8032
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.90s
                      Time elapsed: 00:15:29
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 973/1500 [0m                      

                       Computation: 111468 steps/s (collection: 0.770s, learning 0.112s)
             Mean action noise std: 2.84
          Mean value_function loss: 102.3943
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.8737
                       Mean reward: 540.22
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 107.0537
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.88s
                      Time elapsed: 00:15:30
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 974/1500 [0m                      

                       Computation: 112468 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 2.84
          Mean value_function loss: 118.5466
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.8759
                       Mean reward: 530.13
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 0.4018
    Episode_Reward/rotating_object: 107.9361
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.87s
                      Time elapsed: 00:15:30
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 975/1500 [0m                      

                       Computation: 106413 steps/s (collection: 0.779s, learning 0.145s)
             Mean action noise std: 2.85
          Mean value_function loss: 103.3801
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.8866
                       Mean reward: 540.61
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.4040
    Episode_Reward/rotating_object: 106.8983
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.92s
                      Time elapsed: 00:15:31
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 976/1500 [0m                      

                       Computation: 112760 steps/s (collection: 0.748s, learning 0.124s)
             Mean action noise std: 2.85
          Mean value_function loss: 101.4122
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8999
                       Mean reward: 578.84
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.4034
    Episode_Reward/rotating_object: 105.1922
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.87s
                      Time elapsed: 00:15:32
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 977/1500 [0m                      

                       Computation: 113075 steps/s (collection: 0.759s, learning 0.111s)
             Mean action noise std: 2.86
          Mean value_function loss: 101.9157
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 17.9112
                       Mean reward: 555.68
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 109.2518
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.87s
                      Time elapsed: 00:15:33
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 978/1500 [0m                      

                       Computation: 111391 steps/s (collection: 0.789s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 95.3416
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 17.9183
                       Mean reward: 493.01
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.3998
    Episode_Reward/rotating_object: 105.1457
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.88s
                      Time elapsed: 00:15:34
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 979/1500 [0m                      

                       Computation: 113261 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 2.86
          Mean value_function loss: 98.1913
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.9147
                       Mean reward: 533.19
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.4040
    Episode_Reward/rotating_object: 111.0609
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.87s
                      Time elapsed: 00:15:35
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 980/1500 [0m                      

                       Computation: 110217 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 2.86
          Mean value_function loss: 95.2802
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.9082
                       Mean reward: 537.03
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 109.7442
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.89s
                      Time elapsed: 00:15:36
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 981/1500 [0m                      

                       Computation: 111303 steps/s (collection: 0.766s, learning 0.117s)
             Mean action noise std: 2.86
          Mean value_function loss: 102.2687
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 17.9070
                       Mean reward: 547.59
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.4028
    Episode_Reward/rotating_object: 109.5214
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.88s
                      Time elapsed: 00:15:37
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 982/1500 [0m                      

                       Computation: 110213 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 2.86
          Mean value_function loss: 104.3290
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9060
                       Mean reward: 529.41
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 110.8504
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.89s
                      Time elapsed: 00:15:37
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 983/1500 [0m                      

                       Computation: 110055 steps/s (collection: 0.787s, learning 0.106s)
             Mean action noise std: 2.86
          Mean value_function loss: 92.5336
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 17.9040
                       Mean reward: 589.24
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.4017
    Episode_Reward/rotating_object: 108.3081
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.89s
                      Time elapsed: 00:15:38
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 984/1500 [0m                      

                       Computation: 105710 steps/s (collection: 0.789s, learning 0.141s)
             Mean action noise std: 2.87
          Mean value_function loss: 83.9015
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9163
                       Mean reward: 563.24
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.4003
    Episode_Reward/rotating_object: 106.4875
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.93s
                      Time elapsed: 00:15:39
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 985/1500 [0m                      

                       Computation: 107935 steps/s (collection: 0.764s, learning 0.147s)
             Mean action noise std: 2.87
          Mean value_function loss: 89.6572
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.9255
                       Mean reward: 567.03
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.4027
    Episode_Reward/rotating_object: 108.3760
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.91s
                      Time elapsed: 00:15:40
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 986/1500 [0m                      

                       Computation: 113737 steps/s (collection: 0.766s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 82.3925
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.9278
                       Mean reward: 592.39
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.4051
    Episode_Reward/rotating_object: 113.3797
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.86s
                      Time elapsed: 00:15:41
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 987/1500 [0m                      

                       Computation: 114294 steps/s (collection: 0.761s, learning 0.099s)
             Mean action noise std: 2.87
          Mean value_function loss: 93.7758
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9212
                       Mean reward: 505.18
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.3990
    Episode_Reward/rotating_object: 108.6231
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.86s
                      Time elapsed: 00:15:42
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 988/1500 [0m                      

                       Computation: 110158 steps/s (collection: 0.790s, learning 0.103s)
             Mean action noise std: 2.87
          Mean value_function loss: 89.8900
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9152
                       Mean reward: 522.36
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.4016
    Episode_Reward/rotating_object: 107.8927
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.89s
                      Time elapsed: 00:15:43
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 989/1500 [0m                      

                       Computation: 112204 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 2.88
          Mean value_function loss: 86.3679
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 17.9199
                       Mean reward: 564.01
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.3976
    Episode_Reward/rotating_object: 107.4232
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.88s
                      Time elapsed: 00:15:44
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 990/1500 [0m                      

                       Computation: 110274 steps/s (collection: 0.779s, learning 0.113s)
             Mean action noise std: 2.88
          Mean value_function loss: 87.8061
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.9223
                       Mean reward: 558.06
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.4052
    Episode_Reward/rotating_object: 110.8760
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.89s
                      Time elapsed: 00:15:45
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 991/1500 [0m                      

                       Computation: 111403 steps/s (collection: 0.789s, learning 0.093s)
             Mean action noise std: 2.88
          Mean value_function loss: 85.3082
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9280
                       Mean reward: 538.88
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 108.7817
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.88s
                      Time elapsed: 00:15:45
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 992/1500 [0m                      

                       Computation: 112824 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 2.88
          Mean value_function loss: 92.3457
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.9301
                       Mean reward: 544.13
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.4041
    Episode_Reward/rotating_object: 109.4324
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.87s
                      Time elapsed: 00:15:46
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 993/1500 [0m                      

                       Computation: 105910 steps/s (collection: 0.794s, learning 0.134s)
             Mean action noise std: 2.89
          Mean value_function loss: 91.3892
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9415
                       Mean reward: 531.85
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4045
    Episode_Reward/rotating_object: 110.1448
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.93s
                      Time elapsed: 00:15:47
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 994/1500 [0m                      

                       Computation: 105215 steps/s (collection: 0.814s, learning 0.121s)
             Mean action noise std: 2.89
          Mean value_function loss: 98.0889
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.9488
                       Mean reward: 532.14
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 108.4255
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.93s
                      Time elapsed: 00:15:48
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 995/1500 [0m                      

                       Computation: 108587 steps/s (collection: 0.765s, learning 0.141s)
             Mean action noise std: 2.89
          Mean value_function loss: 89.2074
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.9535
                       Mean reward: 490.71
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 103.6351
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.91s
                      Time elapsed: 00:15:49
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 996/1500 [0m                      

                       Computation: 104726 steps/s (collection: 0.791s, learning 0.148s)
             Mean action noise std: 2.89
          Mean value_function loss: 97.0497
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9546
                       Mean reward: 535.32
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 106.7249
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.94s
                      Time elapsed: 00:15:50
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 997/1500 [0m                      

                       Computation: 112272 steps/s (collection: 0.777s, learning 0.098s)
             Mean action noise std: 2.89
          Mean value_function loss: 94.2990
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.9605
                       Mean reward: 528.69
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 110.7249
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.88s
                      Time elapsed: 00:15:51
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 998/1500 [0m                      

                       Computation: 105526 steps/s (collection: 0.821s, learning 0.111s)
             Mean action noise std: 2.89
          Mean value_function loss: 99.5382
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.9683
                       Mean reward: 543.20
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.4084
    Episode_Reward/rotating_object: 111.1429
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.93s
                      Time elapsed: 00:15:52
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 999/1500 [0m                      

                       Computation: 95806 steps/s (collection: 0.858s, learning 0.168s)
             Mean action noise std: 2.90
          Mean value_function loss: 85.3694
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.9710
                       Mean reward: 557.65
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.4039
    Episode_Reward/rotating_object: 107.8175
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 1.03s
                      Time elapsed: 00:15:53
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1000/1500 [0m                     

                       Computation: 35073 steps/s (collection: 2.695s, learning 0.108s)
             Mean action noise std: 2.90
          Mean value_function loss: 94.7438
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.9715
                       Mean reward: 596.09
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.4077
    Episode_Reward/rotating_object: 113.5917
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.80s
                      Time elapsed: 00:15:56
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1001/1500 [0m                     

                       Computation: 32864 steps/s (collection: 2.859s, learning 0.132s)
             Mean action noise std: 2.90
          Mean value_function loss: 95.2849
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.9726
                       Mean reward: 587.98
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.4072
    Episode_Reward/rotating_object: 109.2436
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 2.99s
                      Time elapsed: 00:15:59
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1002/1500 [0m                     

                       Computation: 32620 steps/s (collection: 2.888s, learning 0.126s)
             Mean action noise std: 2.90
          Mean value_function loss: 94.5343
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9810
                       Mean reward: 527.68
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.4043
    Episode_Reward/rotating_object: 107.7821
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.01s
                      Time elapsed: 00:16:02
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1003/1500 [0m                     

                       Computation: 32492 steps/s (collection: 2.898s, learning 0.127s)
             Mean action noise std: 2.90
          Mean value_function loss: 84.2500
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9798
                       Mean reward: 510.08
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.4043
    Episode_Reward/rotating_object: 106.3600
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.03s
                      Time elapsed: 00:16:05
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1004/1500 [0m                     

                       Computation: 32431 steps/s (collection: 2.896s, learning 0.135s)
             Mean action noise std: 2.90
          Mean value_function loss: 91.6552
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9764
                       Mean reward: 538.01
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 108.8403
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.03s
                      Time elapsed: 00:16:08
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1005/1500 [0m                     

                       Computation: 32967 steps/s (collection: 2.863s, learning 0.119s)
             Mean action noise std: 2.91
          Mean value_function loss: 95.3681
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 17.9784
                       Mean reward: 540.51
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.4121
    Episode_Reward/rotating_object: 110.2015
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 2.98s
                      Time elapsed: 00:16:11
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1006/1500 [0m                     

                       Computation: 33052 steps/s (collection: 2.851s, learning 0.123s)
             Mean action noise std: 2.91
          Mean value_function loss: 97.4255
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.9807
                       Mean reward: 566.00
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4132
    Episode_Reward/rotating_object: 113.7938
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 2.97s
                      Time elapsed: 00:16:14
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1007/1500 [0m                     

                       Computation: 31994 steps/s (collection: 2.952s, learning 0.121s)
             Mean action noise std: 2.91
          Mean value_function loss: 87.7403
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.9780
                       Mean reward: 547.88
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.4114
    Episode_Reward/rotating_object: 111.6913
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.07s
                      Time elapsed: 00:16:17
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1008/1500 [0m                     

                       Computation: 24942 steps/s (collection: 3.840s, learning 0.102s)
             Mean action noise std: 2.91
          Mean value_function loss: 92.3544
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9817
                       Mean reward: 581.19
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4122
    Episode_Reward/rotating_object: 111.2088
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.94s
                      Time elapsed: 00:16:21
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1009/1500 [0m                     

                       Computation: 114022 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 2.91
          Mean value_function loss: 93.5931
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.9832
                       Mean reward: 557.85
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.4083
    Episode_Reward/rotating_object: 109.2842
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.86s
                      Time elapsed: 00:16:22
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1010/1500 [0m                     

                       Computation: 104549 steps/s (collection: 0.740s, learning 0.200s)
             Mean action noise std: 2.91
          Mean value_function loss: 83.7176
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.9854
                       Mean reward: 604.89
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4074
    Episode_Reward/rotating_object: 114.4675
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.94s
                      Time elapsed: 00:16:23
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1011/1500 [0m                     

                       Computation: 112715 steps/s (collection: 0.752s, learning 0.121s)
             Mean action noise std: 2.91
          Mean value_function loss: 106.3910
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9820
                       Mean reward: 555.53
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.4107
    Episode_Reward/rotating_object: 110.3201
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.87s
                      Time elapsed: 00:16:23
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1012/1500 [0m                     

                       Computation: 116186 steps/s (collection: 0.716s, learning 0.131s)
             Mean action noise std: 2.92
          Mean value_function loss: 96.1969
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.9827
                       Mean reward: 552.87
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.4091
    Episode_Reward/rotating_object: 113.8552
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.85s
                      Time elapsed: 00:16:24
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1013/1500 [0m                     

                       Computation: 114948 steps/s (collection: 0.761s, learning 0.094s)
             Mean action noise std: 2.92
          Mean value_function loss: 108.0583
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.9861
                       Mean reward: 522.59
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 0.4077
    Episode_Reward/rotating_object: 109.6045
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.86s
                      Time elapsed: 00:16:25
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1014/1500 [0m                     

                       Computation: 111541 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 2.92
          Mean value_function loss: 103.0556
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9790
                       Mean reward: 546.73
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.4056
    Episode_Reward/rotating_object: 107.3443
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.88s
                      Time elapsed: 00:16:26
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1015/1500 [0m                     

                       Computation: 107397 steps/s (collection: 0.813s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 89.9728
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.9816
                       Mean reward: 547.89
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.4078
    Episode_Reward/rotating_object: 108.3049
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.92s
                      Time elapsed: 00:16:27
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1016/1500 [0m                     

                       Computation: 103080 steps/s (collection: 0.797s, learning 0.157s)
             Mean action noise std: 2.92
          Mean value_function loss: 94.0821
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.9836
                       Mean reward: 572.87
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.4081
    Episode_Reward/rotating_object: 109.9170
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.95s
                      Time elapsed: 00:16:28
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1017/1500 [0m                     

                       Computation: 91017 steps/s (collection: 0.926s, learning 0.154s)
             Mean action noise std: 2.92
          Mean value_function loss: 101.8896
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9850
                       Mean reward: 531.33
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 111.9195
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.08s
                      Time elapsed: 00:16:29
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1018/1500 [0m                     

                       Computation: 102896 steps/s (collection: 0.853s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 94.0752
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.9903
                       Mean reward: 568.91
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.4089
    Episode_Reward/rotating_object: 108.6979
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.96s
                      Time elapsed: 00:16:30
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1019/1500 [0m                     

                       Computation: 115157 steps/s (collection: 0.742s, learning 0.112s)
             Mean action noise std: 2.93
          Mean value_function loss: 88.4888
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9911
                       Mean reward: 589.97
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.4091
    Episode_Reward/rotating_object: 109.6624
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.85s
                      Time elapsed: 00:16:31
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1020/1500 [0m                     

                       Computation: 109926 steps/s (collection: 0.791s, learning 0.104s)
             Mean action noise std: 2.93
          Mean value_function loss: 87.4759
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.9910
                       Mean reward: 549.36
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.4143
    Episode_Reward/rotating_object: 115.4476
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.89s
                      Time elapsed: 00:16:32
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1021/1500 [0m                     

                       Computation: 114534 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 2.93
          Mean value_function loss: 94.7338
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.9945
                       Mean reward: 563.55
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.4050
    Episode_Reward/rotating_object: 109.5395
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.86s
                      Time elapsed: 00:16:32
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1022/1500 [0m                     

                       Computation: 105262 steps/s (collection: 0.768s, learning 0.166s)
             Mean action noise std: 2.94
          Mean value_function loss: 95.3946
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.9989
                       Mean reward: 516.90
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.4067
    Episode_Reward/rotating_object: 109.8789
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.93s
                      Time elapsed: 00:16:33
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1023/1500 [0m                     

                       Computation: 112311 steps/s (collection: 0.767s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 85.4755
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.9951
                       Mean reward: 519.66
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.4010
    Episode_Reward/rotating_object: 109.0930
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.88s
                      Time elapsed: 00:16:34
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1024/1500 [0m                     

                       Computation: 107453 steps/s (collection: 0.764s, learning 0.151s)
             Mean action noise std: 2.94
          Mean value_function loss: 99.2383
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9916
                       Mean reward: 562.26
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.4088
    Episode_Reward/rotating_object: 111.1180
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.91s
                      Time elapsed: 00:16:35
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1025/1500 [0m                     

                       Computation: 112198 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 2.94
          Mean value_function loss: 94.6702
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9856
                       Mean reward: 560.40
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.4055
    Episode_Reward/rotating_object: 109.8691
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.88s
                      Time elapsed: 00:16:36
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1026/1500 [0m                     

                       Computation: 107936 steps/s (collection: 0.801s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 99.9040
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.9810
                       Mean reward: 520.55
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.4114
    Episode_Reward/rotating_object: 110.4872
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.91s
                      Time elapsed: 00:16:37
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1027/1500 [0m                     

                       Computation: 108132 steps/s (collection: 0.820s, learning 0.089s)
             Mean action noise std: 2.94
          Mean value_function loss: 87.4439
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9873
                       Mean reward: 573.14
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.4049
    Episode_Reward/rotating_object: 109.5012
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.91s
                      Time elapsed: 00:16:38
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1028/1500 [0m                     

                       Computation: 115066 steps/s (collection: 0.760s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 99.6766
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.9983
                       Mean reward: 552.96
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.4057
    Episode_Reward/rotating_object: 111.6457
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.85s
                      Time elapsed: 00:16:39
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1029/1500 [0m                     

                       Computation: 111855 steps/s (collection: 0.757s, learning 0.122s)
             Mean action noise std: 2.95
          Mean value_function loss: 83.6020
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.0125
                       Mean reward: 540.47
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 0.4113
    Episode_Reward/rotating_object: 113.1346
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.88s
                      Time elapsed: 00:16:40
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1030/1500 [0m                     

                       Computation: 115831 steps/s (collection: 0.741s, learning 0.108s)
             Mean action noise std: 2.95
          Mean value_function loss: 87.0846
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0134
                       Mean reward: 576.83
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4116
    Episode_Reward/rotating_object: 111.9988
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.85s
                      Time elapsed: 00:16:40
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1031/1500 [0m                     

                       Computation: 117239 steps/s (collection: 0.740s, learning 0.098s)
             Mean action noise std: 2.95
          Mean value_function loss: 80.6752
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.0099
                       Mean reward: 550.85
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.4106
    Episode_Reward/rotating_object: 110.4542
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.84s
                      Time elapsed: 00:16:41
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1032/1500 [0m                     

                       Computation: 114403 steps/s (collection: 0.756s, learning 0.103s)
             Mean action noise std: 2.95
          Mean value_function loss: 83.3747
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.0234
                       Mean reward: 600.42
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.4111
    Episode_Reward/rotating_object: 118.0165
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.86s
                      Time elapsed: 00:16:42
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1033/1500 [0m                     

                       Computation: 114197 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 2.95
          Mean value_function loss: 81.8521
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0276
                       Mean reward: 556.37
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 113.7159
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.86s
                      Time elapsed: 00:16:43
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1034/1500 [0m                     

                       Computation: 116382 steps/s (collection: 0.756s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 95.8392
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0277
                       Mean reward: 575.49
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.4042
    Episode_Reward/rotating_object: 109.1648
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.84s
                      Time elapsed: 00:16:44
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1035/1500 [0m                     

                       Computation: 112953 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 2.96
          Mean value_function loss: 98.2353
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.0267
                       Mean reward: 535.41
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.4092
    Episode_Reward/rotating_object: 109.7813
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.87s
                      Time elapsed: 00:16:45
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1036/1500 [0m                     

                       Computation: 113733 steps/s (collection: 0.744s, learning 0.120s)
             Mean action noise std: 2.96
          Mean value_function loss: 96.4536
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0303
                       Mean reward: 561.95
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.4123
    Episode_Reward/rotating_object: 113.2408
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.86s
                      Time elapsed: 00:16:46
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1037/1500 [0m                     

                       Computation: 110870 steps/s (collection: 0.769s, learning 0.118s)
             Mean action noise std: 2.96
          Mean value_function loss: 99.3111
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 18.0379
                       Mean reward: 537.72
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 0.4064
    Episode_Reward/rotating_object: 111.3759
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.89s
                      Time elapsed: 00:16:47
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1038/1500 [0m                     

                       Computation: 113495 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 2.96
          Mean value_function loss: 101.6712
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.0360
                       Mean reward: 574.90
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 0.4154
    Episode_Reward/rotating_object: 112.6302
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.87s
                      Time elapsed: 00:16:47
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1039/1500 [0m                     

                       Computation: 115809 steps/s (collection: 0.754s, learning 0.095s)
             Mean action noise std: 2.96
          Mean value_function loss: 115.0978
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0293
                       Mean reward: 574.94
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.4124
    Episode_Reward/rotating_object: 112.1516
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.85s
                      Time elapsed: 00:16:48
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1040/1500 [0m                     

                       Computation: 112056 steps/s (collection: 0.765s, learning 0.112s)
             Mean action noise std: 2.96
          Mean value_function loss: 114.2496
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0192
                       Mean reward: 603.84
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.4147
    Episode_Reward/rotating_object: 113.5227
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.88s
                      Time elapsed: 00:16:49
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1041/1500 [0m                     

                       Computation: 116054 steps/s (collection: 0.758s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 95.3620
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0157
                       Mean reward: 524.08
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.4134
    Episode_Reward/rotating_object: 110.8805
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.85s
                      Time elapsed: 00:16:50
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1042/1500 [0m                     

                       Computation: 115341 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 2.97
          Mean value_function loss: 93.8847
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.0172
                       Mean reward: 587.11
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.4090
    Episode_Reward/rotating_object: 110.5379
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.85s
                      Time elapsed: 00:16:51
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1043/1500 [0m                     

                       Computation: 114287 steps/s (collection: 0.763s, learning 0.097s)
             Mean action noise std: 2.97
          Mean value_function loss: 106.9667
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.0123
                       Mean reward: 566.49
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.4101
    Episode_Reward/rotating_object: 109.5657
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.86s
                      Time elapsed: 00:16:52
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1044/1500 [0m                     

                       Computation: 104517 steps/s (collection: 0.799s, learning 0.142s)
             Mean action noise std: 2.97
          Mean value_function loss: 103.1995
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 18.0025
                       Mean reward: 573.61
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 0.4122
    Episode_Reward/rotating_object: 112.2301
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.94s
                      Time elapsed: 00:16:53
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1045/1500 [0m                     

                       Computation: 106259 steps/s (collection: 0.796s, learning 0.130s)
             Mean action noise std: 2.97
          Mean value_function loss: 106.8572
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.0042
                       Mean reward: 505.04
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.4146
    Episode_Reward/rotating_object: 111.5744
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.93s
                      Time elapsed: 00:16:54
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1046/1500 [0m                     

                       Computation: 118664 steps/s (collection: 0.742s, learning 0.087s)
             Mean action noise std: 2.97
          Mean value_function loss: 108.9414
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 18.0106
                       Mean reward: 548.99
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.4163
    Episode_Reward/rotating_object: 112.2327
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.83s
                      Time elapsed: 00:16:54
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1047/1500 [0m                     

                       Computation: 118639 steps/s (collection: 0.729s, learning 0.100s)
             Mean action noise std: 2.97
          Mean value_function loss: 102.5330
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.0118
                       Mean reward: 529.91
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 0.4094
    Episode_Reward/rotating_object: 106.3488
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.83s
                      Time elapsed: 00:16:55
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1048/1500 [0m                     

                       Computation: 116119 steps/s (collection: 0.753s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 90.7816
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0097
                       Mean reward: 586.86
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 112.0565
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.85s
                      Time elapsed: 00:16:56
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1049/1500 [0m                     

                       Computation: 114902 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 99.4143
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.0135
                       Mean reward: 572.58
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.4167
    Episode_Reward/rotating_object: 114.2528
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.86s
                      Time elapsed: 00:16:57
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1050/1500 [0m                     

                       Computation: 118609 steps/s (collection: 0.735s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 92.0849
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.0161
                       Mean reward: 604.90
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.4141
    Episode_Reward/rotating_object: 114.4004
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.83s
                      Time elapsed: 00:16:58
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1051/1500 [0m                     

                       Computation: 111849 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 2.98
          Mean value_function loss: 89.2129
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.0167
                       Mean reward: 570.45
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.4057
    Episode_Reward/rotating_object: 110.7838
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.88s
                      Time elapsed: 00:16:59
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1052/1500 [0m                     

                       Computation: 113125 steps/s (collection: 0.765s, learning 0.104s)
             Mean action noise std: 2.98
          Mean value_function loss: 99.6660
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.0199
                       Mean reward: 546.64
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.4121
    Episode_Reward/rotating_object: 111.1194
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.87s
                      Time elapsed: 00:16:59
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1053/1500 [0m                     

                       Computation: 114066 steps/s (collection: 0.751s, learning 0.111s)
             Mean action noise std: 2.99
          Mean value_function loss: 90.1307
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0248
                       Mean reward: 588.69
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 113.0673
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.86s
                      Time elapsed: 00:17:00
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1054/1500 [0m                     

                       Computation: 115937 steps/s (collection: 0.728s, learning 0.120s)
             Mean action noise std: 2.99
          Mean value_function loss: 104.1469
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0270
                       Mean reward: 546.04
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 110.2281
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.85s
                      Time elapsed: 00:17:01
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1055/1500 [0m                     

                       Computation: 115241 steps/s (collection: 0.759s, learning 0.094s)
             Mean action noise std: 2.99
          Mean value_function loss: 94.1169
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.0279
                       Mean reward: 585.92
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 115.3039
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.85s
                      Time elapsed: 00:17:02
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1056/1500 [0m                     

                       Computation: 111027 steps/s (collection: 0.798s, learning 0.087s)
             Mean action noise std: 2.99
          Mean value_function loss: 103.2407
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0274
                       Mean reward: 573.90
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.4132
    Episode_Reward/rotating_object: 114.5300
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.89s
                      Time elapsed: 00:17:03
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1057/1500 [0m                     

                       Computation: 113588 steps/s (collection: 0.753s, learning 0.112s)
             Mean action noise std: 2.99
          Mean value_function loss: 102.6378
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.0284
                       Mean reward: 550.77
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.4192
    Episode_Reward/rotating_object: 113.4429
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.87s
                      Time elapsed: 00:17:04
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1058/1500 [0m                     

                       Computation: 114279 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 90.2602
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0314
                       Mean reward: 568.58
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 110.4283
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.86s
                      Time elapsed: 00:17:05
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1059/1500 [0m                     

                       Computation: 117681 steps/s (collection: 0.749s, learning 0.087s)
             Mean action noise std: 2.99
          Mean value_function loss: 98.0762
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.0285
                       Mean reward: 574.36
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.4194
    Episode_Reward/rotating_object: 116.9229
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.84s
                      Time elapsed: 00:17:05
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1060/1500 [0m                     

                       Computation: 115957 steps/s (collection: 0.728s, learning 0.120s)
             Mean action noise std: 3.00
          Mean value_function loss: 94.6053
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0270
                       Mean reward: 585.87
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.4151
    Episode_Reward/rotating_object: 114.9478
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.85s
                      Time elapsed: 00:17:06
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1061/1500 [0m                     

                       Computation: 115188 steps/s (collection: 0.751s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 92.9381
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.0327
                       Mean reward: 547.85
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.4147
    Episode_Reward/rotating_object: 115.2492
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.85s
                      Time elapsed: 00:17:07
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1062/1500 [0m                     

                       Computation: 118294 steps/s (collection: 0.733s, learning 0.098s)
             Mean action noise std: 3.00
          Mean value_function loss: 80.7989
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.0396
                       Mean reward: 569.64
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.4162
    Episode_Reward/rotating_object: 116.0809
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.83s
                      Time elapsed: 00:17:08
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1063/1500 [0m                     

                       Computation: 112713 steps/s (collection: 0.760s, learning 0.112s)
             Mean action noise std: 3.00
          Mean value_function loss: 89.4125
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.0422
                       Mean reward: 565.02
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 115.3245
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.87s
                      Time elapsed: 00:17:09
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1064/1500 [0m                     

                       Computation: 116389 steps/s (collection: 0.745s, learning 0.100s)
             Mean action noise std: 3.00
          Mean value_function loss: 82.9815
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0428
                       Mean reward: 556.66
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.4094
    Episode_Reward/rotating_object: 109.1986
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.84s
                      Time elapsed: 00:17:10
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1065/1500 [0m                     

                       Computation: 109796 steps/s (collection: 0.801s, learning 0.095s)
             Mean action noise std: 3.00
          Mean value_function loss: 90.5690
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 18.0497
                       Mean reward: 555.54
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.4115
    Episode_Reward/rotating_object: 113.0374
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.90s
                      Time elapsed: 00:17:11
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1066/1500 [0m                     

                       Computation: 110946 steps/s (collection: 0.764s, learning 0.122s)
             Mean action noise std: 3.00
          Mean value_function loss: 85.9772
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.0490
                       Mean reward: 596.04
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.4099
    Episode_Reward/rotating_object: 113.9710
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.89s
                      Time elapsed: 00:17:11
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1067/1500 [0m                     

                       Computation: 108238 steps/s (collection: 0.763s, learning 0.146s)
             Mean action noise std: 3.01
          Mean value_function loss: 84.2662
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.0454
                       Mean reward: 588.55
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.4096
    Episode_Reward/rotating_object: 114.0934
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.91s
                      Time elapsed: 00:17:12
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1068/1500 [0m                     

                       Computation: 109990 steps/s (collection: 0.750s, learning 0.143s)
             Mean action noise std: 3.01
          Mean value_function loss: 82.5246
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0526
                       Mean reward: 573.58
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.4159
    Episode_Reward/rotating_object: 116.7274
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.89s
                      Time elapsed: 00:17:13
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1069/1500 [0m                     

                       Computation: 119533 steps/s (collection: 0.736s, learning 0.086s)
             Mean action noise std: 3.01
          Mean value_function loss: 91.6750
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0509
                       Mean reward: 584.88
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.4084
    Episode_Reward/rotating_object: 113.8823
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.82s
                      Time elapsed: 00:17:14
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1070/1500 [0m                     

                       Computation: 110193 steps/s (collection: 0.786s, learning 0.106s)
             Mean action noise std: 3.01
          Mean value_function loss: 82.8783
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.0503
                       Mean reward: 604.09
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.4091
    Episode_Reward/rotating_object: 117.3618
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.89s
                      Time elapsed: 00:17:15
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1071/1500 [0m                     

                       Computation: 113437 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 91.5659
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.0468
                       Mean reward: 583.40
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 116.3154
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.87s
                      Time elapsed: 00:17:16
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1072/1500 [0m                     

                       Computation: 114820 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 3.01
          Mean value_function loss: 93.2699
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.0370
                       Mean reward: 589.61
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.4061
    Episode_Reward/rotating_object: 114.7734
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.86s
                      Time elapsed: 00:17:17
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1073/1500 [0m                     

                       Computation: 112655 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 86.6164
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.0405
                       Mean reward: 595.66
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 112.3026
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.87s
                      Time elapsed: 00:17:18
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1074/1500 [0m                     

                       Computation: 120367 steps/s (collection: 0.725s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 91.3566
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.0449
                       Mean reward: 590.19
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.4084
    Episode_Reward/rotating_object: 114.1861
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.82s
                      Time elapsed: 00:17:18
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1075/1500 [0m                     

                       Computation: 114795 steps/s (collection: 0.743s, learning 0.114s)
             Mean action noise std: 3.02
          Mean value_function loss: 89.3740
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0482
                       Mean reward: 569.83
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.4141
    Episode_Reward/rotating_object: 114.4408
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.86s
                      Time elapsed: 00:17:19
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1076/1500 [0m                     

                       Computation: 116333 steps/s (collection: 0.742s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 88.7543
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0494
                       Mean reward: 545.42
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 111.2407
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.85s
                      Time elapsed: 00:17:20
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1077/1500 [0m                     

                       Computation: 119782 steps/s (collection: 0.721s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 88.5667
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.0494
                       Mean reward: 618.82
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4079
    Episode_Reward/rotating_object: 115.0868
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.82s
                      Time elapsed: 00:17:21
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1078/1500 [0m                     

                       Computation: 116644 steps/s (collection: 0.749s, learning 0.094s)
             Mean action noise std: 3.02
          Mean value_function loss: 98.6434
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.0531
                       Mean reward: 583.18
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.4135
    Episode_Reward/rotating_object: 116.2583
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.84s
                      Time elapsed: 00:17:22
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1079/1500 [0m                     

                       Computation: 115406 steps/s (collection: 0.753s, learning 0.099s)
             Mean action noise std: 3.02
          Mean value_function loss: 90.3255
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0469
                       Mean reward: 555.48
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 114.1242
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.85s
                      Time elapsed: 00:17:23
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1080/1500 [0m                     

                       Computation: 113591 steps/s (collection: 0.744s, learning 0.121s)
             Mean action noise std: 3.02
          Mean value_function loss: 93.6251
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0421
                       Mean reward: 591.45
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.4163
    Episode_Reward/rotating_object: 116.3888
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.87s
                      Time elapsed: 00:17:24
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1081/1500 [0m                     

                       Computation: 117828 steps/s (collection: 0.747s, learning 0.088s)
             Mean action noise std: 3.03
          Mean value_function loss: 90.2336
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.0423
                       Mean reward: 565.10
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 116.5539
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.83s
                      Time elapsed: 00:17:24
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1082/1500 [0m                     

                       Computation: 119082 steps/s (collection: 0.732s, learning 0.093s)
             Mean action noise std: 3.03
          Mean value_function loss: 94.5498
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0500
                       Mean reward: 525.41
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.4131
    Episode_Reward/rotating_object: 113.5634
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.83s
                      Time elapsed: 00:17:25
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1083/1500 [0m                     

                       Computation: 117119 steps/s (collection: 0.734s, learning 0.105s)
             Mean action noise std: 3.03
          Mean value_function loss: 86.7863
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0545
                       Mean reward: 603.31
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.4223
    Episode_Reward/rotating_object: 117.8669
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.84s
                      Time elapsed: 00:17:26
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1084/1500 [0m                     

                       Computation: 119529 steps/s (collection: 0.735s, learning 0.087s)
             Mean action noise std: 3.04
          Mean value_function loss: 82.4470
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.0614
                       Mean reward: 623.97
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.4119
    Episode_Reward/rotating_object: 117.0267
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.82s
                      Time elapsed: 00:17:27
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1085/1500 [0m                     

                       Computation: 118921 steps/s (collection: 0.733s, learning 0.094s)
             Mean action noise std: 3.04
          Mean value_function loss: 86.5123
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0679
                       Mean reward: 569.44
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 0.4172
    Episode_Reward/rotating_object: 117.3687
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.83s
                      Time elapsed: 00:17:28
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1086/1500 [0m                     

                       Computation: 113633 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 3.04
          Mean value_function loss: 87.7564
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0671
                       Mean reward: 600.70
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.4172
    Episode_Reward/rotating_object: 114.5240
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.87s
                      Time elapsed: 00:17:29
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1087/1500 [0m                     

                       Computation: 114461 steps/s (collection: 0.735s, learning 0.124s)
             Mean action noise std: 3.04
          Mean value_function loss: 95.9279
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0675
                       Mean reward: 538.57
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.4071
    Episode_Reward/rotating_object: 111.2968
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.86s
                      Time elapsed: 00:17:29
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1088/1500 [0m                     

                       Computation: 116787 steps/s (collection: 0.735s, learning 0.107s)
             Mean action noise std: 3.04
          Mean value_function loss: 91.0213
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.0684
                       Mean reward: 612.70
               Mean episode length: 243.67
    Episode_Reward/reaching_object: 0.4173
    Episode_Reward/rotating_object: 121.1955
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.84s
                      Time elapsed: 00:17:30
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1089/1500 [0m                     

                       Computation: 116882 steps/s (collection: 0.745s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 94.9266
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 18.0746
                       Mean reward: 631.81
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.4176
    Episode_Reward/rotating_object: 118.9870
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.84s
                      Time elapsed: 00:17:31
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1090/1500 [0m                     

                       Computation: 115611 steps/s (collection: 0.759s, learning 0.091s)
             Mean action noise std: 3.05
          Mean value_function loss: 93.7914
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0764
                       Mean reward: 574.51
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.4134
    Episode_Reward/rotating_object: 115.4379
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.85s
                      Time elapsed: 00:17:32
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1091/1500 [0m                     

                       Computation: 111973 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 3.05
          Mean value_function loss: 80.4424
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.0825
                       Mean reward: 580.19
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.4114
    Episode_Reward/rotating_object: 113.9736
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.88s
                      Time elapsed: 00:17:33
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1092/1500 [0m                     

                       Computation: 119233 steps/s (collection: 0.733s, learning 0.091s)
             Mean action noise std: 3.05
          Mean value_function loss: 88.5018
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.0873
                       Mean reward: 596.60
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.4135
    Episode_Reward/rotating_object: 114.8984
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.82s
                      Time elapsed: 00:17:34
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1093/1500 [0m                     

                       Computation: 108879 steps/s (collection: 0.768s, learning 0.135s)
             Mean action noise std: 3.05
          Mean value_function loss: 92.0675
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0826
                       Mean reward: 552.90
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.4119
    Episode_Reward/rotating_object: 113.7032
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.90s
                      Time elapsed: 00:17:35
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1094/1500 [0m                     

                       Computation: 107458 steps/s (collection: 0.780s, learning 0.135s)
             Mean action noise std: 3.06
          Mean value_function loss: 93.9317
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.0887
                       Mean reward: 608.02
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.4110
    Episode_Reward/rotating_object: 117.1374
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.91s
                      Time elapsed: 00:17:35
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1095/1500 [0m                     

                       Computation: 114483 steps/s (collection: 0.734s, learning 0.125s)
             Mean action noise std: 3.06
          Mean value_function loss: 90.0981
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0920
                       Mean reward: 566.68
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.4131
    Episode_Reward/rotating_object: 115.2459
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.86s
                      Time elapsed: 00:17:36
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1096/1500 [0m                     

                       Computation: 116714 steps/s (collection: 0.753s, learning 0.089s)
             Mean action noise std: 3.06
          Mean value_function loss: 90.0004
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.1002
                       Mean reward: 563.58
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 0.4083
    Episode_Reward/rotating_object: 113.0461
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.84s
                      Time elapsed: 00:17:37
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1097/1500 [0m                     

                       Computation: 117955 steps/s (collection: 0.740s, learning 0.093s)
             Mean action noise std: 3.06
          Mean value_function loss: 90.6662
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.1036
                       Mean reward: 565.80
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.4102
    Episode_Reward/rotating_object: 112.5617
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.83s
                      Time elapsed: 00:17:38
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1098/1500 [0m                     

                       Computation: 118180 steps/s (collection: 0.741s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 119.9952
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1059
                       Mean reward: 555.04
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.4175
    Episode_Reward/rotating_object: 115.5239
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.83s
                      Time elapsed: 00:17:39
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1099/1500 [0m                     

                       Computation: 115367 steps/s (collection: 0.753s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 91.7356
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.1093
                       Mean reward: 606.25
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.4146
    Episode_Reward/rotating_object: 118.6059
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.85s
                      Time elapsed: 00:17:40
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1100/1500 [0m                     

                       Computation: 115549 steps/s (collection: 0.730s, learning 0.121s)
             Mean action noise std: 3.07
          Mean value_function loss: 91.6636
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.1149
                       Mean reward: 615.82
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.4047
    Episode_Reward/rotating_object: 119.2321
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.85s
                      Time elapsed: 00:17:41
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1101/1500 [0m                     

                       Computation: 118215 steps/s (collection: 0.733s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 84.7688
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.1141
                       Mean reward: 552.91
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.4076
    Episode_Reward/rotating_object: 116.6311
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.83s
                      Time elapsed: 00:17:41
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1102/1500 [0m                     

                       Computation: 118787 steps/s (collection: 0.739s, learning 0.089s)
             Mean action noise std: 3.07
          Mean value_function loss: 92.6451
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1068
                       Mean reward: 559.87
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.4059
    Episode_Reward/rotating_object: 113.1388
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.83s
                      Time elapsed: 00:17:42
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1103/1500 [0m                     

                       Computation: 119620 steps/s (collection: 0.731s, learning 0.091s)
             Mean action noise std: 3.07
          Mean value_function loss: 82.3569
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.0988
                       Mean reward: 602.66
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.4073
    Episode_Reward/rotating_object: 114.6676
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.82s
                      Time elapsed: 00:17:43
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1104/1500 [0m                     

                       Computation: 114799 steps/s (collection: 0.769s, learning 0.087s)
             Mean action noise std: 3.07
          Mean value_function loss: 95.3475
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.1012
                       Mean reward: 551.91
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.4086
    Episode_Reward/rotating_object: 115.3909
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.86s
                      Time elapsed: 00:17:44
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1105/1500 [0m                     

                       Computation: 119484 steps/s (collection: 0.731s, learning 0.092s)
             Mean action noise std: 3.07
          Mean value_function loss: 93.6094
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1075
                       Mean reward: 528.34
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.4136
    Episode_Reward/rotating_object: 114.6316
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.82s
                      Time elapsed: 00:17:45
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1106/1500 [0m                     

                       Computation: 115485 steps/s (collection: 0.746s, learning 0.106s)
             Mean action noise std: 3.08
          Mean value_function loss: 100.2297
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.1093
                       Mean reward: 586.67
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.4095
    Episode_Reward/rotating_object: 114.3524
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.85s
                      Time elapsed: 00:17:46
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1107/1500 [0m                     

                       Computation: 116091 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 109.3700
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1107
                       Mean reward: 613.50
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.4129
    Episode_Reward/rotating_object: 116.7301
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.85s
                      Time elapsed: 00:17:46
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1108/1500 [0m                     

                       Computation: 103590 steps/s (collection: 0.757s, learning 0.192s)
             Mean action noise std: 3.08
          Mean value_function loss: 101.6087
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1099
                       Mean reward: 601.70
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.4184
    Episode_Reward/rotating_object: 120.1799
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.95s
                      Time elapsed: 00:17:47
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1109/1500 [0m                     

                       Computation: 110258 steps/s (collection: 0.796s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 105.2054
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1134
                       Mean reward: 559.67
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.4184
    Episode_Reward/rotating_object: 114.7774
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.89s
                      Time elapsed: 00:17:48
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1110/1500 [0m                     

                       Computation: 118486 steps/s (collection: 0.740s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 87.5985
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1190
                       Mean reward: 582.74
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.4112
    Episode_Reward/rotating_object: 115.0195
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.83s
                      Time elapsed: 00:17:49
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1111/1500 [0m                     

                       Computation: 113199 steps/s (collection: 0.772s, learning 0.096s)
             Mean action noise std: 3.09
          Mean value_function loss: 101.5528
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1183
                       Mean reward: 558.95
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.4154
    Episode_Reward/rotating_object: 113.5744
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.87s
                      Time elapsed: 00:17:50
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1112/1500 [0m                     

                       Computation: 115627 steps/s (collection: 0.733s, learning 0.117s)
             Mean action noise std: 3.09
          Mean value_function loss: 87.0646
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1169
                       Mean reward: 582.33
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 118.0317
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.85s
                      Time elapsed: 00:17:51
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1113/1500 [0m                     

                       Computation: 106542 steps/s (collection: 0.776s, learning 0.147s)
             Mean action noise std: 3.09
          Mean value_function loss: 94.5101
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1178
                       Mean reward: 568.85
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.4146
    Episode_Reward/rotating_object: 116.8228
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.92s
                      Time elapsed: 00:17:52
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1114/1500 [0m                     

                       Computation: 115183 steps/s (collection: 0.764s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 94.0617
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1182
                       Mean reward: 567.77
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.4176
    Episode_Reward/rotating_object: 115.6222
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.85s
                      Time elapsed: 00:17:53
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1115/1500 [0m                     

                       Computation: 113446 steps/s (collection: 0.731s, learning 0.135s)
             Mean action noise std: 3.10
          Mean value_function loss: 81.7639
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1182
                       Mean reward: 552.55
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 0.4135
    Episode_Reward/rotating_object: 116.1442
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.87s
                      Time elapsed: 00:17:53
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1116/1500 [0m                     

                       Computation: 119484 steps/s (collection: 0.735s, learning 0.088s)
             Mean action noise std: 3.10
          Mean value_function loss: 98.3769
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1252
                       Mean reward: 572.04
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.4186
    Episode_Reward/rotating_object: 115.3623
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.82s
                      Time elapsed: 00:17:54
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1117/1500 [0m                     

                       Computation: 117544 steps/s (collection: 0.740s, learning 0.096s)
             Mean action noise std: 3.10
          Mean value_function loss: 90.7946
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.1196
                       Mean reward: 642.24
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.4188
    Episode_Reward/rotating_object: 119.5545
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.84s
                      Time elapsed: 00:17:55
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1118/1500 [0m                     

                       Computation: 115129 steps/s (collection: 0.760s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 102.3554
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 18.1105
                       Mean reward: 653.95
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 121.3882
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.85s
                      Time elapsed: 00:17:56
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1119/1500 [0m                     

                       Computation: 116667 steps/s (collection: 0.755s, learning 0.088s)
             Mean action noise std: 3.10
          Mean value_function loss: 111.5240
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.1076
                       Mean reward: 576.70
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.4171
    Episode_Reward/rotating_object: 115.0972
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.84s
                      Time elapsed: 00:17:57
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1120/1500 [0m                     

                       Computation: 117546 steps/s (collection: 0.745s, learning 0.092s)
             Mean action noise std: 3.10
          Mean value_function loss: 96.4440
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.1033
                       Mean reward: 601.98
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.4252
    Episode_Reward/rotating_object: 114.1331
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.84s
                      Time elapsed: 00:17:58
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1121/1500 [0m                     

                       Computation: 115605 steps/s (collection: 0.744s, learning 0.106s)
             Mean action noise std: 3.10
          Mean value_function loss: 100.8403
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 18.1033
                       Mean reward: 578.15
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.4215
    Episode_Reward/rotating_object: 115.0829
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.85s
                      Time elapsed: 00:17:58
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1122/1500 [0m                     

                       Computation: 114523 steps/s (collection: 0.747s, learning 0.111s)
             Mean action noise std: 3.10
          Mean value_function loss: 85.6820
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1070
                       Mean reward: 581.85
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.4113
    Episode_Reward/rotating_object: 113.1282
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.86s
                      Time elapsed: 00:17:59
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1123/1500 [0m                     

                       Computation: 117369 steps/s (collection: 0.740s, learning 0.098s)
             Mean action noise std: 3.10
          Mean value_function loss: 78.6771
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.1093
                       Mean reward: 610.56
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.4197
    Episode_Reward/rotating_object: 117.1732
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.84s
                      Time elapsed: 00:18:00
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1124/1500 [0m                     

                       Computation: 114111 steps/s (collection: 0.747s, learning 0.115s)
             Mean action noise std: 3.10
          Mean value_function loss: 83.5861
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.1090
                       Mean reward: 593.18
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.4210
    Episode_Reward/rotating_object: 116.0005
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.86s
                      Time elapsed: 00:18:01
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1125/1500 [0m                     

                       Computation: 114893 steps/s (collection: 0.761s, learning 0.095s)
             Mean action noise std: 3.11
          Mean value_function loss: 85.3603
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.1065
                       Mean reward: 603.32
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 119.7738
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.86s
                      Time elapsed: 00:18:02
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1126/1500 [0m                     

                       Computation: 116272 steps/s (collection: 0.757s, learning 0.089s)
             Mean action noise std: 3.11
          Mean value_function loss: 84.4776
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.1089
                       Mean reward: 571.90
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.4214
    Episode_Reward/rotating_object: 118.9632
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.85s
                      Time elapsed: 00:18:03
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1127/1500 [0m                     

                       Computation: 117066 steps/s (collection: 0.754s, learning 0.086s)
             Mean action noise std: 3.11
          Mean value_function loss: 79.8278
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1105
                       Mean reward: 586.08
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 0.4200
    Episode_Reward/rotating_object: 118.5197
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.84s
                      Time elapsed: 00:18:04
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1128/1500 [0m                     

                       Computation: 111442 steps/s (collection: 0.767s, learning 0.116s)
             Mean action noise std: 3.11
          Mean value_function loss: 82.2678
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1111
                       Mean reward: 552.91
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 117.3972
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.88s
                      Time elapsed: 00:18:04
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1129/1500 [0m                     

                       Computation: 117225 steps/s (collection: 0.732s, learning 0.106s)
             Mean action noise std: 3.12
          Mean value_function loss: 84.5463
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.1122
                       Mean reward: 546.75
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.4166
    Episode_Reward/rotating_object: 112.9507
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.84s
                      Time elapsed: 00:18:05
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1130/1500 [0m                     

                       Computation: 113747 steps/s (collection: 0.758s, learning 0.106s)
             Mean action noise std: 3.12
          Mean value_function loss: 75.7455
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1187
                       Mean reward: 589.14
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.4226
    Episode_Reward/rotating_object: 121.3785
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.86s
                      Time elapsed: 00:18:06
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1131/1500 [0m                     

                       Computation: 118766 steps/s (collection: 0.725s, learning 0.103s)
             Mean action noise std: 3.12
          Mean value_function loss: 86.7140
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.1187
                       Mean reward: 602.81
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.4167
    Episode_Reward/rotating_object: 119.8486
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.83s
                      Time elapsed: 00:18:07
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1132/1500 [0m                     

                       Computation: 113698 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 3.12
          Mean value_function loss: 93.4130
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1219
                       Mean reward: 581.85
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.4149
    Episode_Reward/rotating_object: 117.7546
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.86s
                      Time elapsed: 00:18:08
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1133/1500 [0m                     

                       Computation: 116532 steps/s (collection: 0.756s, learning 0.088s)
             Mean action noise std: 3.12
          Mean value_function loss: 80.7760
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1223
                       Mean reward: 579.80
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 116.9113
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.84s
                      Time elapsed: 00:18:09
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1134/1500 [0m                     

                       Computation: 112747 steps/s (collection: 0.749s, learning 0.123s)
             Mean action noise std: 3.12
          Mean value_function loss: 76.9686
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1217
                       Mean reward: 599.46
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 117.1134
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.87s
                      Time elapsed: 00:18:10
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1135/1500 [0m                     

                       Computation: 120656 steps/s (collection: 0.728s, learning 0.087s)
             Mean action noise std: 3.12
          Mean value_function loss: 77.1104
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1182
                       Mean reward: 593.43
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.4155
    Episode_Reward/rotating_object: 119.4962
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.81s
                      Time elapsed: 00:18:10
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1136/1500 [0m                     

                       Computation: 114345 steps/s (collection: 0.756s, learning 0.104s)
             Mean action noise std: 3.12
          Mean value_function loss: 78.7851
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.1169
                       Mean reward: 573.99
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.4198
    Episode_Reward/rotating_object: 121.8931
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.86s
                      Time elapsed: 00:18:11
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1137/1500 [0m                     

                       Computation: 114921 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 3.12
          Mean value_function loss: 78.6649
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1189
                       Mean reward: 594.02
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 118.6516
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.86s
                      Time elapsed: 00:18:12
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1138/1500 [0m                     

                       Computation: 118455 steps/s (collection: 0.740s, learning 0.090s)
             Mean action noise std: 3.12
          Mean value_function loss: 86.3209
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.1236
                       Mean reward: 616.32
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.4199
    Episode_Reward/rotating_object: 121.1498
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.83s
                      Time elapsed: 00:18:13
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1139/1500 [0m                     

                       Computation: 115484 steps/s (collection: 0.765s, learning 0.086s)
             Mean action noise std: 3.13
          Mean value_function loss: 88.7951
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1279
                       Mean reward: 577.42
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.4129
    Episode_Reward/rotating_object: 115.5027
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.85s
                      Time elapsed: 00:18:14
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1140/1500 [0m                     

                       Computation: 117902 steps/s (collection: 0.733s, learning 0.101s)
             Mean action noise std: 3.13
          Mean value_function loss: 78.6589
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1286
                       Mean reward: 577.85
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 119.5901
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.83s
                      Time elapsed: 00:18:15
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1141/1500 [0m                     

                       Computation: 114402 steps/s (collection: 0.742s, learning 0.117s)
             Mean action noise std: 3.13
          Mean value_function loss: 102.1342
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1282
                       Mean reward: 580.47
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.4152
    Episode_Reward/rotating_object: 115.6826
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.86s
                      Time elapsed: 00:18:15
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1142/1500 [0m                     

                       Computation: 114758 steps/s (collection: 0.757s, learning 0.100s)
             Mean action noise std: 3.13
          Mean value_function loss: 90.2548
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1262
                       Mean reward: 597.15
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 118.1486
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.86s
                      Time elapsed: 00:18:16
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1143/1500 [0m                     

                       Computation: 111914 steps/s (collection: 0.745s, learning 0.134s)
             Mean action noise std: 3.13
          Mean value_function loss: 78.6640
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.1318
                       Mean reward: 594.96
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.4214
    Episode_Reward/rotating_object: 121.1879
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.88s
                      Time elapsed: 00:18:17
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1144/1500 [0m                     

                       Computation: 118745 steps/s (collection: 0.738s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 86.1672
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1309
                       Mean reward: 614.75
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.4197
    Episode_Reward/rotating_object: 119.7541
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.83s
                      Time elapsed: 00:18:18
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1145/1500 [0m                     

                       Computation: 119490 steps/s (collection: 0.736s, learning 0.087s)
             Mean action noise std: 3.14
          Mean value_function loss: 83.7024
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1301
                       Mean reward: 591.85
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 119.3432
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.82s
                      Time elapsed: 00:18:19
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1146/1500 [0m                     

                       Computation: 112467 steps/s (collection: 0.761s, learning 0.113s)
             Mean action noise std: 3.14
          Mean value_function loss: 82.2911
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.1388
                       Mean reward: 589.87
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 0.4228
    Episode_Reward/rotating_object: 119.9928
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.87s
                      Time elapsed: 00:18:20
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1147/1500 [0m                     

                       Computation: 112027 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 83.2396
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1441
                       Mean reward: 618.73
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 117.2226
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.88s
                      Time elapsed: 00:18:21
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1148/1500 [0m                     

                       Computation: 119153 steps/s (collection: 0.729s, learning 0.096s)
             Mean action noise std: 3.14
          Mean value_function loss: 82.5747
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.1472
                       Mean reward: 586.13
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.4220
    Episode_Reward/rotating_object: 117.2038
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.83s
                      Time elapsed: 00:18:21
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1149/1500 [0m                     

                       Computation: 110612 steps/s (collection: 0.729s, learning 0.160s)
             Mean action noise std: 3.14
          Mean value_function loss: 84.2672
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.1446
                       Mean reward: 613.52
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.4255
    Episode_Reward/rotating_object: 121.1022
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.89s
                      Time elapsed: 00:18:22
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1150/1500 [0m                     

                       Computation: 119127 steps/s (collection: 0.739s, learning 0.086s)
             Mean action noise std: 3.15
          Mean value_function loss: 85.0538
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.1532
                       Mean reward: 618.91
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.4170
    Episode_Reward/rotating_object: 117.2481
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.83s
                      Time elapsed: 00:18:23
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1151/1500 [0m                     

                       Computation: 120059 steps/s (collection: 0.725s, learning 0.094s)
             Mean action noise std: 3.15
          Mean value_function loss: 93.2471
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.1554
                       Mean reward: 603.02
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 122.1784
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.82s
                      Time elapsed: 00:18:24
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1152/1500 [0m                     

                       Computation: 117326 steps/s (collection: 0.743s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 86.4162
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.1521
                       Mean reward: 566.39
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 117.3499
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.84s
                      Time elapsed: 00:18:25
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1153/1500 [0m                     

                       Computation: 113912 steps/s (collection: 0.777s, learning 0.086s)
             Mean action noise std: 3.15
          Mean value_function loss: 99.7468
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.1563
                       Mean reward: 596.53
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 0.4136
    Episode_Reward/rotating_object: 115.9350
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.86s
                      Time elapsed: 00:18:26
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1154/1500 [0m                     

                       Computation: 114877 steps/s (collection: 0.748s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 82.9818
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1616
                       Mean reward: 596.03
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 119.0142
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.86s
                      Time elapsed: 00:18:26
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1155/1500 [0m                     

                       Computation: 109111 steps/s (collection: 0.771s, learning 0.130s)
             Mean action noise std: 3.16
          Mean value_function loss: 77.3741
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.1650
                       Mean reward: 591.85
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.4149
    Episode_Reward/rotating_object: 116.2357
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.90s
                      Time elapsed: 00:18:27
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1156/1500 [0m                     

                       Computation: 108540 steps/s (collection: 0.762s, learning 0.144s)
             Mean action noise std: 3.16
          Mean value_function loss: 94.2918
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1722
                       Mean reward: 632.51
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.4183
    Episode_Reward/rotating_object: 118.9674
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.91s
                      Time elapsed: 00:18:28
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1157/1500 [0m                     

                       Computation: 113443 steps/s (collection: 0.735s, learning 0.132s)
             Mean action noise std: 3.16
          Mean value_function loss: 84.8095
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.1733
                       Mean reward: 602.92
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.4173
    Episode_Reward/rotating_object: 121.8787
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.87s
                      Time elapsed: 00:18:29
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1158/1500 [0m                     

                       Computation: 117439 steps/s (collection: 0.746s, learning 0.091s)
             Mean action noise std: 3.16
          Mean value_function loss: 87.1610
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1762
                       Mean reward: 620.12
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.4247
    Episode_Reward/rotating_object: 121.2189
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.84s
                      Time elapsed: 00:18:30
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1159/1500 [0m                     

                       Computation: 120416 steps/s (collection: 0.728s, learning 0.089s)
             Mean action noise std: 3.16
          Mean value_function loss: 80.1734
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1782
                       Mean reward: 596.79
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.4230
    Episode_Reward/rotating_object: 119.0215
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.82s
                      Time elapsed: 00:18:31
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1160/1500 [0m                     

                       Computation: 112913 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 3.16
          Mean value_function loss: 85.3977
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1823
                       Mean reward: 615.67
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 117.6753
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.87s
                      Time elapsed: 00:18:32
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1161/1500 [0m                     

                       Computation: 116503 steps/s (collection: 0.748s, learning 0.096s)
             Mean action noise std: 3.16
          Mean value_function loss: 93.0469
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.1863
                       Mean reward: 590.07
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.4250
    Episode_Reward/rotating_object: 119.1193
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.84s
                      Time elapsed: 00:18:33
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1162/1500 [0m                     

                       Computation: 108471 steps/s (collection: 0.788s, learning 0.119s)
             Mean action noise std: 3.17
          Mean value_function loss: 101.6398
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1908
                       Mean reward: 596.36
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.4259
    Episode_Reward/rotating_object: 119.0631
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.91s
                      Time elapsed: 00:18:33
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1163/1500 [0m                     

                       Computation: 115985 steps/s (collection: 0.747s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 90.4973
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.1900
                       Mean reward: 578.09
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 0.4248
    Episode_Reward/rotating_object: 120.9466
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.85s
                      Time elapsed: 00:18:34
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1164/1500 [0m                     

                       Computation: 116433 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 3.17
          Mean value_function loss: 97.0983
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1930
                       Mean reward: 555.85
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 0.4173
    Episode_Reward/rotating_object: 115.8521
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.84s
                      Time elapsed: 00:18:35
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1165/1500 [0m                     

                       Computation: 112129 steps/s (collection: 0.785s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 96.2967
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.1955
                       Mean reward: 589.80
               Mean episode length: 238.45
    Episode_Reward/reaching_object: 0.4153
    Episode_Reward/rotating_object: 116.4937
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.88s
                      Time elapsed: 00:18:36
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1166/1500 [0m                     

                       Computation: 116730 steps/s (collection: 0.750s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 98.3356
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.1944
                       Mean reward: 577.00
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.4178
    Episode_Reward/rotating_object: 118.6837
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.84s
                      Time elapsed: 00:18:37
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1167/1500 [0m                     

                       Computation: 111758 steps/s (collection: 0.792s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 91.4474
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.1950
                       Mean reward: 581.79
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 0.4108
    Episode_Reward/rotating_object: 115.4241
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.88s
                      Time elapsed: 00:18:38
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1168/1500 [0m                     

                       Computation: 113589 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 83.7717
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.1981
                       Mean reward: 622.08
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.4211
    Episode_Reward/rotating_object: 120.9554
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.87s
                      Time elapsed: 00:18:39
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1169/1500 [0m                     

                       Computation: 112318 steps/s (collection: 0.756s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 96.1328
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2002
                       Mean reward: 602.99
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.4157
    Episode_Reward/rotating_object: 120.0019
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.88s
                      Time elapsed: 00:18:39
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1170/1500 [0m                     

                       Computation: 114047 steps/s (collection: 0.761s, learning 0.100s)
             Mean action noise std: 3.18
          Mean value_function loss: 88.1364
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2036
                       Mean reward: 590.22
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.4238
    Episode_Reward/rotating_object: 120.2486
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.86s
                      Time elapsed: 00:18:40
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1171/1500 [0m                     

                       Computation: 112166 steps/s (collection: 0.753s, learning 0.124s)
             Mean action noise std: 3.18
          Mean value_function loss: 89.7678
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.2077
                       Mean reward: 633.56
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.4198
    Episode_Reward/rotating_object: 117.5726
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.88s
                      Time elapsed: 00:18:41
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1172/1500 [0m                     

                       Computation: 114789 steps/s (collection: 0.761s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 85.3371
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2055
                       Mean reward: 605.27
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.4201
    Episode_Reward/rotating_object: 117.1749
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.86s
                      Time elapsed: 00:18:42
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1173/1500 [0m                     

                       Computation: 113442 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 89.2274
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.2050
                       Mean reward: 583.01
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.4219
    Episode_Reward/rotating_object: 119.8172
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.87s
                      Time elapsed: 00:18:43
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1174/1500 [0m                     

                       Computation: 115603 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 74.4126
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.2074
                       Mean reward: 579.62
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 0.4157
    Episode_Reward/rotating_object: 113.8676
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.85s
                      Time elapsed: 00:18:44
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1175/1500 [0m                     

                       Computation: 108877 steps/s (collection: 0.808s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 75.2266
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.2131
                       Mean reward: 622.62
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 124.8068
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.90s
                      Time elapsed: 00:18:45
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1176/1500 [0m                     

                       Computation: 118116 steps/s (collection: 0.726s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 86.0212
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2160
                       Mean reward: 610.74
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.4210
    Episode_Reward/rotating_object: 122.7032
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.83s
                      Time elapsed: 00:18:46
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1177/1500 [0m                     

                       Computation: 111681 steps/s (collection: 0.751s, learning 0.129s)
             Mean action noise std: 3.19
          Mean value_function loss: 71.9653
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2172
                       Mean reward: 580.92
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.4160
    Episode_Reward/rotating_object: 118.6605
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.88s
                      Time elapsed: 00:18:46
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1178/1500 [0m                     

                       Computation: 110323 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 3.19
          Mean value_function loss: 72.6340
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.2242
                       Mean reward: 604.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 121.6419
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.89s
                      Time elapsed: 00:18:47
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1179/1500 [0m                     

                       Computation: 112930 steps/s (collection: 0.744s, learning 0.126s)
             Mean action noise std: 3.19
          Mean value_function loss: 83.1384
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.2260
                       Mean reward: 634.42
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 0.4181
    Episode_Reward/rotating_object: 121.1754
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.87s
                      Time elapsed: 00:18:48
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1180/1500 [0m                     

                       Computation: 112693 steps/s (collection: 0.774s, learning 0.099s)
             Mean action noise std: 3.19
          Mean value_function loss: 82.5938
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.2301
                       Mean reward: 621.49
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.4148
    Episode_Reward/rotating_object: 121.1032
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.87s
                      Time elapsed: 00:18:49
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1181/1500 [0m                     

                       Computation: 115876 steps/s (collection: 0.744s, learning 0.105s)
             Mean action noise std: 3.19
          Mean value_function loss: 71.5502
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.2284
                       Mean reward: 627.60
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.4168
    Episode_Reward/rotating_object: 120.2081
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.85s
                      Time elapsed: 00:18:50
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1182/1500 [0m                     

                       Computation: 110139 steps/s (collection: 0.749s, learning 0.144s)
             Mean action noise std: 3.19
          Mean value_function loss: 79.6946
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.2289
                       Mean reward: 610.32
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 123.1472
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.89s
                      Time elapsed: 00:18:51
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1183/1500 [0m                     

                       Computation: 108861 steps/s (collection: 0.802s, learning 0.101s)
             Mean action noise std: 3.20
          Mean value_function loss: 69.5279
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2294
                       Mean reward: 609.75
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.4221
    Episode_Reward/rotating_object: 122.2989
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.90s
                      Time elapsed: 00:18:52
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1184/1500 [0m                     

                       Computation: 113649 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 3.20
          Mean value_function loss: 73.9138
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2307
                       Mean reward: 571.77
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.4186
    Episode_Reward/rotating_object: 118.3811
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.86s
                      Time elapsed: 00:18:53
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1185/1500 [0m                     

                       Computation: 113095 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 3.20
          Mean value_function loss: 77.2481
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2294
                       Mean reward: 592.45
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.4178
    Episode_Reward/rotating_object: 116.2678
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.87s
                      Time elapsed: 00:18:53
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1186/1500 [0m                     

                       Computation: 114885 steps/s (collection: 0.751s, learning 0.105s)
             Mean action noise std: 3.20
          Mean value_function loss: 80.8788
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.2332
                       Mean reward: 609.25
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 121.2850
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.86s
                      Time elapsed: 00:18:54
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1187/1500 [0m                     

                       Computation: 120049 steps/s (collection: 0.729s, learning 0.090s)
             Mean action noise std: 3.20
          Mean value_function loss: 98.4187
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.2418
                       Mean reward: 628.94
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.4207
    Episode_Reward/rotating_object: 120.1676
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.82s
                      Time elapsed: 00:18:55
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1188/1500 [0m                     

                       Computation: 115438 steps/s (collection: 0.756s, learning 0.096s)
             Mean action noise std: 3.20
          Mean value_function loss: 72.8003
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2464
                       Mean reward: 582.02
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.4168
    Episode_Reward/rotating_object: 117.1743
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.85s
                      Time elapsed: 00:18:56
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1189/1500 [0m                     

                       Computation: 116238 steps/s (collection: 0.749s, learning 0.097s)
             Mean action noise std: 3.20
          Mean value_function loss: 78.8644
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.2521
                       Mean reward: 607.39
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.4210
    Episode_Reward/rotating_object: 123.3716
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.85s
                      Time elapsed: 00:18:57
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1190/1500 [0m                     

                       Computation: 113458 steps/s (collection: 0.752s, learning 0.114s)
             Mean action noise std: 3.21
          Mean value_function loss: 68.0303
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.2596
                       Mean reward: 614.01
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.4214
    Episode_Reward/rotating_object: 121.3450
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.87s
                      Time elapsed: 00:18:58
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1191/1500 [0m                     

                       Computation: 121391 steps/s (collection: 0.722s, learning 0.088s)
             Mean action noise std: 3.21
          Mean value_function loss: 73.3797
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2633
                       Mean reward: 618.08
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 125.0772
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.81s
                      Time elapsed: 00:18:58
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1192/1500 [0m                     

                       Computation: 107187 steps/s (collection: 0.751s, learning 0.167s)
             Mean action noise std: 3.21
          Mean value_function loss: 73.8837
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2647
                       Mean reward: 593.47
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 0.4189
    Episode_Reward/rotating_object: 119.8617
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.92s
                      Time elapsed: 00:18:59
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1193/1500 [0m                     

                       Computation: 114856 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 81.2271
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.2664
                       Mean reward: 624.62
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.4253
    Episode_Reward/rotating_object: 123.4935
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.86s
                      Time elapsed: 00:19:00
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1194/1500 [0m                     

                       Computation: 117003 steps/s (collection: 0.750s, learning 0.090s)
             Mean action noise std: 3.21
          Mean value_function loss: 81.7958
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.2619
                       Mean reward: 617.91
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.4257
    Episode_Reward/rotating_object: 123.0297
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.84s
                      Time elapsed: 00:19:01
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1195/1500 [0m                     

                       Computation: 112564 steps/s (collection: 0.770s, learning 0.103s)
             Mean action noise std: 3.21
          Mean value_function loss: 89.7025
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.2598
                       Mean reward: 587.31
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 118.8424
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.87s
                      Time elapsed: 00:19:02
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1196/1500 [0m                     

                       Computation: 116339 steps/s (collection: 0.752s, learning 0.093s)
             Mean action noise std: 3.22
          Mean value_function loss: 96.6401
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2642
                       Mean reward: 585.06
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 116.8807
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.84s
                      Time elapsed: 00:19:03
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1197/1500 [0m                     

                       Computation: 111970 steps/s (collection: 0.790s, learning 0.088s)
             Mean action noise std: 3.22
          Mean value_function loss: 102.6568
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.2717
                       Mean reward: 607.68
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.4286
    Episode_Reward/rotating_object: 119.1912
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.88s
                      Time elapsed: 00:19:04
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1198/1500 [0m                     

                       Computation: 106163 steps/s (collection: 0.815s, learning 0.111s)
             Mean action noise std: 3.22
          Mean value_function loss: 85.8707
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2695
                       Mean reward: 600.15
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.4248
    Episode_Reward/rotating_object: 118.6838
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.93s
                      Time elapsed: 00:19:05
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1199/1500 [0m                     

                       Computation: 110105 steps/s (collection: 0.766s, learning 0.127s)
             Mean action noise std: 3.22
          Mean value_function loss: 93.1088
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.2654
                       Mean reward: 580.86
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.4170
    Episode_Reward/rotating_object: 113.9243
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.89s
                      Time elapsed: 00:19:05
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1200/1500 [0m                     

                       Computation: 111814 steps/s (collection: 0.755s, learning 0.124s)
             Mean action noise std: 3.22
          Mean value_function loss: 87.2314
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.2676
                       Mean reward: 607.79
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.4203
    Episode_Reward/rotating_object: 116.5547
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.88s
                      Time elapsed: 00:19:06
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1201/1500 [0m                     

                       Computation: 112943 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 3.22
          Mean value_function loss: 95.2749
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2691
                       Mean reward: 584.15
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.4296
    Episode_Reward/rotating_object: 120.7937
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.87s
                      Time elapsed: 00:19:07
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1202/1500 [0m                     

                       Computation: 116212 steps/s (collection: 0.754s, learning 0.092s)
             Mean action noise std: 3.22
          Mean value_function loss: 89.4540
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2721
                       Mean reward: 616.71
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.4279
    Episode_Reward/rotating_object: 118.5050
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.85s
                      Time elapsed: 00:19:08
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1203/1500 [0m                     

                       Computation: 117176 steps/s (collection: 0.748s, learning 0.090s)
             Mean action noise std: 3.22
          Mean value_function loss: 88.0436
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.2668
                       Mean reward: 605.49
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 121.2069
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.84s
                      Time elapsed: 00:19:09
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1204/1500 [0m                     

                       Computation: 113804 steps/s (collection: 0.757s, learning 0.107s)
             Mean action noise std: 3.22
          Mean value_function loss: 84.4568
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.2690
                       Mean reward: 588.32
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.4262
    Episode_Reward/rotating_object: 118.9240
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.86s
                      Time elapsed: 00:19:10
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1205/1500 [0m                     

                       Computation: 114371 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 3.22
          Mean value_function loss: 82.7882
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2623
                       Mean reward: 604.49
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.4311
    Episode_Reward/rotating_object: 119.2534
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.86s
                      Time elapsed: 00:19:11
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1206/1500 [0m                     

                       Computation: 110773 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 3.23
          Mean value_function loss: 85.8761
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2640
                       Mean reward: 608.63
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.4267
    Episode_Reward/rotating_object: 121.9554
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.89s
                      Time elapsed: 00:19:12
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1207/1500 [0m                     

                       Computation: 115544 steps/s (collection: 0.750s, learning 0.101s)
             Mean action noise std: 3.23
          Mean value_function loss: 83.3952
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2700
                       Mean reward: 639.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4297
    Episode_Reward/rotating_object: 121.7659
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.85s
                      Time elapsed: 00:19:12
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1208/1500 [0m                     

                       Computation: 110511 steps/s (collection: 0.754s, learning 0.136s)
             Mean action noise std: 3.23
          Mean value_function loss: 78.7368
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.2755
                       Mean reward: 588.55
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.4270
    Episode_Reward/rotating_object: 119.2984
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.89s
                      Time elapsed: 00:19:13
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1209/1500 [0m                     

                       Computation: 110621 steps/s (collection: 0.737s, learning 0.151s)
             Mean action noise std: 3.23
          Mean value_function loss: 78.6155
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2762
                       Mean reward: 631.26
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 117.7397
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.89s
                      Time elapsed: 00:19:14
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1210/1500 [0m                     

                       Computation: 113717 steps/s (collection: 0.757s, learning 0.108s)
             Mean action noise std: 3.23
          Mean value_function loss: 80.1075
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2870
                       Mean reward: 588.94
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 118.2424
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.86s
                      Time elapsed: 00:19:15
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1211/1500 [0m                     

                       Computation: 117337 steps/s (collection: 0.723s, learning 0.115s)
             Mean action noise std: 3.24
          Mean value_function loss: 80.4261
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3018
                       Mean reward: 641.00
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.4275
    Episode_Reward/rotating_object: 120.1626
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.84s
                      Time elapsed: 00:19:16
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1212/1500 [0m                     

                       Computation: 115534 steps/s (collection: 0.760s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 77.3744
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.3001
                       Mean reward: 607.01
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.4222
    Episode_Reward/rotating_object: 119.0116
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.85s
                      Time elapsed: 00:19:17
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1213/1500 [0m                     

                       Computation: 112085 steps/s (collection: 0.755s, learning 0.122s)
             Mean action noise std: 3.24
          Mean value_function loss: 93.7358
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.3043
                       Mean reward: 583.62
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.4183
    Episode_Reward/rotating_object: 117.6199
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.88s
                      Time elapsed: 00:19:18
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1214/1500 [0m                     

                       Computation: 115806 steps/s (collection: 0.758s, learning 0.091s)
             Mean action noise std: 3.24
          Mean value_function loss: 83.3239
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3093
                       Mean reward: 570.40
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.4202
    Episode_Reward/rotating_object: 116.1849
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.85s
                      Time elapsed: 00:19:18
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1215/1500 [0m                     

                       Computation: 112868 steps/s (collection: 0.741s, learning 0.130s)
             Mean action noise std: 3.24
          Mean value_function loss: 68.2323
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.3110
                       Mean reward: 610.80
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.4207
    Episode_Reward/rotating_object: 121.9576
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.87s
                      Time elapsed: 00:19:19
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1216/1500 [0m                     

                       Computation: 115728 steps/s (collection: 0.743s, learning 0.106s)
             Mean action noise std: 3.24
          Mean value_function loss: 79.7160
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3074
                       Mean reward: 613.92
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.4265
    Episode_Reward/rotating_object: 124.0141
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.85s
                      Time elapsed: 00:19:20
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1217/1500 [0m                     

                       Computation: 121010 steps/s (collection: 0.724s, learning 0.089s)
             Mean action noise std: 3.24
          Mean value_function loss: 80.7746
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.3111
                       Mean reward: 617.40
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.4199
    Episode_Reward/rotating_object: 120.2736
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.81s
                      Time elapsed: 00:19:21
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1218/1500 [0m                     

                       Computation: 115721 steps/s (collection: 0.745s, learning 0.105s)
             Mean action noise std: 3.25
          Mean value_function loss: 76.9733
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.3135
                       Mean reward: 631.49
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.4242
    Episode_Reward/rotating_object: 122.6178
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.85s
                      Time elapsed: 00:19:22
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1219/1500 [0m                     

                       Computation: 114145 steps/s (collection: 0.773s, learning 0.088s)
             Mean action noise std: 3.25
          Mean value_function loss: 72.1778
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3162
                       Mean reward: 593.31
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 119.4896
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.86s
                      Time elapsed: 00:19:23
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1220/1500 [0m                     

                       Computation: 112751 steps/s (collection: 0.770s, learning 0.102s)
             Mean action noise std: 3.25
          Mean value_function loss: 63.9838
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3162
                       Mean reward: 598.93
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.4241
    Episode_Reward/rotating_object: 123.2916
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.87s
                      Time elapsed: 00:19:24
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1221/1500 [0m                     

                       Computation: 114531 steps/s (collection: 0.760s, learning 0.098s)
             Mean action noise std: 3.25
          Mean value_function loss: 67.2200
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.3171
                       Mean reward: 650.02
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4201
    Episode_Reward/rotating_object: 123.2723
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.86s
                      Time elapsed: 00:19:24
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1222/1500 [0m                     

                       Computation: 109963 steps/s (collection: 0.739s, learning 0.155s)
             Mean action noise std: 3.25
          Mean value_function loss: 97.5862
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.3144
                       Mean reward: 612.96
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 120.3792
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.89s
                      Time elapsed: 00:19:25
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1223/1500 [0m                     

                       Computation: 102233 steps/s (collection: 0.821s, learning 0.141s)
             Mean action noise std: 3.26
          Mean value_function loss: 91.8741
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3167
                       Mean reward: 609.09
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.4220
    Episode_Reward/rotating_object: 124.4967
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.96s
                      Time elapsed: 00:19:26
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1224/1500 [0m                     

                       Computation: 110217 steps/s (collection: 0.738s, learning 0.154s)
             Mean action noise std: 3.26
          Mean value_function loss: 86.2315
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3136
                       Mean reward: 608.33
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.4241
    Episode_Reward/rotating_object: 120.9336
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.89s
                      Time elapsed: 00:19:27
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1225/1500 [0m                     

                       Computation: 116231 steps/s (collection: 0.750s, learning 0.096s)
             Mean action noise std: 3.26
          Mean value_function loss: 95.9827
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.3145
                       Mean reward: 587.98
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 0.4213
    Episode_Reward/rotating_object: 120.7421
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.85s
                      Time elapsed: 00:19:28
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1226/1500 [0m                     

                       Computation: 112744 steps/s (collection: 0.766s, learning 0.106s)
             Mean action noise std: 3.26
          Mean value_function loss: 93.8876
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.3201
                       Mean reward: 607.96
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 119.5603
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.87s
                      Time elapsed: 00:19:29
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1227/1500 [0m                     

                       Computation: 115425 steps/s (collection: 0.747s, learning 0.105s)
             Mean action noise std: 3.26
          Mean value_function loss: 87.2497
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3199
                       Mean reward: 632.43
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.4229
    Episode_Reward/rotating_object: 121.1677
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.85s
                      Time elapsed: 00:19:30
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1228/1500 [0m                     

                       Computation: 113198 steps/s (collection: 0.758s, learning 0.111s)
             Mean action noise std: 3.26
          Mean value_function loss: 81.0708
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3146
                       Mean reward: 638.29
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.4210
    Episode_Reward/rotating_object: 121.0644
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.87s
                      Time elapsed: 00:19:31
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1229/1500 [0m                     

                       Computation: 116459 steps/s (collection: 0.736s, learning 0.108s)
             Mean action noise std: 3.26
          Mean value_function loss: 93.7119
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 18.3169
                       Mean reward: 584.39
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.4250
    Episode_Reward/rotating_object: 121.7342
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.84s
                      Time elapsed: 00:19:31
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1230/1500 [0m                     

                       Computation: 106008 steps/s (collection: 0.762s, learning 0.166s)
             Mean action noise std: 3.27
          Mean value_function loss: 88.5511
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.3161
                       Mean reward: 635.09
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.4279
    Episode_Reward/rotating_object: 124.6123
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.93s
                      Time elapsed: 00:19:32
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1231/1500 [0m                     

                       Computation: 117548 steps/s (collection: 0.749s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 85.6022
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3172
                       Mean reward: 598.82
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 0.4202
    Episode_Reward/rotating_object: 116.3197
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.84s
                      Time elapsed: 00:19:33
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1232/1500 [0m                     

                       Computation: 113071 steps/s (collection: 0.735s, learning 0.134s)
             Mean action noise std: 3.27
          Mean value_function loss: 80.7503
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3330
                       Mean reward: 588.01
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.4268
    Episode_Reward/rotating_object: 122.3492
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.87s
                      Time elapsed: 00:19:34
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1233/1500 [0m                     

                       Computation: 115146 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 3.28
          Mean value_function loss: 73.8242
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3413
                       Mean reward: 629.25
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.4236
    Episode_Reward/rotating_object: 119.7781
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.85s
                      Time elapsed: 00:19:35
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1234/1500 [0m                     

                       Computation: 112674 steps/s (collection: 0.770s, learning 0.102s)
             Mean action noise std: 3.28
          Mean value_function loss: 83.8363
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.3348
                       Mean reward: 587.86
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.4236
    Episode_Reward/rotating_object: 119.9795
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.87s
                      Time elapsed: 00:19:36
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1235/1500 [0m                     

                       Computation: 113444 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 3.28
          Mean value_function loss: 95.6148
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3335
                       Mean reward: 623.35
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.4244
    Episode_Reward/rotating_object: 124.3727
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.87s
                      Time elapsed: 00:19:37
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1236/1500 [0m                     

                       Computation: 110280 steps/s (collection: 0.786s, learning 0.106s)
             Mean action noise std: 3.28
          Mean value_function loss: 85.9390
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.3301
                       Mean reward: 618.10
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.4205
    Episode_Reward/rotating_object: 123.2181
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.89s
                      Time elapsed: 00:19:38
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1237/1500 [0m                     

                       Computation: 112451 steps/s (collection: 0.745s, learning 0.130s)
             Mean action noise std: 3.28
          Mean value_function loss: 88.0849
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3285
                       Mean reward: 602.37
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.4272
    Episode_Reward/rotating_object: 121.2667
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.87s
                      Time elapsed: 00:19:38
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1238/1500 [0m                     

                       Computation: 112973 steps/s (collection: 0.763s, learning 0.107s)
             Mean action noise std: 3.28
          Mean value_function loss: 89.6017
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.3338
                       Mean reward: 623.44
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.4222
    Episode_Reward/rotating_object: 122.1743
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.87s
                      Time elapsed: 00:19:39
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1239/1500 [0m                     

                       Computation: 111834 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 3.29
          Mean value_function loss: 72.4589
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3389
                       Mean reward: 631.08
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 120.7990
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.88s
                      Time elapsed: 00:19:40
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1240/1500 [0m                     

                       Computation: 112073 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 3.29
          Mean value_function loss: 78.1190
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.3400
                       Mean reward: 628.78
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.4209
    Episode_Reward/rotating_object: 118.0604
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.88s
                      Time elapsed: 00:19:41
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1241/1500 [0m                     

                       Computation: 112456 steps/s (collection: 0.775s, learning 0.100s)
             Mean action noise std: 3.29
          Mean value_function loss: 132.6447
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3418
                       Mean reward: 602.29
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.4317
    Episode_Reward/rotating_object: 121.2487
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.87s
                      Time elapsed: 00:19:42
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1242/1500 [0m                     

                       Computation: 113589 steps/s (collection: 0.772s, learning 0.094s)
             Mean action noise std: 3.29
          Mean value_function loss: 140.1129
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.3439
                       Mean reward: 645.28
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.4275
    Episode_Reward/rotating_object: 123.4966
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.87s
                      Time elapsed: 00:19:43
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1243/1500 [0m                     

                       Computation: 113073 steps/s (collection: 0.772s, learning 0.097s)
             Mean action noise std: 3.29
          Mean value_function loss: 97.6099
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3483
                       Mean reward: 612.87
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.4263
    Episode_Reward/rotating_object: 122.5972
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.87s
                      Time elapsed: 00:19:44
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1244/1500 [0m                     

                       Computation: 108926 steps/s (collection: 0.768s, learning 0.135s)
             Mean action noise std: 3.29
          Mean value_function loss: 99.4904
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.3491
                       Mean reward: 579.94
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.4304
    Episode_Reward/rotating_object: 121.7032
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.90s
                      Time elapsed: 00:19:45
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1245/1500 [0m                     

                       Computation: 110152 steps/s (collection: 0.756s, learning 0.137s)
             Mean action noise std: 3.30
          Mean value_function loss: 87.5048
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3497
                       Mean reward: 624.12
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 122.4520
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.89s
                      Time elapsed: 00:19:45
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1246/1500 [0m                     

                       Computation: 114642 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 3.30
          Mean value_function loss: 97.6649
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3499
                       Mean reward: 573.30
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.4245
    Episode_Reward/rotating_object: 120.1446
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.86s
                      Time elapsed: 00:19:46
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1247/1500 [0m                     

                       Computation: 117241 steps/s (collection: 0.746s, learning 0.092s)
             Mean action noise std: 3.30
          Mean value_function loss: 96.1623
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3496
                       Mean reward: 591.17
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.4268
    Episode_Reward/rotating_object: 121.6463
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.84s
                      Time elapsed: 00:19:47
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1248/1500 [0m                     

                       Computation: 113919 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 3.30
          Mean value_function loss: 88.1507
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3519
                       Mean reward: 622.54
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.4250
    Episode_Reward/rotating_object: 121.7784
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.86s
                      Time elapsed: 00:19:48
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1249/1500 [0m                     

                       Computation: 111466 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 3.30
          Mean value_function loss: 92.2233
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3546
                       Mean reward: 565.86
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 119.3730
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.88s
                      Time elapsed: 00:19:49
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1250/1500 [0m                     

                       Computation: 108571 steps/s (collection: 0.806s, learning 0.099s)
             Mean action noise std: 3.31
          Mean value_function loss: 80.0194
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3545
                       Mean reward: 564.70
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 118.1055
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.91s
                      Time elapsed: 00:19:50
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1251/1500 [0m                     

                       Computation: 103958 steps/s (collection: 0.797s, learning 0.149s)
             Mean action noise std: 3.31
          Mean value_function loss: 79.7647
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3568
                       Mean reward: 589.83
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.4254
    Episode_Reward/rotating_object: 119.4966
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.95s
                      Time elapsed: 00:19:51
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1252/1500 [0m                     

                       Computation: 112055 steps/s (collection: 0.772s, learning 0.106s)
             Mean action noise std: 3.31
          Mean value_function loss: 75.7569
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3573
                       Mean reward: 607.06
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.4287
    Episode_Reward/rotating_object: 121.4115
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.88s
                      Time elapsed: 00:19:52
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1253/1500 [0m                     

                       Computation: 110838 steps/s (collection: 0.748s, learning 0.139s)
             Mean action noise std: 3.31
          Mean value_function loss: 89.4948
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3592
                       Mean reward: 624.90
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.4290
    Episode_Reward/rotating_object: 121.1195
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.89s
                      Time elapsed: 00:19:53
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1254/1500 [0m                     

                       Computation: 110420 steps/s (collection: 0.752s, learning 0.139s)
             Mean action noise std: 3.31
          Mean value_function loss: 81.4867
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3639
                       Mean reward: 618.95
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.4215
    Episode_Reward/rotating_object: 120.2961
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.89s
                      Time elapsed: 00:19:53
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1255/1500 [0m                     

                       Computation: 114797 steps/s (collection: 0.757s, learning 0.100s)
             Mean action noise std: 3.31
          Mean value_function loss: 81.7517
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.3698
                       Mean reward: 569.27
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 122.3072
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.86s
                      Time elapsed: 00:19:54
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1256/1500 [0m                     

                       Computation: 115127 steps/s (collection: 0.757s, learning 0.097s)
             Mean action noise std: 3.31
          Mean value_function loss: 76.4316
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3695
                       Mean reward: 578.40
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.4193
    Episode_Reward/rotating_object: 119.6244
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.85s
                      Time elapsed: 00:19:55
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1257/1500 [0m                     

                       Computation: 109668 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 75.8814
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3723
                       Mean reward: 624.64
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.4277
    Episode_Reward/rotating_object: 123.1462
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.90s
                      Time elapsed: 00:19:56
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1258/1500 [0m                     

                       Computation: 112812 steps/s (collection: 0.769s, learning 0.103s)
             Mean action noise std: 3.32
          Mean value_function loss: 75.2422
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3710
                       Mean reward: 587.67
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.4285
    Episode_Reward/rotating_object: 123.9868
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.87s
                      Time elapsed: 00:19:57
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1259/1500 [0m                     

                       Computation: 114546 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 3.32
          Mean value_function loss: 77.6173
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.3742
                       Mean reward: 617.42
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.4283
    Episode_Reward/rotating_object: 126.2932
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.86s
                      Time elapsed: 00:19:58
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1260/1500 [0m                     

                       Computation: 111908 steps/s (collection: 0.751s, learning 0.127s)
             Mean action noise std: 3.32
          Mean value_function loss: 72.9186
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3760
                       Mean reward: 641.61
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 122.7023
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.88s
                      Time elapsed: 00:19:59
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1261/1500 [0m                     

                       Computation: 109489 steps/s (collection: 0.759s, learning 0.139s)
             Mean action noise std: 3.32
          Mean value_function loss: 70.4822
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.3809
                       Mean reward: 595.91
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.4260
    Episode_Reward/rotating_object: 122.6145
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.90s
                      Time elapsed: 00:20:00
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1262/1500 [0m                     

                       Computation: 105714 steps/s (collection: 0.748s, learning 0.182s)
             Mean action noise std: 3.32
          Mean value_function loss: 78.3434
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.3850
                       Mean reward: 610.59
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.4291
    Episode_Reward/rotating_object: 125.3271
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.93s
                      Time elapsed: 00:20:00
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1263/1500 [0m                     

                       Computation: 106582 steps/s (collection: 0.761s, learning 0.162s)
             Mean action noise std: 3.32
          Mean value_function loss: 74.9246
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.3839
                       Mean reward: 631.37
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 126.6069
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.92s
                      Time elapsed: 00:20:01
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1264/1500 [0m                     

                       Computation: 109771 steps/s (collection: 0.774s, learning 0.121s)
             Mean action noise std: 3.32
          Mean value_function loss: 78.3796
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3754
                       Mean reward: 617.11
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.4270
    Episode_Reward/rotating_object: 125.4653
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.90s
                      Time elapsed: 00:20:02
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1265/1500 [0m                     

                       Computation: 113839 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 3.33
          Mean value_function loss: 72.9625
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.3725
                       Mean reward: 616.73
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 121.8730
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.86s
                      Time elapsed: 00:20:03
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1266/1500 [0m                     

                       Computation: 111347 steps/s (collection: 0.778s, learning 0.105s)
             Mean action noise std: 3.33
          Mean value_function loss: 83.6902
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3810
                       Mean reward: 614.66
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.4231
    Episode_Reward/rotating_object: 123.5197
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.88s
                      Time elapsed: 00:20:04
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1267/1500 [0m                     

                       Computation: 116279 steps/s (collection: 0.753s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 74.8735
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.3824
                       Mean reward: 612.70
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 124.8664
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.85s
                      Time elapsed: 00:20:05
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1268/1500 [0m                     

                       Computation: 113049 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 72.2705
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3810
                       Mean reward: 653.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4274
    Episode_Reward/rotating_object: 126.0742
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.87s
                      Time elapsed: 00:20:06
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1269/1500 [0m                     

                       Computation: 113600 steps/s (collection: 0.763s, learning 0.103s)
             Mean action noise std: 3.34
          Mean value_function loss: 76.6852
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.3819
                       Mean reward: 651.88
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 123.9167
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.87s
                      Time elapsed: 00:20:07
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1270/1500 [0m                     

                       Computation: 109896 steps/s (collection: 0.765s, learning 0.129s)
             Mean action noise std: 3.34
          Mean value_function loss: 65.1590
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3845
                       Mean reward: 624.25
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.4265
    Episode_Reward/rotating_object: 121.8807
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.89s
                      Time elapsed: 00:20:07
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1271/1500 [0m                     

                       Computation: 117821 steps/s (collection: 0.745s, learning 0.090s)
             Mean action noise std: 3.34
          Mean value_function loss: 74.4338
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3854
                       Mean reward: 584.41
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.4196
    Episode_Reward/rotating_object: 120.7006
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.83s
                      Time elapsed: 00:20:08
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1272/1500 [0m                     

                       Computation: 116918 steps/s (collection: 0.751s, learning 0.090s)
             Mean action noise std: 3.34
          Mean value_function loss: 68.8104
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3926
                       Mean reward: 601.68
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.4209
    Episode_Reward/rotating_object: 121.3419
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.84s
                      Time elapsed: 00:20:09
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1273/1500 [0m                     

                       Computation: 113591 steps/s (collection: 0.754s, learning 0.111s)
             Mean action noise std: 3.34
          Mean value_function loss: 72.1021
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.3964
                       Mean reward: 657.41
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.4191
    Episode_Reward/rotating_object: 124.8329
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.87s
                      Time elapsed: 00:20:10
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1274/1500 [0m                     

                       Computation: 113927 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 77.8892
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4038
                       Mean reward: 606.64
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 122.3347
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.86s
                      Time elapsed: 00:20:11
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1275/1500 [0m                     

                       Computation: 111153 steps/s (collection: 0.784s, learning 0.100s)
             Mean action noise std: 3.35
          Mean value_function loss: 69.3563
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4025
                       Mean reward: 594.77
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.4190
    Episode_Reward/rotating_object: 120.6620
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.88s
                      Time elapsed: 00:20:12
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1276/1500 [0m                     

                       Computation: 113810 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 3.35
          Mean value_function loss: 74.7121
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4049
                       Mean reward: 612.87
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.4213
    Episode_Reward/rotating_object: 120.5981
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.86s
                      Time elapsed: 00:20:13
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1277/1500 [0m                     

                       Computation: 108201 steps/s (collection: 0.758s, learning 0.151s)
             Mean action noise std: 3.35
          Mean value_function loss: 72.2011
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.4043
                       Mean reward: 589.37
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.4169
    Episode_Reward/rotating_object: 121.5279
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.91s
                      Time elapsed: 00:20:14
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1278/1500 [0m                     

                       Computation: 115174 steps/s (collection: 0.755s, learning 0.098s)
             Mean action noise std: 3.35
          Mean value_function loss: 85.5008
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.4070
                       Mean reward: 587.59
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.4237
    Episode_Reward/rotating_object: 121.4966
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.85s
                      Time elapsed: 00:20:14
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1279/1500 [0m                     

                       Computation: 116633 steps/s (collection: 0.744s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 78.0961
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4104
                       Mean reward: 592.77
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 122.8255
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.84s
                      Time elapsed: 00:20:15
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1280/1500 [0m                     

                       Computation: 112432 steps/s (collection: 0.743s, learning 0.132s)
             Mean action noise std: 3.35
          Mean value_function loss: 70.7056
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.4103
                       Mean reward: 612.71
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.4219
    Episode_Reward/rotating_object: 120.7973
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.87s
                      Time elapsed: 00:20:16
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1281/1500 [0m                     

                       Computation: 105425 steps/s (collection: 0.822s, learning 0.111s)
             Mean action noise std: 3.35
          Mean value_function loss: 80.1169
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4038
                       Mean reward: 620.65
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.4274
    Episode_Reward/rotating_object: 124.5554
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.93s
                      Time elapsed: 00:20:17
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1282/1500 [0m                     

                       Computation: 106805 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 3.35
          Mean value_function loss: 72.8309
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.3992
                       Mean reward: 649.01
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 0.4188
    Episode_Reward/rotating_object: 121.7644
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.92s
                      Time elapsed: 00:20:18
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1283/1500 [0m                     

                       Computation: 111633 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 3.36
          Mean value_function loss: 74.7685
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4003
                       Mean reward: 597.08
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.4275
    Episode_Reward/rotating_object: 122.4467
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.88s
                      Time elapsed: 00:20:19
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1284/1500 [0m                     

                       Computation: 111607 steps/s (collection: 0.778s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 93.6541
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4078
                       Mean reward: 612.94
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.4249
    Episode_Reward/rotating_object: 121.3627
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.88s
                      Time elapsed: 00:20:20
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1285/1500 [0m                     

                       Computation: 107205 steps/s (collection: 0.799s, learning 0.118s)
             Mean action noise std: 3.36
          Mean value_function loss: 83.9512
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4044
                       Mean reward: 621.14
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 0.4174
    Episode_Reward/rotating_object: 120.5174
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.92s
                      Time elapsed: 00:20:21
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1286/1500 [0m                     

                       Computation: 108940 steps/s (collection: 0.796s, learning 0.106s)
             Mean action noise std: 3.36
          Mean value_function loss: 91.6043
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3972
                       Mean reward: 614.48
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.4234
    Episode_Reward/rotating_object: 121.2729
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.90s
                      Time elapsed: 00:20:22
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1287/1500 [0m                     

                       Computation: 107933 steps/s (collection: 0.756s, learning 0.155s)
             Mean action noise std: 3.36
          Mean value_function loss: 96.9366
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.4021
                       Mean reward: 585.97
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.4220
    Episode_Reward/rotating_object: 119.4549
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.91s
                      Time elapsed: 00:20:22
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1288/1500 [0m                     

                       Computation: 113583 steps/s (collection: 0.753s, learning 0.112s)
             Mean action noise std: 3.36
          Mean value_function loss: 85.1872
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4029
                       Mean reward: 624.35
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 0.4171
    Episode_Reward/rotating_object: 120.9366
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.87s
                      Time elapsed: 00:20:23
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1289/1500 [0m                     

                       Computation: 116200 steps/s (collection: 0.750s, learning 0.096s)
             Mean action noise std: 3.36
          Mean value_function loss: 78.1818
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.4075
                       Mean reward: 626.76
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.4241
    Episode_Reward/rotating_object: 124.7348
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.85s
                      Time elapsed: 00:20:24
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1290/1500 [0m                     

                       Computation: 105246 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 3.36
          Mean value_function loss: 91.5596
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.4087
                       Mean reward: 596.09
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 0.4223
    Episode_Reward/rotating_object: 121.2186
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.93s
                      Time elapsed: 00:20:25
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1291/1500 [0m                     

                       Computation: 117072 steps/s (collection: 0.746s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 86.6373
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.4090
                       Mean reward: 626.65
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 0.4262
    Episode_Reward/rotating_object: 123.0062
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.84s
                      Time elapsed: 00:20:26
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1292/1500 [0m                     

                       Computation: 109699 steps/s (collection: 0.802s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 68.3989
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.4118
                       Mean reward: 605.34
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 121.9587
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.90s
                      Time elapsed: 00:20:27
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1293/1500 [0m                     

                       Computation: 104898 steps/s (collection: 0.798s, learning 0.139s)
             Mean action noise std: 3.37
          Mean value_function loss: 77.4433
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.4135
                       Mean reward: 590.12
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.4226
    Episode_Reward/rotating_object: 122.8064
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.94s
                      Time elapsed: 00:20:28
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1294/1500 [0m                     

                       Computation: 109596 steps/s (collection: 0.796s, learning 0.101s)
             Mean action noise std: 3.37
          Mean value_function loss: 91.9773
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4166
                       Mean reward: 667.66
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.4251
    Episode_Reward/rotating_object: 125.5961
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.90s
                      Time elapsed: 00:20:29
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1295/1500 [0m                     

                       Computation: 111932 steps/s (collection: 0.785s, learning 0.093s)
             Mean action noise std: 3.37
          Mean value_function loss: 89.1590
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4226
                       Mean reward: 606.13
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.4187
    Episode_Reward/rotating_object: 116.2622
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.88s
                      Time elapsed: 00:20:30
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1296/1500 [0m                     

                       Computation: 109879 steps/s (collection: 0.761s, learning 0.133s)
             Mean action noise std: 3.38
          Mean value_function loss: 80.5114
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.4293
                       Mean reward: 611.75
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.4198
    Episode_Reward/rotating_object: 120.5272
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.89s
                      Time elapsed: 00:20:30
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1297/1500 [0m                     

                       Computation: 111017 steps/s (collection: 0.753s, learning 0.133s)
             Mean action noise std: 3.38
          Mean value_function loss: 73.7841
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.4342
                       Mean reward: 609.42
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.4177
    Episode_Reward/rotating_object: 122.7576
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.89s
                      Time elapsed: 00:20:31
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1298/1500 [0m                     

                       Computation: 111711 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 92.7907
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.4393
                       Mean reward: 615.08
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.4205
    Episode_Reward/rotating_object: 123.0061
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.88s
                      Time elapsed: 00:20:32
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1299/1500 [0m                     

                       Computation: 102254 steps/s (collection: 0.864s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 76.5176
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4371
                       Mean reward: 604.87
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.4317
    Episode_Reward/rotating_object: 126.0992
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.96s
                      Time elapsed: 00:20:33
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1300/1500 [0m                     

                       Computation: 109439 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 80.1608
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.4404
                       Mean reward: 609.68
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.4174
    Episode_Reward/rotating_object: 118.5000
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.90s
                      Time elapsed: 00:20:34
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1301/1500 [0m                     

                       Computation: 110695 steps/s (collection: 0.769s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 77.4142
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.4487
                       Mean reward: 639.19
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.4262
    Episode_Reward/rotating_object: 124.9272
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.89s
                      Time elapsed: 00:20:35
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1302/1500 [0m                     

                       Computation: 112035 steps/s (collection: 0.776s, learning 0.102s)
             Mean action noise std: 3.38
          Mean value_function loss: 69.6736
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4523
                       Mean reward: 622.93
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.4254
    Episode_Reward/rotating_object: 125.6139
        Episode_Reward/action_rate: -0.1120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.88s
                      Time elapsed: 00:20:36
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1303/1500 [0m                     

                       Computation: 111727 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 3.39
          Mean value_function loss: 66.1526
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4597
                       Mean reward: 619.46
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 123.5409
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.88s
                      Time elapsed: 00:20:37
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1304/1500 [0m                     

                       Computation: 111898 steps/s (collection: 0.761s, learning 0.117s)
             Mean action noise std: 3.39
          Mean value_function loss: 72.9919
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.4713
                       Mean reward: 617.17
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.4219
    Episode_Reward/rotating_object: 123.8482
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.88s
                      Time elapsed: 00:20:38
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1305/1500 [0m                     

                       Computation: 112791 steps/s (collection: 0.757s, learning 0.115s)
             Mean action noise std: 3.39
          Mean value_function loss: 86.9600
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.4689
                       Mean reward: 624.43
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 125.8707
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.87s
                      Time elapsed: 00:20:38
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1306/1500 [0m                     

                       Computation: 108351 steps/s (collection: 0.776s, learning 0.132s)
             Mean action noise std: 3.39
          Mean value_function loss: 86.5899
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4641
                       Mean reward: 623.27
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.4181
    Episode_Reward/rotating_object: 124.1590
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.91s
                      Time elapsed: 00:20:39
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1307/1500 [0m                     

                       Computation: 106904 steps/s (collection: 0.763s, learning 0.156s)
             Mean action noise std: 3.39
          Mean value_function loss: 78.1898
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.4660
                       Mean reward: 626.39
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.4190
    Episode_Reward/rotating_object: 121.8814
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.92s
                      Time elapsed: 00:20:40
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1308/1500 [0m                     

                       Computation: 113400 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 3.39
          Mean value_function loss: 69.1030
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.4634
                       Mean reward: 613.48
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.4222
    Episode_Reward/rotating_object: 126.2913
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.87s
                      Time elapsed: 00:20:41
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1309/1500 [0m                     

                       Computation: 104906 steps/s (collection: 0.822s, learning 0.115s)
             Mean action noise std: 3.40
          Mean value_function loss: 73.9734
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.4701
                       Mean reward: 633.40
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.4230
    Episode_Reward/rotating_object: 123.9920
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.94s
                      Time elapsed: 00:20:42
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1310/1500 [0m                     

                       Computation: 112720 steps/s (collection: 0.764s, learning 0.108s)
             Mean action noise std: 3.40
          Mean value_function loss: 79.7292
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4750
                       Mean reward: 622.86
               Mean episode length: 242.68
    Episode_Reward/reaching_object: 0.4208
    Episode_Reward/rotating_object: 121.3238
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.87s
                      Time elapsed: 00:20:43
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1311/1500 [0m                     

                       Computation: 115626 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 3.40
          Mean value_function loss: 74.9059
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.4717
                       Mean reward: 655.90
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4216
    Episode_Reward/rotating_object: 125.6518
        Episode_Reward/action_rate: -0.1129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.85s
                      Time elapsed: 00:20:44
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1312/1500 [0m                     

                       Computation: 107115 steps/s (collection: 0.800s, learning 0.118s)
             Mean action noise std: 3.40
          Mean value_function loss: 83.7298
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4759
                       Mean reward: 641.98
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 123.8242
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.92s
                      Time elapsed: 00:20:45
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1313/1500 [0m                     

                       Computation: 118257 steps/s (collection: 0.740s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 88.7676
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.4860
                       Mean reward: 614.32
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.4241
    Episode_Reward/rotating_object: 123.4811
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.83s
                      Time elapsed: 00:20:46
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1314/1500 [0m                     

                       Computation: 108162 steps/s (collection: 0.775s, learning 0.134s)
             Mean action noise std: 3.40
          Mean value_function loss: 67.2551
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.4956
                       Mean reward: 622.19
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4293
    Episode_Reward/rotating_object: 121.4368
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.91s
                      Time elapsed: 00:20:46
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1315/1500 [0m                     

                       Computation: 110087 steps/s (collection: 0.770s, learning 0.123s)
             Mean action noise std: 3.40
          Mean value_function loss: 75.5563
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.4953
                       Mean reward: 644.63
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.4238
    Episode_Reward/rotating_object: 123.8783
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.89s
                      Time elapsed: 00:20:47
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1316/1500 [0m                     

                       Computation: 109986 steps/s (collection: 0.755s, learning 0.139s)
             Mean action noise std: 3.40
          Mean value_function loss: 75.6741
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4912
                       Mean reward: 627.17
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.4290
    Episode_Reward/rotating_object: 125.8194
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.89s
                      Time elapsed: 00:20:48
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1317/1500 [0m                     

                       Computation: 110905 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 3.41
          Mean value_function loss: 84.1444
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4888
                       Mean reward: 620.88
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.4322
    Episode_Reward/rotating_object: 130.1211
        Episode_Reward/action_rate: -0.1183
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.89s
                      Time elapsed: 00:20:49
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1318/1500 [0m                     

                       Computation: 105901 steps/s (collection: 0.768s, learning 0.160s)
             Mean action noise std: 3.41
          Mean value_function loss: 69.1911
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.4915
                       Mean reward: 617.71
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.4262
    Episode_Reward/rotating_object: 123.4524
        Episode_Reward/action_rate: -0.1146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.93s
                      Time elapsed: 00:20:50
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1319/1500 [0m                     

                       Computation: 110926 steps/s (collection: 0.776s, learning 0.110s)
             Mean action noise std: 3.41
          Mean value_function loss: 68.8719
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4923
                       Mean reward: 612.62
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.4201
    Episode_Reward/rotating_object: 122.1872
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.89s
                      Time elapsed: 00:20:51
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1320/1500 [0m                     

                       Computation: 110838 steps/s (collection: 0.775s, learning 0.112s)
             Mean action noise std: 3.41
          Mean value_function loss: 66.6926
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.4909
                       Mean reward: 679.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4300
    Episode_Reward/rotating_object: 127.9238
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.89s
                      Time elapsed: 00:20:52
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1321/1500 [0m                     

                       Computation: 114544 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 70.3032
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4901
                       Mean reward: 655.32
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.4293
    Episode_Reward/rotating_object: 126.1848
        Episode_Reward/action_rate: -0.1146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.86s
                      Time elapsed: 00:20:53
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1322/1500 [0m                     

                       Computation: 111911 steps/s (collection: 0.783s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 63.9122
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4845
                       Mean reward: 616.47
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 122.2003
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.88s
                      Time elapsed: 00:20:54
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1323/1500 [0m                     

                       Computation: 112762 steps/s (collection: 0.770s, learning 0.102s)
             Mean action noise std: 3.41
          Mean value_function loss: 73.2443
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.4924
                       Mean reward: 629.18
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.4293
    Episode_Reward/rotating_object: 122.6608
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.87s
                      Time elapsed: 00:20:54
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1324/1500 [0m                     

                       Computation: 108698 steps/s (collection: 0.793s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 68.7562
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.5000
                       Mean reward: 595.81
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.4301
    Episode_Reward/rotating_object: 124.9953
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.90s
                      Time elapsed: 00:20:55
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1325/1500 [0m                     

                       Computation: 111693 steps/s (collection: 0.755s, learning 0.125s)
             Mean action noise std: 3.42
          Mean value_function loss: 65.0852
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.5042
                       Mean reward: 637.61
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.4308
    Episode_Reward/rotating_object: 120.8563
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.88s
                      Time elapsed: 00:20:56
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1326/1500 [0m                     

                       Computation: 115940 steps/s (collection: 0.747s, learning 0.101s)
             Mean action noise std: 3.42
          Mean value_function loss: 68.7170
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.5056
                       Mean reward: 637.40
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.4307
    Episode_Reward/rotating_object: 124.7038
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.85s
                      Time elapsed: 00:20:57
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1327/1500 [0m                     

                       Computation: 107482 steps/s (collection: 0.788s, learning 0.127s)
             Mean action noise std: 3.42
          Mean value_function loss: 73.0531
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5118
                       Mean reward: 648.39
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.4360
    Episode_Reward/rotating_object: 127.5694
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.91s
                      Time elapsed: 00:20:58
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1328/1500 [0m                     

                       Computation: 114443 steps/s (collection: 0.759s, learning 0.100s)
             Mean action noise std: 3.42
          Mean value_function loss: 72.7582
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.5147
                       Mean reward: 623.56
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.4381
    Episode_Reward/rotating_object: 130.0253
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.86s
                      Time elapsed: 00:20:59
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1329/1500 [0m                     

                       Computation: 106048 steps/s (collection: 0.798s, learning 0.129s)
             Mean action noise std: 3.42
          Mean value_function loss: 78.7251
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.5154
                       Mean reward: 649.42
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.4342
    Episode_Reward/rotating_object: 127.6722
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.93s
                      Time elapsed: 00:21:00
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1330/1500 [0m                     

                       Computation: 110409 steps/s (collection: 0.784s, learning 0.107s)
             Mean action noise std: 3.43
          Mean value_function loss: 71.3815
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.5196
                       Mean reward: 644.50
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.4371
    Episode_Reward/rotating_object: 126.7452
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.89s
                      Time elapsed: 00:21:01
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1331/1500 [0m                     

                       Computation: 110641 steps/s (collection: 0.771s, learning 0.117s)
             Mean action noise std: 3.43
          Mean value_function loss: 73.8673
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.5223
                       Mean reward: 604.71
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.4309
    Episode_Reward/rotating_object: 122.3911
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.89s
                      Time elapsed: 00:21:02
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1332/1500 [0m                     

                       Computation: 111319 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 3.43
          Mean value_function loss: 66.9022
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.5228
                       Mean reward: 622.39
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 125.0686
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.88s
                      Time elapsed: 00:21:02
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1333/1500 [0m                     

                       Computation: 47702 steps/s (collection: 1.966s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 76.5854
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5215
                       Mean reward: 637.05
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.4302
    Episode_Reward/rotating_object: 124.2934
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.06s
                      Time elapsed: 00:21:05
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1334/1500 [0m                     

                       Computation: 33525 steps/s (collection: 2.810s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 67.8834
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5187
                       Mean reward: 608.13
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.4359
    Episode_Reward/rotating_object: 126.9373
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 2.93s
                      Time elapsed: 00:21:07
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1335/1500 [0m                     

                       Computation: 32805 steps/s (collection: 2.878s, learning 0.118s)
             Mean action noise std: 3.43
          Mean value_function loss: 70.0920
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.5169
                       Mean reward: 613.67
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.4267
    Episode_Reward/rotating_object: 123.2096
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.00s
                      Time elapsed: 00:21:10
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1336/1500 [0m                     

                       Computation: 31924 steps/s (collection: 2.957s, learning 0.123s)
             Mean action noise std: 3.43
          Mean value_function loss: 81.9445
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.5188
                       Mean reward: 610.62
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 124.7436
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.08s
                      Time elapsed: 00:21:14
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1337/1500 [0m                     

                       Computation: 32479 steps/s (collection: 2.851s, learning 0.176s)
             Mean action noise std: 3.43
          Mean value_function loss: 74.2004
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5194
                       Mean reward: 587.28
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.4250
    Episode_Reward/rotating_object: 121.5739
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.03s
                      Time elapsed: 00:21:17
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1338/1500 [0m                     

                       Computation: 32202 steps/s (collection: 2.892s, learning 0.161s)
             Mean action noise std: 3.43
          Mean value_function loss: 72.9142
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5209
                       Mean reward: 639.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4366
    Episode_Reward/rotating_object: 126.7560
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.05s
                      Time elapsed: 00:21:20
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1339/1500 [0m                     

                       Computation: 32314 steps/s (collection: 2.895s, learning 0.147s)
             Mean action noise std: 3.44
          Mean value_function loss: 68.6427
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 18.5246
                       Mean reward: 586.53
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.4324
    Episode_Reward/rotating_object: 122.7021
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.04s
                      Time elapsed: 00:21:23
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1340/1500 [0m                     

                       Computation: 31723 steps/s (collection: 2.963s, learning 0.136s)
             Mean action noise std: 3.44
          Mean value_function loss: 78.4285
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 18.5281
                       Mean reward: 615.89
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.4348
    Episode_Reward/rotating_object: 122.7460
        Episode_Reward/action_rate: -0.1116
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.10s
                      Time elapsed: 00:21:26
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1341/1500 [0m                     

                       Computation: 21637 steps/s (collection: 4.382s, learning 0.161s)
             Mean action noise std: 3.44
          Mean value_function loss: 78.8795
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5301
                       Mean reward: 602.92
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 123.3972
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.54s
                      Time elapsed: 00:21:30
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1342/1500 [0m                     

                       Computation: 92975 steps/s (collection: 0.911s, learning 0.146s)
             Mean action noise std: 3.44
          Mean value_function loss: 94.7761
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5325
                       Mean reward: 596.61
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.4297
    Episode_Reward/rotating_object: 123.6003
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.06s
                      Time elapsed: 00:21:31
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1343/1500 [0m                     

                       Computation: 105650 steps/s (collection: 0.843s, learning 0.088s)
             Mean action noise std: 3.44
          Mean value_function loss: 100.6764
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.5363
                       Mean reward: 625.70
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.4281
    Episode_Reward/rotating_object: 124.4233
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.93s
                      Time elapsed: 00:21:32
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1344/1500 [0m                     

                       Computation: 114112 steps/s (collection: 0.774s, learning 0.087s)
             Mean action noise std: 3.44
          Mean value_function loss: 90.0140
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.5391
                       Mean reward: 613.49
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 121.3741
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.86s
                      Time elapsed: 00:21:33
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1345/1500 [0m                     

                       Computation: 122366 steps/s (collection: 0.713s, learning 0.090s)
             Mean action noise std: 3.44
          Mean value_function loss: 91.6832
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5403
                       Mean reward: 605.85
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 0.4294
    Episode_Reward/rotating_object: 121.4950
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.80s
                      Time elapsed: 00:21:34
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1346/1500 [0m                     

                       Computation: 112022 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 3.45
          Mean value_function loss: 81.3756
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5320
                       Mean reward: 627.41
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 122.9913
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.88s
                      Time elapsed: 00:21:35
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1347/1500 [0m                     

                       Computation: 113161 steps/s (collection: 0.771s, learning 0.098s)
             Mean action noise std: 3.45
          Mean value_function loss: 87.2455
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.5346
                       Mean reward: 597.63
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.4308
    Episode_Reward/rotating_object: 122.0090
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.87s
                      Time elapsed: 00:21:36
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1348/1500 [0m                     

                       Computation: 107104 steps/s (collection: 0.766s, learning 0.152s)
             Mean action noise std: 3.45
          Mean value_function loss: 77.4712
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5361
                       Mean reward: 622.74
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.4320
    Episode_Reward/rotating_object: 124.9118
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.92s
                      Time elapsed: 00:21:37
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1349/1500 [0m                     

                       Computation: 113269 steps/s (collection: 0.768s, learning 0.100s)
             Mean action noise std: 3.45
          Mean value_function loss: 77.2131
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.5355
                       Mean reward: 612.91
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.4327
    Episode_Reward/rotating_object: 123.3681
        Episode_Reward/action_rate: -0.1117
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.87s
                      Time elapsed: 00:21:37
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1350/1500 [0m                     

                       Computation: 109938 steps/s (collection: 0.776s, learning 0.119s)
             Mean action noise std: 3.45
          Mean value_function loss: 75.5677
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5321
                       Mean reward: 597.61
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.4308
    Episode_Reward/rotating_object: 123.0655
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.89s
                      Time elapsed: 00:21:38
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1351/1500 [0m                     

                       Computation: 110555 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 3.45
          Mean value_function loss: 85.0368
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5255
                       Mean reward: 637.09
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4303
    Episode_Reward/rotating_object: 123.9293
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.89s
                      Time elapsed: 00:21:39
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1352/1500 [0m                     

                       Computation: 119430 steps/s (collection: 0.735s, learning 0.089s)
             Mean action noise std: 3.45
          Mean value_function loss: 80.3971
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5235
                       Mean reward: 595.35
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.4284
    Episode_Reward/rotating_object: 122.7192
        Episode_Reward/action_rate: -0.1105
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.82s
                      Time elapsed: 00:21:40
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1353/1500 [0m                     

                       Computation: 114964 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 3.45
          Mean value_function loss: 74.4000
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.5289
                       Mean reward: 625.03
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.4329
    Episode_Reward/rotating_object: 126.9023
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.86s
                      Time elapsed: 00:21:41
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1354/1500 [0m                     

                       Computation: 113908 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 3.45
          Mean value_function loss: 59.6878
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5378
                       Mean reward: 621.95
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4330
    Episode_Reward/rotating_object: 123.8671
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.86s
                      Time elapsed: 00:21:42
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1355/1500 [0m                     

                       Computation: 112410 steps/s (collection: 0.787s, learning 0.088s)
             Mean action noise std: 3.45
          Mean value_function loss: 71.9856
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5422
                       Mean reward: 653.84
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4361
    Episode_Reward/rotating_object: 127.9513
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.87s
                      Time elapsed: 00:21:43
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1356/1500 [0m                     

                       Computation: 107797 steps/s (collection: 0.756s, learning 0.156s)
             Mean action noise std: 3.46
          Mean value_function loss: 63.7986
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.5367
                       Mean reward: 583.98
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.4274
    Episode_Reward/rotating_object: 123.0299
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.91s
                      Time elapsed: 00:21:44
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1357/1500 [0m                     

                       Computation: 114836 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 69.5973
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5355
                       Mean reward: 626.08
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.4292
    Episode_Reward/rotating_object: 123.7852
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.86s
                      Time elapsed: 00:21:44
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1358/1500 [0m                     

                       Computation: 113880 steps/s (collection: 0.757s, learning 0.107s)
             Mean action noise std: 3.46
          Mean value_function loss: 68.1641
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.5403
                       Mean reward: 646.20
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.4264
    Episode_Reward/rotating_object: 124.1871
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.86s
                      Time elapsed: 00:21:45
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1359/1500 [0m                     

                       Computation: 108361 steps/s (collection: 0.793s, learning 0.114s)
             Mean action noise std: 3.46
          Mean value_function loss: 70.2086
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.5413
                       Mean reward: 640.79
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.4276
    Episode_Reward/rotating_object: 121.9484
        Episode_Reward/action_rate: -0.1099
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.91s
                      Time elapsed: 00:21:46
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1360/1500 [0m                     

                       Computation: 120089 steps/s (collection: 0.734s, learning 0.085s)
             Mean action noise std: 3.46
          Mean value_function loss: 68.4374
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5408
                       Mean reward: 603.37
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.4288
    Episode_Reward/rotating_object: 122.4223
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.82s
                      Time elapsed: 00:21:47
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1361/1500 [0m                     

                       Computation: 115980 steps/s (collection: 0.758s, learning 0.090s)
             Mean action noise std: 3.46
          Mean value_function loss: 69.1815
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5438
                       Mean reward: 618.17
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4271
    Episode_Reward/rotating_object: 122.3568
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.85s
                      Time elapsed: 00:21:48
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1362/1500 [0m                     

                       Computation: 113749 steps/s (collection: 0.778s, learning 0.087s)
             Mean action noise std: 3.47
          Mean value_function loss: 67.8849
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.5494
                       Mean reward: 570.08
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.4278
    Episode_Reward/rotating_object: 119.7032
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.86s
                      Time elapsed: 00:21:49
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1363/1500 [0m                     

                       Computation: 104080 steps/s (collection: 0.790s, learning 0.155s)
             Mean action noise std: 3.47
          Mean value_function loss: 66.8655
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5557
                       Mean reward: 646.20
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.4277
    Episode_Reward/rotating_object: 124.5465
        Episode_Reward/action_rate: -0.1114
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.94s
                      Time elapsed: 00:21:50
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1364/1500 [0m                     

                       Computation: 106694 steps/s (collection: 0.802s, learning 0.120s)
             Mean action noise std: 3.47
          Mean value_function loss: 71.2731
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.5593
                       Mean reward: 624.40
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.4294
    Episode_Reward/rotating_object: 126.3340
        Episode_Reward/action_rate: -0.1136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.92s
                      Time elapsed: 00:21:51
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1365/1500 [0m                     

                       Computation: 115734 steps/s (collection: 0.762s, learning 0.088s)
             Mean action noise std: 3.47
          Mean value_function loss: 72.6374
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5675
                       Mean reward: 640.40
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.4319
    Episode_Reward/rotating_object: 127.6458
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.85s
                      Time elapsed: 00:21:51
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1366/1500 [0m                     

                       Computation: 111544 steps/s (collection: 0.755s, learning 0.126s)
             Mean action noise std: 3.47
          Mean value_function loss: 71.8174
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.5759
                       Mean reward: 647.93
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.4332
    Episode_Reward/rotating_object: 128.1017
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.88s
                      Time elapsed: 00:21:52
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1367/1500 [0m                     

                       Computation: 110374 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 3.48
          Mean value_function loss: 61.9565
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.5751
                       Mean reward: 625.60
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.4340
    Episode_Reward/rotating_object: 125.6006
        Episode_Reward/action_rate: -0.1150
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.89s
                      Time elapsed: 00:21:53
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1368/1500 [0m                     

                       Computation: 117336 steps/s (collection: 0.747s, learning 0.091s)
             Mean action noise std: 3.48
          Mean value_function loss: 78.3910
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.5777
                       Mean reward: 602.91
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.4338
    Episode_Reward/rotating_object: 124.3237
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.84s
                      Time elapsed: 00:21:54
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1369/1500 [0m                     

                       Computation: 113046 steps/s (collection: 0.781s, learning 0.089s)
             Mean action noise std: 3.48
          Mean value_function loss: 69.4036
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.5797
                       Mean reward: 618.50
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.4315
    Episode_Reward/rotating_object: 125.1528
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.87s
                      Time elapsed: 00:21:55
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1370/1500 [0m                     

                       Computation: 112747 steps/s (collection: 0.779s, learning 0.093s)
             Mean action noise std: 3.48
          Mean value_function loss: 72.3145
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5781
                       Mean reward: 645.46
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 125.9303
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.87s
                      Time elapsed: 00:21:56
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1371/1500 [0m                     

                       Computation: 103274 steps/s (collection: 0.805s, learning 0.147s)
             Mean action noise std: 3.48
          Mean value_function loss: 79.3247
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5791
                       Mean reward: 634.83
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 127.6498
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.95s
                      Time elapsed: 00:21:57
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1372/1500 [0m                     

                       Computation: 119587 steps/s (collection: 0.735s, learning 0.087s)
             Mean action noise std: 3.48
          Mean value_function loss: 79.5544
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.5844
                       Mean reward: 627.44
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.4294
    Episode_Reward/rotating_object: 124.8507
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.82s
                      Time elapsed: 00:21:58
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1373/1500 [0m                     

                       Computation: 106751 steps/s (collection: 0.741s, learning 0.180s)
             Mean action noise std: 3.49
          Mean value_function loss: 74.5565
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.5876
                       Mean reward: 665.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4340
    Episode_Reward/rotating_object: 127.0379
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.92s
                      Time elapsed: 00:21:58
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1374/1500 [0m                     

                       Computation: 109175 steps/s (collection: 0.759s, learning 0.142s)
             Mean action noise std: 3.49
          Mean value_function loss: 68.4911
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.6021
                       Mean reward: 640.57
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.4286
    Episode_Reward/rotating_object: 124.6906
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.90s
                      Time elapsed: 00:21:59
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1375/1500 [0m                     

                       Computation: 117606 steps/s (collection: 0.751s, learning 0.085s)
             Mean action noise std: 3.50
          Mean value_function loss: 69.2169
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.6102
                       Mean reward: 611.99
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.4237
    Episode_Reward/rotating_object: 124.4966
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.84s
                      Time elapsed: 00:22:00
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1376/1500 [0m                     

                       Computation: 113830 steps/s (collection: 0.776s, learning 0.088s)
             Mean action noise std: 3.50
          Mean value_function loss: 79.2480
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.6112
                       Mean reward: 598.26
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.4311
    Episode_Reward/rotating_object: 124.3625
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.86s
                      Time elapsed: 00:22:01
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1377/1500 [0m                     

                       Computation: 113467 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 3.50
          Mean value_function loss: 79.5470
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.6129
                       Mean reward: 607.74
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.4318
    Episode_Reward/rotating_object: 122.4710
        Episode_Reward/action_rate: -0.1129
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.87s
                      Time elapsed: 00:22:02
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1378/1500 [0m                     

                       Computation: 114588 steps/s (collection: 0.755s, learning 0.102s)
             Mean action noise std: 3.50
          Mean value_function loss: 73.9834
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.6248
                       Mean reward: 617.02
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.4377
    Episode_Reward/rotating_object: 126.7747
        Episode_Reward/action_rate: -0.1159
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.86s
                      Time elapsed: 00:22:03
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1379/1500 [0m                     

                       Computation: 113862 steps/s (collection: 0.749s, learning 0.114s)
             Mean action noise std: 3.50
          Mean value_function loss: 66.8316
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.6227
                       Mean reward: 631.26
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.4280
    Episode_Reward/rotating_object: 123.1863
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.86s
                      Time elapsed: 00:22:04
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1380/1500 [0m                     

                       Computation: 117094 steps/s (collection: 0.743s, learning 0.097s)
             Mean action noise std: 3.50
          Mean value_function loss: 57.3915
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.6196
                       Mean reward: 620.35
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.4337
    Episode_Reward/rotating_object: 125.5981
        Episode_Reward/action_rate: -0.1144
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.84s
                      Time elapsed: 00:22:05
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1381/1500 [0m                     

                       Computation: 114001 steps/s (collection: 0.726s, learning 0.137s)
             Mean action noise std: 3.50
          Mean value_function loss: 65.9956
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6208
                       Mean reward: 666.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4329
    Episode_Reward/rotating_object: 126.6788
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.86s
                      Time elapsed: 00:22:05
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1382/1500 [0m                     

                       Computation: 107572 steps/s (collection: 0.784s, learning 0.130s)
             Mean action noise std: 3.51
          Mean value_function loss: 70.5037
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6219
                       Mean reward: 645.27
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.4343
    Episode_Reward/rotating_object: 126.5413
        Episode_Reward/action_rate: -0.1164
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.91s
                      Time elapsed: 00:22:06
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1383/1500 [0m                     

                       Computation: 115379 steps/s (collection: 0.744s, learning 0.108s)
             Mean action noise std: 3.51
          Mean value_function loss: 66.9634
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6198
                       Mean reward: 622.44
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.4325
    Episode_Reward/rotating_object: 125.3727
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.85s
                      Time elapsed: 00:22:07
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1384/1500 [0m                     

                       Computation: 111516 steps/s (collection: 0.772s, learning 0.109s)
             Mean action noise std: 3.51
          Mean value_function loss: 81.6696
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.6198
                       Mean reward: 600.69
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 0.4276
    Episode_Reward/rotating_object: 123.5858
        Episode_Reward/action_rate: -0.1154
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.88s
                      Time elapsed: 00:22:08
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1385/1500 [0m                     

                       Computation: 116958 steps/s (collection: 0.735s, learning 0.106s)
             Mean action noise std: 3.51
          Mean value_function loss: 69.2212
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6235
                       Mean reward: 613.78
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.4259
    Episode_Reward/rotating_object: 121.6814
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.84s
                      Time elapsed: 00:22:09
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1386/1500 [0m                     

                       Computation: 115068 steps/s (collection: 0.763s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 80.2596
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.6329
                       Mean reward: 619.28
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.4297
    Episode_Reward/rotating_object: 125.9133
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.85s
                      Time elapsed: 00:22:10
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1387/1500 [0m                     

                       Computation: 115175 steps/s (collection: 0.734s, learning 0.119s)
             Mean action noise std: 3.51
          Mean value_function loss: 68.4810
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.6395
                       Mean reward: 617.60
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 0.4306
    Episode_Reward/rotating_object: 124.5584
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.85s
                      Time elapsed: 00:22:11
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1388/1500 [0m                     

                       Computation: 115944 steps/s (collection: 0.711s, learning 0.137s)
             Mean action noise std: 3.52
          Mean value_function loss: 89.4468
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6421
                       Mean reward: 624.78
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.4307
    Episode_Reward/rotating_object: 122.0081
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.85s
                      Time elapsed: 00:22:11
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1389/1500 [0m                     

                       Computation: 121610 steps/s (collection: 0.719s, learning 0.089s)
             Mean action noise std: 3.52
          Mean value_function loss: 80.6708
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.6432
                       Mean reward: 670.58
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4359
    Episode_Reward/rotating_object: 127.4825
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.81s
                      Time elapsed: 00:22:12
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1390/1500 [0m                     

                       Computation: 119338 steps/s (collection: 0.738s, learning 0.086s)
             Mean action noise std: 3.52
          Mean value_function loss: 68.5612
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.6501
                       Mean reward: 641.71
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.4353
    Episode_Reward/rotating_object: 124.7268
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.82s
                      Time elapsed: 00:22:13
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1391/1500 [0m                     

                       Computation: 114773 steps/s (collection: 0.754s, learning 0.103s)
             Mean action noise std: 3.52
          Mean value_function loss: 70.8891
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6641
                       Mean reward: 647.45
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.4342
    Episode_Reward/rotating_object: 125.6985
        Episode_Reward/action_rate: -0.1195
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.86s
                      Time elapsed: 00:22:14
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1392/1500 [0m                     

                       Computation: 119327 steps/s (collection: 0.734s, learning 0.090s)
             Mean action noise std: 3.53
          Mean value_function loss: 82.5369
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 18.6778
                       Mean reward: 632.18
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.4303
    Episode_Reward/rotating_object: 125.6986
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.82s
                      Time elapsed: 00:22:15
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1393/1500 [0m                     

                       Computation: 119444 steps/s (collection: 0.722s, learning 0.101s)
             Mean action noise std: 3.53
          Mean value_function loss: 84.3032
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.6785
                       Mean reward: 616.36
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.4326
    Episode_Reward/rotating_object: 123.5729
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.82s
                      Time elapsed: 00:22:16
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1394/1500 [0m                     

                       Computation: 118789 steps/s (collection: 0.733s, learning 0.094s)
             Mean action noise std: 3.53
          Mean value_function loss: 68.7121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.6749
                       Mean reward: 663.96
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.4397
    Episode_Reward/rotating_object: 128.8674
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.83s
                      Time elapsed: 00:22:16
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1395/1500 [0m                     

                       Computation: 117480 steps/s (collection: 0.739s, learning 0.098s)
             Mean action noise std: 3.53
          Mean value_function loss: 68.0602
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.6768
                       Mean reward: 646.81
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.4395
    Episode_Reward/rotating_object: 129.2725
        Episode_Reward/action_rate: -0.1229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.84s
                      Time elapsed: 00:22:17
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1396/1500 [0m                     

                       Computation: 116532 steps/s (collection: 0.752s, learning 0.091s)
             Mean action noise std: 3.53
          Mean value_function loss: 75.1687
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.6830
                       Mean reward: 622.96
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.4413
    Episode_Reward/rotating_object: 128.9897
        Episode_Reward/action_rate: -0.1226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.84s
                      Time elapsed: 00:22:18
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1397/1500 [0m                     

                       Computation: 113496 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 3.53
          Mean value_function loss: 67.6434
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.6883
                       Mean reward: 620.20
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.4412
    Episode_Reward/rotating_object: 127.4502
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.87s
                      Time elapsed: 00:22:19
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1398/1500 [0m                     

                       Computation: 119149 steps/s (collection: 0.736s, learning 0.089s)
             Mean action noise std: 3.54
          Mean value_function loss: 72.9637
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.6874
                       Mean reward: 627.42
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.4345
    Episode_Reward/rotating_object: 125.4197
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.83s
                      Time elapsed: 00:22:20
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1399/1500 [0m                     

                       Computation: 118602 steps/s (collection: 0.721s, learning 0.108s)
             Mean action noise std: 3.54
          Mean value_function loss: 75.6957
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.6926
                       Mean reward: 666.95
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.4364
    Episode_Reward/rotating_object: 125.0683
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.83s
                      Time elapsed: 00:22:21
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1400/1500 [0m                     

                       Computation: 117079 steps/s (collection: 0.724s, learning 0.116s)
             Mean action noise std: 3.54
          Mean value_function loss: 77.9427
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7024
                       Mean reward: 624.38
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.4298
    Episode_Reward/rotating_object: 119.2634
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.84s
                      Time elapsed: 00:22:21
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1401/1500 [0m                     

                       Computation: 119487 steps/s (collection: 0.734s, learning 0.089s)
             Mean action noise std: 3.54
          Mean value_function loss: 64.4284
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7075
                       Mean reward: 651.71
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.4334
    Episode_Reward/rotating_object: 124.3964
        Episode_Reward/action_rate: -0.1203
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.82s
                      Time elapsed: 00:22:22
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1402/1500 [0m                     

                       Computation: 117364 steps/s (collection: 0.747s, learning 0.091s)
             Mean action noise std: 3.54
          Mean value_function loss: 71.2960
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.7054
                       Mean reward: 648.29
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.4367
    Episode_Reward/rotating_object: 128.1054
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.84s
                      Time elapsed: 00:22:23
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1403/1500 [0m                     

                       Computation: 117221 steps/s (collection: 0.737s, learning 0.102s)
             Mean action noise std: 3.54
          Mean value_function loss: 63.7172
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.7029
                       Mean reward: 615.01
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.4364
    Episode_Reward/rotating_object: 124.0988
        Episode_Reward/action_rate: -0.1216
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.84s
                      Time elapsed: 00:22:24
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1404/1500 [0m                     

                       Computation: 119157 steps/s (collection: 0.718s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 64.8465
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.7026
                       Mean reward: 617.17
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.4360
    Episode_Reward/rotating_object: 128.2023
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.82s
                      Time elapsed: 00:22:25
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1405/1500 [0m                     

                       Computation: 115834 steps/s (collection: 0.724s, learning 0.125s)
             Mean action noise std: 3.55
          Mean value_function loss: 61.8562
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7072
                       Mean reward: 617.63
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.4277
    Episode_Reward/rotating_object: 122.8183
        Episode_Reward/action_rate: -0.1204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.85s
                      Time elapsed: 00:22:26
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1406/1500 [0m                     

                       Computation: 111707 steps/s (collection: 0.719s, learning 0.161s)
             Mean action noise std: 3.55
          Mean value_function loss: 66.0913
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7091
                       Mean reward: 637.10
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.4355
    Episode_Reward/rotating_object: 125.3682
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.88s
                      Time elapsed: 00:22:26
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1407/1500 [0m                     

                       Computation: 115097 steps/s (collection: 0.719s, learning 0.135s)
             Mean action noise std: 3.55
          Mean value_function loss: 59.5906
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7165
                       Mean reward: 632.80
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.4382
    Episode_Reward/rotating_object: 127.2161
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.85s
                      Time elapsed: 00:22:27
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1408/1500 [0m                     

                       Computation: 119680 steps/s (collection: 0.733s, learning 0.088s)
             Mean action noise std: 3.55
          Mean value_function loss: 80.5295
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.7277
                       Mean reward: 629.19
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.4386
    Episode_Reward/rotating_object: 128.2342
        Episode_Reward/action_rate: -0.1233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.82s
                      Time elapsed: 00:22:28
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1409/1500 [0m                     

                       Computation: 109132 steps/s (collection: 0.815s, learning 0.086s)
             Mean action noise std: 3.55
          Mean value_function loss: 75.2098
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.7282
                       Mean reward: 647.00
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.4323
    Episode_Reward/rotating_object: 126.2916
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.90s
                      Time elapsed: 00:22:29
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1410/1500 [0m                     

                       Computation: 115943 steps/s (collection: 0.735s, learning 0.113s)
             Mean action noise std: 3.55
          Mean value_function loss: 65.2508
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.7228
                       Mean reward: 644.09
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.4335
    Episode_Reward/rotating_object: 124.6087
        Episode_Reward/action_rate: -0.1205
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.85s
                      Time elapsed: 00:22:30
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1411/1500 [0m                     

                       Computation: 115733 steps/s (collection: 0.760s, learning 0.089s)
             Mean action noise std: 3.55
          Mean value_function loss: 79.7315
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.7217
                       Mean reward: 628.05
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.4363
    Episode_Reward/rotating_object: 127.1109
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.85s
                      Time elapsed: 00:22:31
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1412/1500 [0m                     

                       Computation: 107511 steps/s (collection: 0.752s, learning 0.162s)
             Mean action noise std: 3.56
          Mean value_function loss: 68.4404
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.7253
                       Mean reward: 640.88
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.4355
    Episode_Reward/rotating_object: 127.9523
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.91s
                      Time elapsed: 00:22:32
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1413/1500 [0m                     

                       Computation: 118303 steps/s (collection: 0.731s, learning 0.100s)
             Mean action noise std: 3.56
          Mean value_function loss: 69.6887
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7262
                       Mean reward: 598.90
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.4340
    Episode_Reward/rotating_object: 124.1078
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.83s
                      Time elapsed: 00:22:33
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1414/1500 [0m                     

                       Computation: 119955 steps/s (collection: 0.720s, learning 0.099s)
             Mean action noise std: 3.56
          Mean value_function loss: 63.1674
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7253
                       Mean reward: 632.61
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.4380
    Episode_Reward/rotating_object: 128.3012
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.82s
                      Time elapsed: 00:22:33
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1415/1500 [0m                     

                       Computation: 118405 steps/s (collection: 0.742s, learning 0.088s)
             Mean action noise std: 3.56
          Mean value_function loss: 61.6554
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.7300
                       Mean reward: 645.20
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.4378
    Episode_Reward/rotating_object: 128.8609
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.83s
                      Time elapsed: 00:22:34
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1416/1500 [0m                     

                       Computation: 118551 steps/s (collection: 0.741s, learning 0.088s)
             Mean action noise std: 3.56
          Mean value_function loss: 70.5424
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7405
                       Mean reward: 664.39
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.4364
    Episode_Reward/rotating_object: 128.2355
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.83s
                      Time elapsed: 00:22:35
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1417/1500 [0m                     

                       Computation: 120109 steps/s (collection: 0.727s, learning 0.091s)
             Mean action noise std: 3.56
          Mean value_function loss: 69.8565
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7486
                       Mean reward: 647.67
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.4341
    Episode_Reward/rotating_object: 126.7386
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.82s
                      Time elapsed: 00:22:36
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1418/1500 [0m                     

                       Computation: 117816 steps/s (collection: 0.750s, learning 0.085s)
             Mean action noise std: 3.57
          Mean value_function loss: 68.4613
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.7509
                       Mean reward: 613.89
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.4385
    Episode_Reward/rotating_object: 128.2335
        Episode_Reward/action_rate: -0.1259
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.83s
                      Time elapsed: 00:22:37
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1419/1500 [0m                     

                       Computation: 115153 steps/s (collection: 0.718s, learning 0.135s)
             Mean action noise std: 3.57
          Mean value_function loss: 69.0667
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7551
                       Mean reward: 633.96
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.4371
    Episode_Reward/rotating_object: 128.6632
        Episode_Reward/action_rate: -0.1251
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.85s
                      Time elapsed: 00:22:37
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1420/1500 [0m                     

                       Computation: 112367 steps/s (collection: 0.743s, learning 0.132s)
             Mean action noise std: 3.57
          Mean value_function loss: 88.0048
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.7598
                       Mean reward: 652.32
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.4395
    Episode_Reward/rotating_object: 130.4277
        Episode_Reward/action_rate: -0.1277
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.87s
                      Time elapsed: 00:22:38
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1421/1500 [0m                     

                       Computation: 119160 steps/s (collection: 0.734s, learning 0.091s)
             Mean action noise std: 3.57
          Mean value_function loss: 67.0949
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7583
                       Mean reward: 637.82
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.4361
    Episode_Reward/rotating_object: 127.6526
        Episode_Reward/action_rate: -0.1259
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.82s
                      Time elapsed: 00:22:39
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1422/1500 [0m                     

                       Computation: 113868 steps/s (collection: 0.771s, learning 0.092s)
             Mean action noise std: 3.57
          Mean value_function loss: 81.3824
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.7532
                       Mean reward: 638.48
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.4312
    Episode_Reward/rotating_object: 125.5196
        Episode_Reward/action_rate: -0.1231
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.86s
                      Time elapsed: 00:22:40
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1423/1500 [0m                     

                       Computation: 117047 steps/s (collection: 0.747s, learning 0.093s)
             Mean action noise std: 3.57
          Mean value_function loss: 72.6967
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.7511
                       Mean reward: 623.60
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 0.4346
    Episode_Reward/rotating_object: 128.1358
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.84s
                      Time elapsed: 00:22:41
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1424/1500 [0m                     

                       Computation: 117875 steps/s (collection: 0.722s, learning 0.112s)
             Mean action noise std: 3.58
          Mean value_function loss: 69.7589
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.7514
                       Mean reward: 606.77
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.4384
    Episode_Reward/rotating_object: 126.9043
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.83s
                      Time elapsed: 00:22:42
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1425/1500 [0m                     

                       Computation: 110283 steps/s (collection: 0.783s, learning 0.108s)
             Mean action noise std: 3.58
          Mean value_function loss: 80.9366
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.7572
                       Mean reward: 641.85
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.4397
    Episode_Reward/rotating_object: 129.7907
        Episode_Reward/action_rate: -0.1265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.89s
                      Time elapsed: 00:22:43
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1426/1500 [0m                     

                       Computation: 116357 steps/s (collection: 0.736s, learning 0.109s)
             Mean action noise std: 3.58
          Mean value_function loss: 68.1894
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.7611
                       Mean reward: 627.75
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.4375
    Episode_Reward/rotating_object: 127.3290
        Episode_Reward/action_rate: -0.1252
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.84s
                      Time elapsed: 00:22:43
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1427/1500 [0m                     

                       Computation: 117535 steps/s (collection: 0.713s, learning 0.123s)
             Mean action noise std: 3.58
          Mean value_function loss: 72.0843
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.7594
                       Mean reward: 589.16
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.4403
    Episode_Reward/rotating_object: 127.1943
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.84s
                      Time elapsed: 00:22:44
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1428/1500 [0m                     

                       Computation: 118528 steps/s (collection: 0.733s, learning 0.096s)
             Mean action noise std: 3.58
          Mean value_function loss: 62.0858
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7579
                       Mean reward: 611.33
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.4390
    Episode_Reward/rotating_object: 124.3979
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.83s
                      Time elapsed: 00:22:45
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1429/1500 [0m                     

                       Computation: 119017 steps/s (collection: 0.735s, learning 0.091s)
             Mean action noise std: 3.58
          Mean value_function loss: 75.6334
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7607
                       Mean reward: 633.98
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.4408
    Episode_Reward/rotating_object: 126.7103
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.83s
                      Time elapsed: 00:22:46
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1430/1500 [0m                     

                       Computation: 121387 steps/s (collection: 0.722s, learning 0.088s)
             Mean action noise std: 3.59
          Mean value_function loss: 65.6672
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7621
                       Mean reward: 654.74
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.4469
    Episode_Reward/rotating_object: 130.1234
        Episode_Reward/action_rate: -0.1276
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.81s
                      Time elapsed: 00:22:47
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1431/1500 [0m                     

                       Computation: 116618 steps/s (collection: 0.742s, learning 0.101s)
             Mean action noise std: 3.59
          Mean value_function loss: 102.9041
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7725
                       Mean reward: 642.03
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.4390
    Episode_Reward/rotating_object: 125.6815
        Episode_Reward/action_rate: -0.1257
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.84s
                      Time elapsed: 00:22:48
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1432/1500 [0m                     

                       Computation: 114266 steps/s (collection: 0.730s, learning 0.130s)
             Mean action noise std: 3.59
          Mean value_function loss: 84.7625
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.7713
                       Mean reward: 673.07
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.4404
    Episode_Reward/rotating_object: 127.0083
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.86s
                      Time elapsed: 00:22:48
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1433/1500 [0m                     

                       Computation: 115131 steps/s (collection: 0.730s, learning 0.123s)
             Mean action noise std: 3.59
          Mean value_function loss: 64.7981
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.7733
                       Mean reward: 627.73
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.4339
    Episode_Reward/rotating_object: 123.9519
        Episode_Reward/action_rate: -0.1256
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.85s
                      Time elapsed: 00:22:49
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1434/1500 [0m                     

                       Computation: 112171 steps/s (collection: 0.767s, learning 0.109s)
             Mean action noise std: 3.59
          Mean value_function loss: 75.0812
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.7753
                       Mean reward: 633.52
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.4395
    Episode_Reward/rotating_object: 125.8942
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.88s
                      Time elapsed: 00:22:50
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1435/1500 [0m                     

                       Computation: 116638 steps/s (collection: 0.748s, learning 0.095s)
             Mean action noise std: 3.60
          Mean value_function loss: 76.2930
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7749
                       Mean reward: 656.41
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.4367
    Episode_Reward/rotating_object: 123.2243
        Episode_Reward/action_rate: -0.1260
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.84s
                      Time elapsed: 00:22:51
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1436/1500 [0m                     

                       Computation: 115520 steps/s (collection: 0.744s, learning 0.107s)
             Mean action noise std: 3.60
          Mean value_function loss: 80.4791
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.7698
                       Mean reward: 659.37
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.4381
    Episode_Reward/rotating_object: 126.6300
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.85s
                      Time elapsed: 00:22:52
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1437/1500 [0m                     

                       Computation: 116768 steps/s (collection: 0.735s, learning 0.107s)
             Mean action noise std: 3.60
          Mean value_function loss: 70.8176
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7702
                       Mean reward: 634.80
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.4431
    Episode_Reward/rotating_object: 127.5337
        Episode_Reward/action_rate: -0.1284
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.84s
                      Time elapsed: 00:22:53
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1438/1500 [0m                     

                       Computation: 112637 steps/s (collection: 0.754s, learning 0.119s)
             Mean action noise std: 3.60
          Mean value_function loss: 65.3938
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.7743
                       Mean reward: 651.00
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.4413
    Episode_Reward/rotating_object: 127.9506
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.87s
                      Time elapsed: 00:22:54
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1439/1500 [0m                     

                       Computation: 116819 steps/s (collection: 0.738s, learning 0.104s)
             Mean action noise std: 3.60
          Mean value_function loss: 69.2304
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.7810
                       Mean reward: 639.26
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 0.4416
    Episode_Reward/rotating_object: 126.7356
        Episode_Reward/action_rate: -0.1280
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.84s
                      Time elapsed: 00:22:54
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1440/1500 [0m                     

                       Computation: 113851 steps/s (collection: 0.730s, learning 0.134s)
             Mean action noise std: 3.60
          Mean value_function loss: 67.9249
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7805
                       Mean reward: 651.92
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4414
    Episode_Reward/rotating_object: 127.3478
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.86s
                      Time elapsed: 00:22:55
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1441/1500 [0m                     

                       Computation: 119311 steps/s (collection: 0.727s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 78.4020
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.7766
                       Mean reward: 612.19
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.4386
    Episode_Reward/rotating_object: 128.2203
        Episode_Reward/action_rate: -0.1287
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.82s
                      Time elapsed: 00:22:56
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1442/1500 [0m                     

                       Computation: 119983 steps/s (collection: 0.733s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 68.4489
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7708
                       Mean reward: 635.50
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.4418
    Episode_Reward/rotating_object: 125.8280
        Episode_Reward/action_rate: -0.1292
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.82s
                      Time elapsed: 00:22:57
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1443/1500 [0m                     

                       Computation: 119230 steps/s (collection: 0.734s, learning 0.091s)
             Mean action noise std: 3.61
          Mean value_function loss: 65.2068
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.7688
                       Mean reward: 659.14
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.4422
    Episode_Reward/rotating_object: 130.5600
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.82s
                      Time elapsed: 00:22:58
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1444/1500 [0m                     

                       Computation: 116613 steps/s (collection: 0.738s, learning 0.105s)
             Mean action noise std: 3.61
          Mean value_function loss: 65.1377
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.7707
                       Mean reward: 598.85
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.4368
    Episode_Reward/rotating_object: 125.5058
        Episode_Reward/action_rate: -0.1274
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.84s
                      Time elapsed: 00:22:59
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1445/1500 [0m                     

                       Computation: 112296 steps/s (collection: 0.742s, learning 0.133s)
             Mean action noise std: 3.61
          Mean value_function loss: 61.8521
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7720
                       Mean reward: 633.64
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.4334
    Episode_Reward/rotating_object: 123.6817
        Episode_Reward/action_rate: -0.1268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.88s
                      Time elapsed: 00:22:59
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1446/1500 [0m                     

                       Computation: 113077 steps/s (collection: 0.708s, learning 0.161s)
             Mean action noise std: 3.61
          Mean value_function loss: 68.9010
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7703
                       Mean reward: 650.76
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.4419
    Episode_Reward/rotating_object: 128.1264
        Episode_Reward/action_rate: -0.1300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.87s
                      Time elapsed: 00:23:00
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1447/1500 [0m                     

                       Computation: 118732 steps/s (collection: 0.739s, learning 0.089s)
             Mean action noise std: 3.61
          Mean value_function loss: 79.3079
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 18.7762
                       Mean reward: 656.90
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.4426
    Episode_Reward/rotating_object: 126.1355
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.83s
                      Time elapsed: 00:23:01
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1448/1500 [0m                     

                       Computation: 117894 steps/s (collection: 0.747s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 88.9398
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.7804
                       Mean reward: 640.53
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.4410
    Episode_Reward/rotating_object: 127.6213
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.83s
                      Time elapsed: 00:23:02
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1449/1500 [0m                     

                       Computation: 114436 steps/s (collection: 0.755s, learning 0.104s)
             Mean action noise std: 3.61
          Mean value_function loss: 80.9828
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7816
                       Mean reward: 626.02
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.4416
    Episode_Reward/rotating_object: 124.2573
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.86s
                      Time elapsed: 00:23:03
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1450/1500 [0m                     

                       Computation: 118727 steps/s (collection: 0.737s, learning 0.091s)
             Mean action noise std: 3.62
          Mean value_function loss: 71.5442
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.7861
                       Mean reward: 645.16
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.4419
    Episode_Reward/rotating_object: 126.4237
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.83s
                      Time elapsed: 00:23:04
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1451/1500 [0m                     

                       Computation: 118518 steps/s (collection: 0.735s, learning 0.094s)
             Mean action noise std: 3.62
          Mean value_function loss: 77.4358
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.7894
                       Mean reward: 643.83
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.4332
    Episode_Reward/rotating_object: 126.3244
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.83s
                      Time elapsed: 00:23:05
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1452/1500 [0m                     

                       Computation: 113570 steps/s (collection: 0.737s, learning 0.129s)
             Mean action noise std: 3.62
          Mean value_function loss: 79.9329
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7935
                       Mean reward: 579.53
               Mean episode length: 238.66
    Episode_Reward/reaching_object: 0.4369
    Episode_Reward/rotating_object: 124.6721
        Episode_Reward/action_rate: -0.1287
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.87s
                      Time elapsed: 00:23:05
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1453/1500 [0m                     

                       Computation: 120560 steps/s (collection: 0.715s, learning 0.100s)
             Mean action noise std: 3.62
          Mean value_function loss: 68.5323
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7957
                       Mean reward: 638.61
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.4439
    Episode_Reward/rotating_object: 128.2306
        Episode_Reward/action_rate: -0.1316
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.82s
                      Time elapsed: 00:23:06
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1454/1500 [0m                     

                       Computation: 117895 steps/s (collection: 0.745s, learning 0.088s)
             Mean action noise std: 3.62
          Mean value_function loss: 65.5325
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 18.8013
                       Mean reward: 670.92
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.4417
    Episode_Reward/rotating_object: 129.0016
        Episode_Reward/action_rate: -0.1321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.83s
                      Time elapsed: 00:23:07
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1455/1500 [0m                     

                       Computation: 121374 steps/s (collection: 0.722s, learning 0.088s)
             Mean action noise std: 3.62
          Mean value_function loss: 80.8406
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.8091
                       Mean reward: 618.74
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.4387
    Episode_Reward/rotating_object: 127.0993
        Episode_Reward/action_rate: -0.1311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.81s
                      Time elapsed: 00:23:08
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1456/1500 [0m                     

                       Computation: 113038 steps/s (collection: 0.765s, learning 0.105s)
             Mean action noise std: 3.63
          Mean value_function loss: 87.9467
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.8102
                       Mean reward: 637.63
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 0.4386
    Episode_Reward/rotating_object: 125.4566
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.87s
                      Time elapsed: 00:23:09
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1457/1500 [0m                     

                       Computation: 107925 steps/s (collection: 0.777s, learning 0.134s)
             Mean action noise std: 3.63
          Mean value_function loss: 84.1581
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8064
                       Mean reward: 607.39
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.4382
    Episode_Reward/rotating_object: 123.7178
        Episode_Reward/action_rate: -0.1299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.91s
                      Time elapsed: 00:23:10
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1458/1500 [0m                     

                       Computation: 110887 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 3.63
          Mean value_function loss: 78.6300
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8063
                       Mean reward: 655.45
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.4359
    Episode_Reward/rotating_object: 122.3409
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.89s
                      Time elapsed: 00:23:11
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1459/1500 [0m                     

                       Computation: 110279 steps/s (collection: 0.729s, learning 0.162s)
             Mean action noise std: 3.63
          Mean value_function loss: 67.2811
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.8179
                       Mean reward: 578.65
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.4387
    Episode_Reward/rotating_object: 123.8129
        Episode_Reward/action_rate: -0.1312
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.89s
                      Time elapsed: 00:23:11
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1460/1500 [0m                     

                       Computation: 117848 steps/s (collection: 0.703s, learning 0.132s)
             Mean action noise std: 3.63
          Mean value_function loss: 60.8203
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.8250
                       Mean reward: 649.70
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.4467
    Episode_Reward/rotating_object: 129.3517
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.83s
                      Time elapsed: 00:23:12
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1461/1500 [0m                     

                       Computation: 118310 steps/s (collection: 0.738s, learning 0.093s)
             Mean action noise std: 3.64
          Mean value_function loss: 80.7601
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8366
                       Mean reward: 641.59
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.4416
    Episode_Reward/rotating_object: 129.2474
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.83s
                      Time elapsed: 00:23:13
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1462/1500 [0m                     

                       Computation: 116373 steps/s (collection: 0.753s, learning 0.092s)
             Mean action noise std: 3.64
          Mean value_function loss: 75.6201
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8460
                       Mean reward: 634.25
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.4398
    Episode_Reward/rotating_object: 128.0346
        Episode_Reward/action_rate: -0.1351
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.84s
                      Time elapsed: 00:23:14
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1463/1500 [0m                     

                       Computation: 115164 steps/s (collection: 0.759s, learning 0.094s)
             Mean action noise std: 3.64
          Mean value_function loss: 70.2051
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.8504
                       Mean reward: 609.29
               Mean episode length: 242.58
    Episode_Reward/reaching_object: 0.4368
    Episode_Reward/rotating_object: 126.2976
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.85s
                      Time elapsed: 00:23:15
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1464/1500 [0m                     

                       Computation: 115700 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 3.64
          Mean value_function loss: 78.7755
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8508
                       Mean reward: 625.06
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.4407
    Episode_Reward/rotating_object: 127.0507
        Episode_Reward/action_rate: -0.1373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.85s
                      Time elapsed: 00:23:16
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1465/1500 [0m                     

                       Computation: 116567 steps/s (collection: 0.737s, learning 0.106s)
             Mean action noise std: 3.64
          Mean value_function loss: 72.6777
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8499
                       Mean reward: 620.19
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.4417
    Episode_Reward/rotating_object: 125.0515
        Episode_Reward/action_rate: -0.1361
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.84s
                      Time elapsed: 00:23:16
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1466/1500 [0m                     

                       Computation: 114163 steps/s (collection: 0.724s, learning 0.138s)
             Mean action noise std: 3.64
          Mean value_function loss: 72.4985
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.8509
                       Mean reward: 593.85
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.4371
    Episode_Reward/rotating_object: 126.8071
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.86s
                      Time elapsed: 00:23:17
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1467/1500 [0m                     

                       Computation: 121243 steps/s (collection: 0.722s, learning 0.089s)
             Mean action noise std: 3.64
          Mean value_function loss: 82.1819
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.8505
                       Mean reward: 595.10
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.4313
    Episode_Reward/rotating_object: 120.4520
        Episode_Reward/action_rate: -0.1334
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.81s
                      Time elapsed: 00:23:18
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1468/1500 [0m                     

                       Computation: 119458 steps/s (collection: 0.727s, learning 0.096s)
             Mean action noise std: 3.65
          Mean value_function loss: 70.3535
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8547
                       Mean reward: 577.23
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.4243
    Episode_Reward/rotating_object: 121.8513
        Episode_Reward/action_rate: -0.1325
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.82s
                      Time elapsed: 00:23:19
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1469/1500 [0m                     

                       Computation: 116402 steps/s (collection: 0.755s, learning 0.090s)
             Mean action noise std: 3.65
          Mean value_function loss: 68.0426
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.8538
                       Mean reward: 648.72
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.4438
    Episode_Reward/rotating_object: 129.9009
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.84s
                      Time elapsed: 00:23:20
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1470/1500 [0m                     

                       Computation: 116649 steps/s (collection: 0.741s, learning 0.102s)
             Mean action noise std: 3.65
          Mean value_function loss: 69.4776
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.8559
                       Mean reward: 618.22
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.4346
    Episode_Reward/rotating_object: 125.2389
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.84s
                      Time elapsed: 00:23:21
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1471/1500 [0m                     

                       Computation: 110600 steps/s (collection: 0.783s, learning 0.106s)
             Mean action noise std: 3.65
          Mean value_function loss: 63.2249
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 18.8559
                       Mean reward: 637.34
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.4426
    Episode_Reward/rotating_object: 126.3992
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.89s
                      Time elapsed: 00:23:22
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1472/1500 [0m                     

                       Computation: 113792 steps/s (collection: 0.732s, learning 0.132s)
             Mean action noise std: 3.65
          Mean value_function loss: 80.7490
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8564
                       Mean reward: 615.17
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.4364
    Episode_Reward/rotating_object: 125.6315
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.86s
                      Time elapsed: 00:23:22
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1473/1500 [0m                     

                       Computation: 117605 steps/s (collection: 0.730s, learning 0.106s)
             Mean action noise std: 3.65
          Mean value_function loss: 67.5595
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.8558
                       Mean reward: 658.18
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 0.4309
    Episode_Reward/rotating_object: 127.3322
        Episode_Reward/action_rate: -0.1347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.84s
                      Time elapsed: 00:23:23
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1474/1500 [0m                     

                       Computation: 117485 steps/s (collection: 0.732s, learning 0.105s)
             Mean action noise std: 3.65
          Mean value_function loss: 62.9468
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.8514
                       Mean reward: 632.82
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.4370
    Episode_Reward/rotating_object: 126.9607
        Episode_Reward/action_rate: -0.1370
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.84s
                      Time elapsed: 00:23:24
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1475/1500 [0m                     

                       Computation: 118597 steps/s (collection: 0.738s, learning 0.091s)
             Mean action noise std: 3.66
          Mean value_function loss: 66.0064
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8539
                       Mean reward: 619.72
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.4332
    Episode_Reward/rotating_object: 125.6213
        Episode_Reward/action_rate: -0.1358
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.83s
                      Time elapsed: 00:23:25
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1476/1500 [0m                     

                       Computation: 115056 steps/s (collection: 0.751s, learning 0.103s)
             Mean action noise std: 3.66
          Mean value_function loss: 64.5576
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.8612
                       Mean reward: 633.58
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.4403
    Episode_Reward/rotating_object: 127.4251
        Episode_Reward/action_rate: -0.1386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.85s
                      Time elapsed: 00:23:26
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1477/1500 [0m                     

                       Computation: 117245 steps/s (collection: 0.730s, learning 0.108s)
             Mean action noise std: 3.66
          Mean value_function loss: 68.8825
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.8706
                       Mean reward: 601.55
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.4348
    Episode_Reward/rotating_object: 125.5126
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.84s
                      Time elapsed: 00:23:27
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1478/1500 [0m                     

                       Computation: 122953 steps/s (collection: 0.704s, learning 0.095s)
             Mean action noise std: 3.66
          Mean value_function loss: 67.8815
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.8793
                       Mean reward: 628.08
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.4436
    Episode_Reward/rotating_object: 131.6756
        Episode_Reward/action_rate: -0.1393
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.80s
                      Time elapsed: 00:23:27
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1479/1500 [0m                     

                       Computation: 114656 steps/s (collection: 0.737s, learning 0.121s)
             Mean action noise std: 3.67
          Mean value_function loss: 70.1112
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.8830
                       Mean reward: 650.51
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.4368
    Episode_Reward/rotating_object: 125.9743
        Episode_Reward/action_rate: -0.1356
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.86s
                      Time elapsed: 00:23:28
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1480/1500 [0m                     

                       Computation: 115902 steps/s (collection: 0.758s, learning 0.090s)
             Mean action noise std: 3.67
          Mean value_function loss: 73.7315
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.8842
                       Mean reward: 603.19
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.4345
    Episode_Reward/rotating_object: 125.5984
        Episode_Reward/action_rate: -0.1344
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.85s
                      Time elapsed: 00:23:29
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1481/1500 [0m                     

                       Computation: 113638 steps/s (collection: 0.772s, learning 0.093s)
             Mean action noise std: 3.67
          Mean value_function loss: 69.7568
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.8836
                       Mean reward: 624.42
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.4355
    Episode_Reward/rotating_object: 127.2269
        Episode_Reward/action_rate: -0.1359
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.87s
                      Time elapsed: 00:23:30
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1482/1500 [0m                     

                       Computation: 104775 steps/s (collection: 0.808s, learning 0.130s)
             Mean action noise std: 3.67
          Mean value_function loss: 72.9871
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.8793
                       Mean reward: 654.58
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.4389
    Episode_Reward/rotating_object: 129.2219
        Episode_Reward/action_rate: -0.1356
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.94s
                      Time elapsed: 00:23:31
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1483/1500 [0m                     

                       Computation: 113137 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 3.67
          Mean value_function loss: 68.6795
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.8839
                       Mean reward: 643.79
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.4377
    Episode_Reward/rotating_object: 127.1159
        Episode_Reward/action_rate: -0.1371
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.87s
                      Time elapsed: 00:23:32
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1484/1500 [0m                     

                       Computation: 117403 steps/s (collection: 0.747s, learning 0.090s)
             Mean action noise std: 3.67
          Mean value_function loss: 64.6868
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.8899
                       Mean reward: 637.52
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.4422
    Episode_Reward/rotating_object: 129.2444
        Episode_Reward/action_rate: -0.1391
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.84s
                      Time elapsed: 00:23:33
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1485/1500 [0m                     

                       Computation: 111588 steps/s (collection: 0.746s, learning 0.135s)
             Mean action noise std: 3.67
          Mean value_function loss: 66.9931
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.8906
                       Mean reward: 639.65
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.4363
    Episode_Reward/rotating_object: 126.2607
        Episode_Reward/action_rate: -0.1365
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.88s
                      Time elapsed: 00:23:34
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1486/1500 [0m                     

                       Computation: 110544 steps/s (collection: 0.771s, learning 0.119s)
             Mean action noise std: 3.68
          Mean value_function loss: 66.1136
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.8879
                       Mean reward: 650.86
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.4369
    Episode_Reward/rotating_object: 127.5753
        Episode_Reward/action_rate: -0.1374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.89s
                      Time elapsed: 00:23:34
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1487/1500 [0m                     

                       Computation: 118191 steps/s (collection: 0.739s, learning 0.093s)
             Mean action noise std: 3.68
          Mean value_function loss: 62.1367
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.8899
                       Mean reward: 589.58
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.4365
    Episode_Reward/rotating_object: 122.9594
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.83s
                      Time elapsed: 00:23:35
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1488/1500 [0m                     

                       Computation: 120259 steps/s (collection: 0.732s, learning 0.086s)
             Mean action noise std: 3.68
          Mean value_function loss: 72.4275
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8964
                       Mean reward: 643.38
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.4406
    Episode_Reward/rotating_object: 124.9910
        Episode_Reward/action_rate: -0.1362
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.82s
                      Time elapsed: 00:23:36
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1489/1500 [0m                     

                       Computation: 119788 steps/s (collection: 0.724s, learning 0.097s)
             Mean action noise std: 3.68
          Mean value_function loss: 69.0529
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9014
                       Mean reward: 647.29
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.4433
    Episode_Reward/rotating_object: 127.9211
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.82s
                      Time elapsed: 00:23:37
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1490/1500 [0m                     

                       Computation: 120143 steps/s (collection: 0.730s, learning 0.089s)
             Mean action noise std: 3.68
          Mean value_function loss: 66.6302
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.9071
                       Mean reward: 620.42
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.4386
    Episode_Reward/rotating_object: 127.7666
        Episode_Reward/action_rate: -0.1382
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.82s
                      Time elapsed: 00:23:38
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1491/1500 [0m                     

                       Computation: 115626 steps/s (collection: 0.741s, learning 0.110s)
             Mean action noise std: 3.68
          Mean value_function loss: 59.0200
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.9096
                       Mean reward: 649.55
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.4439
    Episode_Reward/rotating_object: 128.0963
        Episode_Reward/action_rate: -0.1400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.85s
                      Time elapsed: 00:23:39
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1492/1500 [0m                     

                       Computation: 117437 steps/s (collection: 0.743s, learning 0.094s)
             Mean action noise std: 3.68
          Mean value_function loss: 70.6671
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9179
                       Mean reward: 623.70
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4392
    Episode_Reward/rotating_object: 127.2782
        Episode_Reward/action_rate: -0.1378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.84s
                      Time elapsed: 00:23:39
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1493/1500 [0m                     

                       Computation: 102949 steps/s (collection: 0.794s, learning 0.161s)
             Mean action noise std: 3.69
          Mean value_function loss: 73.6803
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9166
                       Mean reward: 618.16
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.4438
    Episode_Reward/rotating_object: 128.7594
        Episode_Reward/action_rate: -0.1398
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.95s
                      Time elapsed: 00:23:40
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1494/1500 [0m                     

                       Computation: 111760 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 3.69
          Mean value_function loss: 77.6192
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9054
                       Mean reward: 648.70
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.4433
    Episode_Reward/rotating_object: 130.7595
        Episode_Reward/action_rate: -0.1415
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.88s
                      Time elapsed: 00:23:41
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1495/1500 [0m                     

                       Computation: 103833 steps/s (collection: 0.819s, learning 0.128s)
             Mean action noise std: 3.69
          Mean value_function loss: 78.3698
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.9004
                       Mean reward: 655.45
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.4383
    Episode_Reward/rotating_object: 127.7155
        Episode_Reward/action_rate: -0.1397
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.95s
                      Time elapsed: 00:23:42
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1496/1500 [0m                     

                       Computation: 110432 steps/s (collection: 0.787s, learning 0.103s)
             Mean action noise std: 3.69
          Mean value_function loss: 73.3359
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.9067
                       Mean reward: 635.24
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.4451
    Episode_Reward/rotating_object: 127.1816
        Episode_Reward/action_rate: -0.1408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.89s
                      Time elapsed: 00:23:43
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1497/1500 [0m                     

                       Computation: 118688 steps/s (collection: 0.735s, learning 0.094s)
             Mean action noise std: 3.69
          Mean value_function loss: 74.9781
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.9056
                       Mean reward: 619.92
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.4423
    Episode_Reward/rotating_object: 124.6368
        Episode_Reward/action_rate: -0.1400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.83s
                      Time elapsed: 00:23:44
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1498/1500 [0m                     

                       Computation: 117930 steps/s (collection: 0.739s, learning 0.094s)
             Mean action noise std: 3.69
          Mean value_function loss: 74.9226
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.9064
                       Mean reward: 636.08
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.4426
    Episode_Reward/rotating_object: 125.8982
        Episode_Reward/action_rate: -0.1392
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.83s
                      Time elapsed: 00:23:45
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1499/1500 [0m                     

                       Computation: 114642 steps/s (collection: 0.764s, learning 0.094s)
             Mean action noise std: 3.69
          Mean value_function loss: 77.2467
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.9058
                       Mean reward: 648.04
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.4460
    Episode_Reward/rotating_object: 129.4686
        Episode_Reward/action_rate: -0.1407
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.86s
                      Time elapsed: 00:23:46
                               ETA: 00:00:00

