################################################################################
                      [1m Learning iteration 0/1500 [0m                       

                       Computation: 16913 steps/s (collection: 5.508s, learning 0.304s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 11.3681
                       Mean reward: 0.00
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0003
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.81s
                      Time elapsed: 00:00:05
                               ETA: 02:25:18

################################################################################
                      [1m Learning iteration 1/1500 [0m                       

                       Computation: 34978 steps/s (collection: 2.647s, learning 0.163s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 11.4172
                       Mean reward: 0.01
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0014
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.81s
                      Time elapsed: 00:00:08
                               ETA: 01:47:42

################################################################################
                      [1m Learning iteration 2/1500 [0m                       

                       Computation: 34043 steps/s (collection: 2.748s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.4218
                       Mean reward: 0.01
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0025
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.89s
                      Time elapsed: 00:00:11
                               ETA: 01:35:47

################################################################################
                      [1m Learning iteration 3/1500 [0m                       

                       Computation: 34030 steps/s (collection: 2.738s, learning 0.151s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 11.4421
                       Mean reward: 0.02
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0042
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.89s
                      Time elapsed: 00:00:14
                               ETA: 01:29:48

################################################################################
                      [1m Learning iteration 4/1500 [0m                       

                       Computation: 31524 steps/s (collection: 2.966s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 11.4371
                       Mean reward: 0.02
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0054
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.12s
                      Time elapsed: 00:00:17
                               ETA: 01:27:21

################################################################################
                      [1m Learning iteration 5/1500 [0m                       

                       Computation: 32719 steps/s (collection: 2.836s, learning 0.168s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 11.4509
                       Mean reward: 0.03
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0084
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.00s
                      Time elapsed: 00:00:20
                               ETA: 01:25:13

################################################################################
                      [1m Learning iteration 6/1500 [0m                       

                       Computation: 34233 steps/s (collection: 2.732s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 11.4307
                       Mean reward: 0.05
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0106
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.87s
                      Time elapsed: 00:00:23
                               ETA: 01:23:12

################################################################################
                      [1m Learning iteration 7/1500 [0m                       

                       Computation: 33194 steps/s (collection: 2.815s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 11.4193
                       Mean reward: 0.07
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0146
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 2.96s
                      Time elapsed: 00:00:26
                               ETA: 01:21:58

################################################################################
                      [1m Learning iteration 8/1500 [0m                       

                       Computation: 24665 steps/s (collection: 3.857s, learning 0.128s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 11.4139
                       Mean reward: 0.12
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0209
    Episode_Reward/rotating_object: 0.0001
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.99s
                      Time elapsed: 00:00:30
                               ETA: 01:23:49

################################################################################
                      [1m Learning iteration 9/1500 [0m                       

                       Computation: 110823 steps/s (collection: 0.737s, learning 0.150s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 11.4208
                       Mean reward: 0.13
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0276
    Episode_Reward/rotating_object: 0.0001
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.89s
                      Time elapsed: 00:00:31
                               ETA: 01:17:36

################################################################################
                      [1m Learning iteration 10/1500 [0m                      

                       Computation: 120698 steps/s (collection: 0.707s, learning 0.108s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 11.4289
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0399
    Episode_Reward/rotating_object: 0.0004
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.81s
                      Time elapsed: 00:00:32
                               ETA: 01:12:20

################################################################################
                      [1m Learning iteration 11/1500 [0m                      

                       Computation: 120376 steps/s (collection: 0.701s, learning 0.116s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 11.4654
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0538
    Episode_Reward/rotating_object: 0.0053
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.82s
                      Time elapsed: 00:00:32
                               ETA: 01:07:57

################################################################################
                      [1m Learning iteration 12/1500 [0m                      

                       Computation: 119463 steps/s (collection: 0.727s, learning 0.096s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0601
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.4972
                       Mean reward: 0.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0702
    Episode_Reward/rotating_object: 0.0057
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.82s
                      Time elapsed: 00:00:33
                               ETA: 01:04:15

################################################################################
                      [1m Learning iteration 13/1500 [0m                      

                       Computation: 116186 steps/s (collection: 0.725s, learning 0.122s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0537
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 11.5368
                       Mean reward: 0.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0897
    Episode_Reward/rotating_object: 0.0213
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.85s
                      Time elapsed: 00:00:34
                               ETA: 01:01:07

################################################################################
                      [1m Learning iteration 14/1500 [0m                      

                       Computation: 120008 steps/s (collection: 0.710s, learning 0.110s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1987
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 11.6002
                       Mean reward: 0.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1023
    Episode_Reward/rotating_object: 0.0509
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.82s
                      Time elapsed: 00:00:35
                               ETA: 00:58:21

################################################################################
                      [1m Learning iteration 15/1500 [0m                      

                       Computation: 117104 steps/s (collection: 0.703s, learning 0.136s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.1382
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 11.6557
                       Mean reward: 1.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1224
    Episode_Reward/rotating_object: 0.1017
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.84s
                      Time elapsed: 00:00:36
                               ETA: 00:55:58

################################################################################
                      [1m Learning iteration 16/1500 [0m                      

                       Computation: 119676 steps/s (collection: 0.718s, learning 0.103s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.2354
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.6975
                       Mean reward: 1.37
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.1430
    Episode_Reward/rotating_object: 0.1306
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.82s
                      Time elapsed: 00:00:37
                               ETA: 00:53:50

################################################################################
                      [1m Learning iteration 17/1500 [0m                      

                       Computation: 122361 steps/s (collection: 0.710s, learning 0.093s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.3556
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.7513
                       Mean reward: 1.78
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.1598
    Episode_Reward/rotating_object: 0.1509
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.80s
                      Time elapsed: 00:00:37
                               ETA: 00:51:55

################################################################################
                      [1m Learning iteration 18/1500 [0m                      

                       Computation: 113844 steps/s (collection: 0.758s, learning 0.106s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.4312
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.8310
                       Mean reward: 1.87
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.1713
    Episode_Reward/rotating_object: 0.2747
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.86s
                      Time elapsed: 00:00:38
                               ETA: 00:50:16

################################################################################
                      [1m Learning iteration 19/1500 [0m                      

                       Computation: 121815 steps/s (collection: 0.705s, learning 0.102s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.2233
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 11.8941
                       Mean reward: 2.04
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.1837
    Episode_Reward/rotating_object: 0.3085
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.81s
                      Time elapsed: 00:00:39
                               ETA: 00:48:43

################################################################################
                      [1m Learning iteration 20/1500 [0m                      

                       Computation: 117705 steps/s (collection: 0.708s, learning 0.128s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.3115
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.9326
                       Mean reward: 1.94
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.1870
    Episode_Reward/rotating_object: 0.4029
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.84s
                      Time elapsed: 00:00:40
                               ETA: 00:47:21

################################################################################
                      [1m Learning iteration 21/1500 [0m                      

                       Computation: 124768 steps/s (collection: 0.695s, learning 0.093s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2106
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.9815
                       Mean reward: 2.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1954
    Episode_Reward/rotating_object: 0.3231
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.79s
                      Time elapsed: 00:00:41
                               ETA: 00:46:03

################################################################################
                      [1m Learning iteration 22/1500 [0m                      

                       Computation: 124632 steps/s (collection: 0.703s, learning 0.086s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.3997
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.0429
                       Mean reward: 2.37
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.2064
    Episode_Reward/rotating_object: 0.4859
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.79s
                      Time elapsed: 00:00:41
                               ETA: 00:44:52

################################################################################
                      [1m Learning iteration 23/1500 [0m                      

                       Computation: 119854 steps/s (collection: 0.727s, learning 0.093s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.3149
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.1087
                       Mean reward: 7.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2076
    Episode_Reward/rotating_object: 0.7714
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.82s
                      Time elapsed: 00:00:42
                               ETA: 00:43:48

################################################################################
                      [1m Learning iteration 24/1500 [0m                      

                       Computation: 114497 steps/s (collection: 0.743s, learning 0.116s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.5131
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 12.1908
                       Mean reward: 5.00
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.2128
    Episode_Reward/rotating_object: 0.5078
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                      Time elapsed: 00:00:43
                               ETA: 00:42:52

################################################################################
                      [1m Learning iteration 25/1500 [0m                      

                       Computation: 116018 steps/s (collection: 0.718s, learning 0.129s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.2216
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.2966
                       Mean reward: 3.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2068
    Episode_Reward/rotating_object: 0.5459
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.85s
                      Time elapsed: 00:00:44
                               ETA: 00:41:59

################################################################################
                      [1m Learning iteration 26/1500 [0m                      

                       Computation: 116306 steps/s (collection: 0.730s, learning 0.116s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.2338
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 12.3589
                       Mean reward: 3.50
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.2097
    Episode_Reward/rotating_object: 0.4557
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.85s
                      Time elapsed: 00:00:45
                               ETA: 00:41:11

################################################################################
                      [1m Learning iteration 27/1500 [0m                      

                       Computation: 113777 steps/s (collection: 0.704s, learning 0.160s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2557
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 12.4272
                       Mean reward: 2.42
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.2134
    Episode_Reward/rotating_object: 0.3710
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.86s
                      Time elapsed: 00:00:46
                               ETA: 00:40:26

################################################################################
                      [1m Learning iteration 28/1500 [0m                      

                       Computation: 115813 steps/s (collection: 0.743s, learning 0.106s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.6383
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.4729
                       Mean reward: 4.06
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.2146
    Episode_Reward/rotating_object: 0.7059
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.85s
                      Time elapsed: 00:00:46
                               ETA: 00:39:44

################################################################################
                      [1m Learning iteration 29/1500 [0m                      

                       Computation: 116458 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.4530
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 12.5434
                       Mean reward: 3.07
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.2184
    Episode_Reward/rotating_object: 0.4792
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.84s
                      Time elapsed: 00:00:47
                               ETA: 00:39:04

################################################################################
                      [1m Learning iteration 30/1500 [0m                      

                       Computation: 117798 steps/s (collection: 0.742s, learning 0.092s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2799
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.5918
                       Mean reward: 2.40
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.2237
    Episode_Reward/rotating_object: 0.4242
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.83s
                      Time elapsed: 00:00:48
                               ETA: 00:38:27

################################################################################
                      [1m Learning iteration 31/1500 [0m                      

                       Computation: 108928 steps/s (collection: 0.755s, learning 0.147s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3544
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 12.6577
                       Mean reward: 5.69
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.2253
    Episode_Reward/rotating_object: 0.6634
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.90s
                      Time elapsed: 00:00:49
                               ETA: 00:37:55

################################################################################
                      [1m Learning iteration 32/1500 [0m                      

                       Computation: 116842 steps/s (collection: 0.731s, learning 0.111s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.4429
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 12.6907
                       Mean reward: 5.60
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.2297
    Episode_Reward/rotating_object: 0.7136
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.84s
                      Time elapsed: 00:00:50
                               ETA: 00:37:22

################################################################################
                      [1m Learning iteration 33/1500 [0m                      

                       Computation: 118265 steps/s (collection: 0.717s, learning 0.115s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3458
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.7437
                       Mean reward: 5.35
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.2429
    Episode_Reward/rotating_object: 0.6018
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.83s
                      Time elapsed: 00:00:51
                               ETA: 00:36:50

################################################################################
                      [1m Learning iteration 34/1500 [0m                      

                       Computation: 112936 steps/s (collection: 0.761s, learning 0.109s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.5101
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.7957
                       Mean reward: 3.69
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.2466
    Episode_Reward/rotating_object: 0.5967
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.87s
                      Time elapsed: 00:00:52
                               ETA: 00:36:22

################################################################################
                      [1m Learning iteration 35/1500 [0m                      

                       Computation: 118926 steps/s (collection: 0.737s, learning 0.090s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3378
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 12.8392
                       Mean reward: 4.76
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.2575
    Episode_Reward/rotating_object: 0.5750
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.83s
                      Time elapsed: 00:00:52
                               ETA: 00:35:53

################################################################################
                      [1m Learning iteration 36/1500 [0m                      

                       Computation: 115453 steps/s (collection: 0.737s, learning 0.115s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3230
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.8738
                       Mean reward: 5.14
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 0.2576
    Episode_Reward/rotating_object: 0.7354
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.85s
                      Time elapsed: 00:00:53
                               ETA: 00:35:27

################################################################################
                      [1m Learning iteration 37/1500 [0m                      

                       Computation: 116761 steps/s (collection: 0.752s, learning 0.090s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.6430
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 12.9276
                       Mean reward: 5.49
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.2641
    Episode_Reward/rotating_object: 0.6675
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.84s
                      Time elapsed: 00:00:54
                               ETA: 00:35:02

################################################################################
                      [1m Learning iteration 38/1500 [0m                      

                       Computation: 110292 steps/s (collection: 0.771s, learning 0.120s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7899
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 12.9862
                       Mean reward: 5.95
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.2698
    Episode_Reward/rotating_object: 0.9554
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.89s
                      Time elapsed: 00:00:55
                               ETA: 00:34:41

################################################################################
                      [1m Learning iteration 39/1500 [0m                      

                       Computation: 113016 steps/s (collection: 0.761s, learning 0.108s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7448
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.0351
                       Mean reward: 6.18
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 0.2795
    Episode_Reward/rotating_object: 0.9623
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.87s
                      Time elapsed: 00:00:56
                               ETA: 00:34:19

################################################################################
                      [1m Learning iteration 40/1500 [0m                      

                       Computation: 109149 steps/s (collection: 0.763s, learning 0.138s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.7264
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.0515
                       Mean reward: 5.63
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.2812
    Episode_Reward/rotating_object: 1.0083
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.90s
                      Time elapsed: 00:00:57
                               ETA: 00:33:59

################################################################################
                      [1m Learning iteration 41/1500 [0m                      

                       Computation: 114277 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.7106
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 13.1214
                       Mean reward: 5.81
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 0.2864
    Episode_Reward/rotating_object: 1.0386
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.86s
                      Time elapsed: 00:00:58
                               ETA: 00:33:39

################################################################################
                      [1m Learning iteration 42/1500 [0m                      

                       Computation: 109945 steps/s (collection: 0.770s, learning 0.125s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.4872
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 13.1589
                       Mean reward: 5.62
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.2911
    Episode_Reward/rotating_object: 1.1944
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.89s
                      Time elapsed: 00:00:59
                               ETA: 00:33:21

################################################################################
                      [1m Learning iteration 43/1500 [0m                      

                       Computation: 114339 steps/s (collection: 0.733s, learning 0.127s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.5893
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.1656
                       Mean reward: 8.68
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.2946
    Episode_Reward/rotating_object: 1.2070
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.86s
                      Time elapsed: 00:00:59
                               ETA: 00:33:03

################################################################################
                      [1m Learning iteration 44/1500 [0m                      

                       Computation: 116258 steps/s (collection: 0.756s, learning 0.089s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.5302
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 13.2069
                       Mean reward: 7.43
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 1.6054
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.85s
                      Time elapsed: 00:01:00
                               ETA: 00:32:45

################################################################################
                      [1m Learning iteration 45/1500 [0m                      

                       Computation: 108718 steps/s (collection: 0.811s, learning 0.094s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.6880
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 13.2302
                       Mean reward: 6.35
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.3018
    Episode_Reward/rotating_object: 1.0490
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.90s
                      Time elapsed: 00:01:01
                               ETA: 00:32:29

################################################################################
                      [1m Learning iteration 46/1500 [0m                      

                       Computation: 110708 steps/s (collection: 0.782s, learning 0.106s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.0373
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 13.2471
                       Mean reward: 10.41
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3062
    Episode_Reward/rotating_object: 1.3816
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.89s
                      Time elapsed: 00:01:02
                               ETA: 00:32:14

################################################################################
                      [1m Learning iteration 47/1500 [0m                      

                       Computation: 114877 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.0729
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 13.2739
                       Mean reward: 7.04
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 1.3863
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.86s
                      Time elapsed: 00:01:03
                               ETA: 00:31:58

################################################################################
                      [1m Learning iteration 48/1500 [0m                      

                       Computation: 112308 steps/s (collection: 0.756s, learning 0.120s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.0145
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 13.2938
                       Mean reward: 8.00
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 1.3371
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.88s
                      Time elapsed: 00:01:04
                               ETA: 00:31:44

################################################################################
                      [1m Learning iteration 49/1500 [0m                      

                       Computation: 107930 steps/s (collection: 0.786s, learning 0.125s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.8621
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 13.3182
                       Mean reward: 7.87
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 1.1548
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.91s
                      Time elapsed: 00:01:05
                               ETA: 00:31:31

################################################################################
                      [1m Learning iteration 50/1500 [0m                      

                       Computation: 107281 steps/s (collection: 0.741s, learning 0.175s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.4933
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 13.3348
                       Mean reward: 9.70
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 1.3897
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.92s
                      Time elapsed: 00:01:06
                               ETA: 00:31:19

################################################################################
                      [1m Learning iteration 51/1500 [0m                      

                       Computation: 109836 steps/s (collection: 0.760s, learning 0.135s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.5851
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 13.3540
                       Mean reward: 8.50
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 1.6097
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.90s
                      Time elapsed: 00:01:06
                               ETA: 00:31:06

################################################################################
                      [1m Learning iteration 52/1500 [0m                      

                       Computation: 115188 steps/s (collection: 0.768s, learning 0.086s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.1806
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 13.3739
                       Mean reward: 11.27
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 1.8253
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.85s
                      Time elapsed: 00:01:07
                               ETA: 00:30:53

################################################################################
                      [1m Learning iteration 53/1500 [0m                      

                       Computation: 107594 steps/s (collection: 0.808s, learning 0.105s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.2628
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.3889
                       Mean reward: 9.88
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 1.6462
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.91s
                      Time elapsed: 00:01:08
                               ETA: 00:30:42

################################################################################
                      [1m Learning iteration 54/1500 [0m                      

                       Computation: 108987 steps/s (collection: 0.812s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.3258
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 13.4208
                       Mean reward: 10.16
               Mean episode length: 242.29
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 1.6331
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.90s
                      Time elapsed: 00:01:09
                               ETA: 00:30:31

################################################################################
                      [1m Learning iteration 55/1500 [0m                      

                       Computation: 101161 steps/s (collection: 0.861s, learning 0.111s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.1855
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.4353
                       Mean reward: 9.66
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 1.7229
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.97s
                      Time elapsed: 00:01:10
                               ETA: 00:30:22

################################################################################
                      [1m Learning iteration 56/1500 [0m                      

                       Computation: 110908 steps/s (collection: 0.779s, learning 0.107s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.2022
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 13.4627
                       Mean reward: 11.08
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 1.7188
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.89s
                      Time elapsed: 00:01:11
                               ETA: 00:30:11

################################################################################
                      [1m Learning iteration 57/1500 [0m                      

                       Computation: 105136 steps/s (collection: 0.824s, learning 0.111s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.2663
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 13.4787
                       Mean reward: 14.59
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.3263
    Episode_Reward/rotating_object: 1.8834
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.94s
                      Time elapsed: 00:01:12
                               ETA: 00:30:02

################################################################################
                      [1m Learning iteration 58/1500 [0m                      

                       Computation: 104806 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.3931
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 13.4849
                       Mean reward: 6.58
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 1.4520
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.94s
                      Time elapsed: 00:01:13
                               ETA: 00:29:53

################################################################################
                      [1m Learning iteration 59/1500 [0m                      

                       Computation: 105510 steps/s (collection: 0.812s, learning 0.120s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.2140
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 13.4981
                       Mean reward: 11.39
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 1.6803
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.93s
                      Time elapsed: 00:01:14
                               ETA: 00:29:44

################################################################################
                      [1m Learning iteration 60/1500 [0m                      

                       Computation: 102841 steps/s (collection: 0.841s, learning 0.115s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.2048
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.5144
                       Mean reward: 10.24
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 1.4243
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.96s
                      Time elapsed: 00:01:15
                               ETA: 00:29:37

################################################################################
                      [1m Learning iteration 61/1500 [0m                      

                       Computation: 105385 steps/s (collection: 0.787s, learning 0.146s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.5041
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.5548
                       Mean reward: 14.43
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 1.7982
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.93s
                      Time elapsed: 00:01:16
                               ETA: 00:29:28

################################################################################
                      [1m Learning iteration 62/1500 [0m                      

                       Computation: 105830 steps/s (collection: 0.795s, learning 0.134s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.5406
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 13.5886
                       Mean reward: 8.63
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 1.9162
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.93s
                      Time elapsed: 00:01:17
                               ETA: 00:29:20

################################################################################
                      [1m Learning iteration 63/1500 [0m                      

                       Computation: 108570 steps/s (collection: 0.762s, learning 0.144s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.2254
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.5957
                       Mean reward: 10.34
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 1.7782
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.91s
                      Time elapsed: 00:01:18
                               ETA: 00:29:12

################################################################################
                      [1m Learning iteration 64/1500 [0m                      

                       Computation: 107382 steps/s (collection: 0.782s, learning 0.134s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.3990
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.5939
                       Mean reward: 8.41
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 1.7944
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.92s
                      Time elapsed: 00:01:18
                               ETA: 00:29:04

################################################################################
                      [1m Learning iteration 65/1500 [0m                      

                       Computation: 109858 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.2750
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 13.6155
                       Mean reward: 9.50
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 1.7553
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.89s
                      Time elapsed: 00:01:19
                               ETA: 00:28:56

################################################################################
                      [1m Learning iteration 66/1500 [0m                      

                       Computation: 111473 steps/s (collection: 0.797s, learning 0.085s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.8884
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 13.6353
                       Mean reward: 11.27
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 1.8961
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.88s
                      Time elapsed: 00:01:20
                               ETA: 00:28:47

################################################################################
                      [1m Learning iteration 67/1500 [0m                      

                       Computation: 96112 steps/s (collection: 0.872s, learning 0.151s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.7200
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.6411
                       Mean reward: 10.89
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.3070
    Episode_Reward/rotating_object: 1.7223
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.02s
                      Time elapsed: 00:01:21
                               ETA: 00:28:42

################################################################################
                      [1m Learning iteration 68/1500 [0m                      

                       Computation: 107338 steps/s (collection: 0.824s, learning 0.092s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.4705
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.6626
                       Mean reward: 9.52
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.3101
    Episode_Reward/rotating_object: 1.6143
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.92s
                      Time elapsed: 00:01:22
                               ETA: 00:28:35

################################################################################
                      [1m Learning iteration 69/1500 [0m                      

                       Computation: 108415 steps/s (collection: 0.812s, learning 0.095s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.5518
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 13.6882
                       Mean reward: 12.33
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.3038
    Episode_Reward/rotating_object: 1.7569
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.91s
                      Time elapsed: 00:01:23
                               ETA: 00:28:28

################################################################################
                      [1m Learning iteration 70/1500 [0m                      

                       Computation: 106799 steps/s (collection: 0.824s, learning 0.096s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.6873
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 13.7152
                       Mean reward: 8.08
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 1.5853
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.92s
                      Time elapsed: 00:01:24
                               ETA: 00:28:21

################################################################################
                      [1m Learning iteration 71/1500 [0m                      

                       Computation: 108815 steps/s (collection: 0.799s, learning 0.105s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.0360
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 13.7342
                       Mean reward: 10.09
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 1.8942
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.90s
                      Time elapsed: 00:01:25
                               ETA: 00:28:15

################################################################################
                      [1m Learning iteration 72/1500 [0m                      

                       Computation: 110355 steps/s (collection: 0.770s, learning 0.121s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.9739
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.7497
                       Mean reward: 10.83
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 1.7850
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.89s
                      Time elapsed: 00:01:26
                               ETA: 00:28:08

################################################################################
                      [1m Learning iteration 73/1500 [0m                      

                       Computation: 108149 steps/s (collection: 0.757s, learning 0.152s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.2668
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.7754
                       Mean reward: 9.75
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 1.5537
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.91s
                      Time elapsed: 00:01:27
                               ETA: 00:28:01

################################################################################
                      [1m Learning iteration 74/1500 [0m                      

                       Computation: 101968 steps/s (collection: 0.816s, learning 0.148s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.8786
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.7993
                       Mean reward: 12.03
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 1.9843
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.96s
                      Time elapsed: 00:01:28
                               ETA: 00:27:56

################################################################################
                      [1m Learning iteration 75/1500 [0m                      

                       Computation: 100744 steps/s (collection: 0.837s, learning 0.139s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.0158
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 13.8248
                       Mean reward: 11.40
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 2.0155
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.98s
                      Time elapsed: 00:01:29
                               ETA: 00:27:51

################################################################################
                      [1m Learning iteration 76/1500 [0m                      

                       Computation: 103391 steps/s (collection: 0.841s, learning 0.110s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.1910
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.8342
                       Mean reward: 9.94
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 0.3103
    Episode_Reward/rotating_object: 1.9265
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.95s
                      Time elapsed: 00:01:30
                               ETA: 00:27:46

################################################################################
                      [1m Learning iteration 77/1500 [0m                      

                       Computation: 105147 steps/s (collection: 0.784s, learning 0.151s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.1128
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.8527
                       Mean reward: 10.62
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 0.3055
    Episode_Reward/rotating_object: 1.7299
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.93s
                      Time elapsed: 00:01:31
                               ETA: 00:27:40

################################################################################
                      [1m Learning iteration 78/1500 [0m                      

                       Computation: 113804 steps/s (collection: 0.778s, learning 0.086s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.2830
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.8885
                       Mean reward: 11.65
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.3004
    Episode_Reward/rotating_object: 1.7280
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.86s
                      Time elapsed: 00:01:31
                               ETA: 00:27:34

################################################################################
                      [1m Learning iteration 79/1500 [0m                      

                       Computation: 111599 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.4838
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.9127
                       Mean reward: 11.77
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 2.1686
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.88s
                      Time elapsed: 00:01:32
                               ETA: 00:27:27

################################################################################
                      [1m Learning iteration 80/1500 [0m                      

                       Computation: 112544 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.6810
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 13.9358
                       Mean reward: 8.78
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 2.1426
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.87s
                      Time elapsed: 00:01:33
                               ETA: 00:27:21

################################################################################
                      [1m Learning iteration 81/1500 [0m                      

                       Computation: 113466 steps/s (collection: 0.777s, learning 0.089s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.4151
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 13.9634
                       Mean reward: 11.59
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 2.0435
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.87s
                      Time elapsed: 00:01:34
                               ETA: 00:27:15

################################################################################
                      [1m Learning iteration 82/1500 [0m                      

                       Computation: 112615 steps/s (collection: 0.765s, learning 0.108s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.4175
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 13.9896
                       Mean reward: 15.15
               Mean episode length: 233.47
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 2.2322
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.87s
                      Time elapsed: 00:01:35
                               ETA: 00:27:09

################################################################################
                      [1m Learning iteration 83/1500 [0m                      

                       Computation: 111539 steps/s (collection: 0.771s, learning 0.110s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.4765
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.0088
                       Mean reward: 11.81
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 1.9663
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.88s
                      Time elapsed: 00:01:36
                               ETA: 00:27:03

################################################################################
                      [1m Learning iteration 84/1500 [0m                      

                       Computation: 104252 steps/s (collection: 0.773s, learning 0.170s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.9255
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.0388
                       Mean reward: 11.27
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 1.8528
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.94s
                      Time elapsed: 00:01:37
                               ETA: 00:26:59

################################################################################
                      [1m Learning iteration 85/1500 [0m                      

                       Computation: 105212 steps/s (collection: 0.803s, learning 0.132s)
             Mean action noise std: 1.41
          Mean value_function loss: 3.0150
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.0557
                       Mean reward: 15.07
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 2.2563
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.93s
                      Time elapsed: 00:01:38
                               ETA: 00:26:54

################################################################################
                      [1m Learning iteration 86/1500 [0m                      

                       Computation: 103656 steps/s (collection: 0.784s, learning 0.165s)
             Mean action noise std: 1.42
          Mean value_function loss: 3.5647
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.0750
                       Mean reward: 12.86
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 2.1613
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.95s
                      Time elapsed: 00:01:39
                               ETA: 00:26:50

################################################################################
                      [1m Learning iteration 87/1500 [0m                      

                       Computation: 107803 steps/s (collection: 0.812s, learning 0.100s)
             Mean action noise std: 1.42
          Mean value_function loss: 3.9352
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.0920
                       Mean reward: 9.94
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 0.3184
    Episode_Reward/rotating_object: 2.5941
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.91s
                      Time elapsed: 00:01:40
                               ETA: 00:26:45

################################################################################
                      [1m Learning iteration 88/1500 [0m                      

                       Computation: 111850 steps/s (collection: 0.766s, learning 0.113s)
             Mean action noise std: 1.42
          Mean value_function loss: 3.5272
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.1154
                       Mean reward: 12.42
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 2.4486
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.88s
                      Time elapsed: 00:01:40
                               ETA: 00:26:40

################################################################################
                      [1m Learning iteration 89/1500 [0m                      

                       Computation: 111902 steps/s (collection: 0.789s, learning 0.090s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.3074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.1302
                       Mean reward: 11.73
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 2.6806
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.88s
                      Time elapsed: 00:01:41
                               ETA: 00:26:35

################################################################################
                      [1m Learning iteration 90/1500 [0m                      

                       Computation: 113416 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.9201
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.1357
                       Mean reward: 18.81
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 2.4802
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.87s
                      Time elapsed: 00:01:42
                               ETA: 00:26:30

################################################################################
                      [1m Learning iteration 91/1500 [0m                      

                       Computation: 107055 steps/s (collection: 0.803s, learning 0.115s)
             Mean action noise std: 1.43
          Mean value_function loss: 4.0302
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.1682
                       Mean reward: 11.65
               Mean episode length: 226.94
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 2.1750
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.92s
                      Time elapsed: 00:01:43
                               ETA: 00:26:25

################################################################################
                      [1m Learning iteration 92/1500 [0m                      

                       Computation: 114246 steps/s (collection: 0.766s, learning 0.095s)
             Mean action noise std: 1.44
          Mean value_function loss: 4.2710
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.1908
                       Mean reward: 12.46
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 2.3827
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.86s
                      Time elapsed: 00:01:44
                               ETA: 00:26:20

################################################################################
                      [1m Learning iteration 93/1500 [0m                      

                       Computation: 107166 steps/s (collection: 0.792s, learning 0.125s)
             Mean action noise std: 1.44
          Mean value_function loss: 5.2383
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.2050
                       Mean reward: 16.46
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 3.1442
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.92s
                      Time elapsed: 00:01:45
                               ETA: 00:26:16

################################################################################
                      [1m Learning iteration 94/1500 [0m                      

                       Computation: 115688 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 1.45
          Mean value_function loss: 5.4472
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.2324
                       Mean reward: 17.36
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 0.3220
    Episode_Reward/rotating_object: 2.9279
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.85s
                      Time elapsed: 00:01:46
                               ETA: 00:26:11

################################################################################
                      [1m Learning iteration 95/1500 [0m                      

                       Computation: 107398 steps/s (collection: 0.787s, learning 0.128s)
             Mean action noise std: 1.45
          Mean value_function loss: 4.9403
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.2586
                       Mean reward: 13.60
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 2.5086
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.92s
                      Time elapsed: 00:01:47
                               ETA: 00:26:07

################################################################################
                      [1m Learning iteration 96/1500 [0m                      

                       Computation: 108691 steps/s (collection: 0.780s, learning 0.124s)
             Mean action noise std: 1.46
          Mean value_function loss: 4.1473
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.2851
                       Mean reward: 16.50
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 0.3125
    Episode_Reward/rotating_object: 2.6079
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.90s
                      Time elapsed: 00:01:47
                               ETA: 00:26:03

################################################################################
                      [1m Learning iteration 97/1500 [0m                      

                       Computation: 111606 steps/s (collection: 0.782s, learning 0.099s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.7947
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.2983
                       Mean reward: 14.62
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 2.9268
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.88s
                      Time elapsed: 00:01:48
                               ETA: 00:25:58

################################################################################
                      [1m Learning iteration 98/1500 [0m                      

                       Computation: 108776 steps/s (collection: 0.797s, learning 0.107s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.9212
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.3107
                       Mean reward: 18.18
               Mean episode length: 227.30
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 3.5499
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.90s
                      Time elapsed: 00:01:49
                               ETA: 00:25:54

################################################################################
                      [1m Learning iteration 99/1500 [0m                      

                       Computation: 108152 steps/s (collection: 0.812s, learning 0.097s)
             Mean action noise std: 1.47
          Mean value_function loss: 3.8000
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.3341
                       Mean reward: 12.92
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 0.3018
    Episode_Reward/rotating_object: 2.2522
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.91s
                      Time elapsed: 00:01:50
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 100/1500 [0m                      

                       Computation: 113452 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 1.47
          Mean value_function loss: 4.1061
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.3648
                       Mean reward: 16.04
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 0.3018
    Episode_Reward/rotating_object: 2.5670
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.87s
                      Time elapsed: 00:01:51
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 101/1500 [0m                      

                       Computation: 107853 steps/s (collection: 0.822s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.9993
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.3844
                       Mean reward: 18.45
               Mean episode length: 229.02
    Episode_Reward/reaching_object: 0.3039
    Episode_Reward/rotating_object: 2.9121
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.91s
                      Time elapsed: 00:01:52
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 102/1500 [0m                      

                       Computation: 113798 steps/s (collection: 0.776s, learning 0.088s)
             Mean action noise std: 1.48
          Mean value_function loss: 4.3597
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.4150
                       Mean reward: 13.50
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 2.8904
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.86s
                      Time elapsed: 00:01:53
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 103/1500 [0m                      

                       Computation: 111664 steps/s (collection: 0.782s, learning 0.099s)
             Mean action noise std: 1.48
          Mean value_function loss: 4.9061
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.4214
                       Mean reward: 13.95
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.2884
    Episode_Reward/rotating_object: 2.4103
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.88s
                      Time elapsed: 00:01:54
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 104/1500 [0m                      

                       Computation: 106315 steps/s (collection: 0.787s, learning 0.138s)
             Mean action noise std: 1.48
          Mean value_function loss: 6.4609
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.4282
                       Mean reward: 17.01
               Mean episode length: 220.78
    Episode_Reward/reaching_object: 0.2895
    Episode_Reward/rotating_object: 2.8896
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.92s
                      Time elapsed: 00:01:55
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 105/1500 [0m                      

                       Computation: 104752 steps/s (collection: 0.783s, learning 0.156s)
             Mean action noise std: 1.49
          Mean value_function loss: 5.9409
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.4556
                       Mean reward: 13.85
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 0.2864
    Episode_Reward/rotating_object: 2.7184
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.94s
                      Time elapsed: 00:01:56
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 106/1500 [0m                      

                       Computation: 109350 steps/s (collection: 0.771s, learning 0.128s)
             Mean action noise std: 1.49
          Mean value_function loss: 6.1836
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.4599
                       Mean reward: 16.53
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 0.2841
    Episode_Reward/rotating_object: 3.0121
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.90s
                      Time elapsed: 00:01:56
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 107/1500 [0m                      

                       Computation: 114799 steps/s (collection: 0.760s, learning 0.097s)
             Mean action noise std: 1.49
          Mean value_function loss: 6.3359
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.4556
                       Mean reward: 16.24
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 2.7695
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.86s
                      Time elapsed: 00:01:57
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 108/1500 [0m                      

                       Computation: 109732 steps/s (collection: 0.807s, learning 0.089s)
             Mean action noise std: 1.49
          Mean value_function loss: 7.2209
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 14.4673
                       Mean reward: 19.41
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 0.2885
    Episode_Reward/rotating_object: 3.3673
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.90s
                      Time elapsed: 00:01:58
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 109/1500 [0m                      

                       Computation: 103132 steps/s (collection: 0.833s, learning 0.120s)
             Mean action noise std: 1.49
          Mean value_function loss: 5.7828
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.4778
                       Mean reward: 13.18
               Mean episode length: 226.06
    Episode_Reward/reaching_object: 0.2668
    Episode_Reward/rotating_object: 2.7804
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.95s
                      Time elapsed: 00:01:59
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 110/1500 [0m                      

                       Computation: 111745 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 1.50
          Mean value_function loss: 6.8888
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.5012
                       Mean reward: 18.05
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 0.2801
    Episode_Reward/rotating_object: 3.4102
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.88s
                      Time elapsed: 00:02:00
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 111/1500 [0m                      

                       Computation: 109920 steps/s (collection: 0.810s, learning 0.085s)
             Mean action noise std: 1.50
          Mean value_function loss: 7.1533
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.5315
                       Mean reward: 13.35
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 0.2717
    Episode_Reward/rotating_object: 3.1923
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.89s
                      Time elapsed: 00:02:01
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 112/1500 [0m                      

                       Computation: 112595 steps/s (collection: 0.768s, learning 0.105s)
             Mean action noise std: 1.51
          Mean value_function loss: 7.2201
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.5481
                       Mean reward: 18.31
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 0.2754
    Episode_Reward/rotating_object: 3.5237
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.87s
                      Time elapsed: 00:02:02
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 113/1500 [0m                      

                       Computation: 111297 steps/s (collection: 0.772s, learning 0.111s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.8588
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.5539
                       Mean reward: 20.94
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 0.2788
    Episode_Reward/rotating_object: 3.7550
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.88s
                      Time elapsed: 00:02:03
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 114/1500 [0m                      

                       Computation: 114027 steps/s (collection: 0.766s, learning 0.096s)
             Mean action noise std: 1.51
          Mean value_function loss: 7.8009
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.5629
                       Mean reward: 15.36
               Mean episode length: 233.40
    Episode_Reward/reaching_object: 0.2790
    Episode_Reward/rotating_object: 3.2031
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.86s
                      Time elapsed: 00:02:04
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 115/1500 [0m                      

                       Computation: 109588 steps/s (collection: 0.755s, learning 0.142s)
             Mean action noise std: 1.51
          Mean value_function loss: 8.2780
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.5818
                       Mean reward: 24.12
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.2780
    Episode_Reward/rotating_object: 3.5321
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.90s
                      Time elapsed: 00:02:04
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 116/1500 [0m                      

                       Computation: 115183 steps/s (collection: 0.755s, learning 0.099s)
             Mean action noise std: 1.52
          Mean value_function loss: 8.1351
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.5970
                       Mean reward: 20.74
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 0.2741
    Episode_Reward/rotating_object: 3.5034
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.85s
                      Time elapsed: 00:02:05
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 117/1500 [0m                      

                       Computation: 115535 steps/s (collection: 0.762s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 7.9818
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.6099
                       Mean reward: 22.32
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.2708
    Episode_Reward/rotating_object: 3.9475
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.85s
                      Time elapsed: 00:02:06
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 118/1500 [0m                      

                       Computation: 112419 steps/s (collection: 0.765s, learning 0.110s)
             Mean action noise std: 1.53
          Mean value_function loss: 8.5452
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.6461
                       Mean reward: 23.00
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.2726
    Episode_Reward/rotating_object: 3.5273
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.87s
                      Time elapsed: 00:02:07
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 119/1500 [0m                      

                       Computation: 110978 steps/s (collection: 0.794s, learning 0.092s)
             Mean action noise std: 1.53
          Mean value_function loss: 9.4622
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.6601
                       Mean reward: 18.21
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 0.2717
    Episode_Reward/rotating_object: 4.2354
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.89s
                      Time elapsed: 00:02:08
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 120/1500 [0m                      

                       Computation: 109846 steps/s (collection: 0.803s, learning 0.092s)
             Mean action noise std: 1.53
          Mean value_function loss: 9.2571
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.6675
                       Mean reward: 25.24
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 0.2699
    Episode_Reward/rotating_object: 4.2163
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.89s
                      Time elapsed: 00:02:09
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 121/1500 [0m                      

                       Computation: 117215 steps/s (collection: 0.753s, learning 0.086s)
             Mean action noise std: 1.53
          Mean value_function loss: 8.7354
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.6809
                       Mean reward: 25.01
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.2708
    Episode_Reward/rotating_object: 4.4918
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.84s
                      Time elapsed: 00:02:10
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 122/1500 [0m                      

                       Computation: 114554 steps/s (collection: 0.758s, learning 0.100s)
             Mean action noise std: 1.54
          Mean value_function loss: 8.7097
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.6956
                       Mean reward: 17.89
               Mean episode length: 234.27
    Episode_Reward/reaching_object: 0.2641
    Episode_Reward/rotating_object: 4.4807
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.86s
                      Time elapsed: 00:02:11
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 123/1500 [0m                      

                       Computation: 107280 steps/s (collection: 0.772s, learning 0.144s)
             Mean action noise std: 1.54
          Mean value_function loss: 10.3622
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.7002
                       Mean reward: 28.47
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 0.2617
    Episode_Reward/rotating_object: 4.8261
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.92s
                      Time elapsed: 00:02:11
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 124/1500 [0m                      

                       Computation: 112859 steps/s (collection: 0.771s, learning 0.100s)
             Mean action noise std: 1.54
          Mean value_function loss: 11.2358
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.6972
                       Mean reward: 16.15
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 0.2610
    Episode_Reward/rotating_object: 4.0474
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.87s
                      Time elapsed: 00:02:12
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 125/1500 [0m                      

                       Computation: 115883 steps/s (collection: 0.761s, learning 0.087s)
             Mean action noise std: 1.54
          Mean value_function loss: 9.7750
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7001
                       Mean reward: 20.71
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 0.2543
    Episode_Reward/rotating_object: 4.3247
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.85s
                      Time elapsed: 00:02:13
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 126/1500 [0m                      

                       Computation: 109900 steps/s (collection: 0.783s, learning 0.112s)
             Mean action noise std: 1.54
          Mean value_function loss: 9.7071
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7172
                       Mean reward: 23.77
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 0.2592
    Episode_Reward/rotating_object: 3.6744
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.89s
                      Time elapsed: 00:02:14
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 127/1500 [0m                      

                       Computation: 112961 steps/s (collection: 0.784s, learning 0.087s)
             Mean action noise std: 1.54
          Mean value_function loss: 9.7807
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.7306
                       Mean reward: 27.50
               Mean episode length: 242.58
    Episode_Reward/reaching_object: 0.2559
    Episode_Reward/rotating_object: 4.1312
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.87s
                      Time elapsed: 00:02:15
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 128/1500 [0m                      

                       Computation: 112274 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.55
          Mean value_function loss: 10.6343
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.7357
                       Mean reward: 28.38
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.2566
    Episode_Reward/rotating_object: 5.8089
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.88s
                      Time elapsed: 00:02:16
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 129/1500 [0m                      

                       Computation: 110295 steps/s (collection: 0.758s, learning 0.133s)
             Mean action noise std: 1.55
          Mean value_function loss: 11.5412
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.7382
                       Mean reward: 25.05
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.2619
    Episode_Reward/rotating_object: 5.1920
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.89s
                      Time elapsed: 00:02:17
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 130/1500 [0m                      

                       Computation: 107028 steps/s (collection: 0.781s, learning 0.137s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.1491
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7405
                       Mean reward: 29.55
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 0.2635
    Episode_Reward/rotating_object: 5.7160
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.92s
                      Time elapsed: 00:02:18
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 131/1500 [0m                      

                       Computation: 112735 steps/s (collection: 0.772s, learning 0.100s)
             Mean action noise std: 1.55
          Mean value_function loss: 10.6990
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.7482
                       Mean reward: 28.26
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 0.2594
    Episode_Reward/rotating_object: 5.9846
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.87s
                      Time elapsed: 00:02:18
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 132/1500 [0m                      

                       Computation: 117115 steps/s (collection: 0.751s, learning 0.089s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.4073
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.7551
                       Mean reward: 28.85
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.2584
    Episode_Reward/rotating_object: 6.2853
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.84s
                      Time elapsed: 00:02:19
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 133/1500 [0m                      

                       Computation: 105361 steps/s (collection: 0.811s, learning 0.122s)
             Mean action noise std: 1.55
          Mean value_function loss: 11.3003
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7658
                       Mean reward: 28.22
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 0.2583
    Episode_Reward/rotating_object: 5.2218
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.93s
                      Time elapsed: 00:02:20
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 134/1500 [0m                      

                       Computation: 109921 steps/s (collection: 0.790s, learning 0.104s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.2595
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.7770
                       Mean reward: 26.33
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 0.2602
    Episode_Reward/rotating_object: 5.7305
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.89s
                      Time elapsed: 00:02:21
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 135/1500 [0m                      

                       Computation: 111461 steps/s (collection: 0.769s, learning 0.113s)
             Mean action noise std: 1.56
          Mean value_function loss: 10.2058
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.7864
                       Mean reward: 33.11
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 0.2507
    Episode_Reward/rotating_object: 5.4947
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.88s
                      Time elapsed: 00:02:22
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 136/1500 [0m                      

                       Computation: 110590 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 1.56
          Mean value_function loss: 10.9890
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.7974
                       Mean reward: 29.07
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 0.2646
    Episode_Reward/rotating_object: 5.3444
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.89s
                      Time elapsed: 00:02:23
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 137/1500 [0m                      

                       Computation: 113611 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 1.56
          Mean value_function loss: 10.9086
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8175
                       Mean reward: 31.82
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 0.2553
    Episode_Reward/rotating_object: 5.9966
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.87s
                      Time elapsed: 00:02:24
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 138/1500 [0m                      

                       Computation: 110901 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 1.56
          Mean value_function loss: 11.3442
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.8156
                       Mean reward: 33.99
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 0.2632
    Episode_Reward/rotating_object: 6.7088
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.89s
                      Time elapsed: 00:02:25
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 139/1500 [0m                      

                       Computation: 111848 steps/s (collection: 0.761s, learning 0.118s)
             Mean action noise std: 1.57
          Mean value_function loss: 8.8404
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.8166
                       Mean reward: 37.71
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 0.2562
    Episode_Reward/rotating_object: 7.3587
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.88s
                      Time elapsed: 00:02:26
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 140/1500 [0m                      

                       Computation: 108659 steps/s (collection: 0.760s, learning 0.145s)
             Mean action noise std: 1.57
          Mean value_function loss: 8.4818
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.8208
                       Mean reward: 23.56
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 0.2673
    Episode_Reward/rotating_object: 5.7684
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.90s
                      Time elapsed: 00:02:26
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 141/1500 [0m                      

                       Computation: 110826 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 9.6055
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8281
                       Mean reward: 31.35
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.2659
    Episode_Reward/rotating_object: 6.1801
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.89s
                      Time elapsed: 00:02:27
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 142/1500 [0m                      

                       Computation: 115352 steps/s (collection: 0.756s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 11.6708
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8322
                       Mean reward: 18.13
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 0.2660
    Episode_Reward/rotating_object: 5.2517
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.85s
                      Time elapsed: 00:02:28
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 143/1500 [0m                      

                       Computation: 104823 steps/s (collection: 0.846s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 13.5746
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8436
                       Mean reward: 26.60
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.2724
    Episode_Reward/rotating_object: 6.4447
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.94s
                      Time elapsed: 00:02:29
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 144/1500 [0m                      

                       Computation: 106515 steps/s (collection: 0.801s, learning 0.122s)
             Mean action noise std: 1.57
          Mean value_function loss: 14.4491
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 14.8492
                       Mean reward: 36.86
               Mean episode length: 220.57
    Episode_Reward/reaching_object: 0.2686
    Episode_Reward/rotating_object: 5.8566
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.92s
                      Time elapsed: 00:02:30
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 145/1500 [0m                      

                       Computation: 110194 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.4824
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.8420
                       Mean reward: 38.55
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 0.2747
    Episode_Reward/rotating_object: 7.5440
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.89s
                      Time elapsed: 00:02:31
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 146/1500 [0m                      

                       Computation: 111955 steps/s (collection: 0.774s, learning 0.105s)
             Mean action noise std: 1.57
          Mean value_function loss: 14.0715
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8435
                       Mean reward: 38.81
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 0.2675
    Episode_Reward/rotating_object: 7.3174
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.88s
                      Time elapsed: 00:02:32
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 147/1500 [0m                      

                       Computation: 106323 steps/s (collection: 0.793s, learning 0.132s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.3024
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.8371
                       Mean reward: 46.39
               Mean episode length: 230.35
    Episode_Reward/reaching_object: 0.2819
    Episode_Reward/rotating_object: 8.9456
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.92s
                      Time elapsed: 00:02:33
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 148/1500 [0m                      

                       Computation: 113592 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 1.58
          Mean value_function loss: 14.3400
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.8433
                       Mean reward: 40.24
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.2839
    Episode_Reward/rotating_object: 9.4905
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.87s
                      Time elapsed: 00:02:34
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 149/1500 [0m                      

                       Computation: 107033 steps/s (collection: 0.752s, learning 0.166s)
             Mean action noise std: 1.58
          Mean value_function loss: 15.3939
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.8523
                       Mean reward: 52.91
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.2905
    Episode_Reward/rotating_object: 10.9334
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.92s
                      Time elapsed: 00:02:35
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 150/1500 [0m                      

                       Computation: 108464 steps/s (collection: 0.758s, learning 0.149s)
             Mean action noise std: 1.58
          Mean value_function loss: 16.9703
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8550
                       Mean reward: 44.31
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.2852
    Episode_Reward/rotating_object: 9.0206
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.91s
                      Time elapsed: 00:02:35
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 151/1500 [0m                      

                       Computation: 113404 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 1.58
          Mean value_function loss: 15.7876
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8597
                       Mean reward: 56.10
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.2883
    Episode_Reward/rotating_object: 10.5182
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.87s
                      Time elapsed: 00:02:36
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 152/1500 [0m                      

                       Computation: 110839 steps/s (collection: 0.779s, learning 0.108s)
             Mean action noise std: 1.58
          Mean value_function loss: 15.5038
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8657
                       Mean reward: 39.32
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.2955
    Episode_Reward/rotating_object: 8.9405
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.89s
                      Time elapsed: 00:02:37
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 153/1500 [0m                      

                       Computation: 110845 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 1.58
          Mean value_function loss: 18.4796
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8694
                       Mean reward: 57.27
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.2901
    Episode_Reward/rotating_object: 9.6662
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.89s
                      Time elapsed: 00:02:38
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 154/1500 [0m                      

                       Computation: 106997 steps/s (collection: 0.810s, learning 0.109s)
             Mean action noise std: 1.59
          Mean value_function loss: 15.0118
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.8867
                       Mean reward: 48.05
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 0.2898
    Episode_Reward/rotating_object: 8.9751
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.92s
                      Time elapsed: 00:02:39
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 155/1500 [0m                      

                       Computation: 110315 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 16.6011
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.9071
                       Mean reward: 56.16
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 0.3039
    Episode_Reward/rotating_object: 10.9064
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.89s
                      Time elapsed: 00:02:40
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 156/1500 [0m                      

                       Computation: 106369 steps/s (collection: 0.811s, learning 0.113s)
             Mean action noise std: 1.60
          Mean value_function loss: 18.0551
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9311
                       Mean reward: 49.59
               Mean episode length: 217.64
    Episode_Reward/reaching_object: 0.2896
    Episode_Reward/rotating_object: 9.5424
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.92s
                      Time elapsed: 00:02:41
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 157/1500 [0m                      

                       Computation: 106054 steps/s (collection: 0.802s, learning 0.125s)
             Mean action noise std: 1.60
          Mean value_function loss: 19.1457
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.9503
                       Mean reward: 39.80
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 0.2851
    Episode_Reward/rotating_object: 8.0516
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.93s
                      Time elapsed: 00:02:42
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 158/1500 [0m                      

                       Computation: 104146 steps/s (collection: 0.844s, learning 0.100s)
             Mean action noise std: 1.60
          Mean value_function loss: 18.8584
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9657
                       Mean reward: 46.78
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.2900
    Episode_Reward/rotating_object: 9.4504
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.94s
                      Time elapsed: 00:02:43
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 159/1500 [0m                      

                       Computation: 103590 steps/s (collection: 0.789s, learning 0.160s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.2999
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.9870
                       Mean reward: 48.49
               Mean episode length: 218.45
    Episode_Reward/reaching_object: 0.2790
    Episode_Reward/rotating_object: 7.8048
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.95s
                      Time elapsed: 00:02:44
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 160/1500 [0m                      

                       Computation: 107019 steps/s (collection: 0.802s, learning 0.117s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.4840
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.9917
                       Mean reward: 60.11
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.2853
    Episode_Reward/rotating_object: 10.2850
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.92s
                      Time elapsed: 00:02:45
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 161/1500 [0m                      

                       Computation: 109016 steps/s (collection: 0.778s, learning 0.124s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.0076
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 15.0001
                       Mean reward: 54.86
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 0.2815
    Episode_Reward/rotating_object: 9.0616
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.90s
                      Time elapsed: 00:02:45
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 162/1500 [0m                      

                       Computation: 106855 steps/s (collection: 0.795s, learning 0.125s)
             Mean action noise std: 1.61
          Mean value_function loss: 17.3553
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 15.0043
                       Mean reward: 52.27
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.2953
    Episode_Reward/rotating_object: 9.7958
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.92s
                      Time elapsed: 00:02:46
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 163/1500 [0m                      

                       Computation: 112760 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 1.61
          Mean value_function loss: 12.9275
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.0038
                       Mean reward: 52.01
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 0.2850
    Episode_Reward/rotating_object: 9.4634
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.87s
                      Time elapsed: 00:02:47
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 164/1500 [0m                      

                       Computation: 111948 steps/s (collection: 0.774s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 12.4889
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.0008
                       Mean reward: 62.93
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 0.2891
    Episode_Reward/rotating_object: 10.8341
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.88s
                      Time elapsed: 00:02:48
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 165/1500 [0m                      

                       Computation: 103074 steps/s (collection: 0.841s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 16.4142
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.0044
                       Mean reward: 54.05
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.2904
    Episode_Reward/rotating_object: 9.9559
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.95s
                      Time elapsed: 00:02:49
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 166/1500 [0m                      

                       Computation: 111964 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.3179
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 15.0124
                       Mean reward: 60.84
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 0.2785
    Episode_Reward/rotating_object: 10.7653
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.88s
                      Time elapsed: 00:02:50
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 167/1500 [0m                      

                       Computation: 111593 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 1.61
          Mean value_function loss: 18.5623
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.0137
                       Mean reward: 45.81
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 0.2868
    Episode_Reward/rotating_object: 9.0428
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.88s
                      Time elapsed: 00:02:51
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 168/1500 [0m                      

                       Computation: 108878 steps/s (collection: 0.798s, learning 0.105s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.5898
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.0202
                       Mean reward: 62.94
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.2884
    Episode_Reward/rotating_object: 11.1073
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.90s
                      Time elapsed: 00:02:52
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 169/1500 [0m                      

                       Computation: 106144 steps/s (collection: 0.782s, learning 0.144s)
             Mean action noise std: 1.62
          Mean value_function loss: 18.3732
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.0350
                       Mean reward: 37.44
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.2839
    Episode_Reward/rotating_object: 10.9817
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.93s
                      Time elapsed: 00:02:53
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 170/1500 [0m                      

                       Computation: 113620 steps/s (collection: 0.760s, learning 0.105s)
             Mean action noise std: 1.62
          Mean value_function loss: 17.2846
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.0421
                       Mean reward: 59.30
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.2820
    Episode_Reward/rotating_object: 11.5043
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.87s
                      Time elapsed: 00:02:54
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 171/1500 [0m                      

                       Computation: 112552 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 18.6373
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.0487
                       Mean reward: 60.47
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 0.2796
    Episode_Reward/rotating_object: 11.4047
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.87s
                      Time elapsed: 00:02:54
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 172/1500 [0m                      

                       Computation: 107340 steps/s (collection: 0.802s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 20.7957
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 15.0396
                       Mean reward: 63.95
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 0.2760
    Episode_Reward/rotating_object: 12.4350
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.92s
                      Time elapsed: 00:02:55
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 173/1500 [0m                      

                       Computation: 114402 steps/s (collection: 0.768s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 21.1408
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.0419
                       Mean reward: 44.38
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.2754
    Episode_Reward/rotating_object: 10.8400
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.86s
                      Time elapsed: 00:02:56
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 174/1500 [0m                      

                       Computation: 107328 steps/s (collection: 0.800s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 22.1316
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.0555
                       Mean reward: 52.72
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 0.2805
    Episode_Reward/rotating_object: 11.6871
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.92s
                      Time elapsed: 00:02:57
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 175/1500 [0m                      

                       Computation: 112597 steps/s (collection: 0.760s, learning 0.113s)
             Mean action noise std: 1.63
          Mean value_function loss: 20.3545
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.0684
                       Mean reward: 59.83
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 0.2807
    Episode_Reward/rotating_object: 13.4989
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.87s
                      Time elapsed: 00:02:58
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 176/1500 [0m                      

                       Computation: 111367 steps/s (collection: 0.774s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 23.5840
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 15.0770
                       Mean reward: 57.34
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 0.2771
    Episode_Reward/rotating_object: 11.1858
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.88s
                      Time elapsed: 00:02:59
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 177/1500 [0m                      

                       Computation: 110656 steps/s (collection: 0.766s, learning 0.123s)
             Mean action noise std: 1.63
          Mean value_function loss: 28.7023
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.0839
                       Mean reward: 63.53
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 0.2745
    Episode_Reward/rotating_object: 13.6599
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.89s
                      Time elapsed: 00:03:00
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 178/1500 [0m                      

                       Computation: 109074 steps/s (collection: 0.789s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 27.7389
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.0975
                       Mean reward: 66.32
               Mean episode length: 229.51
    Episode_Reward/reaching_object: 0.2837
    Episode_Reward/rotating_object: 13.6370
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.90s
                      Time elapsed: 00:03:01
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 179/1500 [0m                      

                       Computation: 106070 steps/s (collection: 0.775s, learning 0.152s)
             Mean action noise std: 1.64
          Mean value_function loss: 27.3895
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.1182
                       Mean reward: 47.34
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 0.2880
    Episode_Reward/rotating_object: 13.1484
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.93s
                      Time elapsed: 00:03:02
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 180/1500 [0m                      

                       Computation: 109703 steps/s (collection: 0.784s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 23.5754
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.1495
                       Mean reward: 55.61
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 0.2870
    Episode_Reward/rotating_object: 11.5321
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.90s
                      Time elapsed: 00:03:02
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 181/1500 [0m                      

                       Computation: 107831 steps/s (collection: 0.775s, learning 0.137s)
             Mean action noise std: 1.65
          Mean value_function loss: 24.6738
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.1755
                       Mean reward: 59.15
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 0.2946
    Episode_Reward/rotating_object: 12.9187
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.91s
                      Time elapsed: 00:03:03
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 182/1500 [0m                      

                       Computation: 111068 steps/s (collection: 0.794s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 23.1269
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.1851
                       Mean reward: 66.27
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 0.2773
    Episode_Reward/rotating_object: 10.9963
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.89s
                      Time elapsed: 00:03:04
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 183/1500 [0m                      

                       Computation: 114613 steps/s (collection: 0.768s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 21.9519
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.1903
                       Mean reward: 53.50
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 0.2799
    Episode_Reward/rotating_object: 10.9742
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.86s
                      Time elapsed: 00:03:05
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 184/1500 [0m                      

                       Computation: 110412 steps/s (collection: 0.788s, learning 0.103s)
             Mean action noise std: 1.66
          Mean value_function loss: 21.4213
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.2123
                       Mean reward: 45.64
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 0.2861
    Episode_Reward/rotating_object: 9.2152
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.89s
                      Time elapsed: 00:03:06
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 185/1500 [0m                      

                       Computation: 112418 steps/s (collection: 0.787s, learning 0.088s)
             Mean action noise std: 1.66
          Mean value_function loss: 22.1234
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.2392
                       Mean reward: 56.80
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.2791
    Episode_Reward/rotating_object: 9.8718
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.87s
                      Time elapsed: 00:03:07
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 186/1500 [0m                      

                       Computation: 108348 steps/s (collection: 0.808s, learning 0.099s)
             Mean action noise std: 1.67
          Mean value_function loss: 19.9387
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.2598
                       Mean reward: 43.28
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.2815
    Episode_Reward/rotating_object: 10.1685
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.91s
                      Time elapsed: 00:03:08
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 187/1500 [0m                      

                       Computation: 111829 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 1.67
          Mean value_function loss: 19.0113
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 15.2687
                       Mean reward: 54.20
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.2792
    Episode_Reward/rotating_object: 11.3536
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.88s
                      Time elapsed: 00:03:09
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 188/1500 [0m                      

                       Computation: 114663 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 1.67
          Mean value_function loss: 19.4437
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.2722
                       Mean reward: 53.81
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.2915
    Episode_Reward/rotating_object: 11.4377
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.86s
                      Time elapsed: 00:03:10
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 189/1500 [0m                      

                       Computation: 109811 steps/s (collection: 0.776s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 20.3809
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.2753
                       Mean reward: 63.82
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.2854
    Episode_Reward/rotating_object: 11.9615
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.90s
                      Time elapsed: 00:03:10
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 190/1500 [0m                      

                       Computation: 111860 steps/s (collection: 0.772s, learning 0.107s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.5358
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2824
                       Mean reward: 53.24
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 13.0910
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.88s
                      Time elapsed: 00:03:11
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 191/1500 [0m                      

                       Computation: 106167 steps/s (collection: 0.804s, learning 0.122s)
             Mean action noise std: 1.67
          Mean value_function loss: 24.9774
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.2829
                       Mean reward: 57.38
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.2854
    Episode_Reward/rotating_object: 11.0915
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.93s
                      Time elapsed: 00:03:12
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 192/1500 [0m                      

                       Computation: 110069 steps/s (collection: 0.790s, learning 0.103s)
             Mean action noise std: 1.68
          Mean value_function loss: 22.3061
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.2850
                       Mean reward: 67.09
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.2829
    Episode_Reward/rotating_object: 11.6806
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.89s
                      Time elapsed: 00:03:13
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 193/1500 [0m                      

                       Computation: 107099 steps/s (collection: 0.822s, learning 0.096s)
             Mean action noise std: 1.68
          Mean value_function loss: 22.0459
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3064
                       Mean reward: 82.97
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.2948
    Episode_Reward/rotating_object: 15.2146
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.92s
                      Time elapsed: 00:03:14
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 194/1500 [0m                      

                       Computation: 107590 steps/s (collection: 0.816s, learning 0.098s)
             Mean action noise std: 1.68
          Mean value_function loss: 22.4348
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3109
                       Mean reward: 77.20
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.2857
    Episode_Reward/rotating_object: 11.6837
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.91s
                      Time elapsed: 00:03:15
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 195/1500 [0m                      

                       Computation: 112880 steps/s (collection: 0.765s, learning 0.106s)
             Mean action noise std: 1.68
          Mean value_function loss: 21.3097
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.3125
                       Mean reward: 57.39
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.2909
    Episode_Reward/rotating_object: 11.6460
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.87s
                      Time elapsed: 00:03:16
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 196/1500 [0m                      

                       Computation: 110457 steps/s (collection: 0.790s, learning 0.100s)
             Mean action noise std: 1.69
          Mean value_function loss: 25.0124
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.3258
                       Mean reward: 93.56
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 14.3007
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.89s
                      Time elapsed: 00:03:17
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 197/1500 [0m                      

                       Computation: 114291 steps/s (collection: 0.767s, learning 0.094s)
             Mean action noise std: 1.69
          Mean value_function loss: 25.3930
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.3455
                       Mean reward: 67.05
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.2855
    Episode_Reward/rotating_object: 10.9534
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.86s
                      Time elapsed: 00:03:18
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 198/1500 [0m                      

                       Computation: 112131 steps/s (collection: 0.748s, learning 0.129s)
             Mean action noise std: 1.69
          Mean value_function loss: 25.0219
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.3493
                       Mean reward: 65.51
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.2881
    Episode_Reward/rotating_object: 12.1218
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.88s
                      Time elapsed: 00:03:18
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 199/1500 [0m                      

                       Computation: 111265 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 1.69
          Mean value_function loss: 27.5238
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.3536
                       Mean reward: 48.28
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.2795
    Episode_Reward/rotating_object: 12.7135
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.88s
                      Time elapsed: 00:03:19
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 200/1500 [0m                      

                       Computation: 104238 steps/s (collection: 0.794s, learning 0.149s)
             Mean action noise std: 1.69
          Mean value_function loss: 26.5642
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.3583
                       Mean reward: 69.32
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.2778
    Episode_Reward/rotating_object: 12.8455
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.94s
                      Time elapsed: 00:03:20
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 201/1500 [0m                      

                       Computation: 111855 steps/s (collection: 0.779s, learning 0.100s)
             Mean action noise std: 1.69
          Mean value_function loss: 26.2453
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.3596
                       Mean reward: 66.89
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.2753
    Episode_Reward/rotating_object: 13.8451
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.88s
                      Time elapsed: 00:03:21
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 202/1500 [0m                      

                       Computation: 112896 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 1.69
          Mean value_function loss: 27.4943
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3568
                       Mean reward: 73.63
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 0.2935
    Episode_Reward/rotating_object: 13.7464
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.87s
                      Time elapsed: 00:03:22
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 203/1500 [0m                      

                       Computation: 109248 steps/s (collection: 0.791s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 22.5571
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.3654
                       Mean reward: 95.61
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 0.2822
    Episode_Reward/rotating_object: 13.7385
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.90s
                      Time elapsed: 00:03:23
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 204/1500 [0m                      

                       Computation: 107933 steps/s (collection: 0.784s, learning 0.127s)
             Mean action noise std: 1.70
          Mean value_function loss: 24.2816
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.3805
                       Mean reward: 80.02
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 0.2852
    Episode_Reward/rotating_object: 14.3112
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.91s
                      Time elapsed: 00:03:24
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 205/1500 [0m                      

                       Computation: 108789 steps/s (collection: 0.794s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 25.8921
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.3874
                       Mean reward: 81.04
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.2868
    Episode_Reward/rotating_object: 15.2161
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.90s
                      Time elapsed: 00:03:25
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 206/1500 [0m                      

                       Computation: 108145 steps/s (collection: 0.787s, learning 0.122s)
             Mean action noise std: 1.70
          Mean value_function loss: 27.9750
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.3896
                       Mean reward: 56.07
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.2789
    Episode_Reward/rotating_object: 12.4505
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.91s
                      Time elapsed: 00:03:26
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 207/1500 [0m                      

                       Computation: 113136 steps/s (collection: 0.767s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 31.6222
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4008
                       Mean reward: 71.32
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 0.2824
    Episode_Reward/rotating_object: 12.5136
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.87s
                      Time elapsed: 00:03:27
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 208/1500 [0m                      

                       Computation: 104072 steps/s (collection: 0.799s, learning 0.145s)
             Mean action noise std: 1.71
          Mean value_function loss: 31.6044
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.4219
                       Mean reward: 69.98
               Mean episode length: 232.40
    Episode_Reward/reaching_object: 0.2805
    Episode_Reward/rotating_object: 12.4485
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.94s
                      Time elapsed: 00:03:27
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 209/1500 [0m                      

                       Computation: 108239 steps/s (collection: 0.790s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 29.9574
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.4285
                       Mean reward: 58.74
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 0.2791
    Episode_Reward/rotating_object: 12.3926
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.91s
                      Time elapsed: 00:03:28
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 210/1500 [0m                      

                       Computation: 109977 steps/s (collection: 0.777s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 34.9378
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4470
                       Mean reward: 58.42
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 0.2879
    Episode_Reward/rotating_object: 13.6686
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.89s
                      Time elapsed: 00:03:29
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 211/1500 [0m                      

                       Computation: 109788 steps/s (collection: 0.793s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 35.5484
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.4568
                       Mean reward: 66.29
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 0.2829
    Episode_Reward/rotating_object: 13.5312
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.90s
                      Time elapsed: 00:03:30
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 212/1500 [0m                      

                       Computation: 111251 steps/s (collection: 0.797s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 32.7416
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4610
                       Mean reward: 88.80
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 0.2801
    Episode_Reward/rotating_object: 14.3998
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.88s
                      Time elapsed: 00:03:31
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 213/1500 [0m                      

                       Computation: 105363 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 32.6239
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4663
                       Mean reward: 93.65
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 0.2865
    Episode_Reward/rotating_object: 14.5947
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.93s
                      Time elapsed: 00:03:32
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 214/1500 [0m                      

                       Computation: 111576 steps/s (collection: 0.792s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.1141
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4833
                       Mean reward: 62.23
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 0.2841
    Episode_Reward/rotating_object: 15.6953
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.88s
                      Time elapsed: 00:03:33
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 215/1500 [0m                      

                       Computation: 104219 steps/s (collection: 0.827s, learning 0.116s)
             Mean action noise std: 1.73
          Mean value_function loss: 33.1571
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5036
                       Mean reward: 72.06
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.2842
    Episode_Reward/rotating_object: 14.5414
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.94s
                      Time elapsed: 00:03:34
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 216/1500 [0m                      

                       Computation: 107720 steps/s (collection: 0.827s, learning 0.085s)
             Mean action noise std: 1.73
          Mean value_function loss: 32.9122
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5106
                       Mean reward: 66.14
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.2772
    Episode_Reward/rotating_object: 11.6968
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.91s
                      Time elapsed: 00:03:35
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 217/1500 [0m                      

                       Computation: 107201 steps/s (collection: 0.795s, learning 0.122s)
             Mean action noise std: 1.74
          Mean value_function loss: 34.7343
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5283
                       Mean reward: 77.59
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.2761
    Episode_Reward/rotating_object: 12.9141
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.92s
                      Time elapsed: 00:03:36
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 218/1500 [0m                      

                       Computation: 115233 steps/s (collection: 0.761s, learning 0.092s)
             Mean action noise std: 1.74
          Mean value_function loss: 32.4850
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5356
                       Mean reward: 48.65
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.2708
    Episode_Reward/rotating_object: 10.9857
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.85s
                      Time elapsed: 00:03:36
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 219/1500 [0m                      

                       Computation: 115590 steps/s (collection: 0.750s, learning 0.100s)
             Mean action noise std: 1.74
          Mean value_function loss: 37.9146
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5420
                       Mean reward: 76.39
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 0.2842
    Episode_Reward/rotating_object: 16.4000
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.85s
                      Time elapsed: 00:03:37
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 220/1500 [0m                      

                       Computation: 104092 steps/s (collection: 0.831s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 40.3217
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5562
                       Mean reward: 71.48
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 0.2700
    Episode_Reward/rotating_object: 12.5113
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.94s
                      Time elapsed: 00:03:38
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 221/1500 [0m                      

                       Computation: 108168 steps/s (collection: 0.818s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 38.8382
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5726
                       Mean reward: 67.00
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 0.2686
    Episode_Reward/rotating_object: 13.3259
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.91s
                      Time elapsed: 00:03:39
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 222/1500 [0m                      

                       Computation: 114706 steps/s (collection: 0.764s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 41.0439
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5765
                       Mean reward: 70.45
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 0.2708
    Episode_Reward/rotating_object: 14.3161
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.86s
                      Time elapsed: 00:03:40
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 223/1500 [0m                      

                       Computation: 108562 steps/s (collection: 0.811s, learning 0.095s)
             Mean action noise std: 1.75
          Mean value_function loss: 46.5828
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.5752
                       Mean reward: 67.94
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 0.2689
    Episode_Reward/rotating_object: 13.2558
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.91s
                      Time elapsed: 00:03:41
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 224/1500 [0m                      

                       Computation: 112802 steps/s (collection: 0.779s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 48.3511
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.5800
                       Mean reward: 68.65
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 0.2774
    Episode_Reward/rotating_object: 14.3851
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.87s
                      Time elapsed: 00:03:42
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 225/1500 [0m                      

                       Computation: 109166 steps/s (collection: 0.800s, learning 0.101s)
             Mean action noise std: 1.75
          Mean value_function loss: 45.6285
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5850
                       Mean reward: 74.66
               Mean episode length: 228.47
    Episode_Reward/reaching_object: 0.2651
    Episode_Reward/rotating_object: 13.1740
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.90s
                      Time elapsed: 00:03:43
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 226/1500 [0m                      

                       Computation: 108108 steps/s (collection: 0.789s, learning 0.120s)
             Mean action noise std: 1.76
          Mean value_function loss: 38.5473
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.5981
                       Mean reward: 85.32
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 0.2648
    Episode_Reward/rotating_object: 15.6130
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.91s
                      Time elapsed: 00:03:44
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 227/1500 [0m                      

                       Computation: 104142 steps/s (collection: 0.751s, learning 0.193s)
             Mean action noise std: 1.76
          Mean value_function loss: 41.7144
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6057
                       Mean reward: 100.22
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.2740
    Episode_Reward/rotating_object: 17.6395
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.94s
                      Time elapsed: 00:03:45
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 228/1500 [0m                      

                       Computation: 113445 steps/s (collection: 0.761s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 48.5755
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.6111
                       Mean reward: 94.88
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.2816
    Episode_Reward/rotating_object: 16.8377
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.87s
                      Time elapsed: 00:03:45
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 229/1500 [0m                      

                       Computation: 110257 steps/s (collection: 0.771s, learning 0.121s)
             Mean action noise std: 1.76
          Mean value_function loss: 50.6265
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6257
                       Mean reward: 65.97
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 0.2738
    Episode_Reward/rotating_object: 14.3452
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.89s
                      Time elapsed: 00:03:46
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 230/1500 [0m                      

                       Computation: 109887 steps/s (collection: 0.793s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 47.6574
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6349
                       Mean reward: 74.53
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 0.2706
    Episode_Reward/rotating_object: 15.5805
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.89s
                      Time elapsed: 00:03:47
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 231/1500 [0m                      

                       Computation: 108365 steps/s (collection: 0.792s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 46.6185
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6440
                       Mean reward: 75.57
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.2758
    Episode_Reward/rotating_object: 15.3255
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.91s
                      Time elapsed: 00:03:48
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 232/1500 [0m                      

                       Computation: 107242 steps/s (collection: 0.801s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 49.1579
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6668
                       Mean reward: 83.21
               Mean episode length: 226.31
    Episode_Reward/reaching_object: 0.2711
    Episode_Reward/rotating_object: 15.7692
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.92s
                      Time elapsed: 00:03:49
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 233/1500 [0m                      

                       Computation: 106675 steps/s (collection: 0.808s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 47.4055
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.6823
                       Mean reward: 60.77
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 0.2667
    Episode_Reward/rotating_object: 15.0659
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.92s
                      Time elapsed: 00:03:50
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 234/1500 [0m                      

                       Computation: 109370 steps/s (collection: 0.806s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 47.8158
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6864
                       Mean reward: 83.90
               Mean episode length: 226.97
    Episode_Reward/reaching_object: 0.2603
    Episode_Reward/rotating_object: 14.5815
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.90s
                      Time elapsed: 00:03:51
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 235/1500 [0m                      

                       Computation: 108014 steps/s (collection: 0.811s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 46.2786
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6840
                       Mean reward: 74.42
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.2532
    Episode_Reward/rotating_object: 13.5004
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.91s
                      Time elapsed: 00:03:52
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 236/1500 [0m                      

                       Computation: 107793 steps/s (collection: 0.775s, learning 0.137s)
             Mean action noise std: 1.78
          Mean value_function loss: 41.1571
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.6860
                       Mean reward: 51.69
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.2602
    Episode_Reward/rotating_object: 14.3356
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.91s
                      Time elapsed: 00:03:53
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 237/1500 [0m                      

                       Computation: 94167 steps/s (collection: 0.902s, learning 0.142s)
             Mean action noise std: 1.78
          Mean value_function loss: 38.4531
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6887
                       Mean reward: 67.60
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 0.2608
    Episode_Reward/rotating_object: 15.0095
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.04s
                      Time elapsed: 00:03:54
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 238/1500 [0m                      

                       Computation: 110583 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 40.0680
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6857
                       Mean reward: 89.03
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.2693
    Episode_Reward/rotating_object: 16.4586
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.89s
                      Time elapsed: 00:03:55
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 239/1500 [0m                      

                       Computation: 106276 steps/s (collection: 0.780s, learning 0.145s)
             Mean action noise std: 1.78
          Mean value_function loss: 45.0812
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6881
                       Mean reward: 83.07
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 0.2697
    Episode_Reward/rotating_object: 18.5832
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.92s
                      Time elapsed: 00:03:56
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 240/1500 [0m                      

                       Computation: 109512 steps/s (collection: 0.777s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 40.5815
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6869
                       Mean reward: 82.95
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.2659
    Episode_Reward/rotating_object: 17.3375
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.90s
                      Time elapsed: 00:03:56
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 241/1500 [0m                      

                       Computation: 110189 steps/s (collection: 0.745s, learning 0.147s)
             Mean action noise std: 1.79
          Mean value_function loss: 35.1500
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.6900
                       Mean reward: 71.71
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 0.2712
    Episode_Reward/rotating_object: 17.7988
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.89s
                      Time elapsed: 00:03:57
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 242/1500 [0m                      

                       Computation: 113170 steps/s (collection: 0.775s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 38.0031
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6979
                       Mean reward: 98.46
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 0.2754
    Episode_Reward/rotating_object: 20.7219
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.87s
                      Time elapsed: 00:03:58
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 243/1500 [0m                      

                       Computation: 111988 steps/s (collection: 0.769s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 41.6572
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6989
                       Mean reward: 101.38
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.2768
    Episode_Reward/rotating_object: 20.5516
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.88s
                      Time elapsed: 00:03:59
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 244/1500 [0m                      

                       Computation: 105204 steps/s (collection: 0.763s, learning 0.171s)
             Mean action noise std: 1.79
          Mean value_function loss: 41.7524
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.7040
                       Mean reward: 105.55
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.2765
    Episode_Reward/rotating_object: 19.3219
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.93s
                      Time elapsed: 00:04:00
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 245/1500 [0m                      

                       Computation: 111093 steps/s (collection: 0.795s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 44.1421
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7115
                       Mean reward: 98.76
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 0.2802
    Episode_Reward/rotating_object: 20.6701
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.88s
                      Time elapsed: 00:04:01
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 246/1500 [0m                      

                       Computation: 110919 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 1.79
          Mean value_function loss: 55.9548
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7176
                       Mean reward: 79.23
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.2835
    Episode_Reward/rotating_object: 16.5094
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.89s
                      Time elapsed: 00:04:02
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 247/1500 [0m                      

                       Computation: 115286 steps/s (collection: 0.763s, learning 0.090s)
             Mean action noise std: 1.79
          Mean value_function loss: 50.5605
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7227
                       Mean reward: 96.63
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.2796
    Episode_Reward/rotating_object: 20.4567
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.85s
                      Time elapsed: 00:04:03
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 248/1500 [0m                      

                       Computation: 110634 steps/s (collection: 0.768s, learning 0.121s)
             Mean action noise std: 1.80
          Mean value_function loss: 54.7792
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.7274
                       Mean reward: 131.15
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 22.1634
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.89s
                      Time elapsed: 00:04:04
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 249/1500 [0m                      

                       Computation: 109108 steps/s (collection: 0.757s, learning 0.144s)
             Mean action noise std: 1.80
          Mean value_function loss: 51.9437
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7322
                       Mean reward: 97.83
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.2920
    Episode_Reward/rotating_object: 21.8512
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.90s
                      Time elapsed: 00:04:04
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 250/1500 [0m                      

                       Computation: 113322 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 47.3772
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7340
                       Mean reward: 118.37
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 21.7862
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.87s
                      Time elapsed: 00:04:05
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 251/1500 [0m                      

                       Computation: 114421 steps/s (collection: 0.760s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 48.5413
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7300
                       Mean reward: 96.26
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 0.2928
    Episode_Reward/rotating_object: 21.7153
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.86s
                      Time elapsed: 00:04:06
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 252/1500 [0m                      

                       Computation: 106987 steps/s (collection: 0.800s, learning 0.119s)
             Mean action noise std: 1.80
          Mean value_function loss: 49.9236
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7294
                       Mean reward: 84.80
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 0.2867
    Episode_Reward/rotating_object: 19.9963
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.92s
                      Time elapsed: 00:04:07
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 253/1500 [0m                      

                       Computation: 109128 steps/s (collection: 0.789s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 51.9386
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7372
                       Mean reward: 131.66
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 23.7212
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.90s
                      Time elapsed: 00:04:08
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 254/1500 [0m                      

                       Computation: 108221 steps/s (collection: 0.781s, learning 0.127s)
             Mean action noise std: 1.80
          Mean value_function loss: 44.5744
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.7407
                       Mean reward: 115.07
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 0.2800
    Episode_Reward/rotating_object: 23.9268
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.91s
                      Time elapsed: 00:04:09
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 255/1500 [0m                      

                       Computation: 109324 steps/s (collection: 0.790s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 45.3723
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7510
                       Mean reward: 104.87
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 0.2801
    Episode_Reward/rotating_object: 22.1992
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.90s
                      Time elapsed: 00:04:10
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 256/1500 [0m                      

                       Computation: 111491 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 46.1297
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.7600
                       Mean reward: 122.06
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.2828
    Episode_Reward/rotating_object: 23.0301
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.88s
                      Time elapsed: 00:04:11
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 257/1500 [0m                      

                       Computation: 108600 steps/s (collection: 0.774s, learning 0.132s)
             Mean action noise std: 1.81
          Mean value_function loss: 42.8674
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.7624
                       Mean reward: 117.17
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 0.2774
    Episode_Reward/rotating_object: 22.4506
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.91s
                      Time elapsed: 00:04:12
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 258/1500 [0m                      

                       Computation: 111344 steps/s (collection: 0.753s, learning 0.130s)
             Mean action noise std: 1.81
          Mean value_function loss: 46.3926
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7708
                       Mean reward: 107.97
               Mean episode length: 234.51
    Episode_Reward/reaching_object: 0.2757
    Episode_Reward/rotating_object: 20.9716
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.88s
                      Time elapsed: 00:04:12
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 259/1500 [0m                      

                       Computation: 113113 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 1.81
          Mean value_function loss: 46.8624
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7763
                       Mean reward: 106.09
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 0.2796
    Episode_Reward/rotating_object: 22.6994
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.87s
                      Time elapsed: 00:04:13
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 260/1500 [0m                      

                       Computation: 114998 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 1.81
          Mean value_function loss: 49.4456
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7804
                       Mean reward: 136.03
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.2812
    Episode_Reward/rotating_object: 24.4042
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.85s
                      Time elapsed: 00:04:14
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 261/1500 [0m                      

                       Computation: 105031 steps/s (collection: 0.822s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 51.2347
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8062
                       Mean reward: 107.62
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 0.2904
    Episode_Reward/rotating_object: 23.4572
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.94s
                      Time elapsed: 00:04:15
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 262/1500 [0m                      

                       Computation: 111781 steps/s (collection: 0.766s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 56.2739
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8152
                       Mean reward: 136.31
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.2902
    Episode_Reward/rotating_object: 27.5816
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.88s
                      Time elapsed: 00:04:16
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 263/1500 [0m                      

                       Computation: 110115 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 56.1981
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8213
                       Mean reward: 129.66
               Mean episode length: 236.08
    Episode_Reward/reaching_object: 0.2853
    Episode_Reward/rotating_object: 23.0481
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.89s
                      Time elapsed: 00:04:17
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 264/1500 [0m                      

                       Computation: 110348 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 62.4140
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8202
                       Mean reward: 125.74
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 0.2823
    Episode_Reward/rotating_object: 23.7931
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.89s
                      Time elapsed: 00:04:18
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 265/1500 [0m                      

                       Computation: 114700 steps/s (collection: 0.757s, learning 0.100s)
             Mean action noise std: 1.82
          Mean value_function loss: 65.3277
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8219
                       Mean reward: 105.35
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.2949
    Episode_Reward/rotating_object: 21.8022
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.86s
                      Time elapsed: 00:04:19
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 266/1500 [0m                      

                       Computation: 105963 steps/s (collection: 0.792s, learning 0.136s)
             Mean action noise std: 1.83
          Mean value_function loss: 53.7115
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8288
                       Mean reward: 97.90
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 0.2909
    Episode_Reward/rotating_object: 21.2643
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.93s
                      Time elapsed: 00:04:20
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 267/1500 [0m                      

                       Computation: 103200 steps/s (collection: 0.800s, learning 0.152s)
             Mean action noise std: 1.83
          Mean value_function loss: 54.4578
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8404
                       Mean reward: 122.72
               Mean episode length: 226.19
    Episode_Reward/reaching_object: 0.2922
    Episode_Reward/rotating_object: 24.7347
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.95s
                      Time elapsed: 00:04:21
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 268/1500 [0m                      

                       Computation: 107681 steps/s (collection: 0.762s, learning 0.151s)
             Mean action noise std: 1.83
          Mean value_function loss: 49.1906
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8467
                       Mean reward: 111.52
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 24.1052
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.91s
                      Time elapsed: 00:04:21
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 269/1500 [0m                      

                       Computation: 106034 steps/s (collection: 0.787s, learning 0.140s)
             Mean action noise std: 1.83
          Mean value_function loss: 46.5542
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.8478
                       Mean reward: 137.93
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.2905
    Episode_Reward/rotating_object: 25.7914
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.93s
                      Time elapsed: 00:04:22
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 270/1500 [0m                      

                       Computation: 110475 steps/s (collection: 0.773s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 54.8553
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8500
                       Mean reward: 128.81
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 0.2949
    Episode_Reward/rotating_object: 23.9649
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.89s
                      Time elapsed: 00:04:23
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 271/1500 [0m                      

                       Computation: 112062 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 55.6447
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.8518
                       Mean reward: 142.84
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 0.2909
    Episode_Reward/rotating_object: 24.7692
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.88s
                      Time elapsed: 00:04:24
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 272/1500 [0m                      

                       Computation: 113222 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 57.2211
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8575
                       Mean reward: 140.97
               Mean episode length: 235.25
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 27.7684
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.87s
                      Time elapsed: 00:04:25
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 273/1500 [0m                      

                       Computation: 111680 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 69.9846
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8635
                       Mean reward: 130.37
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.2913
    Episode_Reward/rotating_object: 25.1453
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.88s
                      Time elapsed: 00:04:26
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 274/1500 [0m                      

                       Computation: 113865 steps/s (collection: 0.767s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 62.3860
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.8763
                       Mean reward: 117.09
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 0.2972
    Episode_Reward/rotating_object: 27.0200
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.86s
                      Time elapsed: 00:04:27
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 275/1500 [0m                      

                       Computation: 109634 steps/s (collection: 0.788s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 65.7659
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8800
                       Mean reward: 162.71
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3011
    Episode_Reward/rotating_object: 29.7691
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.90s
                      Time elapsed: 00:04:28
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 276/1500 [0m                      

                       Computation: 109585 steps/s (collection: 0.764s, learning 0.133s)
             Mean action noise std: 1.84
          Mean value_function loss: 70.1284
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8797
                       Mean reward: 127.87
               Mean episode length: 230.91
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 29.2322
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.90s
                      Time elapsed: 00:04:29
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 277/1500 [0m                      

                       Computation: 110950 steps/s (collection: 0.769s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 62.4228
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.8903
                       Mean reward: 133.24
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 0.3026
    Episode_Reward/rotating_object: 25.7577
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.89s
                      Time elapsed: 00:04:29
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 278/1500 [0m                      

                       Computation: 109527 steps/s (collection: 0.783s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 63.2810
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.9043
                       Mean reward: 152.58
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 27.9671
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.90s
                      Time elapsed: 00:04:30
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 279/1500 [0m                      

                       Computation: 111416 steps/s (collection: 0.792s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 63.3692
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.9089
                       Mean reward: 130.56
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 28.4904
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.88s
                      Time elapsed: 00:04:31
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 280/1500 [0m                      

                       Computation: 111743 steps/s (collection: 0.785s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 57.9926
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9158
                       Mean reward: 124.85
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.3033
    Episode_Reward/rotating_object: 25.7337
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.88s
                      Time elapsed: 00:04:32
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 281/1500 [0m                      

                       Computation: 110992 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 66.2911
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9211
                       Mean reward: 137.56
               Mean episode length: 234.07
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 28.2626
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.89s
                      Time elapsed: 00:04:33
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 282/1500 [0m                      

                       Computation: 114795 steps/s (collection: 0.763s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 67.6481
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9279
                       Mean reward: 159.82
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.3031
    Episode_Reward/rotating_object: 28.3127
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.86s
                      Time elapsed: 00:04:34
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 283/1500 [0m                      

                       Computation: 110940 steps/s (collection: 0.788s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 65.9543
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9221
                       Mean reward: 165.29
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 29.6951
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.89s
                      Time elapsed: 00:04:35
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 284/1500 [0m                      

                       Computation: 113260 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.6372
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.9115
                       Mean reward: 139.23
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 28.5706
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.87s
                      Time elapsed: 00:04:36
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 285/1500 [0m                      

                       Computation: 104145 steps/s (collection: 0.785s, learning 0.159s)
             Mean action noise std: 1.85
          Mean value_function loss: 68.5715
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9089
                       Mean reward: 182.70
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.3058
    Episode_Reward/rotating_object: 29.0277
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.94s
                      Time elapsed: 00:04:37
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 286/1500 [0m                      

                       Computation: 111727 steps/s (collection: 0.758s, learning 0.122s)
             Mean action noise std: 1.85
          Mean value_function loss: 59.3279
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9170
                       Mean reward: 152.74
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.3027
    Episode_Reward/rotating_object: 26.4244
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.88s
                      Time elapsed: 00:04:37
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 287/1500 [0m                      

                       Computation: 115060 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 64.5045
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.9280
                       Mean reward: 148.67
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 0.3035
    Episode_Reward/rotating_object: 31.4607
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.85s
                      Time elapsed: 00:04:38
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 288/1500 [0m                      

                       Computation: 111309 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 69.2792
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9334
                       Mean reward: 134.67
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 28.5983
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.88s
                      Time elapsed: 00:04:39
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 289/1500 [0m                      

                       Computation: 111667 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 73.7739
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9393
                       Mean reward: 160.41
               Mean episode length: 233.91
    Episode_Reward/reaching_object: 0.3041
    Episode_Reward/rotating_object: 30.1630
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.88s
                      Time elapsed: 00:04:40
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 290/1500 [0m                      

                       Computation: 113635 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 1.86
          Mean value_function loss: 58.5365
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9437
                       Mean reward: 147.39
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 0.3043
    Episode_Reward/rotating_object: 28.6632
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.87s
                      Time elapsed: 00:04:41
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 291/1500 [0m                      

                       Computation: 112370 steps/s (collection: 0.788s, learning 0.087s)
             Mean action noise std: 1.86
          Mean value_function loss: 63.4342
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9473
                       Mean reward: 181.01
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.2970
    Episode_Reward/rotating_object: 32.7058
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.87s
                      Time elapsed: 00:04:42
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 292/1500 [0m                      

                       Computation: 112170 steps/s (collection: 0.782s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 60.2490
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9547
                       Mean reward: 120.99
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.3077
    Episode_Reward/rotating_object: 29.5854
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.88s
                      Time elapsed: 00:04:43
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 293/1500 [0m                      

                       Computation: 107033 steps/s (collection: 0.764s, learning 0.155s)
             Mean action noise std: 1.86
          Mean value_function loss: 57.2126
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9661
                       Mean reward: 113.59
               Mean episode length: 231.43
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 28.3895
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.92s
                      Time elapsed: 00:04:44
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 294/1500 [0m                      

                       Computation: 114719 steps/s (collection: 0.766s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 58.0808
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9715
                       Mean reward: 166.81
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 0.2962
    Episode_Reward/rotating_object: 29.7364
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.86s
                      Time elapsed: 00:04:44
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 295/1500 [0m                      

                       Computation: 116270 steps/s (collection: 0.751s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 50.9325
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.9690
                       Mean reward: 170.82
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.3022
    Episode_Reward/rotating_object: 31.0058
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.85s
                      Time elapsed: 00:04:45
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 296/1500 [0m                      

                       Computation: 113724 steps/s (collection: 0.764s, learning 0.101s)
             Mean action noise std: 1.86
          Mean value_function loss: 58.0777
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9721
                       Mean reward: 174.60
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.3021
    Episode_Reward/rotating_object: 36.0524
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.86s
                      Time elapsed: 00:04:46
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 297/1500 [0m                      

                       Computation: 114323 steps/s (collection: 0.762s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 62.6068
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 15.9716
                       Mean reward: 118.42
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 0.2935
    Episode_Reward/rotating_object: 30.9503
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.86s
                      Time elapsed: 00:04:47
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 298/1500 [0m                      

                       Computation: 107280 steps/s (collection: 0.829s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 66.3278
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9654
                       Mean reward: 155.76
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 0.2974
    Episode_Reward/rotating_object: 32.2448
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.92s
                      Time elapsed: 00:04:48
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 299/1500 [0m                      

                       Computation: 111913 steps/s (collection: 0.781s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 60.1872
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.9684
                       Mean reward: 153.67
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 0.3082
    Episode_Reward/rotating_object: 27.7529
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.88s
                      Time elapsed: 00:04:49
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 300/1500 [0m                      

                       Computation: 108335 steps/s (collection: 0.783s, learning 0.125s)
             Mean action noise std: 1.87
          Mean value_function loss: 67.6481
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9769
                       Mean reward: 137.31
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.3031
    Episode_Reward/rotating_object: 30.8100
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.91s
                      Time elapsed: 00:04:50
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 301/1500 [0m                      

                       Computation: 109838 steps/s (collection: 0.779s, learning 0.116s)
             Mean action noise std: 1.87
          Mean value_function loss: 70.3149
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9840
                       Mean reward: 133.28
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 33.0383
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.89s
                      Time elapsed: 00:04:51
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 302/1500 [0m                      

                       Computation: 106795 steps/s (collection: 0.759s, learning 0.161s)
             Mean action noise std: 1.87
          Mean value_function loss: 68.3084
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.9903
                       Mean reward: 149.52
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 30.9137
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.92s
                      Time elapsed: 00:04:51
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 303/1500 [0m                      

                       Computation: 101624 steps/s (collection: 0.796s, learning 0.171s)
             Mean action noise std: 1.87
          Mean value_function loss: 59.8896
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.9979
                       Mean reward: 139.56
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 31.4297
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.97s
                      Time elapsed: 00:04:52
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 304/1500 [0m                      

                       Computation: 100350 steps/s (collection: 0.840s, learning 0.140s)
             Mean action noise std: 1.87
          Mean value_function loss: 68.0974
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.0066
                       Mean reward: 150.54
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 30.9759
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.98s
                      Time elapsed: 00:04:53
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 305/1500 [0m                      

                       Computation: 110989 steps/s (collection: 0.798s, learning 0.088s)
             Mean action noise std: 1.88
          Mean value_function loss: 54.0616
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0187
                       Mean reward: 171.14
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 30.8316
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.89s
                      Time elapsed: 00:04:54
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 306/1500 [0m                      

                       Computation: 112637 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 66.2983
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0251
                       Mean reward: 148.91
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.3063
    Episode_Reward/rotating_object: 32.6265
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.87s
                      Time elapsed: 00:04:55
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 307/1500 [0m                      

                       Computation: 109111 steps/s (collection: 0.816s, learning 0.085s)
             Mean action noise std: 1.88
          Mean value_function loss: 65.2372
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0267
                       Mean reward: 158.06
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 29.3194
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.90s
                      Time elapsed: 00:04:56
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 308/1500 [0m                      

                       Computation: 110060 steps/s (collection: 0.793s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 71.7189
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0377
                       Mean reward: 162.42
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 31.9472
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.89s
                      Time elapsed: 00:04:57
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 309/1500 [0m                      

                       Computation: 110066 steps/s (collection: 0.780s, learning 0.113s)
             Mean action noise std: 1.88
          Mean value_function loss: 71.9330
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.0424
                       Mean reward: 177.53
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 31.0343
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.89s
                      Time elapsed: 00:04:58
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 310/1500 [0m                      

                       Computation: 107632 steps/s (collection: 0.807s, learning 0.106s)
             Mean action noise std: 1.88
          Mean value_function loss: 78.5418
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.0445
                       Mean reward: 183.55
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 35.6258
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.91s
                      Time elapsed: 00:04:59
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 311/1500 [0m                      

                       Computation: 108638 steps/s (collection: 0.789s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 83.7129
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.0456
                       Mean reward: 154.03
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 0.3123
    Episode_Reward/rotating_object: 35.8850
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.90s
                      Time elapsed: 00:05:00
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 312/1500 [0m                      

                       Computation: 107416 steps/s (collection: 0.788s, learning 0.127s)
             Mean action noise std: 1.88
          Mean value_function loss: 89.6192
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0510
                       Mean reward: 144.34
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 32.8210
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.92s
                      Time elapsed: 00:05:01
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 313/1500 [0m                      

                       Computation: 107161 steps/s (collection: 0.787s, learning 0.131s)
             Mean action noise std: 1.88
          Mean value_function loss: 78.2522
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0549
                       Mean reward: 169.15
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 34.2812
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.92s
                      Time elapsed: 00:05:02
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 314/1500 [0m                      

                       Computation: 105839 steps/s (collection: 0.755s, learning 0.174s)
             Mean action noise std: 1.89
          Mean value_function loss: 75.6433
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0551
                       Mean reward: 208.53
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.3123
    Episode_Reward/rotating_object: 31.9421
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.93s
                      Time elapsed: 00:05:02
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 315/1500 [0m                      

                       Computation: 107818 steps/s (collection: 0.774s, learning 0.138s)
             Mean action noise std: 1.89
          Mean value_function loss: 88.5435
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.0573
                       Mean reward: 178.67
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 34.0533
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.91s
                      Time elapsed: 00:05:03
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 316/1500 [0m                      

                       Computation: 112768 steps/s (collection: 0.766s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 71.2165
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0620
                       Mean reward: 140.66
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 34.6588
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.87s
                      Time elapsed: 00:05:04
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 317/1500 [0m                      

                       Computation: 111227 steps/s (collection: 0.773s, learning 0.111s)
             Mean action noise std: 1.89
          Mean value_function loss: 79.9089
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0661
                       Mean reward: 211.78
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 35.3706
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.88s
                      Time elapsed: 00:05:05
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 318/1500 [0m                      

                       Computation: 114753 steps/s (collection: 0.771s, learning 0.086s)
             Mean action noise std: 1.89
          Mean value_function loss: 75.4690
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0630
                       Mean reward: 189.97
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 32.9274
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.86s
                      Time elapsed: 00:05:06
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 319/1500 [0m                      

                       Computation: 110902 steps/s (collection: 0.770s, learning 0.116s)
             Mean action noise std: 1.89
          Mean value_function loss: 76.2880
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0652
                       Mean reward: 192.38
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 36.4782
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.89s
                      Time elapsed: 00:05:07
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 320/1500 [0m                      

                       Computation: 100275 steps/s (collection: 0.827s, learning 0.153s)
             Mean action noise std: 1.89
          Mean value_function loss: 79.2034
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0719
                       Mean reward: 161.95
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 34.5446
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.98s
                      Time elapsed: 00:05:08
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 321/1500 [0m                      

                       Computation: 110724 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 1.89
          Mean value_function loss: 71.4531
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.0810
                       Mean reward: 163.82
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.3138
    Episode_Reward/rotating_object: 33.1344
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.89s
                      Time elapsed: 00:05:09
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 322/1500 [0m                      

                       Computation: 108782 steps/s (collection: 0.778s, learning 0.126s)
             Mean action noise std: 1.90
          Mean value_function loss: 66.8775
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0855
                       Mean reward: 167.95
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 35.4035
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.90s
                      Time elapsed: 00:05:10
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 323/1500 [0m                      

                       Computation: 107342 steps/s (collection: 0.785s, learning 0.131s)
             Mean action noise std: 1.90
          Mean value_function loss: 61.1594
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0860
                       Mean reward: 180.38
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 35.4180
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.92s
                      Time elapsed: 00:05:11
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 324/1500 [0m                      

                       Computation: 106294 steps/s (collection: 0.755s, learning 0.170s)
             Mean action noise std: 1.90
          Mean value_function loss: 60.3170
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.0824
                       Mean reward: 171.55
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 33.9438
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.92s
                      Time elapsed: 00:05:11
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 325/1500 [0m                      

                       Computation: 106908 steps/s (collection: 0.788s, learning 0.132s)
             Mean action noise std: 1.90
          Mean value_function loss: 59.7221
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0835
                       Mean reward: 159.19
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 0.3074
    Episode_Reward/rotating_object: 32.8044
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.92s
                      Time elapsed: 00:05:12
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 326/1500 [0m                      

                       Computation: 109366 steps/s (collection: 0.792s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 58.6571
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0883
                       Mean reward: 198.90
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 35.8584
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.90s
                      Time elapsed: 00:05:13
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 327/1500 [0m                      

                       Computation: 108512 steps/s (collection: 0.786s, learning 0.120s)
             Mean action noise std: 1.90
          Mean value_function loss: 70.8662
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0891
                       Mean reward: 146.61
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.3045
    Episode_Reward/rotating_object: 31.7720
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.91s
                      Time elapsed: 00:05:14
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 328/1500 [0m                      

                       Computation: 107807 steps/s (collection: 0.795s, learning 0.117s)
             Mean action noise std: 1.90
          Mean value_function loss: 58.6007
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.0911
                       Mean reward: 179.72
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 0.3065
    Episode_Reward/rotating_object: 34.5361
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.91s
                      Time elapsed: 00:05:15
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 329/1500 [0m                      

                       Computation: 105882 steps/s (collection: 0.814s, learning 0.114s)
             Mean action noise std: 1.90
          Mean value_function loss: 66.4970
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.0974
                       Mean reward: 171.24
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 0.3095
    Episode_Reward/rotating_object: 32.4885
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.93s
                      Time elapsed: 00:05:16
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 330/1500 [0m                      

                       Computation: 114235 steps/s (collection: 0.774s, learning 0.086s)
             Mean action noise std: 1.90
          Mean value_function loss: 66.3450
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0999
                       Mean reward: 173.15
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 33.5769
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.86s
                      Time elapsed: 00:05:17
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 331/1500 [0m                      

                       Computation: 111276 steps/s (collection: 0.771s, learning 0.113s)
             Mean action noise std: 1.90
          Mean value_function loss: 60.2446
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.0994
                       Mean reward: 155.45
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 34.8080
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.88s
                      Time elapsed: 00:05:18
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 332/1500 [0m                      

                       Computation: 102721 steps/s (collection: 0.811s, learning 0.146s)
             Mean action noise std: 1.90
          Mean value_function loss: 65.8925
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0935
                       Mean reward: 194.57
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.3123
    Episode_Reward/rotating_object: 35.4995
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.96s
                      Time elapsed: 00:05:19
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 333/1500 [0m                      

                       Computation: 45896 steps/s (collection: 2.037s, learning 0.105s)
             Mean action noise std: 1.90
          Mean value_function loss: 62.5614
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0929
                       Mean reward: 225.89
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 37.8779
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.14s
                      Time elapsed: 00:05:21
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 334/1500 [0m                      

                       Computation: 33812 steps/s (collection: 2.780s, learning 0.127s)
             Mean action noise std: 1.91
          Mean value_function loss: 67.0474
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.1007
                       Mean reward: 187.31
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 34.9005
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.91s
                      Time elapsed: 00:05:24
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 335/1500 [0m                      

                       Computation: 31852 steps/s (collection: 2.944s, learning 0.142s)
             Mean action noise std: 1.91
          Mean value_function loss: 67.2845
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1110
                       Mean reward: 188.98
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 36.4884
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.09s
                      Time elapsed: 00:05:27
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 336/1500 [0m                      

                       Computation: 33058 steps/s (collection: 2.851s, learning 0.122s)
             Mean action noise std: 1.91
          Mean value_function loss: 66.6588
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1172
                       Mean reward: 171.50
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 36.7313
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 2.97s
                      Time elapsed: 00:05:30
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 337/1500 [0m                      

                       Computation: 33293 steps/s (collection: 2.831s, learning 0.121s)
             Mean action noise std: 1.91
          Mean value_function loss: 69.6346
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.1234
                       Mean reward: 191.75
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 36.1727
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 2.95s
                      Time elapsed: 00:05:33
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 338/1500 [0m                      

                       Computation: 32086 steps/s (collection: 2.935s, learning 0.129s)
             Mean action noise std: 1.91
          Mean value_function loss: 70.0853
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.1324
                       Mean reward: 156.54
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 35.4266
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.06s
                      Time elapsed: 00:05:36
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 339/1500 [0m                      

                       Computation: 32164 steps/s (collection: 2.915s, learning 0.142s)
             Mean action noise std: 1.91
          Mean value_function loss: 71.7275
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.1305
                       Mean reward: 189.12
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 0.3184
    Episode_Reward/rotating_object: 38.7774
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.06s
                      Time elapsed: 00:05:39
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 340/1500 [0m                      

                       Computation: 29605 steps/s (collection: 3.175s, learning 0.146s)
             Mean action noise std: 1.91
          Mean value_function loss: 77.1793
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.1292
                       Mean reward: 159.15
               Mean episode length: 234.19
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 37.3862
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.32s
                      Time elapsed: 00:05:42
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 341/1500 [0m                      

                       Computation: 21663 steps/s (collection: 4.339s, learning 0.198s)
             Mean action noise std: 1.91
          Mean value_function loss: 76.0181
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.1275
                       Mean reward: 181.85
               Mean episode length: 232.61
    Episode_Reward/reaching_object: 0.3305
    Episode_Reward/rotating_object: 37.4274
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.54s
                      Time elapsed: 00:05:47
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 342/1500 [0m                      

                       Computation: 108416 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 78.2367
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.1324
                       Mean reward: 205.68
               Mean episode length: 228.22
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 35.8565
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.91s
                      Time elapsed: 00:05:48
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 343/1500 [0m                      

                       Computation: 108633 steps/s (collection: 0.785s, learning 0.120s)
             Mean action noise std: 1.92
          Mean value_function loss: 66.4539
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1424
                       Mean reward: 217.40
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.3276
    Episode_Reward/rotating_object: 39.3645
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.90s
                      Time elapsed: 00:05:49
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 344/1500 [0m                      

                       Computation: 106833 steps/s (collection: 0.793s, learning 0.128s)
             Mean action noise std: 1.92
          Mean value_function loss: 72.2673
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.1478
                       Mean reward: 201.31
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.3370
    Episode_Reward/rotating_object: 40.0360
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.92s
                      Time elapsed: 00:05:50
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 345/1500 [0m                      

                       Computation: 110843 steps/s (collection: 0.759s, learning 0.128s)
             Mean action noise std: 1.92
          Mean value_function loss: 83.2720
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1534
                       Mean reward: 207.63
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.3270
    Episode_Reward/rotating_object: 38.7811
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.89s
                      Time elapsed: 00:05:50
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 346/1500 [0m                      

                       Computation: 110704 steps/s (collection: 0.771s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 95.4652
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1574
                       Mean reward: 183.02
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 36.9053
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.89s
                      Time elapsed: 00:05:51
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 347/1500 [0m                      

                       Computation: 105837 steps/s (collection: 0.809s, learning 0.120s)
             Mean action noise std: 1.92
          Mean value_function loss: 92.6085
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1632
                       Mean reward: 185.56
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.3284
    Episode_Reward/rotating_object: 40.1758
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.93s
                      Time elapsed: 00:05:52
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 348/1500 [0m                      

                       Computation: 113117 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 88.0366
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1614
                       Mean reward: 198.18
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.3359
    Episode_Reward/rotating_object: 41.0858
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.87s
                      Time elapsed: 00:05:53
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 349/1500 [0m                      

                       Computation: 108416 steps/s (collection: 0.790s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 85.7176
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1646
                       Mean reward: 230.90
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.3424
    Episode_Reward/rotating_object: 42.2613
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.91s
                      Time elapsed: 00:05:54
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 350/1500 [0m                      

                       Computation: 98493 steps/s (collection: 0.872s, learning 0.126s)
             Mean action noise std: 1.93
          Mean value_function loss: 89.1177
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.1660
                       Mean reward: 196.57
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 0.3393
    Episode_Reward/rotating_object: 38.6328
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.00s
                      Time elapsed: 00:05:55
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 351/1500 [0m                      

                       Computation: 100238 steps/s (collection: 0.866s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 106.1296
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1667
                       Mean reward: 167.88
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.3271
    Episode_Reward/rotating_object: 35.0180
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.98s
                      Time elapsed: 00:05:56
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 352/1500 [0m                      

                       Computation: 105001 steps/s (collection: 0.826s, learning 0.111s)
             Mean action noise std: 1.93
          Mean value_function loss: 126.1288
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.1672
                       Mean reward: 187.36
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 38.1265
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.94s
                      Time elapsed: 00:05:57
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 353/1500 [0m                      

                       Computation: 110114 steps/s (collection: 0.783s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 104.6123
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1726
                       Mean reward: 235.79
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.3310
    Episode_Reward/rotating_object: 39.8218
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.89s
                      Time elapsed: 00:05:58
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 354/1500 [0m                      

                       Computation: 109248 steps/s (collection: 0.785s, learning 0.115s)
             Mean action noise std: 1.93
          Mean value_function loss: 97.7243
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1788
                       Mean reward: 205.91
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 0.3411
    Episode_Reward/rotating_object: 40.2625
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.90s
                      Time elapsed: 00:05:59
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 355/1500 [0m                      

                       Computation: 102805 steps/s (collection: 0.747s, learning 0.209s)
             Mean action noise std: 1.93
          Mean value_function loss: 86.0929
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1889
                       Mean reward: 228.16
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.3348
    Episode_Reward/rotating_object: 39.5274
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.96s
                      Time elapsed: 00:06:00
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 356/1500 [0m                      

                       Computation: 96761 steps/s (collection: 0.862s, learning 0.154s)
             Mean action noise std: 1.94
          Mean value_function loss: 91.3504
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1965
                       Mean reward: 195.35
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 37.4170
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.02s
                      Time elapsed: 00:06:01
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 357/1500 [0m                      

                       Computation: 112401 steps/s (collection: 0.778s, learning 0.097s)
             Mean action noise std: 1.94
          Mean value_function loss: 84.5178
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.2036
                       Mean reward: 240.66
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.3335
    Episode_Reward/rotating_object: 42.8122
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.87s
                      Time elapsed: 00:06:02
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 358/1500 [0m                      

                       Computation: 107813 steps/s (collection: 0.823s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 87.2508
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.2139
                       Mean reward: 175.60
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.3346
    Episode_Reward/rotating_object: 39.1432
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.91s
                      Time elapsed: 00:06:02
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 359/1500 [0m                      

                       Computation: 110701 steps/s (collection: 0.770s, learning 0.118s)
             Mean action noise std: 1.94
          Mean value_function loss: 84.6349
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2156
                       Mean reward: 191.99
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 36.5691
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.89s
                      Time elapsed: 00:06:03
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 360/1500 [0m                      

                       Computation: 108601 steps/s (collection: 0.789s, learning 0.117s)
             Mean action noise std: 1.94
          Mean value_function loss: 87.7245
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.2174
                       Mean reward: 200.20
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.3369
    Episode_Reward/rotating_object: 40.2423
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.91s
                      Time elapsed: 00:06:04
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 361/1500 [0m                      

                       Computation: 107745 steps/s (collection: 0.797s, learning 0.115s)
             Mean action noise std: 1.94
          Mean value_function loss: 102.1331
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.2141
                       Mean reward: 232.53
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 0.3322
    Episode_Reward/rotating_object: 38.9041
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.91s
                      Time elapsed: 00:06:05
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 362/1500 [0m                      

                       Computation: 112961 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 1.94
          Mean value_function loss: 104.6574
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2139
                       Mean reward: 185.21
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 0.3323
    Episode_Reward/rotating_object: 37.2656
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.87s
                      Time elapsed: 00:06:06
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 363/1500 [0m                      

                       Computation: 103170 steps/s (collection: 0.839s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 98.1801
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2145
                       Mean reward: 201.84
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 37.6090
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.95s
                      Time elapsed: 00:06:07
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 364/1500 [0m                      

                       Computation: 113773 steps/s (collection: 0.777s, learning 0.087s)
             Mean action noise std: 1.94
          Mean value_function loss: 95.6099
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.2164
                       Mean reward: 247.26
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3354
    Episode_Reward/rotating_object: 43.1648
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.86s
                      Time elapsed: 00:06:08
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 365/1500 [0m                      

                       Computation: 106920 steps/s (collection: 0.766s, learning 0.154s)
             Mean action noise std: 1.95
          Mean value_function loss: 90.0386
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.2163
                       Mean reward: 221.11
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.3311
    Episode_Reward/rotating_object: 41.8001
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.92s
                      Time elapsed: 00:06:09
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 366/1500 [0m                      

                       Computation: 106606 steps/s (collection: 0.802s, learning 0.120s)
             Mean action noise std: 1.95
          Mean value_function loss: 84.7738
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.2206
                       Mean reward: 207.04
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 0.3344
    Episode_Reward/rotating_object: 40.3682
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.92s
                      Time elapsed: 00:06:10
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 367/1500 [0m                      

                       Computation: 113947 steps/s (collection: 0.763s, learning 0.100s)
             Mean action noise std: 1.95
          Mean value_function loss: 99.4541
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 16.2263
                       Mean reward: 218.20
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.3401
    Episode_Reward/rotating_object: 39.8123
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.86s
                      Time elapsed: 00:06:11
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 368/1500 [0m                      

                       Computation: 109629 steps/s (collection: 0.771s, learning 0.125s)
             Mean action noise std: 1.95
          Mean value_function loss: 89.9183
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.2259
                       Mean reward: 174.26
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 0.3304
    Episode_Reward/rotating_object: 40.1343
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.90s
                      Time elapsed: 00:06:11
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 369/1500 [0m                      

                       Computation: 110485 steps/s (collection: 0.781s, learning 0.109s)
             Mean action noise std: 1.95
          Mean value_function loss: 92.4007
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2189
                       Mean reward: 218.17
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.3415
    Episode_Reward/rotating_object: 42.6531
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.89s
                      Time elapsed: 00:06:12
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 370/1500 [0m                      

                       Computation: 107648 steps/s (collection: 0.795s, learning 0.119s)
             Mean action noise std: 1.95
          Mean value_function loss: 93.9079
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.2152
                       Mean reward: 215.31
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 0.3355
    Episode_Reward/rotating_object: 45.5247
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.91s
                      Time elapsed: 00:06:13
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 371/1500 [0m                      

                       Computation: 106976 steps/s (collection: 0.806s, learning 0.113s)
             Mean action noise std: 1.95
          Mean value_function loss: 95.0481
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2193
                       Mean reward: 176.17
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 43.0978
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.92s
                      Time elapsed: 00:06:14
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 372/1500 [0m                      

                       Computation: 112165 steps/s (collection: 0.782s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 98.3068
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.2299
                       Mean reward: 233.15
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.3336
    Episode_Reward/rotating_object: 45.2589
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.88s
                      Time elapsed: 00:06:15
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 373/1500 [0m                      

                       Computation: 108558 steps/s (collection: 0.804s, learning 0.101s)
             Mean action noise std: 1.95
          Mean value_function loss: 98.7762
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2279
                       Mean reward: 207.63
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.3280
    Episode_Reward/rotating_object: 37.7332
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.91s
                      Time elapsed: 00:06:16
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 374/1500 [0m                      

                       Computation: 108501 steps/s (collection: 0.779s, learning 0.127s)
             Mean action noise std: 1.95
          Mean value_function loss: 94.2513
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.2315
                       Mean reward: 187.96
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 0.3260
    Episode_Reward/rotating_object: 38.5527
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.91s
                      Time elapsed: 00:06:17
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 375/1500 [0m                      

                       Computation: 106641 steps/s (collection: 0.796s, learning 0.126s)
             Mean action noise std: 1.95
          Mean value_function loss: 105.4392
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.2338
                       Mean reward: 180.11
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 0.3260
    Episode_Reward/rotating_object: 39.9287
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.92s
                      Time elapsed: 00:06:18
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 376/1500 [0m                      

                       Computation: 101992 steps/s (collection: 0.814s, learning 0.150s)
             Mean action noise std: 1.96
          Mean value_function loss: 96.0475
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.2367
                       Mean reward: 213.81
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 0.3327
    Episode_Reward/rotating_object: 41.7578
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.96s
                      Time elapsed: 00:06:19
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 377/1500 [0m                      

                       Computation: 100580 steps/s (collection: 0.842s, learning 0.135s)
             Mean action noise std: 1.96
          Mean value_function loss: 87.1940
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2436
                       Mean reward: 179.39
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 39.6773
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.98s
                      Time elapsed: 00:06:20
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 378/1500 [0m                      

                       Computation: 87310 steps/s (collection: 0.902s, learning 0.224s)
             Mean action noise std: 1.96
          Mean value_function loss: 97.1993
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.2459
                       Mean reward: 197.61
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 38.4966
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.13s
                      Time elapsed: 00:06:21
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 379/1500 [0m                      

                       Computation: 89729 steps/s (collection: 0.889s, learning 0.207s)
             Mean action noise std: 1.96
          Mean value_function loss: 89.5082
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.2483
                       Mean reward: 180.09
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 0.3270
    Episode_Reward/rotating_object: 38.3911
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.10s
                      Time elapsed: 00:06:22
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 380/1500 [0m                      

                       Computation: 91837 steps/s (collection: 0.971s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 76.2029
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.2514
                       Mean reward: 196.53
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.3388
    Episode_Reward/rotating_object: 44.3615
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.07s
                      Time elapsed: 00:06:23
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 381/1500 [0m                      

                       Computation: 109215 steps/s (collection: 0.789s, learning 0.111s)
             Mean action noise std: 1.96
          Mean value_function loss: 82.8741
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2573
                       Mean reward: 225.04
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 0.3342
    Episode_Reward/rotating_object: 45.3861
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.90s
                      Time elapsed: 00:06:24
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 382/1500 [0m                      

                       Computation: 105432 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 80.0923
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2615
                       Mean reward: 206.71
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.3293
    Episode_Reward/rotating_object: 43.9537
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.93s
                      Time elapsed: 00:06:25
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 383/1500 [0m                      

                       Computation: 106523 steps/s (collection: 0.793s, learning 0.130s)
             Mean action noise std: 1.96
          Mean value_function loss: 78.4091
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2675
                       Mean reward: 197.81
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 0.3327
    Episode_Reward/rotating_object: 42.1526
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.92s
                      Time elapsed: 00:06:26
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 384/1500 [0m                      

                       Computation: 102274 steps/s (collection: 0.793s, learning 0.168s)
             Mean action noise std: 1.97
          Mean value_function loss: 86.9034
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2698
                       Mean reward: 246.68
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.3321
    Episode_Reward/rotating_object: 42.4589
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.96s
                      Time elapsed: 00:06:27
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 385/1500 [0m                      

                       Computation: 112306 steps/s (collection: 0.788s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 81.2162
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2738
                       Mean reward: 189.11
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 42.6551
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.88s
                      Time elapsed: 00:06:28
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 386/1500 [0m                      

                       Computation: 102628 steps/s (collection: 0.790s, learning 0.168s)
             Mean action noise std: 1.97
          Mean value_function loss: 72.3640
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.2807
                       Mean reward: 212.57
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 44.1850
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.96s
                      Time elapsed: 00:06:29
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 387/1500 [0m                      

                       Computation: 107395 steps/s (collection: 0.797s, learning 0.118s)
             Mean action noise std: 1.97
          Mean value_function loss: 76.0996
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.2835
                       Mean reward: 262.17
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.3308
    Episode_Reward/rotating_object: 47.2656
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.92s
                      Time elapsed: 00:06:29
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 388/1500 [0m                      

                       Computation: 106377 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 75.0270
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2820
                       Mean reward: 201.33
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.3298
    Episode_Reward/rotating_object: 45.7952
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.92s
                      Time elapsed: 00:06:30
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 389/1500 [0m                      

                       Computation: 105181 steps/s (collection: 0.820s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 81.4288
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2845
                       Mean reward: 213.38
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 41.9825
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.93s
                      Time elapsed: 00:06:31
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 390/1500 [0m                      

                       Computation: 108632 steps/s (collection: 0.818s, learning 0.087s)
             Mean action noise std: 1.97
          Mean value_function loss: 82.0888
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.2909
                       Mean reward: 214.98
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 41.5246
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.90s
                      Time elapsed: 00:06:32
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 391/1500 [0m                      

                       Computation: 108342 steps/s (collection: 0.798s, learning 0.110s)
             Mean action noise std: 1.97
          Mean value_function loss: 96.8495
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.2989
                       Mean reward: 284.97
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 44.2280
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.91s
                      Time elapsed: 00:06:33
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 392/1500 [0m                      

                       Computation: 107262 steps/s (collection: 0.808s, learning 0.108s)
             Mean action noise std: 1.98
          Mean value_function loss: 97.8646
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.3096
                       Mean reward: 236.84
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 0.3276
    Episode_Reward/rotating_object: 44.6804
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.92s
                      Time elapsed: 00:06:34
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 393/1500 [0m                      

                       Computation: 114074 steps/s (collection: 0.756s, learning 0.105s)
             Mean action noise std: 1.98
          Mean value_function loss: 104.8503
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3122
                       Mean reward: 188.61
               Mean episode length: 228.53
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 43.4969
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.86s
                      Time elapsed: 00:06:35
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 394/1500 [0m                      

                       Computation: 105570 steps/s (collection: 0.809s, learning 0.123s)
             Mean action noise std: 1.98
          Mean value_function loss: 96.2239
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3156
                       Mean reward: 222.77
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 43.9644
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.93s
                      Time elapsed: 00:06:36
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 395/1500 [0m                      

                       Computation: 107540 steps/s (collection: 0.789s, learning 0.126s)
             Mean action noise std: 1.98
          Mean value_function loss: 82.8995
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.3182
                       Mean reward: 210.10
               Mean episode length: 222.43
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 44.0874
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.91s
                      Time elapsed: 00:06:37
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 396/1500 [0m                      

                       Computation: 104876 steps/s (collection: 0.811s, learning 0.127s)
             Mean action noise std: 1.98
          Mean value_function loss: 100.4129
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3205
                       Mean reward: 208.35
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 0.3279
    Episode_Reward/rotating_object: 45.1764
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.94s
                      Time elapsed: 00:06:38
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 397/1500 [0m                      

                       Computation: 112954 steps/s (collection: 0.772s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 98.5865
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3219
                       Mean reward: 233.44
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 41.4058
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.87s
                      Time elapsed: 00:06:39
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 398/1500 [0m                      

                       Computation: 112634 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 1.98
          Mean value_function loss: 97.4118
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3281
                       Mean reward: 244.11
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 0.3311
    Episode_Reward/rotating_object: 45.8160
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.87s
                      Time elapsed: 00:06:39
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 399/1500 [0m                      

                       Computation: 111180 steps/s (collection: 0.799s, learning 0.086s)
             Mean action noise std: 1.98
          Mean value_function loss: 100.4604
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3341
                       Mean reward: 217.72
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 0.3279
    Episode_Reward/rotating_object: 45.1890
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.88s
                      Time elapsed: 00:06:40
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 400/1500 [0m                      

                       Computation: 106516 steps/s (collection: 0.794s, learning 0.129s)
             Mean action noise std: 1.98
          Mean value_function loss: 91.3283
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.3339
                       Mean reward: 252.41
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 0.3357
    Episode_Reward/rotating_object: 49.7017
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.92s
                      Time elapsed: 00:06:41
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 401/1500 [0m                      

                       Computation: 106022 steps/s (collection: 0.814s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 85.6033
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.3376
                       Mean reward: 230.43
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.3361
    Episode_Reward/rotating_object: 47.5451
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.93s
                      Time elapsed: 00:06:42
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 402/1500 [0m                      

                       Computation: 113253 steps/s (collection: 0.751s, learning 0.117s)
             Mean action noise std: 1.99
          Mean value_function loss: 91.9622
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3352
                       Mean reward: 245.83
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3271
    Episode_Reward/rotating_object: 47.6150
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.87s
                      Time elapsed: 00:06:43
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 403/1500 [0m                      

                       Computation: 110176 steps/s (collection: 0.801s, learning 0.091s)
             Mean action noise std: 1.99
          Mean value_function loss: 88.8135
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3353
                       Mean reward: 238.48
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.3271
    Episode_Reward/rotating_object: 43.6674
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.89s
                      Time elapsed: 00:06:44
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 404/1500 [0m                      

                       Computation: 107202 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 1.99
          Mean value_function loss: 89.5257
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3367
                       Mean reward: 238.27
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 0.3370
    Episode_Reward/rotating_object: 44.2418
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.92s
                      Time elapsed: 00:06:45
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 405/1500 [0m                      

                       Computation: 106042 steps/s (collection: 0.788s, learning 0.139s)
             Mean action noise std: 1.99
          Mean value_function loss: 83.4794
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3351
                       Mean reward: 250.01
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.3327
    Episode_Reward/rotating_object: 46.7729
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.93s
                      Time elapsed: 00:06:46
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 406/1500 [0m                      

                       Computation: 108486 steps/s (collection: 0.817s, learning 0.089s)
             Mean action noise std: 1.99
          Mean value_function loss: 94.0131
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3378
                       Mean reward: 249.63
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.3343
    Episode_Reward/rotating_object: 47.9651
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.91s
                      Time elapsed: 00:06:47
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 407/1500 [0m                      

                       Computation: 107581 steps/s (collection: 0.804s, learning 0.110s)
             Mean action noise std: 1.99
          Mean value_function loss: 92.9809
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.3486
                       Mean reward: 249.52
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 0.3332
    Episode_Reward/rotating_object: 45.5582
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.91s
                      Time elapsed: 00:06:48
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 408/1500 [0m                      

                       Computation: 109679 steps/s (collection: 0.780s, learning 0.116s)
             Mean action noise std: 1.99
          Mean value_function loss: 88.2359
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.3554
                       Mean reward: 213.48
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 0.3309
    Episode_Reward/rotating_object: 44.7719
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.90s
                      Time elapsed: 00:06:49
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 409/1500 [0m                      

                       Computation: 109540 steps/s (collection: 0.776s, learning 0.121s)
             Mean action noise std: 1.99
          Mean value_function loss: 79.8356
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.3612
                       Mean reward: 211.66
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 0.3392
    Episode_Reward/rotating_object: 47.6204
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.90s
                      Time elapsed: 00:06:49
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 410/1500 [0m                      

                       Computation: 109772 steps/s (collection: 0.797s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 85.9500
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.3620
                       Mean reward: 205.41
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 41.1159
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.90s
                      Time elapsed: 00:06:50
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 411/1500 [0m                      

                       Computation: 108410 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 1.99
          Mean value_function loss: 86.5351
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3598
                       Mean reward: 218.10
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 42.8407
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.91s
                      Time elapsed: 00:06:51
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 412/1500 [0m                      

                       Computation: 107763 steps/s (collection: 0.800s, learning 0.112s)
             Mean action noise std: 1.99
          Mean value_function loss: 86.3387
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.3565
                       Mean reward: 222.85
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 45.2014
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.91s
                      Time elapsed: 00:06:52
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 413/1500 [0m                      

                       Computation: 102705 steps/s (collection: 0.858s, learning 0.100s)
             Mean action noise std: 2.00
          Mean value_function loss: 84.2005
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3577
                       Mean reward: 256.12
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 46.0401
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.96s
                      Time elapsed: 00:06:53
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 414/1500 [0m                      

                       Computation: 111355 steps/s (collection: 0.782s, learning 0.101s)
             Mean action noise std: 2.00
          Mean value_function loss: 92.5227
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.3553
                       Mean reward: 243.96
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 0.3291
    Episode_Reward/rotating_object: 45.3891
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.88s
                      Time elapsed: 00:06:54
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 415/1500 [0m                      

                       Computation: 101325 steps/s (collection: 0.856s, learning 0.114s)
             Mean action noise std: 2.00
          Mean value_function loss: 87.5049
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.3527
                       Mean reward: 276.06
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 0.3263
    Episode_Reward/rotating_object: 48.0064
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.97s
                      Time elapsed: 00:06:55
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 416/1500 [0m                      

                       Computation: 104821 steps/s (collection: 0.829s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 77.6791
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.3563
                       Mean reward: 208.94
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.3324
    Episode_Reward/rotating_object: 45.3317
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.94s
                      Time elapsed: 00:06:56
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 417/1500 [0m                      

                       Computation: 100271 steps/s (collection: 0.836s, learning 0.144s)
             Mean action noise std: 2.00
          Mean value_function loss: 83.7521
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3556
                       Mean reward: 220.67
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 44.0518
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.98s
                      Time elapsed: 00:06:57
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 418/1500 [0m                      

                       Computation: 105741 steps/s (collection: 0.830s, learning 0.100s)
             Mean action noise std: 2.00
          Mean value_function loss: 80.9718
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.3577
                       Mean reward: 216.76
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.3278
    Episode_Reward/rotating_object: 43.5445
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.93s
                      Time elapsed: 00:06:58
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 419/1500 [0m                      

                       Computation: 100873 steps/s (collection: 0.830s, learning 0.144s)
             Mean action noise std: 2.00
          Mean value_function loss: 85.1712
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3622
                       Mean reward: 231.04
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 0.3401
    Episode_Reward/rotating_object: 52.9234
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.97s
                      Time elapsed: 00:06:59
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 420/1500 [0m                      

                       Computation: 95470 steps/s (collection: 0.843s, learning 0.187s)
             Mean action noise std: 2.00
          Mean value_function loss: 73.1638
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.3710
                       Mean reward: 250.46
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.3324
    Episode_Reward/rotating_object: 47.8874
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 1.03s
                      Time elapsed: 00:07:00
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 421/1500 [0m                      

                       Computation: 93528 steps/s (collection: 0.920s, learning 0.131s)
             Mean action noise std: 2.00
          Mean value_function loss: 83.3296
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.3758
                       Mean reward: 223.03
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 0.3320
    Episode_Reward/rotating_object: 49.3052
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.05s
                      Time elapsed: 00:07:01
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 422/1500 [0m                      

                       Computation: 111160 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 2.00
          Mean value_function loss: 83.2412
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3800
                       Mean reward: 240.72
               Mean episode length: 238.81
    Episode_Reward/reaching_object: 0.3417
    Episode_Reward/rotating_object: 48.3788
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.88s
                      Time elapsed: 00:07:02
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 423/1500 [0m                      

                       Computation: 103332 steps/s (collection: 0.798s, learning 0.153s)
             Mean action noise std: 2.01
          Mean value_function loss: 90.7476
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.3820
                       Mean reward: 214.89
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 0.3323
    Episode_Reward/rotating_object: 48.8673
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.95s
                      Time elapsed: 00:07:03
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 424/1500 [0m                      

                       Computation: 102207 steps/s (collection: 0.807s, learning 0.155s)
             Mean action noise std: 2.01
          Mean value_function loss: 91.7926
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3877
                       Mean reward: 276.40
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 0.3436
    Episode_Reward/rotating_object: 50.2442
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.96s
                      Time elapsed: 00:07:04
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 425/1500 [0m                      

                       Computation: 109266 steps/s (collection: 0.782s, learning 0.118s)
             Mean action noise std: 2.01
          Mean value_function loss: 85.4799
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.3921
                       Mean reward: 247.61
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.3352
    Episode_Reward/rotating_object: 49.1509
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.90s
                      Time elapsed: 00:07:05
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 426/1500 [0m                      

                       Computation: 103025 steps/s (collection: 0.806s, learning 0.148s)
             Mean action noise std: 2.01
          Mean value_function loss: 88.1547
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.3953
                       Mean reward: 245.67
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 0.3411
    Episode_Reward/rotating_object: 45.4040
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.95s
                      Time elapsed: 00:07:06
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 427/1500 [0m                      

                       Computation: 104858 steps/s (collection: 0.819s, learning 0.119s)
             Mean action noise std: 2.01
          Mean value_function loss: 95.2698
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.4056
                       Mean reward: 257.25
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.3397
    Episode_Reward/rotating_object: 48.7875
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.94s
                      Time elapsed: 00:07:06
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 428/1500 [0m                      

                       Computation: 108761 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 2.01
          Mean value_function loss: 87.5194
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.4117
                       Mean reward: 268.80
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.3461
    Episode_Reward/rotating_object: 51.3677
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.90s
                      Time elapsed: 00:07:07
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 429/1500 [0m                      

                       Computation: 101772 steps/s (collection: 0.844s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 84.7506
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4089
                       Mean reward: 214.00
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 0.3427
    Episode_Reward/rotating_object: 42.9697
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.97s
                      Time elapsed: 00:07:08
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 430/1500 [0m                      

                       Computation: 109392 steps/s (collection: 0.807s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 89.9645
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.4113
                       Mean reward: 253.81
               Mean episode length: 237.41
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 50.4087
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.90s
                      Time elapsed: 00:07:09
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 431/1500 [0m                      

                       Computation: 104055 steps/s (collection: 0.834s, learning 0.111s)
             Mean action noise std: 2.02
          Mean value_function loss: 76.4400
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.4136
                       Mean reward: 247.89
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 0.3383
    Episode_Reward/rotating_object: 47.1666
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.94s
                      Time elapsed: 00:07:10
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 432/1500 [0m                      

                       Computation: 103802 steps/s (collection: 0.819s, learning 0.128s)
             Mean action noise std: 2.02
          Mean value_function loss: 81.9996
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 16.4139
                       Mean reward: 260.67
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.3362
    Episode_Reward/rotating_object: 49.3852
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.95s
                      Time elapsed: 00:07:11
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 433/1500 [0m                      

                       Computation: 94660 steps/s (collection: 0.877s, learning 0.162s)
             Mean action noise std: 2.02
          Mean value_function loss: 82.9684
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4152
                       Mean reward: 254.83
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 0.3352
    Episode_Reward/rotating_object: 45.3764
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.04s
                      Time elapsed: 00:07:12
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 434/1500 [0m                      

                       Computation: 98063 steps/s (collection: 0.891s, learning 0.112s)
             Mean action noise std: 2.02
          Mean value_function loss: 82.1550
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4271
                       Mean reward: 210.91
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 0.3423
    Episode_Reward/rotating_object: 50.4089
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 1.00s
                      Time elapsed: 00:07:13
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 435/1500 [0m                      

                       Computation: 109723 steps/s (collection: 0.804s, learning 0.092s)
             Mean action noise std: 2.02
          Mean value_function loss: 91.2946
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.4383
                       Mean reward: 231.67
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.3280
    Episode_Reward/rotating_object: 47.6908
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.90s
                      Time elapsed: 00:07:14
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 436/1500 [0m                      

                       Computation: 108450 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 2.02
          Mean value_function loss: 81.5643
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.4459
                       Mean reward: 226.49
               Mean episode length: 239.05
    Episode_Reward/reaching_object: 0.3350
    Episode_Reward/rotating_object: 45.2225
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.91s
                      Time elapsed: 00:07:15
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 437/1500 [0m                      

                       Computation: 113129 steps/s (collection: 0.764s, learning 0.105s)
             Mean action noise std: 2.03
          Mean value_function loss: 87.5330
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.4449
                       Mean reward: 269.44
               Mean episode length: 231.38
    Episode_Reward/reaching_object: 0.3416
    Episode_Reward/rotating_object: 52.1669
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.87s
                      Time elapsed: 00:07:16
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 438/1500 [0m                      

                       Computation: 102722 steps/s (collection: 0.789s, learning 0.168s)
             Mean action noise std: 2.03
          Mean value_function loss: 86.8370
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.4538
                       Mean reward: 257.66
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.3376
    Episode_Reward/rotating_object: 53.9880
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.96s
                      Time elapsed: 00:07:17
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 439/1500 [0m                      

                       Computation: 105315 steps/s (collection: 0.793s, learning 0.140s)
             Mean action noise std: 2.03
          Mean value_function loss: 87.4344
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.4594
                       Mean reward: 244.97
               Mean episode length: 242.10
    Episode_Reward/reaching_object: 0.3372
    Episode_Reward/rotating_object: 50.4552
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.93s
                      Time elapsed: 00:07:18
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 440/1500 [0m                      

                       Computation: 96948 steps/s (collection: 0.869s, learning 0.145s)
             Mean action noise std: 2.03
          Mean value_function loss: 95.0286
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4588
                       Mean reward: 279.72
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.3391
    Episode_Reward/rotating_object: 50.9131
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.01s
                      Time elapsed: 00:07:19
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 441/1500 [0m                      

                       Computation: 104416 steps/s (collection: 0.815s, learning 0.126s)
             Mean action noise std: 2.03
          Mean value_function loss: 91.2323
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.4658
                       Mean reward: 238.96
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.3352
    Episode_Reward/rotating_object: 47.5257
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.94s
                      Time elapsed: 00:07:20
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 442/1500 [0m                      

                       Computation: 97374 steps/s (collection: 0.837s, learning 0.173s)
             Mean action noise std: 2.03
          Mean value_function loss: 92.9903
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4745
                       Mean reward: 220.85
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 0.3379
    Episode_Reward/rotating_object: 49.3997
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 1.01s
                      Time elapsed: 00:07:21
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 443/1500 [0m                      

                       Computation: 102718 steps/s (collection: 0.791s, learning 0.166s)
             Mean action noise std: 2.03
          Mean value_function loss: 91.9443
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.4773
                       Mean reward: 280.34
               Mean episode length: 238.81
    Episode_Reward/reaching_object: 0.3439
    Episode_Reward/rotating_object: 51.3956
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.96s
                      Time elapsed: 00:07:22
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 444/1500 [0m                      

                       Computation: 103215 steps/s (collection: 0.807s, learning 0.146s)
             Mean action noise std: 2.04
          Mean value_function loss: 98.8804
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4815
                       Mean reward: 278.78
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.3421
    Episode_Reward/rotating_object: 50.6003
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.95s
                      Time elapsed: 00:07:23
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 445/1500 [0m                      

                       Computation: 101659 steps/s (collection: 0.816s, learning 0.151s)
             Mean action noise std: 2.04
          Mean value_function loss: 89.3941
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.4826
                       Mean reward: 243.68
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.3368
    Episode_Reward/rotating_object: 51.0593
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.97s
                      Time elapsed: 00:07:24
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 446/1500 [0m                      

                       Computation: 100139 steps/s (collection: 0.819s, learning 0.162s)
             Mean action noise std: 2.04
          Mean value_function loss: 80.6994
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.4881
                       Mean reward: 265.83
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 0.3465
    Episode_Reward/rotating_object: 53.2245
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.98s
                      Time elapsed: 00:07:25
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 447/1500 [0m                      

                       Computation: 103157 steps/s (collection: 0.815s, learning 0.138s)
             Mean action noise std: 2.04
          Mean value_function loss: 92.1574
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.4928
                       Mean reward: 229.15
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.3501
    Episode_Reward/rotating_object: 52.9495
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.95s
                      Time elapsed: 00:07:25
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 448/1500 [0m                      

                       Computation: 108451 steps/s (collection: 0.798s, learning 0.108s)
             Mean action noise std: 2.04
          Mean value_function loss: 89.3704
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.4992
                       Mean reward: 251.68
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.3468
    Episode_Reward/rotating_object: 53.9947
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.91s
                      Time elapsed: 00:07:26
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 449/1500 [0m                      

                       Computation: 110524 steps/s (collection: 0.778s, learning 0.111s)
             Mean action noise std: 2.04
          Mean value_function loss: 95.3551
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.5029
                       Mean reward: 277.27
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.3532
    Episode_Reward/rotating_object: 53.5552
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.89s
                      Time elapsed: 00:07:27
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 450/1500 [0m                      

                       Computation: 107327 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 2.05
          Mean value_function loss: 102.2993
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5057
                       Mean reward: 258.19
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 0.3455
    Episode_Reward/rotating_object: 50.9411
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.92s
                      Time elapsed: 00:07:28
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 451/1500 [0m                      

                       Computation: 109365 steps/s (collection: 0.800s, learning 0.099s)
             Mean action noise std: 2.05
          Mean value_function loss: 99.7506
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.5136
                       Mean reward: 235.87
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.3400
    Episode_Reward/rotating_object: 48.6800
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.90s
                      Time elapsed: 00:07:29
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 452/1500 [0m                      

                       Computation: 105120 steps/s (collection: 0.785s, learning 0.151s)
             Mean action noise std: 2.05
          Mean value_function loss: 100.1792
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5137
                       Mean reward: 208.17
               Mean episode length: 229.00
    Episode_Reward/reaching_object: 0.3393
    Episode_Reward/rotating_object: 49.1884
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.94s
                      Time elapsed: 00:07:30
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 453/1500 [0m                      

                       Computation: 108269 steps/s (collection: 0.799s, learning 0.109s)
             Mean action noise std: 2.05
          Mean value_function loss: 94.0978
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.5208
                       Mean reward: 271.28
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.3418
    Episode_Reward/rotating_object: 51.7364
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.91s
                      Time elapsed: 00:07:31
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 454/1500 [0m                      

                       Computation: 105775 steps/s (collection: 0.817s, learning 0.113s)
             Mean action noise std: 2.05
          Mean value_function loss: 97.0632
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.5290
                       Mean reward: 238.98
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.3347
    Episode_Reward/rotating_object: 50.0805
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.93s
                      Time elapsed: 00:07:32
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 455/1500 [0m                      

                       Computation: 105546 steps/s (collection: 0.806s, learning 0.125s)
             Mean action noise std: 2.05
          Mean value_function loss: 85.2918
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.5363
                       Mean reward: 253.41
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 0.3460
    Episode_Reward/rotating_object: 52.4360
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.93s
                      Time elapsed: 00:07:33
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 456/1500 [0m                      

                       Computation: 98379 steps/s (collection: 0.897s, learning 0.102s)
             Mean action noise std: 2.06
          Mean value_function loss: 87.6192
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.5377
                       Mean reward: 304.60
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.3458
    Episode_Reward/rotating_object: 56.5823
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.00s
                      Time elapsed: 00:07:34
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 457/1500 [0m                      

                       Computation: 101080 steps/s (collection: 0.813s, learning 0.160s)
             Mean action noise std: 2.06
          Mean value_function loss: 79.5508
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.5360
                       Mean reward: 273.37
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.3384
    Episode_Reward/rotating_object: 53.7028
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.97s
                      Time elapsed: 00:07:35
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 458/1500 [0m                      

                       Computation: 104811 steps/s (collection: 0.802s, learning 0.136s)
             Mean action noise std: 2.06
          Mean value_function loss: 91.4550
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.5379
                       Mean reward: 275.49
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.3351
    Episode_Reward/rotating_object: 50.4516
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.94s
                      Time elapsed: 00:07:36
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 459/1500 [0m                      

                       Computation: 105085 steps/s (collection: 0.794s, learning 0.142s)
             Mean action noise std: 2.06
          Mean value_function loss: 83.1846
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5350
                       Mean reward: 261.82
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.3431
    Episode_Reward/rotating_object: 53.6115
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.94s
                      Time elapsed: 00:07:37
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 460/1500 [0m                      

                       Computation: 107268 steps/s (collection: 0.789s, learning 0.127s)
             Mean action noise std: 2.06
          Mean value_function loss: 92.4522
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.5349
                       Mean reward: 243.95
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 0.3339
    Episode_Reward/rotating_object: 50.9367
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.92s
                      Time elapsed: 00:07:38
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 461/1500 [0m                      

                       Computation: 97587 steps/s (collection: 0.870s, learning 0.138s)
             Mean action noise std: 2.06
          Mean value_function loss: 91.0996
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.5441
                       Mean reward: 281.72
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.3397
    Episode_Reward/rotating_object: 56.7724
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 1.01s
                      Time elapsed: 00:07:39
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 462/1500 [0m                      

                       Computation: 100176 steps/s (collection: 0.814s, learning 0.167s)
             Mean action noise std: 2.06
          Mean value_function loss: 86.4373
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 16.5570
                       Mean reward: 278.67
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.3464
    Episode_Reward/rotating_object: 53.5543
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.98s
                      Time elapsed: 00:07:40
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 463/1500 [0m                      

                       Computation: 101097 steps/s (collection: 0.813s, learning 0.160s)
             Mean action noise std: 2.06
          Mean value_function loss: 90.6639
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 16.5582
                       Mean reward: 254.76
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.3427
    Episode_Reward/rotating_object: 51.3351
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.97s
                      Time elapsed: 00:07:41
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 464/1500 [0m                      

                       Computation: 109668 steps/s (collection: 0.802s, learning 0.095s)
             Mean action noise std: 2.07
          Mean value_function loss: 94.4190
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.5595
                       Mean reward: 249.55
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.3370
    Episode_Reward/rotating_object: 52.6719
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.90s
                      Time elapsed: 00:07:41
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 465/1500 [0m                      

                       Computation: 110800 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 2.07
          Mean value_function loss: 91.6980
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.5654
                       Mean reward: 265.54
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 0.3456
    Episode_Reward/rotating_object: 55.1728
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.89s
                      Time elapsed: 00:07:42
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 466/1500 [0m                      

                       Computation: 107358 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 2.07
          Mean value_function loss: 94.3109
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.5742
                       Mean reward: 275.97
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.3567
    Episode_Reward/rotating_object: 54.5043
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.92s
                      Time elapsed: 00:07:43
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 467/1500 [0m                      

                       Computation: 111282 steps/s (collection: 0.770s, learning 0.114s)
             Mean action noise std: 2.07
          Mean value_function loss: 95.8066
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.5810
                       Mean reward: 254.39
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3561
    Episode_Reward/rotating_object: 57.1075
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.88s
                      Time elapsed: 00:07:44
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 468/1500 [0m                      

                       Computation: 104254 steps/s (collection: 0.828s, learning 0.115s)
             Mean action noise std: 2.07
          Mean value_function loss: 103.4777
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.5828
                       Mean reward: 273.76
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 0.3431
    Episode_Reward/rotating_object: 48.8279
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.94s
                      Time elapsed: 00:07:45
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 469/1500 [0m                      

                       Computation: 106095 steps/s (collection: 0.816s, learning 0.111s)
             Mean action noise std: 2.07
          Mean value_function loss: 107.6726
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5785
                       Mean reward: 263.64
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 0.3520
    Episode_Reward/rotating_object: 54.9988
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.93s
                      Time elapsed: 00:07:46
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 470/1500 [0m                      

                       Computation: 105089 steps/s (collection: 0.825s, learning 0.111s)
             Mean action noise std: 2.07
          Mean value_function loss: 101.3459
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.5815
                       Mean reward: 264.60
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 0.3473
    Episode_Reward/rotating_object: 50.8117
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.94s
                      Time elapsed: 00:07:47
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 471/1500 [0m                      

                       Computation: 111294 steps/s (collection: 0.787s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 97.6684
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.5889
                       Mean reward: 236.80
               Mean episode length: 229.28
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 48.7700
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.88s
                      Time elapsed: 00:07:48
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 472/1500 [0m                      

                       Computation: 103981 steps/s (collection: 0.784s, learning 0.162s)
             Mean action noise std: 2.08
          Mean value_function loss: 98.1546
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.5943
                       Mean reward: 262.18
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.3514
    Episode_Reward/rotating_object: 51.2367
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.95s
                      Time elapsed: 00:07:49
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 473/1500 [0m                      

                       Computation: 101012 steps/s (collection: 0.808s, learning 0.165s)
             Mean action noise std: 2.08
          Mean value_function loss: 90.7330
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.5984
                       Mean reward: 301.79
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3614
    Episode_Reward/rotating_object: 56.0999
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.97s
                      Time elapsed: 00:07:50
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 474/1500 [0m                      

                       Computation: 109287 steps/s (collection: 0.793s, learning 0.106s)
             Mean action noise std: 2.08
          Mean value_function loss: 89.4465
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.5932
                       Mean reward: 280.81
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 0.3554
    Episode_Reward/rotating_object: 53.2793
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.90s
                      Time elapsed: 00:07:51
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 475/1500 [0m                      

                       Computation: 105547 steps/s (collection: 0.784s, learning 0.148s)
             Mean action noise std: 2.08
          Mean value_function loss: 86.0925
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.6003
                       Mean reward: 288.32
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.3647
    Episode_Reward/rotating_object: 54.8873
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.93s
                      Time elapsed: 00:07:52
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 476/1500 [0m                      

                       Computation: 103906 steps/s (collection: 0.844s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 79.3109
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6032
                       Mean reward: 315.83
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 0.3571
    Episode_Reward/rotating_object: 56.0972
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.95s
                      Time elapsed: 00:07:52
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 477/1500 [0m                      

                       Computation: 113693 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 2.08
          Mean value_function loss: 86.4970
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6082
                       Mean reward: 260.28
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.3554
    Episode_Reward/rotating_object: 57.3712
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.86s
                      Time elapsed: 00:07:53
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 478/1500 [0m                      

                       Computation: 107407 steps/s (collection: 0.811s, learning 0.104s)
             Mean action noise std: 2.08
          Mean value_function loss: 83.2722
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.6173
                       Mean reward: 261.03
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.3537
    Episode_Reward/rotating_object: 56.3186
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.92s
                      Time elapsed: 00:07:54
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 479/1500 [0m                      

                       Computation: 109821 steps/s (collection: 0.804s, learning 0.091s)
             Mean action noise std: 2.09
          Mean value_function loss: 88.2572
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.6218
                       Mean reward: 238.18
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 0.3467
    Episode_Reward/rotating_object: 52.8022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.90s
                      Time elapsed: 00:07:55
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 480/1500 [0m                      

                       Computation: 107188 steps/s (collection: 0.805s, learning 0.113s)
             Mean action noise std: 2.09
          Mean value_function loss: 93.1757
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.6192
                       Mean reward: 253.65
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 53.1349
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.92s
                      Time elapsed: 00:07:56
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 481/1500 [0m                      

                       Computation: 111078 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 2.09
          Mean value_function loss: 84.7891
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 16.6191
                       Mean reward: 256.40
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.3545
    Episode_Reward/rotating_object: 53.2606
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.88s
                      Time elapsed: 00:07:57
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 482/1500 [0m                      

                       Computation: 109364 steps/s (collection: 0.806s, learning 0.093s)
             Mean action noise std: 2.09
          Mean value_function loss: 83.4023
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.6196
                       Mean reward: 264.99
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.3407
    Episode_Reward/rotating_object: 54.5259
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.90s
                      Time elapsed: 00:07:58
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 483/1500 [0m                      

                       Computation: 111801 steps/s (collection: 0.774s, learning 0.105s)
             Mean action noise std: 2.09
          Mean value_function loss: 72.8675
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.6304
                       Mean reward: 289.43
               Mean episode length: 240.12
    Episode_Reward/reaching_object: 0.3530
    Episode_Reward/rotating_object: 56.2684
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.88s
                      Time elapsed: 00:07:59
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 484/1500 [0m                      

                       Computation: 99403 steps/s (collection: 0.859s, learning 0.130s)
             Mean action noise std: 2.09
          Mean value_function loss: 80.5481
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6371
                       Mean reward: 293.74
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.3459
    Episode_Reward/rotating_object: 55.1726
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.99s
                      Time elapsed: 00:08:00
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 485/1500 [0m                      

                       Computation: 109921 steps/s (collection: 0.783s, learning 0.111s)
             Mean action noise std: 2.10
          Mean value_function loss: 83.6699
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.6403
                       Mean reward: 296.84
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.3426
    Episode_Reward/rotating_object: 55.0470
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.89s
                      Time elapsed: 00:08:01
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 486/1500 [0m                      

                       Computation: 107072 steps/s (collection: 0.789s, learning 0.130s)
             Mean action noise std: 2.10
          Mean value_function loss: 89.1906
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6432
                       Mean reward: 273.33
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.3471
    Episode_Reward/rotating_object: 56.6893
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.92s
                      Time elapsed: 00:08:02
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 487/1500 [0m                      

                       Computation: 107352 steps/s (collection: 0.775s, learning 0.141s)
             Mean action noise std: 2.10
          Mean value_function loss: 85.9174
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.6406
                       Mean reward: 290.49
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.3371
    Episode_Reward/rotating_object: 53.9935
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.92s
                      Time elapsed: 00:08:02
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 488/1500 [0m                      

                       Computation: 108564 steps/s (collection: 0.795s, learning 0.110s)
             Mean action noise std: 2.10
          Mean value_function loss: 80.1227
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.6406
                       Mean reward: 280.58
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.3420
    Episode_Reward/rotating_object: 57.3025
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.91s
                      Time elapsed: 00:08:03
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 489/1500 [0m                      

                       Computation: 108796 steps/s (collection: 0.781s, learning 0.123s)
             Mean action noise std: 2.10
          Mean value_function loss: 86.0408
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6481
                       Mean reward: 287.50
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3430
    Episode_Reward/rotating_object: 57.3581
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.90s
                      Time elapsed: 00:08:04
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 490/1500 [0m                      

                       Computation: 107998 steps/s (collection: 0.805s, learning 0.106s)
             Mean action noise std: 2.10
          Mean value_function loss: 83.9393
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.6599
                       Mean reward: 270.58
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.3382
    Episode_Reward/rotating_object: 53.4506
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.91s
                      Time elapsed: 00:08:05
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 491/1500 [0m                      

                       Computation: 103156 steps/s (collection: 0.836s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 84.0011
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.6665
                       Mean reward: 277.58
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.3491
    Episode_Reward/rotating_object: 54.6454
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.95s
                      Time elapsed: 00:08:06
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 492/1500 [0m                      

                       Computation: 104920 steps/s (collection: 0.821s, learning 0.116s)
             Mean action noise std: 2.11
          Mean value_function loss: 84.9949
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.6664
                       Mean reward: 282.56
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.3447
    Episode_Reward/rotating_object: 56.0972
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.94s
                      Time elapsed: 00:08:07
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 493/1500 [0m                      

                       Computation: 98744 steps/s (collection: 0.842s, learning 0.153s)
             Mean action noise std: 2.11
          Mean value_function loss: 86.1198
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6731
                       Mean reward: 265.67
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.3388
    Episode_Reward/rotating_object: 51.8938
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 1.00s
                      Time elapsed: 00:08:08
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 494/1500 [0m                      

                       Computation: 106969 steps/s (collection: 0.825s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 90.3982
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.6899
                       Mean reward: 299.01
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.3448
    Episode_Reward/rotating_object: 54.9409
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.92s
                      Time elapsed: 00:08:09
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 495/1500 [0m                      

                       Computation: 101245 steps/s (collection: 0.851s, learning 0.120s)
             Mean action noise std: 2.12
          Mean value_function loss: 82.3426
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.6957
                       Mean reward: 300.60
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 0.3439
    Episode_Reward/rotating_object: 56.9447
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.97s
                      Time elapsed: 00:08:10
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 496/1500 [0m                      

                       Computation: 99609 steps/s (collection: 0.855s, learning 0.132s)
             Mean action noise std: 2.12
          Mean value_function loss: 87.4337
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7037
                       Mean reward: 306.59
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.3426
    Episode_Reward/rotating_object: 54.5876
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.99s
                      Time elapsed: 00:08:11
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 497/1500 [0m                      

                       Computation: 101555 steps/s (collection: 0.839s, learning 0.129s)
             Mean action noise std: 2.12
          Mean value_function loss: 82.9797
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7051
                       Mean reward: 252.86
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.3526
    Episode_Reward/rotating_object: 54.5944
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.97s
                      Time elapsed: 00:08:12
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 498/1500 [0m                      

                       Computation: 108898 steps/s (collection: 0.799s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 95.7581
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.7031
                       Mean reward: 275.88
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.3477
    Episode_Reward/rotating_object: 50.1273
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.90s
                      Time elapsed: 00:08:13
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 499/1500 [0m                      

                       Computation: 107138 steps/s (collection: 0.793s, learning 0.124s)
             Mean action noise std: 2.12
          Mean value_function loss: 92.4225
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.7077
                       Mean reward: 295.33
               Mean episode length: 236.85
    Episode_Reward/reaching_object: 0.3533
    Episode_Reward/rotating_object: 57.7153
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.92s
                      Time elapsed: 00:08:14
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 500/1500 [0m                      

                       Computation: 103163 steps/s (collection: 0.805s, learning 0.148s)
             Mean action noise std: 2.12
          Mean value_function loss: 100.3159
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 16.7146
                       Mean reward: 287.73
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.3577
    Episode_Reward/rotating_object: 56.3803
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.95s
                      Time elapsed: 00:08:15
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 501/1500 [0m                      

                       Computation: 106906 steps/s (collection: 0.806s, learning 0.114s)
             Mean action noise std: 2.12
          Mean value_function loss: 90.6484
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.7149
                       Mean reward: 261.59
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 0.3426
    Episode_Reward/rotating_object: 54.8123
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.92s
                      Time elapsed: 00:08:16
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 502/1500 [0m                      

                       Computation: 104593 steps/s (collection: 0.804s, learning 0.136s)
             Mean action noise std: 2.12
          Mean value_function loss: 96.5455
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7180
                       Mean reward: 305.59
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.3563
    Episode_Reward/rotating_object: 56.3368
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.94s
                      Time elapsed: 00:08:17
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 503/1500 [0m                      

                       Computation: 105823 steps/s (collection: 0.801s, learning 0.128s)
             Mean action noise std: 2.13
          Mean value_function loss: 104.5528
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7268
                       Mean reward: 295.28
               Mean episode length: 232.83
    Episode_Reward/reaching_object: 0.3472
    Episode_Reward/rotating_object: 57.6760
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.93s
                      Time elapsed: 00:08:17
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 504/1500 [0m                      

                       Computation: 100237 steps/s (collection: 0.835s, learning 0.146s)
             Mean action noise std: 2.13
          Mean value_function loss: 100.0525
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.7280
                       Mean reward: 274.88
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.3620
    Episode_Reward/rotating_object: 60.1462
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.98s
                      Time elapsed: 00:08:18
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 505/1500 [0m                      

                       Computation: 113415 steps/s (collection: 0.772s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 97.5143
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7249
                       Mean reward: 272.14
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.3513
    Episode_Reward/rotating_object: 55.8963
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.87s
                      Time elapsed: 00:08:19
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 506/1500 [0m                      

                       Computation: 104423 steps/s (collection: 0.834s, learning 0.107s)
             Mean action noise std: 2.13
          Mean value_function loss: 96.4164
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7211
                       Mean reward: 271.00
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 0.3577
    Episode_Reward/rotating_object: 58.2258
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.94s
                      Time elapsed: 00:08:20
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 507/1500 [0m                      

                       Computation: 110768 steps/s (collection: 0.790s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 87.5986
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7212
                       Mean reward: 270.71
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 0.3626
    Episode_Reward/rotating_object: 58.6724
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.89s
                      Time elapsed: 00:08:21
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 508/1500 [0m                      

                       Computation: 108880 steps/s (collection: 0.805s, learning 0.098s)
             Mean action noise std: 2.13
          Mean value_function loss: 91.9756
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.7287
                       Mean reward: 304.78
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 0.3602
    Episode_Reward/rotating_object: 57.2136
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.90s
                      Time elapsed: 00:08:22
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 509/1500 [0m                      

                       Computation: 108693 steps/s (collection: 0.794s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 98.9696
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.7273
                       Mean reward: 307.94
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.3604
    Episode_Reward/rotating_object: 63.5122
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.90s
                      Time elapsed: 00:08:23
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 510/1500 [0m                      

                       Computation: 106755 steps/s (collection: 0.796s, learning 0.125s)
             Mean action noise std: 2.13
          Mean value_function loss: 94.6117
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 16.7248
                       Mean reward: 259.21
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 54.8892
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.92s
                      Time elapsed: 00:08:24
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 511/1500 [0m                      

                       Computation: 109146 steps/s (collection: 0.788s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 85.1482
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7268
                       Mean reward: 285.24
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3564
    Episode_Reward/rotating_object: 58.5188
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.90s
                      Time elapsed: 00:08:25
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 512/1500 [0m                      

                       Computation: 108533 steps/s (collection: 0.807s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 93.3177
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7294
                       Mean reward: 321.71
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 0.3541
    Episode_Reward/rotating_object: 60.5678
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.91s
                      Time elapsed: 00:08:26
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 513/1500 [0m                      

                       Computation: 105943 steps/s (collection: 0.789s, learning 0.139s)
             Mean action noise std: 2.13
          Mean value_function loss: 100.1645
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.7213
                       Mean reward: 273.52
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.3603
    Episode_Reward/rotating_object: 57.1456
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.93s
                      Time elapsed: 00:08:27
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 514/1500 [0m                      

                       Computation: 109985 steps/s (collection: 0.788s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 93.9306
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.7242
                       Mean reward: 341.57
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.3574
    Episode_Reward/rotating_object: 57.2709
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.89s
                      Time elapsed: 00:08:27
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 515/1500 [0m                      

                       Computation: 101671 steps/s (collection: 0.802s, learning 0.165s)
             Mean action noise std: 2.13
          Mean value_function loss: 97.5215
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.7276
                       Mean reward: 286.12
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 0.3521
    Episode_Reward/rotating_object: 55.5901
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.97s
                      Time elapsed: 00:08:28
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 516/1500 [0m                      

                       Computation: 104773 steps/s (collection: 0.804s, learning 0.135s)
             Mean action noise std: 2.13
          Mean value_function loss: 97.5758
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.7285
                       Mean reward: 274.35
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.3617
    Episode_Reward/rotating_object: 58.7393
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.94s
                      Time elapsed: 00:08:29
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 517/1500 [0m                      

                       Computation: 107709 steps/s (collection: 0.792s, learning 0.121s)
             Mean action noise std: 2.13
          Mean value_function loss: 97.0841
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.7279
                       Mean reward: 303.39
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 0.3472
    Episode_Reward/rotating_object: 58.2648
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.91s
                      Time elapsed: 00:08:30
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 518/1500 [0m                      

                       Computation: 110297 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 86.7881
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.7327
                       Mean reward: 318.63
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 0.3540
    Episode_Reward/rotating_object: 61.1041
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.89s
                      Time elapsed: 00:08:31
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 519/1500 [0m                      

                       Computation: 111240 steps/s (collection: 0.786s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 86.9916
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7357
                       Mean reward: 287.79
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.3596
    Episode_Reward/rotating_object: 63.2248
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.88s
                      Time elapsed: 00:08:32
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 520/1500 [0m                      

                       Computation: 109012 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 2.14
          Mean value_function loss: 89.6750
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.7326
                       Mean reward: 320.95
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3623
    Episode_Reward/rotating_object: 60.3014
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.90s
                      Time elapsed: 00:08:33
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 521/1500 [0m                      

                       Computation: 109684 steps/s (collection: 0.784s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 94.4028
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.7327
                       Mean reward: 334.56
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 0.3539
    Episode_Reward/rotating_object: 58.0594
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.90s
                      Time elapsed: 00:08:34
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 522/1500 [0m                      

                       Computation: 105575 steps/s (collection: 0.818s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 91.1867
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.7428
                       Mean reward: 317.41
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3519
    Episode_Reward/rotating_object: 61.6037
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.93s
                      Time elapsed: 00:08:35
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 523/1500 [0m                      

                       Computation: 108232 steps/s (collection: 0.809s, learning 0.100s)
             Mean action noise std: 2.14
          Mean value_function loss: 89.7904
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.7560
                       Mean reward: 331.71
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 0.3484
    Episode_Reward/rotating_object: 60.4606
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.91s
                      Time elapsed: 00:08:36
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 524/1500 [0m                      

                       Computation: 113815 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 88.1562
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.7672
                       Mean reward: 311.28
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 0.3541
    Episode_Reward/rotating_object: 61.4467
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.86s
                      Time elapsed: 00:08:37
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 525/1500 [0m                      

                       Computation: 99575 steps/s (collection: 0.822s, learning 0.166s)
             Mean action noise std: 2.15
          Mean value_function loss: 85.3335
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7802
                       Mean reward: 288.71
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 0.3518
    Episode_Reward/rotating_object: 58.5104
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.99s
                      Time elapsed: 00:08:38
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 526/1500 [0m                      

                       Computation: 106011 steps/s (collection: 0.768s, learning 0.159s)
             Mean action noise std: 2.15
          Mean value_function loss: 97.1996
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7863
                       Mean reward: 294.35
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.3489
    Episode_Reward/rotating_object: 58.7608
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.93s
                      Time elapsed: 00:08:39
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 527/1500 [0m                      

                       Computation: 107971 steps/s (collection: 0.774s, learning 0.136s)
             Mean action noise std: 2.15
          Mean value_function loss: 94.4994
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7891
                       Mean reward: 271.01
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.3470
    Episode_Reward/rotating_object: 56.2371
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.91s
                      Time elapsed: 00:08:39
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 528/1500 [0m                      

                       Computation: 108839 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 101.6296
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.7948
                       Mean reward: 295.77
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 0.3567
    Episode_Reward/rotating_object: 58.6306
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.90s
                      Time elapsed: 00:08:40
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 529/1500 [0m                      

                       Computation: 108138 steps/s (collection: 0.810s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 102.1069
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.8019
                       Mean reward: 322.62
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.3526
    Episode_Reward/rotating_object: 59.7367
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.91s
                      Time elapsed: 00:08:41
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 530/1500 [0m                      

                       Computation: 109195 steps/s (collection: 0.803s, learning 0.098s)
             Mean action noise std: 2.15
          Mean value_function loss: 95.9641
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.7957
                       Mean reward: 236.19
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.3506
    Episode_Reward/rotating_object: 58.5026
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.90s
                      Time elapsed: 00:08:42
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 531/1500 [0m                      

                       Computation: 112163 steps/s (collection: 0.784s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 95.4209
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.7979
                       Mean reward: 318.62
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.3578
    Episode_Reward/rotating_object: 61.4547
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.88s
                      Time elapsed: 00:08:43
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 532/1500 [0m                      

                       Computation: 109621 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 97.8270
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.8005
                       Mean reward: 327.70
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.3508
    Episode_Reward/rotating_object: 60.7371
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.90s
                      Time elapsed: 00:08:44
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 533/1500 [0m                      

                       Computation: 108750 steps/s (collection: 0.808s, learning 0.096s)
             Mean action noise std: 2.16
          Mean value_function loss: 98.5184
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7977
                       Mean reward: 288.72
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.3570
    Episode_Reward/rotating_object: 60.5071
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.90s
                      Time elapsed: 00:08:45
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 534/1500 [0m                      

                       Computation: 110350 steps/s (collection: 0.791s, learning 0.100s)
             Mean action noise std: 2.16
          Mean value_function loss: 96.6651
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.8017
                       Mean reward: 313.64
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.3436
    Episode_Reward/rotating_object: 60.2889
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.89s
                      Time elapsed: 00:08:46
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 535/1500 [0m                      

                       Computation: 101888 steps/s (collection: 0.813s, learning 0.151s)
             Mean action noise std: 2.16
          Mean value_function loss: 98.2294
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.8122
                       Mean reward: 330.21
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 0.3569
    Episode_Reward/rotating_object: 64.4749
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.96s
                      Time elapsed: 00:08:47
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 536/1500 [0m                      

                       Computation: 110799 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 2.16
          Mean value_function loss: 91.5861
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8184
                       Mean reward: 300.65
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.3524
    Episode_Reward/rotating_object: 60.5926
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.89s
                      Time elapsed: 00:08:48
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 537/1500 [0m                      

                       Computation: 103180 steps/s (collection: 0.781s, learning 0.172s)
             Mean action noise std: 2.16
          Mean value_function loss: 101.2891
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8300
                       Mean reward: 317.87
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.3496
    Episode_Reward/rotating_object: 61.1957
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.95s
                      Time elapsed: 00:08:49
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 538/1500 [0m                      

                       Computation: 109118 steps/s (collection: 0.802s, learning 0.099s)
             Mean action noise std: 2.17
          Mean value_function loss: 105.3792
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.8316
                       Mean reward: 305.37
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.3518
    Episode_Reward/rotating_object: 57.8882
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.90s
                      Time elapsed: 00:08:49
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 539/1500 [0m                      

                       Computation: 109255 steps/s (collection: 0.779s, learning 0.120s)
             Mean action noise std: 2.17
          Mean value_function loss: 102.5700
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.8366
                       Mean reward: 314.86
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3547
    Episode_Reward/rotating_object: 61.5731
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.90s
                      Time elapsed: 00:08:50
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 540/1500 [0m                      

                       Computation: 108480 steps/s (collection: 0.796s, learning 0.110s)
             Mean action noise std: 2.17
          Mean value_function loss: 97.9680
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.8463
                       Mean reward: 287.40
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.3572
    Episode_Reward/rotating_object: 60.0881
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.91s
                      Time elapsed: 00:08:51
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 541/1500 [0m                      

                       Computation: 107233 steps/s (collection: 0.809s, learning 0.108s)
             Mean action noise std: 2.17
          Mean value_function loss: 102.2997
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.8626
                       Mean reward: 308.54
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.3495
    Episode_Reward/rotating_object: 55.8706
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.92s
                      Time elapsed: 00:08:52
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 542/1500 [0m                      

                       Computation: 108475 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 105.7924
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.8748
                       Mean reward: 281.00
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 60.3385
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.91s
                      Time elapsed: 00:08:53
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 543/1500 [0m                      

                       Computation: 108557 steps/s (collection: 0.810s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 104.4976
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 16.8797
                       Mean reward: 237.24
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.3521
    Episode_Reward/rotating_object: 59.6763
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.91s
                      Time elapsed: 00:08:54
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 544/1500 [0m                      

                       Computation: 102942 steps/s (collection: 0.831s, learning 0.124s)
             Mean action noise std: 2.18
          Mean value_function loss: 105.6762
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.8787
                       Mean reward: 284.31
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 0.3548
    Episode_Reward/rotating_object: 59.3206
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.95s
                      Time elapsed: 00:08:55
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 545/1500 [0m                      

                       Computation: 106253 steps/s (collection: 0.819s, learning 0.106s)
             Mean action noise std: 2.18
          Mean value_function loss: 97.6304
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.8780
                       Mean reward: 344.69
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 0.3476
    Episode_Reward/rotating_object: 60.9253
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.93s
                      Time elapsed: 00:08:56
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 546/1500 [0m                      

                       Computation: 95404 steps/s (collection: 0.866s, learning 0.165s)
             Mean action noise std: 2.18
          Mean value_function loss: 94.0242
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.8854
                       Mean reward: 309.14
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.3624
    Episode_Reward/rotating_object: 59.9543
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 1.03s
                      Time elapsed: 00:08:57
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 547/1500 [0m                      

                       Computation: 96764 steps/s (collection: 0.896s, learning 0.120s)
             Mean action noise std: 2.18
          Mean value_function loss: 96.0592
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.8901
                       Mean reward: 274.24
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 0.3578
    Episode_Reward/rotating_object: 60.7559
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 1.02s
                      Time elapsed: 00:08:58
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 548/1500 [0m                      

                       Computation: 104695 steps/s (collection: 0.845s, learning 0.094s)
             Mean action noise std: 2.19
          Mean value_function loss: 107.5672
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.8894
                       Mean reward: 268.22
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.3538
    Episode_Reward/rotating_object: 59.1944
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.94s
                      Time elapsed: 00:08:59
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 549/1500 [0m                      

                       Computation: 108300 steps/s (collection: 0.801s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 90.0531
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.8908
                       Mean reward: 357.41
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 0.3531
    Episode_Reward/rotating_object: 61.0645
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.91s
                      Time elapsed: 00:09:00
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 550/1500 [0m                      

                       Computation: 106322 steps/s (collection: 0.820s, learning 0.104s)
             Mean action noise std: 2.19
          Mean value_function loss: 99.0457
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.8955
                       Mean reward: 300.11
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3588
    Episode_Reward/rotating_object: 61.5418
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.92s
                      Time elapsed: 00:09:01
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 551/1500 [0m                      

                       Computation: 105959 steps/s (collection: 0.793s, learning 0.135s)
             Mean action noise std: 2.19
          Mean value_function loss: 96.0349
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8992
                       Mean reward: 288.88
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.3544
    Episode_Reward/rotating_object: 63.2012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.93s
                      Time elapsed: 00:09:02
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 552/1500 [0m                      

                       Computation: 97043 steps/s (collection: 0.904s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 98.1136
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.9092
                       Mean reward: 310.44
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.3628
    Episode_Reward/rotating_object: 61.9013
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 1.01s
                      Time elapsed: 00:09:03
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 553/1500 [0m                      

                       Computation: 100555 steps/s (collection: 0.808s, learning 0.170s)
             Mean action noise std: 2.20
          Mean value_function loss: 99.7932
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.9218
                       Mean reward: 339.32
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.3548
    Episode_Reward/rotating_object: 66.8085
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.98s
                      Time elapsed: 00:09:04
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 554/1500 [0m                      

                       Computation: 101045 steps/s (collection: 0.819s, learning 0.154s)
             Mean action noise std: 2.20
          Mean value_function loss: 92.8689
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9281
                       Mean reward: 324.41
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.3498
    Episode_Reward/rotating_object: 63.3782
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.97s
                      Time elapsed: 00:09:05
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 555/1500 [0m                      

                       Computation: 100050 steps/s (collection: 0.802s, learning 0.181s)
             Mean action noise std: 2.20
          Mean value_function loss: 97.7849
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.9310
                       Mean reward: 263.35
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.3546
    Episode_Reward/rotating_object: 60.4594
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.98s
                      Time elapsed: 00:09:06
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 556/1500 [0m                      

                       Computation: 102925 steps/s (collection: 0.820s, learning 0.135s)
             Mean action noise std: 2.20
          Mean value_function loss: 96.9949
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9301
                       Mean reward: 345.14
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.3557
    Episode_Reward/rotating_object: 65.7814
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.96s
                      Time elapsed: 00:09:06
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 557/1500 [0m                      

                       Computation: 98071 steps/s (collection: 0.819s, learning 0.183s)
             Mean action noise std: 2.20
          Mean value_function loss: 90.7137
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.9277
                       Mean reward: 327.57
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.3517
    Episode_Reward/rotating_object: 62.8209
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 1.00s
                      Time elapsed: 00:09:07
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 558/1500 [0m                      

                       Computation: 97502 steps/s (collection: 0.851s, learning 0.157s)
             Mean action noise std: 2.20
          Mean value_function loss: 99.0264
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.9374
                       Mean reward: 329.03
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.3591
    Episode_Reward/rotating_object: 62.3473
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 1.01s
                      Time elapsed: 00:09:08
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 559/1500 [0m                      

                       Computation: 98694 steps/s (collection: 0.832s, learning 0.164s)
             Mean action noise std: 2.21
          Mean value_function loss: 113.4385
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 16.9492
                       Mean reward: 308.69
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.3575
    Episode_Reward/rotating_object: 59.6277
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 1.00s
                      Time elapsed: 00:09:09
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 560/1500 [0m                      

                       Computation: 102707 steps/s (collection: 0.821s, learning 0.137s)
             Mean action noise std: 2.21
          Mean value_function loss: 124.9807
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9547
                       Mean reward: 334.30
               Mean episode length: 241.99
    Episode_Reward/reaching_object: 0.3638
    Episode_Reward/rotating_object: 62.0628
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.96s
                      Time elapsed: 00:09:10
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 561/1500 [0m                      

                       Computation: 103601 steps/s (collection: 0.839s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 114.3443
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.9600
                       Mean reward: 298.40
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3597
    Episode_Reward/rotating_object: 60.3604
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.95s
                      Time elapsed: 00:09:11
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 562/1500 [0m                      

                       Computation: 98297 steps/s (collection: 0.830s, learning 0.170s)
             Mean action noise std: 2.21
          Mean value_function loss: 109.6389
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.9697
                       Mean reward: 295.72
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 0.3628
    Episode_Reward/rotating_object: 59.0329
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 1.00s
                      Time elapsed: 00:09:12
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 563/1500 [0m                      

                       Computation: 101959 steps/s (collection: 0.824s, learning 0.140s)
             Mean action noise std: 2.22
          Mean value_function loss: 108.5656
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.9748
                       Mean reward: 296.21
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 0.3655
    Episode_Reward/rotating_object: 62.4642
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.96s
                      Time elapsed: 00:09:13
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 564/1500 [0m                      

                       Computation: 94775 steps/s (collection: 0.858s, learning 0.180s)
             Mean action noise std: 2.22
          Mean value_function loss: 107.4078
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.9868
                       Mean reward: 318.32
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.3660
    Episode_Reward/rotating_object: 64.8532
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 1.04s
                      Time elapsed: 00:09:14
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 565/1500 [0m                      

                       Computation: 101620 steps/s (collection: 0.829s, learning 0.138s)
             Mean action noise std: 2.22
          Mean value_function loss: 98.1092
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.9972
                       Mean reward: 321.57
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 0.3622
    Episode_Reward/rotating_object: 63.9188
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.97s
                      Time elapsed: 00:09:15
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 566/1500 [0m                      

                       Computation: 106356 steps/s (collection: 0.822s, learning 0.102s)
             Mean action noise std: 2.22
          Mean value_function loss: 109.6467
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.0023
                       Mean reward: 335.61
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 0.3608
    Episode_Reward/rotating_object: 67.2529
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.92s
                      Time elapsed: 00:09:16
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 567/1500 [0m                      

                       Computation: 96107 steps/s (collection: 0.845s, learning 0.178s)
             Mean action noise std: 2.22
          Mean value_function loss: 100.4272
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.0026
                       Mean reward: 293.40
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 0.3608
    Episode_Reward/rotating_object: 60.3758
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 1.02s
                      Time elapsed: 00:09:17
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 568/1500 [0m                      

                       Computation: 101650 steps/s (collection: 0.859s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 119.6403
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.9940
                       Mean reward: 329.12
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 0.3644
    Episode_Reward/rotating_object: 64.9483
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.97s
                      Time elapsed: 00:09:18
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 569/1500 [0m                      

                       Computation: 103611 steps/s (collection: 0.847s, learning 0.102s)
             Mean action noise std: 2.23
          Mean value_function loss: 106.6237
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.0030
                       Mean reward: 280.85
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.3663
    Episode_Reward/rotating_object: 62.8451
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.95s
                      Time elapsed: 00:09:19
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 570/1500 [0m                      

                       Computation: 105760 steps/s (collection: 0.834s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 116.0454
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.0086
                       Mean reward: 351.61
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 0.3722
    Episode_Reward/rotating_object: 65.3462
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.93s
                      Time elapsed: 00:09:20
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 571/1500 [0m                      

                       Computation: 100522 steps/s (collection: 0.861s, learning 0.117s)
             Mean action noise std: 2.23
          Mean value_function loss: 119.8138
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 17.0138
                       Mean reward: 322.17
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.3725
    Episode_Reward/rotating_object: 66.4218
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.98s
                      Time elapsed: 00:09:21
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 572/1500 [0m                      

                       Computation: 109483 steps/s (collection: 0.798s, learning 0.100s)
             Mean action noise std: 2.23
          Mean value_function loss: 116.5964
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0177
                       Mean reward: 354.27
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.3714
    Episode_Reward/rotating_object: 60.8166
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.90s
                      Time elapsed: 00:09:22
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 573/1500 [0m                      

                       Computation: 104295 steps/s (collection: 0.832s, learning 0.111s)
             Mean action noise std: 2.23
          Mean value_function loss: 112.1745
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.0227
                       Mean reward: 328.61
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.3753
    Episode_Reward/rotating_object: 64.2599
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.94s
                      Time elapsed: 00:09:23
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 574/1500 [0m                      

                       Computation: 104088 steps/s (collection: 0.828s, learning 0.116s)
             Mean action noise std: 2.24
          Mean value_function loss: 113.3015
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 17.0319
                       Mean reward: 371.10
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.3677
    Episode_Reward/rotating_object: 60.4240
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.94s
                      Time elapsed: 00:09:24
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 575/1500 [0m                      

                       Computation: 107266 steps/s (collection: 0.816s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 111.3833
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.0440
                       Mean reward: 333.06
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 0.3627
    Episode_Reward/rotating_object: 62.6396
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.92s
                      Time elapsed: 00:09:25
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 576/1500 [0m                      

                       Computation: 101746 steps/s (collection: 0.842s, learning 0.124s)
             Mean action noise std: 2.24
          Mean value_function loss: 93.5494
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 17.0555
                       Mean reward: 313.58
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 0.3667
    Episode_Reward/rotating_object: 62.6932
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.97s
                      Time elapsed: 00:09:26
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 577/1500 [0m                      

                       Computation: 104419 steps/s (collection: 0.841s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 106.4288
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.0530
                       Mean reward: 331.58
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 0.3633
    Episode_Reward/rotating_object: 63.6702
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.94s
                      Time elapsed: 00:09:27
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 578/1500 [0m                      

                       Computation: 104944 steps/s (collection: 0.837s, learning 0.100s)
             Mean action noise std: 2.25
          Mean value_function loss: 105.6205
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0556
                       Mean reward: 339.87
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 0.3647
    Episode_Reward/rotating_object: 64.3312
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.94s
                      Time elapsed: 00:09:28
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 579/1500 [0m                      

                       Computation: 104752 steps/s (collection: 0.823s, learning 0.116s)
             Mean action noise std: 2.25
          Mean value_function loss: 95.1617
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.0683
                       Mean reward: 346.65
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 0.3739
    Episode_Reward/rotating_object: 66.6462
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.94s
                      Time elapsed: 00:09:29
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 580/1500 [0m                      

                       Computation: 101587 steps/s (collection: 0.826s, learning 0.142s)
             Mean action noise std: 2.25
          Mean value_function loss: 104.5272
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.0776
                       Mean reward: 297.02
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 0.3697
    Episode_Reward/rotating_object: 66.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.97s
                      Time elapsed: 00:09:30
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 581/1500 [0m                      

                       Computation: 98675 steps/s (collection: 0.877s, learning 0.120s)
             Mean action noise std: 2.25
          Mean value_function loss: 106.1884
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.0775
                       Mean reward: 355.93
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.3581
    Episode_Reward/rotating_object: 64.5352
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 1.00s
                      Time elapsed: 00:09:31
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 582/1500 [0m                      

                       Computation: 104336 steps/s (collection: 0.791s, learning 0.152s)
             Mean action noise std: 2.26
          Mean value_function loss: 111.6779
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.0830
                       Mean reward: 313.15
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.3661
    Episode_Reward/rotating_object: 61.8083
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.94s
                      Time elapsed: 00:09:32
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 583/1500 [0m                      

                       Computation: 102803 steps/s (collection: 0.823s, learning 0.133s)
             Mean action noise std: 2.26
          Mean value_function loss: 102.2632
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.0828
                       Mean reward: 300.25
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 0.3514
    Episode_Reward/rotating_object: 61.3906
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.96s
                      Time elapsed: 00:09:32
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 584/1500 [0m                      

                       Computation: 102017 steps/s (collection: 0.826s, learning 0.138s)
             Mean action noise std: 2.26
          Mean value_function loss: 104.0613
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.0844
                       Mean reward: 325.56
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.3610
    Episode_Reward/rotating_object: 64.6943
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.96s
                      Time elapsed: 00:09:33
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 585/1500 [0m                      

                       Computation: 96329 steps/s (collection: 0.839s, learning 0.181s)
             Mean action noise std: 2.26
          Mean value_function loss: 110.4827
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.0936
                       Mean reward: 338.58
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.3692
    Episode_Reward/rotating_object: 67.7588
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 1.02s
                      Time elapsed: 00:09:34
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 586/1500 [0m                      

                       Computation: 98180 steps/s (collection: 0.834s, learning 0.167s)
             Mean action noise std: 2.26
          Mean value_function loss: 125.9191
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.0989
                       Mean reward: 324.82
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.3764
    Episode_Reward/rotating_object: 64.1989
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.00s
                      Time elapsed: 00:09:35
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 587/1500 [0m                      

                       Computation: 98818 steps/s (collection: 0.845s, learning 0.150s)
             Mean action noise std: 2.27
          Mean value_function loss: 116.0564
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.1018
                       Mean reward: 340.47
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 62.1801
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.99s
                      Time elapsed: 00:09:36
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 588/1500 [0m                      

                       Computation: 98869 steps/s (collection: 0.855s, learning 0.139s)
             Mean action noise std: 2.27
          Mean value_function loss: 102.2872
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1093
                       Mean reward: 272.69
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.3684
    Episode_Reward/rotating_object: 59.1898
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.99s
                      Time elapsed: 00:09:37
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 589/1500 [0m                      

                       Computation: 97360 steps/s (collection: 0.809s, learning 0.201s)
             Mean action noise std: 2.27
          Mean value_function loss: 115.8335
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1141
                       Mean reward: 308.72
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 0.3683
    Episode_Reward/rotating_object: 65.3021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 1.01s
                      Time elapsed: 00:09:38
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 590/1500 [0m                      

                       Computation: 99261 steps/s (collection: 0.836s, learning 0.154s)
             Mean action noise std: 2.27
          Mean value_function loss: 113.3685
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1088
                       Mean reward: 308.68
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.3603
    Episode_Reward/rotating_object: 60.5079
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.99s
                      Time elapsed: 00:09:39
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 591/1500 [0m                      

                       Computation: 104096 steps/s (collection: 0.794s, learning 0.151s)
             Mean action noise std: 2.27
          Mean value_function loss: 110.5206
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.1042
                       Mean reward: 296.94
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 62.3884
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.94s
                      Time elapsed: 00:09:40
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 592/1500 [0m                      

                       Computation: 103124 steps/s (collection: 0.819s, learning 0.135s)
             Mean action noise std: 2.27
          Mean value_function loss: 126.3173
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.1016
                       Mean reward: 371.27
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 64.5862
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.95s
                      Time elapsed: 00:09:41
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 593/1500 [0m                      

                       Computation: 98307 steps/s (collection: 0.861s, learning 0.139s)
             Mean action noise std: 2.27
          Mean value_function loss: 113.9157
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.0965
                       Mean reward: 314.12
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.3670
    Episode_Reward/rotating_object: 60.2715
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 1.00s
                      Time elapsed: 00:09:42
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 594/1500 [0m                      

                       Computation: 102960 steps/s (collection: 0.837s, learning 0.118s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.9443
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.0936
                       Mean reward: 352.08
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 68.5865
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.95s
                      Time elapsed: 00:09:43
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 595/1500 [0m                      

                       Computation: 104211 steps/s (collection: 0.839s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 99.7656
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.0998
                       Mean reward: 316.99
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 67.5574
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.94s
                      Time elapsed: 00:09:44
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 596/1500 [0m                      

                       Computation: 105674 steps/s (collection: 0.837s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 101.7848
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1024
                       Mean reward: 308.48
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 0.3747
    Episode_Reward/rotating_object: 66.8032
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.93s
                      Time elapsed: 00:09:45
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 597/1500 [0m                      

                       Computation: 107667 steps/s (collection: 0.820s, learning 0.093s)
             Mean action noise std: 2.28
          Mean value_function loss: 100.7874
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.1150
                       Mean reward: 348.31
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.3685
    Episode_Reward/rotating_object: 66.4800
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.91s
                      Time elapsed: 00:09:46
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 598/1500 [0m                      

                       Computation: 109332 steps/s (collection: 0.804s, learning 0.096s)
             Mean action noise std: 2.28
          Mean value_function loss: 107.6751
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1194
                       Mean reward: 314.66
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 0.3664
    Episode_Reward/rotating_object: 64.5174
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.90s
                      Time elapsed: 00:09:47
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 599/1500 [0m                      

                       Computation: 100921 steps/s (collection: 0.867s, learning 0.107s)
             Mean action noise std: 2.28
          Mean value_function loss: 98.6805
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 17.1249
                       Mean reward: 343.53
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3610
    Episode_Reward/rotating_object: 70.2308
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.97s
                      Time elapsed: 00:09:48
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 600/1500 [0m                      

                       Computation: 93139 steps/s (collection: 0.890s, learning 0.165s)
             Mean action noise std: 2.28
          Mean value_function loss: 113.3574
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.1263
                       Mean reward: 331.67
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.3768
    Episode_Reward/rotating_object: 70.1886
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 1.06s
                      Time elapsed: 00:09:49
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 601/1500 [0m                      

                       Computation: 101018 steps/s (collection: 0.846s, learning 0.128s)
             Mean action noise std: 2.29
          Mean value_function loss: 110.9354
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1264
                       Mean reward: 337.97
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 68.4062
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.97s
                      Time elapsed: 00:09:50
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 602/1500 [0m                      

                       Computation: 104311 steps/s (collection: 0.810s, learning 0.132s)
             Mean action noise std: 2.29
          Mean value_function loss: 120.1408
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1267
                       Mean reward: 342.75
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 68.5604
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.94s
                      Time elapsed: 00:09:51
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 603/1500 [0m                      

                       Computation: 101933 steps/s (collection: 0.846s, learning 0.118s)
             Mean action noise std: 2.29
          Mean value_function loss: 108.4124
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1323
                       Mean reward: 347.93
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.3682
    Episode_Reward/rotating_object: 67.0626
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.96s
                      Time elapsed: 00:09:52
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 604/1500 [0m                      

                       Computation: 102306 steps/s (collection: 0.841s, learning 0.120s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.0863
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 17.1438
                       Mean reward: 354.81
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 0.3730
    Episode_Reward/rotating_object: 70.1113
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.96s
                      Time elapsed: 00:09:53
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 605/1500 [0m                      

                       Computation: 91008 steps/s (collection: 0.935s, learning 0.145s)
             Mean action noise std: 2.29
          Mean value_function loss: 114.6464
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.1498
                       Mean reward: 302.74
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 0.3727
    Episode_Reward/rotating_object: 66.7552
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 1.08s
                      Time elapsed: 00:09:54
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 606/1500 [0m                      

                       Computation: 101083 steps/s (collection: 0.877s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 112.3658
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.1550
                       Mean reward: 366.79
               Mean episode length: 238.68
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 66.7543
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.97s
                      Time elapsed: 00:09:55
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 607/1500 [0m                      

                       Computation: 110338 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 121.7455
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.1531
                       Mean reward: 324.15
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.3751
    Episode_Reward/rotating_object: 64.8415
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.89s
                      Time elapsed: 00:09:56
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 608/1500 [0m                      

                       Computation: 106425 steps/s (collection: 0.826s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 109.7999
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.1591
                       Mean reward: 321.19
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 0.3801
    Episode_Reward/rotating_object: 66.6573
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.92s
                      Time elapsed: 00:09:57
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 609/1500 [0m                      

                       Computation: 104830 steps/s (collection: 0.820s, learning 0.117s)
             Mean action noise std: 2.30
          Mean value_function loss: 113.2984
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.1637
                       Mean reward: 352.11
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3770
    Episode_Reward/rotating_object: 67.1992
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.94s
                      Time elapsed: 00:09:58
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 610/1500 [0m                      

                       Computation: 92164 steps/s (collection: 0.850s, learning 0.216s)
             Mean action noise std: 2.30
          Mean value_function loss: 107.7719
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.1679
                       Mean reward: 345.52
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 0.3630
    Episode_Reward/rotating_object: 68.3087
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 1.07s
                      Time elapsed: 00:09:59
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 611/1500 [0m                      

                       Computation: 98530 steps/s (collection: 0.890s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 103.7615
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.1636
                       Mean reward: 406.29
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3742
    Episode_Reward/rotating_object: 71.5282
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 1.00s
                      Time elapsed: 00:10:00
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 612/1500 [0m                      

                       Computation: 104191 steps/s (collection: 0.799s, learning 0.144s)
             Mean action noise std: 2.30
          Mean value_function loss: 112.2891
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.1600
                       Mean reward: 308.62
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.3601
    Episode_Reward/rotating_object: 62.3065
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.94s
                      Time elapsed: 00:10:01
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 613/1500 [0m                      

                       Computation: 103905 steps/s (collection: 0.804s, learning 0.143s)
             Mean action noise std: 2.30
          Mean value_function loss: 97.7383
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1573
                       Mean reward: 394.12
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.3710
    Episode_Reward/rotating_object: 71.5828
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.95s
                      Time elapsed: 00:10:02
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 614/1500 [0m                      

                       Computation: 103446 steps/s (collection: 0.794s, learning 0.156s)
             Mean action noise std: 2.31
          Mean value_function loss: 104.7806
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.1761
                       Mean reward: 301.78
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 0.3620
    Episode_Reward/rotating_object: 64.5616
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.95s
                      Time elapsed: 00:10:03
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 615/1500 [0m                      

                       Computation: 105108 steps/s (collection: 0.826s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 104.6584
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1825
                       Mean reward: 336.70
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.3726
    Episode_Reward/rotating_object: 69.9522
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.94s
                      Time elapsed: 00:10:03
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 616/1500 [0m                      

                       Computation: 107822 steps/s (collection: 0.803s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 109.7176
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.1891
                       Mean reward: 371.03
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 0.3617
    Episode_Reward/rotating_object: 68.0682
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 0.91s
                      Time elapsed: 00:10:04
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 617/1500 [0m                      

                       Computation: 101472 steps/s (collection: 0.817s, learning 0.152s)
             Mean action noise std: 2.31
          Mean value_function loss: 114.2741
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.1906
                       Mean reward: 379.50
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.3646
    Episode_Reward/rotating_object: 70.8835
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.97s
                      Time elapsed: 00:10:05
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 618/1500 [0m                      

                       Computation: 107714 steps/s (collection: 0.810s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 109.1119
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.1982
                       Mean reward: 320.90
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.3585
    Episode_Reward/rotating_object: 70.3821
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.91s
                      Time elapsed: 00:10:06
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 619/1500 [0m                      

                       Computation: 107201 steps/s (collection: 0.824s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 102.2313
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2216
                       Mean reward: 386.23
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.3688
    Episode_Reward/rotating_object: 71.8663
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.92s
                      Time elapsed: 00:10:07
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 620/1500 [0m                      

                       Computation: 103118 steps/s (collection: 0.839s, learning 0.114s)
             Mean action noise std: 2.32
          Mean value_function loss: 108.1314
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.2332
                       Mean reward: 340.31
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.3731
    Episode_Reward/rotating_object: 69.3997
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.95s
                      Time elapsed: 00:10:08
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 621/1500 [0m                      

                       Computation: 105878 steps/s (collection: 0.815s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 113.4811
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.2403
                       Mean reward: 311.24
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 0.3717
    Episode_Reward/rotating_object: 67.8970
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.93s
                      Time elapsed: 00:10:09
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 622/1500 [0m                      

                       Computation: 102100 steps/s (collection: 0.842s, learning 0.121s)
             Mean action noise std: 2.33
          Mean value_function loss: 112.5392
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.2502
                       Mean reward: 315.17
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3617
    Episode_Reward/rotating_object: 67.4393
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.96s
                      Time elapsed: 00:10:10
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 623/1500 [0m                      

                       Computation: 100566 steps/s (collection: 0.844s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 106.3739
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.2560
                       Mean reward: 328.90
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 0.3674
    Episode_Reward/rotating_object: 71.0999
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.98s
                      Time elapsed: 00:10:11
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 624/1500 [0m                      

                       Computation: 89005 steps/s (collection: 0.994s, learning 0.110s)
             Mean action noise std: 2.33
          Mean value_function loss: 102.9222
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2623
                       Mean reward: 331.06
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.3623
    Episode_Reward/rotating_object: 62.5947
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 1.10s
                      Time elapsed: 00:10:12
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 625/1500 [0m                      

                       Computation: 104939 steps/s (collection: 0.818s, learning 0.119s)
             Mean action noise std: 2.34
          Mean value_function loss: 98.5565
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.2785
                       Mean reward: 357.28
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 70.3412
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.94s
                      Time elapsed: 00:10:13
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 626/1500 [0m                      

                       Computation: 91478 steps/s (collection: 0.926s, learning 0.148s)
             Mean action noise std: 2.34
          Mean value_function loss: 106.2252
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2812
                       Mean reward: 385.27
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3651
    Episode_Reward/rotating_object: 66.9252
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 1.07s
                      Time elapsed: 00:10:14
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 627/1500 [0m                      

                       Computation: 105944 steps/s (collection: 0.837s, learning 0.091s)
             Mean action noise std: 2.34
          Mean value_function loss: 99.3509
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2859
                       Mean reward: 363.30
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 0.3675
    Episode_Reward/rotating_object: 67.2867
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.93s
                      Time elapsed: 00:10:15
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 628/1500 [0m                      

                       Computation: 107071 steps/s (collection: 0.813s, learning 0.105s)
             Mean action noise std: 2.34
          Mean value_function loss: 93.0366
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.2865
                       Mean reward: 350.22
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.3642
    Episode_Reward/rotating_object: 64.1003
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.92s
                      Time elapsed: 00:10:16
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 629/1500 [0m                      

                       Computation: 94788 steps/s (collection: 0.871s, learning 0.167s)
             Mean action noise std: 2.34
          Mean value_function loss: 90.6074
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.2936
                       Mean reward: 373.45
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 0.3504
    Episode_Reward/rotating_object: 66.3664
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 1.04s
                      Time elapsed: 00:10:17
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 630/1500 [0m                      

                       Computation: 105189 steps/s (collection: 0.817s, learning 0.118s)
             Mean action noise std: 2.35
          Mean value_function loss: 93.7691
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.3086
                       Mean reward: 367.25
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.3571
    Episode_Reward/rotating_object: 70.9940
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.93s
                      Time elapsed: 00:10:18
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 631/1500 [0m                      

                       Computation: 106800 steps/s (collection: 0.828s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 95.5527
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.3183
                       Mean reward: 413.49
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 0.3642
    Episode_Reward/rotating_object: 71.7362
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.92s
                      Time elapsed: 00:10:19
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 632/1500 [0m                      

                       Computation: 110756 steps/s (collection: 0.794s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 101.4866
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3280
                       Mean reward: 324.44
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.3593
    Episode_Reward/rotating_object: 70.9844
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.89s
                      Time elapsed: 00:10:20
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 633/1500 [0m                      

                       Computation: 106925 steps/s (collection: 0.820s, learning 0.099s)
             Mean action noise std: 2.35
          Mean value_function loss: 99.3757
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.3366
                       Mean reward: 377.17
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 0.3562
    Episode_Reward/rotating_object: 69.0071
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.92s
                      Time elapsed: 00:10:21
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 634/1500 [0m                      

                       Computation: 104366 steps/s (collection: 0.814s, learning 0.128s)
             Mean action noise std: 2.36
          Mean value_function loss: 103.0348
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.3431
                       Mean reward: 321.10
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 0.3609
    Episode_Reward/rotating_object: 68.4036
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.94s
                      Time elapsed: 00:10:22
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 635/1500 [0m                      

                       Computation: 104201 steps/s (collection: 0.817s, learning 0.126s)
             Mean action noise std: 2.36
          Mean value_function loss: 104.6914
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3476
                       Mean reward: 332.25
               Mean episode length: 237.07
    Episode_Reward/reaching_object: 0.3642
    Episode_Reward/rotating_object: 64.0665
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.94s
                      Time elapsed: 00:10:23
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 636/1500 [0m                      

                       Computation: 109009 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 2.37
          Mean value_function loss: 103.1832
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.3576
                       Mean reward: 347.04
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.3584
    Episode_Reward/rotating_object: 70.3268
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.90s
                      Time elapsed: 00:10:23
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 637/1500 [0m                      

                       Computation: 97860 steps/s (collection: 0.831s, learning 0.174s)
             Mean action noise std: 2.37
          Mean value_function loss: 107.2159
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.3708
                       Mean reward: 346.02
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 0.3530
    Episode_Reward/rotating_object: 64.4191
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 1.00s
                      Time elapsed: 00:10:24
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 638/1500 [0m                      

                       Computation: 105854 steps/s (collection: 0.788s, learning 0.141s)
             Mean action noise std: 2.37
          Mean value_function loss: 107.1716
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.3774
                       Mean reward: 303.97
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.3597
    Episode_Reward/rotating_object: 64.3715
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.93s
                      Time elapsed: 00:10:25
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 639/1500 [0m                      

                       Computation: 104037 steps/s (collection: 0.789s, learning 0.156s)
             Mean action noise std: 2.37
          Mean value_function loss: 105.3428
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.3766
                       Mean reward: 331.49
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.3537
    Episode_Reward/rotating_object: 69.7045
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.94s
                      Time elapsed: 00:10:26
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 640/1500 [0m                      

                       Computation: 103644 steps/s (collection: 0.789s, learning 0.160s)
             Mean action noise std: 2.37
          Mean value_function loss: 98.1830
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.3653
                       Mean reward: 381.04
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 0.3694
    Episode_Reward/rotating_object: 75.0618
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.95s
                      Time elapsed: 00:10:27
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 641/1500 [0m                      

                       Computation: 108904 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 100.4354
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.3671
                       Mean reward: 362.21
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.3643
    Episode_Reward/rotating_object: 71.6293
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.90s
                      Time elapsed: 00:10:28
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 642/1500 [0m                      

                       Computation: 101478 steps/s (collection: 0.835s, learning 0.134s)
             Mean action noise std: 2.38
          Mean value_function loss: 99.7749
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.3766
                       Mean reward: 373.06
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 0.3705
    Episode_Reward/rotating_object: 72.4608
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.97s
                      Time elapsed: 00:10:29
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 643/1500 [0m                      

                       Computation: 110234 steps/s (collection: 0.795s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 103.2856
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.3819
                       Mean reward: 358.94
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.3788
    Episode_Reward/rotating_object: 72.5858
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.89s
                      Time elapsed: 00:10:30
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 644/1500 [0m                      

                       Computation: 102295 steps/s (collection: 0.835s, learning 0.126s)
             Mean action noise std: 2.39
          Mean value_function loss: 100.9742
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.3936
                       Mean reward: 332.92
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 0.3640
    Episode_Reward/rotating_object: 61.5225
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.96s
                      Time elapsed: 00:10:31
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 645/1500 [0m                      

                       Computation: 106226 steps/s (collection: 0.801s, learning 0.125s)
             Mean action noise std: 2.39
          Mean value_function loss: 108.9992
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.3996
                       Mean reward: 348.91
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.3650
    Episode_Reward/rotating_object: 70.7983
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.93s
                      Time elapsed: 00:10:32
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 646/1500 [0m                      

                       Computation: 105557 steps/s (collection: 0.833s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 112.0251
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3986
                       Mean reward: 338.13
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 69.2194
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.93s
                      Time elapsed: 00:10:33
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 647/1500 [0m                      

                       Computation: 105169 steps/s (collection: 0.828s, learning 0.107s)
             Mean action noise std: 2.39
          Mean value_function loss: 104.8416
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.3917
                       Mean reward: 319.15
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3659
    Episode_Reward/rotating_object: 71.9057
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.93s
                      Time elapsed: 00:10:34
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 648/1500 [0m                      

                       Computation: 101420 steps/s (collection: 0.851s, learning 0.119s)
             Mean action noise std: 2.39
          Mean value_function loss: 118.7380
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.3912
                       Mean reward: 338.89
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 0.3710
    Episode_Reward/rotating_object: 73.1820
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.97s
                      Time elapsed: 00:10:35
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 649/1500 [0m                      

                       Computation: 101830 steps/s (collection: 0.843s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 112.5676
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3835
                       Mean reward: 345.90
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.3635
    Episode_Reward/rotating_object: 71.6088
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.97s
                      Time elapsed: 00:10:36
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 650/1500 [0m                      

                       Computation: 103017 steps/s (collection: 0.819s, learning 0.135s)
             Mean action noise std: 2.39
          Mean value_function loss: 102.1664
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 17.3811
                       Mean reward: 327.84
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.3739
    Episode_Reward/rotating_object: 71.3052
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.95s
                      Time elapsed: 00:10:37
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 651/1500 [0m                      

                       Computation: 105401 steps/s (collection: 0.826s, learning 0.107s)
             Mean action noise std: 2.39
          Mean value_function loss: 96.6325
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.3843
                       Mean reward: 355.30
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.3693
    Episode_Reward/rotating_object: 72.3585
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.93s
                      Time elapsed: 00:10:38
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 652/1500 [0m                      

                       Computation: 105414 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 102.2084
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3885
                       Mean reward: 388.56
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.3707
    Episode_Reward/rotating_object: 71.2086
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.93s
                      Time elapsed: 00:10:39
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 653/1500 [0m                      

                       Computation: 103179 steps/s (collection: 0.847s, learning 0.106s)
             Mean action noise std: 2.40
          Mean value_function loss: 101.0815
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3912
                       Mean reward: 360.80
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.3725
    Episode_Reward/rotating_object: 70.6362
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.95s
                      Time elapsed: 00:10:40
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 654/1500 [0m                      

                       Computation: 103633 steps/s (collection: 0.818s, learning 0.131s)
             Mean action noise std: 2.39
          Mean value_function loss: 110.5888
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.3824
                       Mean reward: 330.63
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.3596
    Episode_Reward/rotating_object: 67.0341
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.95s
                      Time elapsed: 00:10:40
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 655/1500 [0m                      

                       Computation: 96254 steps/s (collection: 0.845s, learning 0.176s)
             Mean action noise std: 2.40
          Mean value_function loss: 111.0509
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.3780
                       Mean reward: 366.71
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.3702
    Episode_Reward/rotating_object: 67.5525
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 1.02s
                      Time elapsed: 00:10:41
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 656/1500 [0m                      

                       Computation: 99334 steps/s (collection: 0.838s, learning 0.152s)
             Mean action noise std: 2.40
          Mean value_function loss: 110.7460
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.3819
                       Mean reward: 373.02
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.3712
    Episode_Reward/rotating_object: 72.2380
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.99s
                      Time elapsed: 00:10:42
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 657/1500 [0m                      

                       Computation: 104626 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 2.40
          Mean value_function loss: 111.2085
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.3840
                       Mean reward: 364.33
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.3651
    Episode_Reward/rotating_object: 72.6763
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.94s
                      Time elapsed: 00:10:43
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 658/1500 [0m                      

                       Computation: 97276 steps/s (collection: 0.833s, learning 0.178s)
             Mean action noise std: 2.40
          Mean value_function loss: 111.0389
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3809
                       Mean reward: 302.35
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 70.5174
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.01s
                      Time elapsed: 00:10:44
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 659/1500 [0m                      

                       Computation: 103533 steps/s (collection: 0.845s, learning 0.104s)
             Mean action noise std: 2.40
          Mean value_function loss: 106.5078
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.3727
                       Mean reward: 343.87
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.3693
    Episode_Reward/rotating_object: 69.0472
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.95s
                      Time elapsed: 00:10:45
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 660/1500 [0m                      

                       Computation: 103843 steps/s (collection: 0.813s, learning 0.133s)
             Mean action noise std: 2.40
          Mean value_function loss: 94.2472
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3715
                       Mean reward: 341.59
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 70.5486
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.95s
                      Time elapsed: 00:10:46
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 661/1500 [0m                      

                       Computation: 102483 steps/s (collection: 0.821s, learning 0.138s)
             Mean action noise std: 2.40
          Mean value_function loss: 93.8894
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3699
                       Mean reward: 354.83
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.3673
    Episode_Reward/rotating_object: 72.4548
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.96s
                      Time elapsed: 00:10:47
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 662/1500 [0m                      

                       Computation: 105691 steps/s (collection: 0.805s, learning 0.125s)
             Mean action noise std: 2.40
          Mean value_function loss: 109.9694
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.3759
                       Mean reward: 333.15
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.3643
    Episode_Reward/rotating_object: 72.0568
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.93s
                      Time elapsed: 00:10:48
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 663/1500 [0m                      

                       Computation: 102106 steps/s (collection: 0.860s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 114.0823
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.3834
                       Mean reward: 414.74
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.3653
    Episode_Reward/rotating_object: 76.9712
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.96s
                      Time elapsed: 00:10:49
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 664/1500 [0m                      

                       Computation: 113095 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 124.2466
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 17.3936
                       Mean reward: 334.77
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.3706
    Episode_Reward/rotating_object: 71.7128
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.87s
                      Time elapsed: 00:10:50
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 665/1500 [0m                      

                       Computation: 104429 steps/s (collection: 0.844s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 122.6765
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 17.3950
                       Mean reward: 381.23
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 73.2992
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.94s
                      Time elapsed: 00:10:51
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 666/1500 [0m                      

                       Computation: 63618 steps/s (collection: 1.441s, learning 0.104s)
             Mean action noise std: 2.41
          Mean value_function loss: 112.6552
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 17.3969
                       Mean reward: 314.51
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3777
    Episode_Reward/rotating_object: 70.6864
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.55s
                      Time elapsed: 00:10:53
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 667/1500 [0m                      

                       Computation: 34661 steps/s (collection: 2.721s, learning 0.115s)
             Mean action noise std: 2.41
          Mean value_function loss: 105.2190
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.3986
                       Mean reward: 352.69
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.3825
    Episode_Reward/rotating_object: 67.1897
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 2.84s
                      Time elapsed: 00:10:55
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 668/1500 [0m                      

                       Computation: 33674 steps/s (collection: 2.800s, learning 0.119s)
             Mean action noise std: 2.41
          Mean value_function loss: 112.0035
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.3997
                       Mean reward: 408.07
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3742
    Episode_Reward/rotating_object: 73.3815
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 2.92s
                      Time elapsed: 00:10:58
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 669/1500 [0m                      

                       Computation: 33213 steps/s (collection: 2.823s, learning 0.137s)
             Mean action noise std: 2.41
          Mean value_function loss: 112.0258
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3963
                       Mean reward: 329.82
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.3658
    Episode_Reward/rotating_object: 65.7092
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 2.96s
                      Time elapsed: 00:11:01
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 670/1500 [0m                      

                       Computation: 34014 steps/s (collection: 2.766s, learning 0.124s)
             Mean action noise std: 2.42
          Mean value_function loss: 112.3647
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4080
                       Mean reward: 396.94
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3735
    Episode_Reward/rotating_object: 70.7786
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 2.89s
                      Time elapsed: 00:11:04
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 671/1500 [0m                      

                       Computation: 34338 steps/s (collection: 2.744s, learning 0.119s)
             Mean action noise std: 2.42
          Mean value_function loss: 104.1259
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4117
                       Mean reward: 382.22
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 0.3789
    Episode_Reward/rotating_object: 75.8202
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 2.86s
                      Time elapsed: 00:11:07
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 672/1500 [0m                      

                       Computation: 32290 steps/s (collection: 2.903s, learning 0.142s)
             Mean action noise std: 2.42
          Mean value_function loss: 101.5632
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.4146
                       Mean reward: 382.22
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.3747
    Episode_Reward/rotating_object: 76.5615
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.04s
                      Time elapsed: 00:11:10
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 673/1500 [0m                      

                       Computation: 32192 steps/s (collection: 2.910s, learning 0.143s)
             Mean action noise std: 2.42
          Mean value_function loss: 116.2524
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4161
                       Mean reward: 335.30
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.3775
    Episode_Reward/rotating_object: 74.4160
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.05s
                      Time elapsed: 00:11:13
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 674/1500 [0m                      

                       Computation: 33603 steps/s (collection: 2.815s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 113.8559
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4195
                       Mean reward: 382.44
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.3826
    Episode_Reward/rotating_object: 76.0447
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 2.93s
                      Time elapsed: 00:11:16
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 675/1500 [0m                      

                       Computation: 31196 steps/s (collection: 3.028s, learning 0.123s)
             Mean action noise std: 2.42
          Mean value_function loss: 112.6179
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4183
                       Mean reward: 389.22
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.3765
    Episode_Reward/rotating_object: 71.1884
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.15s
                      Time elapsed: 00:11:19
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 676/1500 [0m                      

                       Computation: 107350 steps/s (collection: 0.807s, learning 0.109s)
             Mean action noise std: 2.43
          Mean value_function loss: 104.8856
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.4265
                       Mean reward: 389.13
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.3788
    Episode_Reward/rotating_object: 75.1446
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.92s
                      Time elapsed: 00:11:20
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 677/1500 [0m                      

                       Computation: 109750 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 2.43
          Mean value_function loss: 110.0026
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.4315
                       Mean reward: 377.49
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.3814
    Episode_Reward/rotating_object: 73.8242
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.90s
                      Time elapsed: 00:11:21
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 678/1500 [0m                      

                       Computation: 105879 steps/s (collection: 0.816s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 116.4218
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.4328
                       Mean reward: 393.41
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.3819
    Episode_Reward/rotating_object: 77.3743
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.93s
                      Time elapsed: 00:11:22
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 679/1500 [0m                      

                       Computation: 106118 steps/s (collection: 0.804s, learning 0.122s)
             Mean action noise std: 2.43
          Mean value_function loss: 113.0309
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.4324
                       Mean reward: 395.36
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.3892
    Episode_Reward/rotating_object: 81.4389
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.93s
                      Time elapsed: 00:11:23
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 680/1500 [0m                      

                       Computation: 105496 steps/s (collection: 0.795s, learning 0.137s)
             Mean action noise std: 2.43
          Mean value_function loss: 107.4670
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.4333
                       Mean reward: 410.81
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.3839
    Episode_Reward/rotating_object: 79.3504
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.93s
                      Time elapsed: 00:11:24
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 681/1500 [0m                      

                       Computation: 109985 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 2.44
          Mean value_function loss: 117.2214
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4285
                       Mean reward: 363.65
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.3825
    Episode_Reward/rotating_object: 73.6230
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.89s
                      Time elapsed: 00:11:25
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 682/1500 [0m                      

                       Computation: 106470 steps/s (collection: 0.757s, learning 0.166s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.5174
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 17.4339
                       Mean reward: 348.81
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 74.2740
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.92s
                      Time elapsed: 00:11:26
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 683/1500 [0m                      

                       Computation: 109984 steps/s (collection: 0.785s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 112.6796
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4415
                       Mean reward: 383.53
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.3815
    Episode_Reward/rotating_object: 75.0812
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.89s
                      Time elapsed: 00:11:26
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 684/1500 [0m                      

                       Computation: 107949 steps/s (collection: 0.784s, learning 0.127s)
             Mean action noise std: 2.44
          Mean value_function loss: 110.1054
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.4528
                       Mean reward: 397.41
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.3755
    Episode_Reward/rotating_object: 72.8405
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.91s
                      Time elapsed: 00:11:27
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 685/1500 [0m                      

                       Computation: 107152 steps/s (collection: 0.802s, learning 0.116s)
             Mean action noise std: 2.45
          Mean value_function loss: 114.7874
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4598
                       Mean reward: 334.88
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.3786
    Episode_Reward/rotating_object: 73.6622
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.92s
                      Time elapsed: 00:11:28
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 686/1500 [0m                      

                       Computation: 107450 steps/s (collection: 0.803s, learning 0.112s)
             Mean action noise std: 2.45
          Mean value_function loss: 117.1736
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.4644
                       Mean reward: 356.36
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 71.0896
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.91s
                      Time elapsed: 00:11:29
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 687/1500 [0m                      

                       Computation: 107603 steps/s (collection: 0.801s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 115.5249
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4699
                       Mean reward: 376.62
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 0.3826
    Episode_Reward/rotating_object: 72.5720
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.91s
                      Time elapsed: 00:11:30
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 688/1500 [0m                      

                       Computation: 110757 steps/s (collection: 0.796s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 127.1233
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4686
                       Mean reward: 377.64
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 0.3765
    Episode_Reward/rotating_object: 75.1191
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.89s
                      Time elapsed: 00:11:31
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 689/1500 [0m                      

                       Computation: 111623 steps/s (collection: 0.790s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 111.6931
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4744
                       Mean reward: 391.24
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.3767
    Episode_Reward/rotating_object: 75.5390
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.88s
                      Time elapsed: 00:11:32
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 690/1500 [0m                      

                       Computation: 108061 steps/s (collection: 0.813s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 105.0610
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.4877
                       Mean reward: 345.55
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.3796
    Episode_Reward/rotating_object: 71.9321
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.91s
                      Time elapsed: 00:11:33
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 691/1500 [0m                      

                       Computation: 106095 steps/s (collection: 0.795s, learning 0.132s)
             Mean action noise std: 2.46
          Mean value_function loss: 111.7073
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.4963
                       Mean reward: 365.79
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.3793
    Episode_Reward/rotating_object: 73.4005
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.93s
                      Time elapsed: 00:11:34
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 692/1500 [0m                      

                       Computation: 108321 steps/s (collection: 0.787s, learning 0.121s)
             Mean action noise std: 2.47
          Mean value_function loss: 112.1653
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.5070
                       Mean reward: 370.00
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.3821
    Episode_Reward/rotating_object: 74.8468
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.91s
                      Time elapsed: 00:11:35
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 693/1500 [0m                      

                       Computation: 105895 steps/s (collection: 0.798s, learning 0.131s)
             Mean action noise std: 2.47
          Mean value_function loss: 110.5928
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.5133
                       Mean reward: 405.05
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.3874
    Episode_Reward/rotating_object: 76.7344
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.93s
                      Time elapsed: 00:11:36
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 694/1500 [0m                      

                       Computation: 107659 steps/s (collection: 0.784s, learning 0.129s)
             Mean action noise std: 2.47
          Mean value_function loss: 123.1475
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.5259
                       Mean reward: 348.93
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3794
    Episode_Reward/rotating_object: 74.9717
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.91s
                      Time elapsed: 00:11:36
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 695/1500 [0m                      

                       Computation: 111954 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 108.4815
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.5363
                       Mean reward: 377.23
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.3808
    Episode_Reward/rotating_object: 73.9674
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.88s
                      Time elapsed: 00:11:37
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 696/1500 [0m                      

                       Computation: 110678 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 2.48
          Mean value_function loss: 117.2678
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.5434
                       Mean reward: 332.12
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 72.8874
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.89s
                      Time elapsed: 00:11:38
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 697/1500 [0m                      

                       Computation: 104221 steps/s (collection: 0.823s, learning 0.121s)
             Mean action noise std: 2.48
          Mean value_function loss: 116.3486
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5493
                       Mean reward: 367.85
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 0.3724
    Episode_Reward/rotating_object: 72.9830
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.94s
                      Time elapsed: 00:11:39
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 698/1500 [0m                      

                       Computation: 110089 steps/s (collection: 0.807s, learning 0.086s)
             Mean action noise std: 2.49
          Mean value_function loss: 117.7478
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.5616
                       Mean reward: 358.07
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.3754
    Episode_Reward/rotating_object: 71.5460
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.89s
                      Time elapsed: 00:11:40
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 699/1500 [0m                      

                       Computation: 104996 steps/s (collection: 0.805s, learning 0.131s)
             Mean action noise std: 2.49
          Mean value_function loss: 130.3945
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5613
                       Mean reward: 395.89
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.3820
    Episode_Reward/rotating_object: 75.9789
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.94s
                      Time elapsed: 00:11:41
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 700/1500 [0m                      

                       Computation: 111642 steps/s (collection: 0.789s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 107.5569
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.5612
                       Mean reward: 345.21
               Mean episode length: 233.71
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 71.7251
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.88s
                      Time elapsed: 00:11:42
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 701/1500 [0m                      

                       Computation: 113282 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 105.9906
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.5575
                       Mean reward: 389.68
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.3746
    Episode_Reward/rotating_object: 73.2481
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.87s
                      Time elapsed: 00:11:43
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 702/1500 [0m                      

                       Computation: 101381 steps/s (collection: 0.792s, learning 0.178s)
             Mean action noise std: 2.49
          Mean value_function loss: 97.6091
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5621
                       Mean reward: 385.84
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 74.9195
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.97s
                      Time elapsed: 00:11:44
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 703/1500 [0m                      

                       Computation: 113765 steps/s (collection: 0.769s, learning 0.095s)
             Mean action noise std: 2.50
          Mean value_function loss: 102.1345
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5653
                       Mean reward: 354.90
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.3753
    Episode_Reward/rotating_object: 74.8686
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.86s
                      Time elapsed: 00:11:45
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 704/1500 [0m                      

                       Computation: 107071 steps/s (collection: 0.763s, learning 0.155s)
             Mean action noise std: 2.50
          Mean value_function loss: 104.5582
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.5712
                       Mean reward: 375.65
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.3821
    Episode_Reward/rotating_object: 73.2261
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.92s
                      Time elapsed: 00:11:46
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 705/1500 [0m                      

                       Computation: 110580 steps/s (collection: 0.802s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 110.1242
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 17.5757
                       Mean reward: 348.62
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3698
    Episode_Reward/rotating_object: 68.3478
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.89s
                      Time elapsed: 00:11:46
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 706/1500 [0m                      

                       Computation: 117345 steps/s (collection: 0.752s, learning 0.086s)
             Mean action noise std: 2.50
          Mean value_function loss: 102.7102
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.5728
                       Mean reward: 378.98
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.3784
    Episode_Reward/rotating_object: 76.3196
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.84s
                      Time elapsed: 00:11:47
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 707/1500 [0m                      

                       Computation: 111009 steps/s (collection: 0.797s, learning 0.089s)
             Mean action noise std: 2.50
          Mean value_function loss: 110.2521
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.5658
                       Mean reward: 369.66
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.3669
    Episode_Reward/rotating_object: 76.4218
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.89s
                      Time elapsed: 00:11:48
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 708/1500 [0m                      

                       Computation: 113202 steps/s (collection: 0.771s, learning 0.097s)
             Mean action noise std: 2.50
          Mean value_function loss: 99.3449
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 17.5708
                       Mean reward: 413.63
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.3699
    Episode_Reward/rotating_object: 73.4722
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.87s
                      Time elapsed: 00:11:49
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 709/1500 [0m                      

                       Computation: 112537 steps/s (collection: 0.776s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 103.0710
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.5734
                       Mean reward: 412.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 79.1174
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.87s
                      Time elapsed: 00:11:50
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 710/1500 [0m                      

                       Computation: 108394 steps/s (collection: 0.776s, learning 0.131s)
             Mean action noise std: 2.51
          Mean value_function loss: 109.8224
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.5743
                       Mean reward: 395.70
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.3705
    Episode_Reward/rotating_object: 75.9233
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.91s
                      Time elapsed: 00:11:51
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 711/1500 [0m                      

                       Computation: 110079 steps/s (collection: 0.759s, learning 0.134s)
             Mean action noise std: 2.51
          Mean value_function loss: 99.4007
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5733
                       Mean reward: 366.69
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.3714
    Episode_Reward/rotating_object: 74.2175
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.89s
                      Time elapsed: 00:11:52
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 712/1500 [0m                      

                       Computation: 113921 steps/s (collection: 0.760s, learning 0.103s)
             Mean action noise std: 2.51
          Mean value_function loss: 102.8543
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5698
                       Mean reward: 376.45
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.3701
    Episode_Reward/rotating_object: 72.1613
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.86s
                      Time elapsed: 00:11:53
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 713/1500 [0m                      

                       Computation: 112346 steps/s (collection: 0.765s, learning 0.110s)
             Mean action noise std: 2.51
          Mean value_function loss: 97.3521
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.5666
                       Mean reward: 423.67
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.3731
    Episode_Reward/rotating_object: 79.7858
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.88s
                      Time elapsed: 00:11:53
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 714/1500 [0m                      

                       Computation: 106235 steps/s (collection: 0.803s, learning 0.123s)
             Mean action noise std: 2.51
          Mean value_function loss: 96.8714
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.5645
                       Mean reward: 426.41
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3642
    Episode_Reward/rotating_object: 76.7997
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.93s
                      Time elapsed: 00:11:54
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 715/1500 [0m                      

                       Computation: 110971 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 2.51
          Mean value_function loss: 108.9210
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.5576
                       Mean reward: 374.55
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.3733
    Episode_Reward/rotating_object: 77.9031
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.89s
                      Time elapsed: 00:11:55
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 716/1500 [0m                      

                       Computation: 102393 steps/s (collection: 0.845s, learning 0.115s)
             Mean action noise std: 2.52
          Mean value_function loss: 105.0603
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.5557
                       Mean reward: 485.68
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.3808
    Episode_Reward/rotating_object: 85.5626
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.96s
                      Time elapsed: 00:11:56
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 717/1500 [0m                      

                       Computation: 101089 steps/s (collection: 0.861s, learning 0.111s)
             Mean action noise std: 2.52
          Mean value_function loss: 101.2023
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.5622
                       Mean reward: 365.31
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.3712
    Episode_Reward/rotating_object: 76.6457
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.97s
                      Time elapsed: 00:11:57
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 718/1500 [0m                      

                       Computation: 110868 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 101.0352
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.5637
                       Mean reward: 354.45
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3793
    Episode_Reward/rotating_object: 77.1202
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.89s
                      Time elapsed: 00:11:58
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 719/1500 [0m                      

                       Computation: 109015 steps/s (collection: 0.783s, learning 0.119s)
             Mean action noise std: 2.52
          Mean value_function loss: 98.3836
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.5673
                       Mean reward: 385.41
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.3715
    Episode_Reward/rotating_object: 77.0568
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.90s
                      Time elapsed: 00:11:59
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 720/1500 [0m                      

                       Computation: 110282 steps/s (collection: 0.780s, learning 0.112s)
             Mean action noise std: 2.53
          Mean value_function loss: 100.8676
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 17.5707
                       Mean reward: 381.61
               Mean episode length: 242.04
    Episode_Reward/reaching_object: 0.3693
    Episode_Reward/rotating_object: 74.5896
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.89s
                      Time elapsed: 00:12:00
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 721/1500 [0m                      

                       Computation: 94941 steps/s (collection: 0.891s, learning 0.144s)
             Mean action noise std: 2.53
          Mean value_function loss: 109.0776
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 17.5764
                       Mean reward: 453.93
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.3671
    Episode_Reward/rotating_object: 75.0623
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.04s
                      Time elapsed: 00:12:01
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 722/1500 [0m                      

                       Computation: 110438 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 2.53
          Mean value_function loss: 101.2364
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.5819
                       Mean reward: 385.68
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3773
    Episode_Reward/rotating_object: 77.2453
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.89s
                      Time elapsed: 00:12:02
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 723/1500 [0m                      

                       Computation: 104522 steps/s (collection: 0.787s, learning 0.153s)
             Mean action noise std: 2.53
          Mean value_function loss: 95.4710
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.5981
                       Mean reward: 389.52
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3714
    Episode_Reward/rotating_object: 76.5455
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.94s
                      Time elapsed: 00:12:03
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 724/1500 [0m                      

                       Computation: 106059 steps/s (collection: 0.775s, learning 0.152s)
             Mean action noise std: 2.53
          Mean value_function loss: 98.7864
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.6047
                       Mean reward: 369.19
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.3761
    Episode_Reward/rotating_object: 72.9374
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.93s
                      Time elapsed: 00:12:04
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 725/1500 [0m                      

                       Computation: 111843 steps/s (collection: 0.757s, learning 0.122s)
             Mean action noise std: 2.53
          Mean value_function loss: 102.5579
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.6074
                       Mean reward: 384.37
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.3836
    Episode_Reward/rotating_object: 80.5642
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.88s
                      Time elapsed: 00:12:05
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 726/1500 [0m                      

                       Computation: 112975 steps/s (collection: 0.766s, learning 0.105s)
             Mean action noise std: 2.54
          Mean value_function loss: 100.2626
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.6077
                       Mean reward: 372.30
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.3708
    Episode_Reward/rotating_object: 75.3787
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.87s
                      Time elapsed: 00:12:05
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 727/1500 [0m                      

                       Computation: 115853 steps/s (collection: 0.761s, learning 0.087s)
             Mean action noise std: 2.54
          Mean value_function loss: 106.9762
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6128
                       Mean reward: 343.22
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.3776
    Episode_Reward/rotating_object: 79.2029
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.85s
                      Time elapsed: 00:12:06
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 728/1500 [0m                      

                       Computation: 114253 steps/s (collection: 0.747s, learning 0.113s)
             Mean action noise std: 2.54
          Mean value_function loss: 101.0124
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.6150
                       Mean reward: 411.87
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 0.3712
    Episode_Reward/rotating_object: 82.3416
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.86s
                      Time elapsed: 00:12:07
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 729/1500 [0m                      

                       Computation: 112501 steps/s (collection: 0.771s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 105.1682
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6204
                       Mean reward: 390.23
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.3796
    Episode_Reward/rotating_object: 78.8909
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.87s
                      Time elapsed: 00:12:08
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 730/1500 [0m                      

                       Computation: 114220 steps/s (collection: 0.759s, learning 0.102s)
             Mean action noise std: 2.54
          Mean value_function loss: 113.1215
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.6157
                       Mean reward: 396.11
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 76.4059
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.86s
                      Time elapsed: 00:12:09
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 731/1500 [0m                      

                       Computation: 109754 steps/s (collection: 0.776s, learning 0.120s)
             Mean action noise std: 2.54
          Mean value_function loss: 115.5382
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.6169
                       Mean reward: 406.91
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 0.3795
    Episode_Reward/rotating_object: 78.7497
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.90s
                      Time elapsed: 00:12:10
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 732/1500 [0m                      

                       Computation: 106939 steps/s (collection: 0.749s, learning 0.171s)
             Mean action noise std: 2.55
          Mean value_function loss: 102.9485
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6305
                       Mean reward: 388.68
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 0.3729
    Episode_Reward/rotating_object: 74.9984
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.92s
                      Time elapsed: 00:12:11
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 733/1500 [0m                      

                       Computation: 104642 steps/s (collection: 0.847s, learning 0.092s)
             Mean action noise std: 2.55
          Mean value_function loss: 95.4234
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6381
                       Mean reward: 414.41
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.3859
    Episode_Reward/rotating_object: 79.6484
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.94s
                      Time elapsed: 00:12:12
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 734/1500 [0m                      

                       Computation: 103127 steps/s (collection: 0.822s, learning 0.131s)
             Mean action noise std: 2.56
          Mean value_function loss: 96.0969
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.6534
                       Mean reward: 378.73
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.3884
    Episode_Reward/rotating_object: 77.0436
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.95s
                      Time elapsed: 00:12:13
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 735/1500 [0m                      

                       Computation: 117065 steps/s (collection: 0.732s, learning 0.108s)
             Mean action noise std: 2.56
          Mean value_function loss: 110.2492
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.6670
                       Mean reward: 362.81
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 0.3824
    Episode_Reward/rotating_object: 77.0348
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.84s
                      Time elapsed: 00:12:13
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 736/1500 [0m                      

                       Computation: 113356 steps/s (collection: 0.767s, learning 0.100s)
             Mean action noise std: 2.56
          Mean value_function loss: 106.5463
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.6712
                       Mean reward: 416.57
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 80.0804
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.87s
                      Time elapsed: 00:12:14
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 737/1500 [0m                      

                       Computation: 115433 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 2.56
          Mean value_function loss: 109.3091
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.6765
                       Mean reward: 398.65
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.3821
    Episode_Reward/rotating_object: 76.9919
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.85s
                      Time elapsed: 00:12:15
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 738/1500 [0m                      

                       Computation: 100829 steps/s (collection: 0.827s, learning 0.148s)
             Mean action noise std: 2.56
          Mean value_function loss: 112.4762
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.6726
                       Mean reward: 411.59
               Mean episode length: 244.51
    Episode_Reward/reaching_object: 0.3836
    Episode_Reward/rotating_object: 81.8456
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.97s
                      Time elapsed: 00:12:16
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 739/1500 [0m                      

                       Computation: 117090 steps/s (collection: 0.750s, learning 0.089s)
             Mean action noise std: 2.56
          Mean value_function loss: 116.4622
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.6713
                       Mean reward: 409.94
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3891
    Episode_Reward/rotating_object: 79.4836
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.84s
                      Time elapsed: 00:12:17
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 740/1500 [0m                      

                       Computation: 111034 steps/s (collection: 0.765s, learning 0.121s)
             Mean action noise std: 2.57
          Mean value_function loss: 118.2608
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.6707
                       Mean reward: 407.57
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 0.3790
    Episode_Reward/rotating_object: 77.0169
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.89s
                      Time elapsed: 00:12:18
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 741/1500 [0m                      

                       Computation: 109943 steps/s (collection: 0.771s, learning 0.123s)
             Mean action noise std: 2.57
          Mean value_function loss: 131.9503
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6766
                       Mean reward: 434.15
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.3818
    Episode_Reward/rotating_object: 81.0456
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.89s
                      Time elapsed: 00:12:19
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 742/1500 [0m                      

                       Computation: 113719 steps/s (collection: 0.751s, learning 0.113s)
             Mean action noise std: 2.57
          Mean value_function loss: 128.9328
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.6857
                       Mean reward: 394.98
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.3837
    Episode_Reward/rotating_object: 79.8108
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.86s
                      Time elapsed: 00:12:20
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 743/1500 [0m                      

                       Computation: 118087 steps/s (collection: 0.735s, learning 0.097s)
             Mean action noise std: 2.57
          Mean value_function loss: 116.3094
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6806
                       Mean reward: 389.01
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 0.3867
    Episode_Reward/rotating_object: 80.4307
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.83s
                      Time elapsed: 00:12:20
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 744/1500 [0m                      

                       Computation: 115018 steps/s (collection: 0.767s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 105.8001
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.6819
                       Mean reward: 420.97
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.3844
    Episode_Reward/rotating_object: 78.6207
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.85s
                      Time elapsed: 00:12:21
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 745/1500 [0m                      

                       Computation: 115156 steps/s (collection: 0.767s, learning 0.087s)
             Mean action noise std: 2.58
          Mean value_function loss: 114.8728
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6973
                       Mean reward: 366.84
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 77.5150
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.85s
                      Time elapsed: 00:12:22
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 746/1500 [0m                      

                       Computation: 108789 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 2.58
          Mean value_function loss: 116.4122
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.7040
                       Mean reward: 359.62
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.3859
    Episode_Reward/rotating_object: 78.1251
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.90s
                      Time elapsed: 00:12:23
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 747/1500 [0m                      

                       Computation: 111294 steps/s (collection: 0.770s, learning 0.113s)
             Mean action noise std: 2.58
          Mean value_function loss: 123.0204
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7023
                       Mean reward: 385.16
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 0.3973
    Episode_Reward/rotating_object: 80.2959
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.88s
                      Time elapsed: 00:12:24
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 748/1500 [0m                      

                       Computation: 110276 steps/s (collection: 0.766s, learning 0.125s)
             Mean action noise std: 2.58
          Mean value_function loss: 117.0805
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7022
                       Mean reward: 391.28
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.3926
    Episode_Reward/rotating_object: 80.3819
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.89s
                      Time elapsed: 00:12:25
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 749/1500 [0m                      

                       Computation: 111000 steps/s (collection: 0.758s, learning 0.128s)
             Mean action noise std: 2.59
          Mean value_function loss: 114.6349
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.7062
                       Mean reward: 343.74
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 80.8947
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.89s
                      Time elapsed: 00:12:26
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 750/1500 [0m                      

                       Computation: 111067 steps/s (collection: 0.746s, learning 0.139s)
             Mean action noise std: 2.59
          Mean value_function loss: 116.8502
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.7118
                       Mean reward: 413.31
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.3970
    Episode_Reward/rotating_object: 81.2113
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.89s
                      Time elapsed: 00:12:27
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 751/1500 [0m                      

                       Computation: 112263 steps/s (collection: 0.758s, learning 0.117s)
             Mean action noise std: 2.59
          Mean value_function loss: 116.2339
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.7209
                       Mean reward: 378.81
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3919
    Episode_Reward/rotating_object: 73.8423
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.88s
                      Time elapsed: 00:12:27
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 752/1500 [0m                      

                       Computation: 116297 steps/s (collection: 0.756s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 123.5354
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.7289
                       Mean reward: 382.83
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.3846
    Episode_Reward/rotating_object: 73.5684
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.85s
                      Time elapsed: 00:12:28
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 753/1500 [0m                      

                       Computation: 110459 steps/s (collection: 0.786s, learning 0.104s)
             Mean action noise std: 2.60
          Mean value_function loss: 118.5680
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.7260
                       Mean reward: 415.43
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.3872
    Episode_Reward/rotating_object: 73.2027
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.89s
                      Time elapsed: 00:12:29
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 754/1500 [0m                      

                       Computation: 113454 steps/s (collection: 0.772s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 116.4104
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.7281
                       Mean reward: 373.22
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3950
    Episode_Reward/rotating_object: 75.5851
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.87s
                      Time elapsed: 00:12:30
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 755/1500 [0m                      

                       Computation: 115994 steps/s (collection: 0.762s, learning 0.086s)
             Mean action noise std: 2.60
          Mean value_function loss: 121.6672
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.7268
                       Mean reward: 376.29
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.3910
    Episode_Reward/rotating_object: 74.7252
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.85s
                      Time elapsed: 00:12:31
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 756/1500 [0m                      

                       Computation: 111924 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 2.60
          Mean value_function loss: 114.9076
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.7294
                       Mean reward: 382.49
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 0.4039
    Episode_Reward/rotating_object: 82.7262
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.88s
                      Time elapsed: 00:12:32
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 757/1500 [0m                      

                       Computation: 111962 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 2.60
          Mean value_function loss: 122.8601
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.7391
                       Mean reward: 448.45
               Mean episode length: 242.64
    Episode_Reward/reaching_object: 0.3934
    Episode_Reward/rotating_object: 80.6576
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.88s
                      Time elapsed: 00:12:33
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 758/1500 [0m                      

                       Computation: 108700 steps/s (collection: 0.746s, learning 0.159s)
             Mean action noise std: 2.60
          Mean value_function loss: 139.0830
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7442
                       Mean reward: 411.29
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.3987
    Episode_Reward/rotating_object: 78.8842
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.90s
                      Time elapsed: 00:12:34
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 759/1500 [0m                      

                       Computation: 112729 steps/s (collection: 0.757s, learning 0.115s)
             Mean action noise std: 2.61
          Mean value_function loss: 119.4174
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.7466
                       Mean reward: 406.57
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.3916
    Episode_Reward/rotating_object: 75.7934
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.87s
                      Time elapsed: 00:12:34
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 760/1500 [0m                      

                       Computation: 113362 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 123.2976
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7478
                       Mean reward: 416.13
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 80.1381
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.87s
                      Time elapsed: 00:12:35
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 761/1500 [0m                      

                       Computation: 111087 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 2.61
          Mean value_function loss: 132.2707
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.7537
                       Mean reward: 400.31
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.4038
    Episode_Reward/rotating_object: 78.6250
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.88s
                      Time elapsed: 00:12:36
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 762/1500 [0m                      

                       Computation: 111274 steps/s (collection: 0.771s, learning 0.113s)
             Mean action noise std: 2.61
          Mean value_function loss: 124.1792
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 17.7528
                       Mean reward: 424.64
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.3958
    Episode_Reward/rotating_object: 78.1066
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.88s
                      Time elapsed: 00:12:37
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 763/1500 [0m                      

                       Computation: 111760 steps/s (collection: 0.783s, learning 0.097s)
             Mean action noise std: 2.61
          Mean value_function loss: 127.6130
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.7546
                       Mean reward: 404.64
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.3988
    Episode_Reward/rotating_object: 78.6556
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.88s
                      Time elapsed: 00:12:38
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 764/1500 [0m                      

                       Computation: 112990 steps/s (collection: 0.781s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 123.2848
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.7773
                       Mean reward: 402.52
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.4024
    Episode_Reward/rotating_object: 78.9021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.87s
                      Time elapsed: 00:12:39
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 765/1500 [0m                      

                       Computation: 109285 steps/s (collection: 0.752s, learning 0.148s)
             Mean action noise std: 2.62
          Mean value_function loss: 103.2295
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.7874
                       Mean reward: 387.12
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.4002
    Episode_Reward/rotating_object: 78.5023
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.90s
                      Time elapsed: 00:12:40
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 766/1500 [0m                      

                       Computation: 106573 steps/s (collection: 0.762s, learning 0.161s)
             Mean action noise std: 2.63
          Mean value_function loss: 107.4001
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8051
                       Mean reward: 455.51
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 81.3700
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.92s
                      Time elapsed: 00:12:41
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 767/1500 [0m                      

                       Computation: 108848 steps/s (collection: 0.756s, learning 0.147s)
             Mean action noise std: 2.63
          Mean value_function loss: 104.8461
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.8158
                       Mean reward: 457.98
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 81.7922
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.90s
                      Time elapsed: 00:12:42
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 768/1500 [0m                      

                       Computation: 113301 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 2.63
          Mean value_function loss: 99.2568
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8228
                       Mean reward: 377.36
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 80.0742
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.87s
                      Time elapsed: 00:12:42
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 769/1500 [0m                      

                       Computation: 106190 steps/s (collection: 0.805s, learning 0.121s)
             Mean action noise std: 2.63
          Mean value_function loss: 107.6667
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.8197
                       Mean reward: 372.62
               Mean episode length: 240.28
    Episode_Reward/reaching_object: 0.3845
    Episode_Reward/rotating_object: 79.0749
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.93s
                      Time elapsed: 00:12:43
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 770/1500 [0m                      

                       Computation: 113491 steps/s (collection: 0.755s, learning 0.111s)
             Mean action noise std: 2.63
          Mean value_function loss: 99.9257
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8142
                       Mean reward: 397.39
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3859
    Episode_Reward/rotating_object: 80.8807
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.87s
                      Time elapsed: 00:12:44
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 771/1500 [0m                      

                       Computation: 108441 steps/s (collection: 0.800s, learning 0.107s)
             Mean action noise std: 2.64
          Mean value_function loss: 106.5468
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8139
                       Mean reward: 409.11
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3843
    Episode_Reward/rotating_object: 82.2168
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.91s
                      Time elapsed: 00:12:45
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 772/1500 [0m                      

                       Computation: 114252 steps/s (collection: 0.751s, learning 0.110s)
             Mean action noise std: 2.64
          Mean value_function loss: 110.3557
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.8178
                       Mean reward: 377.97
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.3763
    Episode_Reward/rotating_object: 81.5375
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.86s
                      Time elapsed: 00:12:46
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 773/1500 [0m                      

                       Computation: 113936 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 114.7046
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8258
                       Mean reward: 399.23
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.3723
    Episode_Reward/rotating_object: 78.7969
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.86s
                      Time elapsed: 00:12:47
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 774/1500 [0m                      

                       Computation: 113426 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 109.8754
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8332
                       Mean reward: 435.50
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.3862
    Episode_Reward/rotating_object: 82.0955
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.87s
                      Time elapsed: 00:12:48
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 775/1500 [0m                      

                       Computation: 104725 steps/s (collection: 0.778s, learning 0.161s)
             Mean action noise std: 2.65
          Mean value_function loss: 99.3446
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.8386
                       Mean reward: 415.99
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.3809
    Episode_Reward/rotating_object: 80.7109
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.94s
                      Time elapsed: 00:12:49
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 776/1500 [0m                      

                       Computation: 114654 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 112.6799
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 17.8393
                       Mean reward: 374.81
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 79.2695
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.86s
                      Time elapsed: 00:12:49
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 777/1500 [0m                      

                       Computation: 115414 steps/s (collection: 0.750s, learning 0.102s)
             Mean action noise std: 2.65
          Mean value_function loss: 120.6711
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.8407
                       Mean reward: 415.10
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 80.7081
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.85s
                      Time elapsed: 00:12:50
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 778/1500 [0m                      

                       Computation: 112780 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 99.3642
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8426
                       Mean reward: 396.56
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.3811
    Episode_Reward/rotating_object: 81.9254
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.87s
                      Time elapsed: 00:12:51
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 779/1500 [0m                      

                       Computation: 108996 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 112.1430
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8404
                       Mean reward: 424.14
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.3903
    Episode_Reward/rotating_object: 86.9397
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.90s
                      Time elapsed: 00:12:52
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 780/1500 [0m                      

                       Computation: 110632 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 114.8616
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.8362
                       Mean reward: 415.71
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.3831
    Episode_Reward/rotating_object: 81.0075
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.89s
                      Time elapsed: 00:12:53
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 781/1500 [0m                      

                       Computation: 107457 steps/s (collection: 0.800s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 118.8377
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.8377
                       Mean reward: 415.83
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.3818
    Episode_Reward/rotating_object: 85.4307
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.91s
                      Time elapsed: 00:12:54
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 782/1500 [0m                      

                       Computation: 109406 steps/s (collection: 0.760s, learning 0.139s)
             Mean action noise std: 2.66
          Mean value_function loss: 127.4092
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.8465
                       Mean reward: 449.12
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.3819
    Episode_Reward/rotating_object: 88.0408
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.90s
                      Time elapsed: 00:12:55
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 783/1500 [0m                      

                       Computation: 104491 steps/s (collection: 0.833s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 131.4751
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8589
                       Mean reward: 409.77
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.3840
    Episode_Reward/rotating_object: 82.8308
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.94s
                      Time elapsed: 00:12:56
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 784/1500 [0m                      

                       Computation: 107360 steps/s (collection: 0.774s, learning 0.141s)
             Mean action noise std: 2.67
          Mean value_function loss: 108.4018
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.8726
                       Mean reward: 453.23
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.3928
    Episode_Reward/rotating_object: 89.4183
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.92s
                      Time elapsed: 00:12:57
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 785/1500 [0m                      

                       Computation: 104379 steps/s (collection: 0.779s, learning 0.163s)
             Mean action noise std: 2.67
          Mean value_function loss: 117.0270
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8809
                       Mean reward: 397.32
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.3852
    Episode_Reward/rotating_object: 86.0214
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.94s
                      Time elapsed: 00:12:58
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 786/1500 [0m                      

                       Computation: 107205 steps/s (collection: 0.760s, learning 0.157s)
             Mean action noise std: 2.67
          Mean value_function loss: 113.7707
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8937
                       Mean reward: 413.08
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.3855
    Episode_Reward/rotating_object: 84.3854
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.92s
                      Time elapsed: 00:12:59
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 787/1500 [0m                      

                       Computation: 115064 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 2.68
          Mean value_function loss: 115.8393
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9044
                       Mean reward: 428.84
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.3892
    Episode_Reward/rotating_object: 83.4585
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.85s
                      Time elapsed: 00:12:59
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 788/1500 [0m                      

                       Computation: 112182 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 114.9335
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.9103
                       Mean reward: 429.16
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.3842
    Episode_Reward/rotating_object: 83.0785
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.88s
                      Time elapsed: 00:13:00
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 789/1500 [0m                      

                       Computation: 114416 steps/s (collection: 0.764s, learning 0.095s)
             Mean action noise std: 2.68
          Mean value_function loss: 102.1074
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9114
                       Mean reward: 401.45
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.3791
    Episode_Reward/rotating_object: 80.5666
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.86s
                      Time elapsed: 00:13:01
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 790/1500 [0m                      

                       Computation: 107743 steps/s (collection: 0.820s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 104.2643
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.9180
                       Mean reward: 408.11
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.3877
    Episode_Reward/rotating_object: 81.9450
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.91s
                      Time elapsed: 00:13:02
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 791/1500 [0m                      

                       Computation: 115842 steps/s (collection: 0.759s, learning 0.090s)
             Mean action noise std: 2.69
          Mean value_function loss: 101.6672
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9307
                       Mean reward: 443.38
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.3828
    Episode_Reward/rotating_object: 81.3078
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.85s
                      Time elapsed: 00:13:03
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 792/1500 [0m                      

                       Computation: 106865 steps/s (collection: 0.769s, learning 0.151s)
             Mean action noise std: 2.69
          Mean value_function loss: 114.1536
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 17.9407
                       Mean reward: 406.24
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.3934
    Episode_Reward/rotating_object: 88.3726
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.92s
                      Time elapsed: 00:13:04
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 793/1500 [0m                      

                       Computation: 115397 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 103.0257
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.9419
                       Mean reward: 409.28
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 0.3780
    Episode_Reward/rotating_object: 81.4360
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.85s
                      Time elapsed: 00:13:05
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 794/1500 [0m                      

                       Computation: 110362 steps/s (collection: 0.755s, learning 0.136s)
             Mean action noise std: 2.70
          Mean value_function loss: 109.3170
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.9511
                       Mean reward: 437.17
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.3817
    Episode_Reward/rotating_object: 84.7079
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.89s
                      Time elapsed: 00:13:06
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 795/1500 [0m                      

                       Computation: 111549 steps/s (collection: 0.769s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 90.6418
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 17.9575
                       Mean reward: 452.81
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.3827
    Episode_Reward/rotating_object: 83.2056
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.88s
                      Time elapsed: 00:13:06
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 796/1500 [0m                      

                       Computation: 108597 steps/s (collection: 0.781s, learning 0.124s)
             Mean action noise std: 2.70
          Mean value_function loss: 94.0884
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9662
                       Mean reward: 439.93
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.3853
    Episode_Reward/rotating_object: 85.5342
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.91s
                      Time elapsed: 00:13:07
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 797/1500 [0m                      

                       Computation: 111101 steps/s (collection: 0.764s, learning 0.121s)
             Mean action noise std: 2.71
          Mean value_function loss: 98.1709
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.9740
                       Mean reward: 409.22
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.3742
    Episode_Reward/rotating_object: 84.0892
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.88s
                      Time elapsed: 00:13:08
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 798/1500 [0m                      

                       Computation: 113909 steps/s (collection: 0.772s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 101.9263
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9815
                       Mean reward: 363.47
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 0.3786
    Episode_Reward/rotating_object: 80.7218
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.86s
                      Time elapsed: 00:13:09
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 799/1500 [0m                      

                       Computation: 104455 steps/s (collection: 0.808s, learning 0.134s)
             Mean action noise std: 2.71
          Mean value_function loss: 106.6979
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.9869
                       Mean reward: 431.76
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 0.3760
    Episode_Reward/rotating_object: 83.6724
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.94s
                      Time elapsed: 00:13:10
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 800/1500 [0m                      

                       Computation: 101394 steps/s (collection: 0.861s, learning 0.109s)
             Mean action noise std: 2.72
          Mean value_function loss: 106.6541
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 17.9984
                       Mean reward: 419.83
               Mean episode length: 242.60
    Episode_Reward/reaching_object: 0.3744
    Episode_Reward/rotating_object: 82.7071
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.97s
                      Time elapsed: 00:13:11
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 801/1500 [0m                      

                       Computation: 112820 steps/s (collection: 0.762s, learning 0.110s)
             Mean action noise std: 2.72
          Mean value_function loss: 108.5601
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0004
                       Mean reward: 424.75
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.3764
    Episode_Reward/rotating_object: 87.5551
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.87s
                      Time elapsed: 00:13:12
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 802/1500 [0m                      

                       Computation: 106233 steps/s (collection: 0.815s, learning 0.110s)
             Mean action noise std: 2.72
          Mean value_function loss: 114.6298
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.0036
                       Mean reward: 438.70
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 87.6902
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.93s
                      Time elapsed: 00:13:13
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 803/1500 [0m                      

                       Computation: 113198 steps/s (collection: 0.782s, learning 0.087s)
             Mean action noise std: 2.72
          Mean value_function loss: 104.6235
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0089
                       Mean reward: 421.59
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 0.3706
    Episode_Reward/rotating_object: 79.9169
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.87s
                      Time elapsed: 00:13:14
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 804/1500 [0m                      

                       Computation: 108171 steps/s (collection: 0.775s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 111.9315
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0134
                       Mean reward: 432.74
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3843
    Episode_Reward/rotating_object: 81.6916
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.91s
                      Time elapsed: 00:13:15
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 805/1500 [0m                      

                       Computation: 102337 steps/s (collection: 0.843s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 101.3469
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.0150
                       Mean reward: 434.43
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 0.3816
    Episode_Reward/rotating_object: 84.4211
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.96s
                      Time elapsed: 00:13:16
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 806/1500 [0m                      

                       Computation: 103495 steps/s (collection: 0.796s, learning 0.154s)
             Mean action noise std: 2.73
          Mean value_function loss: 96.2857
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.0143
                       Mean reward: 416.29
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.3810
    Episode_Reward/rotating_object: 86.6071
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.95s
                      Time elapsed: 00:13:16
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 807/1500 [0m                      

                       Computation: 95519 steps/s (collection: 0.943s, learning 0.086s)
             Mean action noise std: 2.73
          Mean value_function loss: 91.7002
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.0170
                       Mean reward: 478.52
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.3903
    Episode_Reward/rotating_object: 86.5709
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 1.03s
                      Time elapsed: 00:13:17
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 808/1500 [0m                      

                       Computation: 117731 steps/s (collection: 0.740s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 94.5633
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0087
                       Mean reward: 428.79
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.3839
    Episode_Reward/rotating_object: 86.2192
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.83s
                      Time elapsed: 00:13:18
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 809/1500 [0m                      

                       Computation: 113138 steps/s (collection: 0.781s, learning 0.088s)
             Mean action noise std: 2.73
          Mean value_function loss: 101.0596
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0125
                       Mean reward: 413.37
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.3867
    Episode_Reward/rotating_object: 85.4412
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.87s
                      Time elapsed: 00:13:19
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 810/1500 [0m                      

                       Computation: 115458 steps/s (collection: 0.765s, learning 0.087s)
             Mean action noise std: 2.74
          Mean value_function loss: 106.0662
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.0145
                       Mean reward: 432.53
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.3750
    Episode_Reward/rotating_object: 85.1824
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.85s
                      Time elapsed: 00:13:20
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 811/1500 [0m                      

                       Computation: 116091 steps/s (collection: 0.761s, learning 0.086s)
             Mean action noise std: 2.74
          Mean value_function loss: 95.6021
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.0172
                       Mean reward: 402.90
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.3702
    Episode_Reward/rotating_object: 79.9495
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.85s
                      Time elapsed: 00:13:21
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 812/1500 [0m                      

                       Computation: 111353 steps/s (collection: 0.763s, learning 0.120s)
             Mean action noise std: 2.74
          Mean value_function loss: 89.3856
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 18.0126
                       Mean reward: 420.70
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.3816
    Episode_Reward/rotating_object: 84.2654
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.88s
                      Time elapsed: 00:13:22
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 813/1500 [0m                      

                       Computation: 112862 steps/s (collection: 0.767s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 108.8546
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0094
                       Mean reward: 434.96
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 87.1138
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.87s
                      Time elapsed: 00:13:23
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 814/1500 [0m                      

                       Computation: 116358 steps/s (collection: 0.749s, learning 0.096s)
             Mean action noise std: 2.75
          Mean value_function loss: 99.3035
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.0189
                       Mean reward: 424.13
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 0.3720
    Episode_Reward/rotating_object: 86.9647
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.84s
                      Time elapsed: 00:13:23
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 815/1500 [0m                      

                       Computation: 117451 steps/s (collection: 0.750s, learning 0.087s)
             Mean action noise std: 2.75
          Mean value_function loss: 96.5746
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.0303
                       Mean reward: 456.06
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 89.9346
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.84s
                      Time elapsed: 00:13:24
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 816/1500 [0m                      

                       Computation: 111036 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 92.8810
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.0338
                       Mean reward: 443.36
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.3810
    Episode_Reward/rotating_object: 88.2237
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.89s
                      Time elapsed: 00:13:25
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 817/1500 [0m                      

                       Computation: 114650 steps/s (collection: 0.756s, learning 0.101s)
             Mean action noise std: 2.75
          Mean value_function loss: 91.0078
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 18.0426
                       Mean reward: 430.10
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.3745
    Episode_Reward/rotating_object: 86.9116
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.86s
                      Time elapsed: 00:13:26
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 818/1500 [0m                      

                       Computation: 117952 steps/s (collection: 0.748s, learning 0.086s)
             Mean action noise std: 2.76
          Mean value_function loss: 108.1024
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.0472
                       Mean reward: 450.73
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 88.4658
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.83s
                      Time elapsed: 00:13:27
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 819/1500 [0m                      

                       Computation: 115395 steps/s (collection: 0.746s, learning 0.106s)
             Mean action noise std: 2.76
          Mean value_function loss: 103.4108
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0491
                       Mean reward: 397.05
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 0.3670
    Episode_Reward/rotating_object: 85.4107
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.85s
                      Time elapsed: 00:13:28
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 820/1500 [0m                      

                       Computation: 111143 steps/s (collection: 0.769s, learning 0.115s)
             Mean action noise std: 2.76
          Mean value_function loss: 97.3918
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0578
                       Mean reward: 398.91
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3765
    Episode_Reward/rotating_object: 84.4549
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.88s
                      Time elapsed: 00:13:29
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 821/1500 [0m                      

                       Computation: 116425 steps/s (collection: 0.753s, learning 0.092s)
             Mean action noise std: 2.76
          Mean value_function loss: 99.6468
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0548
                       Mean reward: 381.94
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.3771
    Episode_Reward/rotating_object: 81.6851
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.84s
                      Time elapsed: 00:13:29
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 822/1500 [0m                      

                       Computation: 115455 steps/s (collection: 0.749s, learning 0.103s)
             Mean action noise std: 2.77
          Mean value_function loss: 103.0402
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0711
                       Mean reward: 461.07
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.3828
    Episode_Reward/rotating_object: 91.0912
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.85s
                      Time elapsed: 00:13:30
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 823/1500 [0m                      

                       Computation: 113669 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 2.78
          Mean value_function loss: 107.2862
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0916
                       Mean reward: 416.43
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.3799
    Episode_Reward/rotating_object: 85.9770
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.86s
                      Time elapsed: 00:13:31
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 824/1500 [0m                      

                       Computation: 105722 steps/s (collection: 0.814s, learning 0.116s)
             Mean action noise std: 2.78
          Mean value_function loss: 104.1787
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.1054
                       Mean reward: 477.38
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.3869
    Episode_Reward/rotating_object: 89.5583
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.93s
                      Time elapsed: 00:13:32
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 825/1500 [0m                      

                       Computation: 107382 steps/s (collection: 0.820s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 113.6448
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.1106
                       Mean reward: 460.28
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.3759
    Episode_Reward/rotating_object: 87.1803
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.92s
                      Time elapsed: 00:13:33
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 826/1500 [0m                      

                       Computation: 109910 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 104.1639
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.1157
                       Mean reward: 424.10
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.3817
    Episode_Reward/rotating_object: 87.0791
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.89s
                      Time elapsed: 00:13:34
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 827/1500 [0m                      

                       Computation: 95146 steps/s (collection: 0.877s, learning 0.156s)
             Mean action noise std: 2.79
          Mean value_function loss: 103.6710
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1230
                       Mean reward: 403.49
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 87.1761
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.03s
                      Time elapsed: 00:13:35
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 828/1500 [0m                      

                       Computation: 94560 steps/s (collection: 0.882s, learning 0.157s)
             Mean action noise std: 2.79
          Mean value_function loss: 102.8242
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.1300
                       Mean reward: 457.90
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.3824
    Episode_Reward/rotating_object: 84.8139
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 1.04s
                      Time elapsed: 00:13:36
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 829/1500 [0m                      

                       Computation: 94647 steps/s (collection: 0.881s, learning 0.157s)
             Mean action noise std: 2.79
          Mean value_function loss: 101.3151
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.1267
                       Mean reward: 438.50
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.3840
    Episode_Reward/rotating_object: 85.9594
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 1.04s
                      Time elapsed: 00:13:37
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 830/1500 [0m                      

                       Computation: 108711 steps/s (collection: 0.818s, learning 0.087s)
             Mean action noise std: 2.79
          Mean value_function loss: 112.4619
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.1372
                       Mean reward: 387.45
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.3818
    Episode_Reward/rotating_object: 85.8895
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.90s
                      Time elapsed: 00:13:38
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 831/1500 [0m                      

                       Computation: 110505 steps/s (collection: 0.768s, learning 0.122s)
             Mean action noise std: 2.79
          Mean value_function loss: 115.9433
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.1456
                       Mean reward: 440.18
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.3877
    Episode_Reward/rotating_object: 83.4372
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.89s
                      Time elapsed: 00:13:39
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 832/1500 [0m                      

                       Computation: 104606 steps/s (collection: 0.786s, learning 0.154s)
             Mean action noise std: 2.80
          Mean value_function loss: 113.5508
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.1440
                       Mean reward: 409.42
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.3825
    Episode_Reward/rotating_object: 86.2572
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.94s
                      Time elapsed: 00:13:40
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 833/1500 [0m                      

                       Computation: 110337 steps/s (collection: 0.762s, learning 0.129s)
             Mean action noise std: 2.80
          Mean value_function loss: 114.7560
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.1483
                       Mean reward: 417.71
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.3952
    Episode_Reward/rotating_object: 82.9878
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.89s
                      Time elapsed: 00:13:41
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 834/1500 [0m                      

                       Computation: 105605 steps/s (collection: 0.767s, learning 0.163s)
             Mean action noise std: 2.80
          Mean value_function loss: 112.6276
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.1560
                       Mean reward: 402.13
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 0.3849
    Episode_Reward/rotating_object: 85.7373
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.93s
                      Time elapsed: 00:13:42
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 835/1500 [0m                      

                       Computation: 113915 steps/s (collection: 0.771s, learning 0.092s)
             Mean action noise std: 2.80
          Mean value_function loss: 120.1058
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1546
                       Mean reward: 443.98
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.3876
    Episode_Reward/rotating_object: 85.9302
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.86s
                      Time elapsed: 00:13:42
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 836/1500 [0m                      

                       Computation: 113295 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 2.81
          Mean value_function loss: 114.1294
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1646
                       Mean reward: 444.63
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 88.7558
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.87s
                      Time elapsed: 00:13:43
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 837/1500 [0m                      

                       Computation: 113725 steps/s (collection: 0.779s, learning 0.085s)
             Mean action noise std: 2.81
          Mean value_function loss: 104.2404
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.1755
                       Mean reward: 367.95
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 0.3799
    Episode_Reward/rotating_object: 80.7201
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.86s
                      Time elapsed: 00:13:44
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 838/1500 [0m                      

                       Computation: 113187 steps/s (collection: 0.758s, learning 0.111s)
             Mean action noise std: 2.81
          Mean value_function loss: 93.7299
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.1809
                       Mean reward: 432.65
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 85.8860
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.87s
                      Time elapsed: 00:13:45
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 839/1500 [0m                      

                       Computation: 104743 steps/s (collection: 0.814s, learning 0.124s)
             Mean action noise std: 2.82
          Mean value_function loss: 108.1200
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.1865
                       Mean reward: 403.88
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.3774
    Episode_Reward/rotating_object: 80.5784
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.94s
                      Time elapsed: 00:13:46
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 840/1500 [0m                      

                       Computation: 107531 steps/s (collection: 0.803s, learning 0.112s)
             Mean action noise std: 2.82
          Mean value_function loss: 113.1791
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.1868
                       Mean reward: 428.01
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.3902
    Episode_Reward/rotating_object: 86.9073
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.91s
                      Time elapsed: 00:13:47
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 841/1500 [0m                      

                       Computation: 107566 steps/s (collection: 0.806s, learning 0.108s)
             Mean action noise std: 2.82
          Mean value_function loss: 116.8328
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.1923
                       Mean reward: 423.43
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.3849
    Episode_Reward/rotating_object: 84.9346
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.91s
                      Time elapsed: 00:13:48
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 842/1500 [0m                      

                       Computation: 105206 steps/s (collection: 0.800s, learning 0.134s)
             Mean action noise std: 2.82
          Mean value_function loss: 119.0566
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1999
                       Mean reward: 405.65
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 0.3837
    Episode_Reward/rotating_object: 86.8411
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.93s
                      Time elapsed: 00:13:49
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 843/1500 [0m                      

                       Computation: 100048 steps/s (collection: 0.797s, learning 0.186s)
             Mean action noise std: 2.82
          Mean value_function loss: 117.3116
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1959
                       Mean reward: 455.68
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.3847
    Episode_Reward/rotating_object: 83.7778
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.98s
                      Time elapsed: 00:13:50
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 844/1500 [0m                      

                       Computation: 104307 steps/s (collection: 0.832s, learning 0.110s)
             Mean action noise std: 2.83
          Mean value_function loss: 108.7283
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.1926
                       Mean reward: 384.91
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 0.3809
    Episode_Reward/rotating_object: 82.5173
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.94s
                      Time elapsed: 00:13:51
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 845/1500 [0m                      

                       Computation: 112436 steps/s (collection: 0.764s, learning 0.111s)
             Mean action noise std: 2.83
          Mean value_function loss: 112.4365
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.1950
                       Mean reward: 422.12
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.3877
    Episode_Reward/rotating_object: 82.0090
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.87s
                      Time elapsed: 00:13:52
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 846/1500 [0m                      

                       Computation: 108950 steps/s (collection: 0.755s, learning 0.147s)
             Mean action noise std: 2.83
          Mean value_function loss: 113.3316
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2024
                       Mean reward: 430.34
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 83.6703
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.90s
                      Time elapsed: 00:13:52
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 847/1500 [0m                      

                       Computation: 107059 steps/s (collection: 0.803s, learning 0.116s)
             Mean action noise std: 2.83
          Mean value_function loss: 103.1734
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2038
                       Mean reward: 422.54
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.3869
    Episode_Reward/rotating_object: 85.5644
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.92s
                      Time elapsed: 00:13:53
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 848/1500 [0m                      

                       Computation: 111029 steps/s (collection: 0.778s, learning 0.107s)
             Mean action noise std: 2.84
          Mean value_function loss: 104.5243
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.2076
                       Mean reward: 407.06
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.3856
    Episode_Reward/rotating_object: 85.0990
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.89s
                      Time elapsed: 00:13:54
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 849/1500 [0m                      

                       Computation: 112115 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 110.8672
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 18.2126
                       Mean reward: 418.75
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.3762
    Episode_Reward/rotating_object: 84.0132
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.88s
                      Time elapsed: 00:13:55
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 850/1500 [0m                      

                       Computation: 114465 steps/s (collection: 0.772s, learning 0.087s)
             Mean action noise std: 2.84
          Mean value_function loss: 108.2044
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.2182
                       Mean reward: 438.18
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.3833
    Episode_Reward/rotating_object: 85.0642
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.86s
                      Time elapsed: 00:13:56
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 851/1500 [0m                      

                       Computation: 108827 steps/s (collection: 0.789s, learning 0.114s)
             Mean action noise std: 2.84
          Mean value_function loss: 98.8457
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2223
                       Mean reward: 405.60
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.3773
    Episode_Reward/rotating_object: 84.0214
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.90s
                      Time elapsed: 00:13:57
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 852/1500 [0m                      

                       Computation: 98479 steps/s (collection: 0.904s, learning 0.094s)
             Mean action noise std: 2.84
          Mean value_function loss: 100.9101
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.2231
                       Mean reward: 430.91
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 84.6909
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 1.00s
                      Time elapsed: 00:13:58
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 853/1500 [0m                      

                       Computation: 110869 steps/s (collection: 0.799s, learning 0.088s)
             Mean action noise std: 2.84
          Mean value_function loss: 108.0710
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.2257
                       Mean reward: 386.93
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 79.7915
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.89s
                      Time elapsed: 00:13:59
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 854/1500 [0m                      

                       Computation: 106313 steps/s (collection: 0.781s, learning 0.144s)
             Mean action noise std: 2.85
          Mean value_function loss: 109.5733
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.2275
                       Mean reward: 424.70
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 85.2512
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.92s
                      Time elapsed: 00:14:00
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 855/1500 [0m                      

                       Computation: 104854 steps/s (collection: 0.806s, learning 0.132s)
             Mean action noise std: 2.85
          Mean value_function loss: 108.6415
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2349
                       Mean reward: 445.68
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.3805
    Episode_Reward/rotating_object: 83.7663
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.94s
                      Time elapsed: 00:14:01
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 856/1500 [0m                      

                       Computation: 112005 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 2.85
          Mean value_function loss: 105.3098
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.2487
                       Mean reward: 408.00
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 0.3773
    Episode_Reward/rotating_object: 84.5429
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.88s
                      Time elapsed: 00:14:02
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 857/1500 [0m                      

                       Computation: 114146 steps/s (collection: 0.758s, learning 0.103s)
             Mean action noise std: 2.85
          Mean value_function loss: 109.9060
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.2494
                       Mean reward: 439.74
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.3790
    Episode_Reward/rotating_object: 88.9758
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.86s
                      Time elapsed: 00:14:02
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 858/1500 [0m                      

                       Computation: 112566 steps/s (collection: 0.770s, learning 0.103s)
             Mean action noise std: 2.86
          Mean value_function loss: 113.4374
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.2605
                       Mean reward: 460.31
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.3812
    Episode_Reward/rotating_object: 86.2953
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.87s
                      Time elapsed: 00:14:03
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 859/1500 [0m                      

                       Computation: 108993 steps/s (collection: 0.763s, learning 0.139s)
             Mean action noise std: 2.86
          Mean value_function loss: 103.1749
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.2610
                       Mean reward: 461.33
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 88.5423
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.90s
                      Time elapsed: 00:14:04
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 860/1500 [0m                      

                       Computation: 96522 steps/s (collection: 0.901s, learning 0.118s)
             Mean action noise std: 2.86
          Mean value_function loss: 99.1316
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.2724
                       Mean reward: 457.11
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.3833
    Episode_Reward/rotating_object: 87.0874
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 1.02s
                      Time elapsed: 00:14:05
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 861/1500 [0m                      

                       Computation: 106174 steps/s (collection: 0.807s, learning 0.119s)
             Mean action noise std: 2.87
          Mean value_function loss: 113.8423
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2808
                       Mean reward: 403.44
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.3916
    Episode_Reward/rotating_object: 88.8908
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.93s
                      Time elapsed: 00:14:06
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 862/1500 [0m                      

                       Computation: 102722 steps/s (collection: 0.825s, learning 0.132s)
             Mean action noise std: 2.87
          Mean value_function loss: 108.2505
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.2849
                       Mean reward: 440.36
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.3855
    Episode_Reward/rotating_object: 88.0922
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.96s
                      Time elapsed: 00:14:07
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 863/1500 [0m                      

                       Computation: 107002 steps/s (collection: 0.820s, learning 0.099s)
             Mean action noise std: 2.87
          Mean value_function loss: 110.2422
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.2929
                       Mean reward: 425.64
               Mean episode length: 243.63
    Episode_Reward/reaching_object: 0.3842
    Episode_Reward/rotating_object: 86.1265
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.92s
                      Time elapsed: 00:14:08
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 864/1500 [0m                      

                       Computation: 109685 steps/s (collection: 0.783s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 116.8197
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.2960
                       Mean reward: 420.22
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.3896
    Episode_Reward/rotating_object: 83.7030
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.90s
                      Time elapsed: 00:14:09
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 865/1500 [0m                      

                       Computation: 107477 steps/s (collection: 0.806s, learning 0.109s)
             Mean action noise std: 2.87
          Mean value_function loss: 121.5271
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2924
                       Mean reward: 420.05
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 85.6877
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.91s
                      Time elapsed: 00:14:10
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 866/1500 [0m                      

                       Computation: 113424 steps/s (collection: 0.761s, learning 0.106s)
             Mean action noise std: 2.88
          Mean value_function loss: 118.6039
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.2941
                       Mean reward: 402.78
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3803
    Episode_Reward/rotating_object: 82.4809
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.87s
                      Time elapsed: 00:14:11
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 867/1500 [0m                      

                       Computation: 110438 steps/s (collection: 0.756s, learning 0.135s)
             Mean action noise std: 2.88
          Mean value_function loss: 116.8579
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.3005
                       Mean reward: 453.00
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 0.3938
    Episode_Reward/rotating_object: 88.0711
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.89s
                      Time elapsed: 00:14:12
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 868/1500 [0m                      

                       Computation: 103840 steps/s (collection: 0.768s, learning 0.179s)
             Mean action noise std: 2.88
          Mean value_function loss: 107.9931
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3093
                       Mean reward: 434.19
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.3866
    Episode_Reward/rotating_object: 84.1575
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.95s
                      Time elapsed: 00:14:13
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 869/1500 [0m                      

                       Computation: 99998 steps/s (collection: 0.804s, learning 0.179s)
             Mean action noise std: 2.88
          Mean value_function loss: 123.8086
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.3106
                       Mean reward: 455.19
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.3947
    Episode_Reward/rotating_object: 88.2429
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.98s
                      Time elapsed: 00:14:13
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 870/1500 [0m                      

                       Computation: 101145 steps/s (collection: 0.831s, learning 0.141s)
             Mean action noise std: 2.88
          Mean value_function loss: 115.6302
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3084
                       Mean reward: 466.35
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.3873
    Episode_Reward/rotating_object: 87.9748
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.97s
                      Time elapsed: 00:14:14
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 871/1500 [0m                      

                       Computation: 105804 steps/s (collection: 0.804s, learning 0.125s)
             Mean action noise std: 2.88
          Mean value_function loss: 127.6517
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3044
                       Mean reward: 436.43
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.3993
    Episode_Reward/rotating_object: 88.1004
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.93s
                      Time elapsed: 00:14:15
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 872/1500 [0m                      

                       Computation: 111033 steps/s (collection: 0.789s, learning 0.096s)
             Mean action noise std: 2.89
          Mean value_function loss: 112.2913
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.2964
                       Mean reward: 440.83
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 87.2619
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.89s
                      Time elapsed: 00:14:16
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 873/1500 [0m                      

                       Computation: 112842 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 108.6116
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.2965
                       Mean reward: 427.25
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.3930
    Episode_Reward/rotating_object: 86.6644
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.87s
                      Time elapsed: 00:14:17
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 874/1500 [0m                      

                       Computation: 116402 steps/s (collection: 0.754s, learning 0.090s)
             Mean action noise std: 2.89
          Mean value_function loss: 107.9676
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3053
                       Mean reward: 469.99
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 90.9362
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.84s
                      Time elapsed: 00:14:18
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 875/1500 [0m                      

                       Computation: 109009 steps/s (collection: 0.811s, learning 0.091s)
             Mean action noise std: 2.89
          Mean value_function loss: 118.1231
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3085
                       Mean reward: 469.50
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 83.9899
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.90s
                      Time elapsed: 00:14:19
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 876/1500 [0m                      

                       Computation: 109866 steps/s (collection: 0.781s, learning 0.113s)
             Mean action noise std: 2.89
          Mean value_function loss: 110.4987
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.3075
                       Mean reward: 411.44
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.3905
    Episode_Reward/rotating_object: 83.9471
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.89s
                      Time elapsed: 00:14:20
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 877/1500 [0m                      

                       Computation: 109100 steps/s (collection: 0.772s, learning 0.129s)
             Mean action noise std: 2.89
          Mean value_function loss: 107.2035
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3089
                       Mean reward: 459.62
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 86.3017
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.90s
                      Time elapsed: 00:14:21
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 878/1500 [0m                      

                       Computation: 114878 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 2.89
          Mean value_function loss: 113.0590
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3094
                       Mean reward: 442.36
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3916
    Episode_Reward/rotating_object: 86.5305
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.86s
                      Time elapsed: 00:14:22
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 879/1500 [0m                      

                       Computation: 106295 steps/s (collection: 0.756s, learning 0.169s)
             Mean action noise std: 2.89
          Mean value_function loss: 103.5155
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3083
                       Mean reward: 422.88
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 84.8840
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.92s
                      Time elapsed: 00:14:22
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 880/1500 [0m                      

                       Computation: 96279 steps/s (collection: 0.856s, learning 0.165s)
             Mean action noise std: 2.90
          Mean value_function loss: 104.5839
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3060
                       Mean reward: 466.36
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.3983
    Episode_Reward/rotating_object: 88.0244
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 1.02s
                      Time elapsed: 00:14:23
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 881/1500 [0m                      

                       Computation: 97004 steps/s (collection: 0.841s, learning 0.173s)
             Mean action noise std: 2.90
          Mean value_function loss: 110.8724
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.3092
                       Mean reward: 456.81
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 84.7205
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.01s
                      Time elapsed: 00:14:24
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 882/1500 [0m                      

                       Computation: 93513 steps/s (collection: 0.860s, learning 0.191s)
             Mean action noise std: 2.90
          Mean value_function loss: 111.8387
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.3161
                       Mean reward: 430.85
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 90.0472
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 1.05s
                      Time elapsed: 00:14:26
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 883/1500 [0m                      

                       Computation: 98939 steps/s (collection: 0.850s, learning 0.143s)
             Mean action noise std: 2.90
          Mean value_function loss: 109.5907
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.3149
                       Mean reward: 443.12
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.3907
    Episode_Reward/rotating_object: 86.3245
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.99s
                      Time elapsed: 00:14:27
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 884/1500 [0m                      

                       Computation: 100471 steps/s (collection: 0.803s, learning 0.176s)
             Mean action noise std: 2.90
          Mean value_function loss: 104.0160
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3147
                       Mean reward: 427.27
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 85.4961
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.98s
                      Time elapsed: 00:14:28
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 885/1500 [0m                      

                       Computation: 115842 steps/s (collection: 0.750s, learning 0.099s)
             Mean action noise std: 2.90
          Mean value_function loss: 107.2382
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.3191
                       Mean reward: 458.95
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 85.5220
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.85s
                      Time elapsed: 00:14:28
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 886/1500 [0m                      

                       Computation: 111352 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 2.91
          Mean value_function loss: 107.2982
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3254
                       Mean reward: 428.56
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 86.1748
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.88s
                      Time elapsed: 00:14:29
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 887/1500 [0m                      

                       Computation: 112431 steps/s (collection: 0.760s, learning 0.114s)
             Mean action noise std: 2.91
          Mean value_function loss: 107.4708
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3314
                       Mean reward: 406.05
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.3893
    Episode_Reward/rotating_object: 85.7183
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.87s
                      Time elapsed: 00:14:30
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 888/1500 [0m                      

                       Computation: 115342 steps/s (collection: 0.747s, learning 0.106s)
             Mean action noise std: 2.91
          Mean value_function loss: 107.6828
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.3290
                       Mean reward: 397.87
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 84.4250
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.85s
                      Time elapsed: 00:14:31
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 889/1500 [0m                      

                       Computation: 111002 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 2.91
          Mean value_function loss: 112.2154
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.3207
                       Mean reward: 428.87
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 86.7943
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.89s
                      Time elapsed: 00:14:32
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 890/1500 [0m                      

                       Computation: 113157 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 2.91
          Mean value_function loss: 114.3380
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3105
                       Mean reward: 441.26
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.3958
    Episode_Reward/rotating_object: 89.3859
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.87s
                      Time elapsed: 00:14:33
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 891/1500 [0m                      

                       Computation: 106063 steps/s (collection: 0.795s, learning 0.132s)
             Mean action noise std: 2.91
          Mean value_function loss: 116.9829
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.3116
                       Mean reward: 475.66
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 85.2401
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.93s
                      Time elapsed: 00:14:34
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 892/1500 [0m                      

                       Computation: 107022 steps/s (collection: 0.771s, learning 0.148s)
             Mean action noise std: 2.92
          Mean value_function loss: 107.3167
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3163
                       Mean reward: 490.18
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 93.1734
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.92s
                      Time elapsed: 00:14:35
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 893/1500 [0m                      

                       Computation: 105680 steps/s (collection: 0.775s, learning 0.155s)
             Mean action noise std: 2.92
          Mean value_function loss: 108.9132
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.3200
                       Mean reward: 463.35
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 89.5850
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.93s
                      Time elapsed: 00:14:36
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 894/1500 [0m                      

                       Computation: 110968 steps/s (collection: 0.796s, learning 0.090s)
             Mean action noise std: 2.92
          Mean value_function loss: 101.2925
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.3182
                       Mean reward: 430.84
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 88.3999
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.89s
                      Time elapsed: 00:14:36
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 895/1500 [0m                      

                       Computation: 111227 steps/s (collection: 0.769s, learning 0.115s)
             Mean action noise std: 2.92
          Mean value_function loss: 107.5444
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3270
                       Mean reward: 446.56
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 86.0871
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.88s
                      Time elapsed: 00:14:37
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 896/1500 [0m                      

                       Computation: 95560 steps/s (collection: 0.869s, learning 0.160s)
             Mean action noise std: 2.93
          Mean value_function loss: 107.4186
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 18.3321
                       Mean reward: 439.83
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.3960
    Episode_Reward/rotating_object: 89.0627
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 1.03s
                      Time elapsed: 00:14:38
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 897/1500 [0m                      

                       Computation: 104148 steps/s (collection: 0.807s, learning 0.137s)
             Mean action noise std: 2.93
          Mean value_function loss: 104.7923
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.3340
                       Mean reward: 490.29
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.3903
    Episode_Reward/rotating_object: 91.9002
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.94s
                      Time elapsed: 00:14:39
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 898/1500 [0m                      

                       Computation: 109253 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 2.93
          Mean value_function loss: 103.0433
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.3387
                       Mean reward: 445.06
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 88.1532
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.90s
                      Time elapsed: 00:14:40
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 899/1500 [0m                      

                       Computation: 107221 steps/s (collection: 0.812s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 107.0384
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.3427
                       Mean reward: 426.82
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.3892
    Episode_Reward/rotating_object: 86.7436
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.92s
                      Time elapsed: 00:14:41
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 900/1500 [0m                      

                       Computation: 114479 steps/s (collection: 0.753s, learning 0.106s)
             Mean action noise std: 2.94
          Mean value_function loss: 110.4815
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.3493
                       Mean reward: 479.88
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.3920
    Episode_Reward/rotating_object: 91.5742
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.86s
                      Time elapsed: 00:14:42
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 901/1500 [0m                      

                       Computation: 108082 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 2.94
          Mean value_function loss: 101.7953
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.3467
                       Mean reward: 432.77
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.3841
    Episode_Reward/rotating_object: 87.6014
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.91s
                      Time elapsed: 00:14:43
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 902/1500 [0m                      

                       Computation: 95304 steps/s (collection: 0.868s, learning 0.163s)
             Mean action noise std: 2.94
          Mean value_function loss: 105.8084
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3431
                       Mean reward: 465.42
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 90.8307
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 1.03s
                      Time elapsed: 00:14:44
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 903/1500 [0m                      

                       Computation: 103167 steps/s (collection: 0.834s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 109.7121
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.3537
                       Mean reward: 471.38
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.3882
    Episode_Reward/rotating_object: 88.0542
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.95s
                      Time elapsed: 00:14:45
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 904/1500 [0m                      

                       Computation: 108965 steps/s (collection: 0.788s, learning 0.114s)
             Mean action noise std: 2.95
          Mean value_function loss: 103.8742
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.3612
                       Mean reward: 454.53
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 91.7728
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.90s
                      Time elapsed: 00:14:46
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 905/1500 [0m                      

                       Computation: 108196 steps/s (collection: 0.798s, learning 0.110s)
             Mean action noise std: 2.95
          Mean value_function loss: 104.4676
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.3641
                       Mean reward: 462.63
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.3944
    Episode_Reward/rotating_object: 96.4208
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.91s
                      Time elapsed: 00:14:47
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 906/1500 [0m                      

                       Computation: 108284 steps/s (collection: 0.784s, learning 0.124s)
             Mean action noise std: 2.95
          Mean value_function loss: 102.5514
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.3613
                       Mean reward: 460.98
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.3899
    Episode_Reward/rotating_object: 88.4910
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.91s
                      Time elapsed: 00:14:48
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 907/1500 [0m                      

                       Computation: 108222 steps/s (collection: 0.782s, learning 0.127s)
             Mean action noise std: 2.95
          Mean value_function loss: 99.5337
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.3722
                       Mean reward: 382.55
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 0.3878
    Episode_Reward/rotating_object: 82.9565
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.91s
                      Time elapsed: 00:14:48
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 908/1500 [0m                      

                       Computation: 105533 steps/s (collection: 0.807s, learning 0.124s)
             Mean action noise std: 2.96
          Mean value_function loss: 103.2746
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3777
                       Mean reward: 411.11
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.3955
    Episode_Reward/rotating_object: 89.3025
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.93s
                      Time elapsed: 00:14:49
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 909/1500 [0m                      

                       Computation: 111360 steps/s (collection: 0.771s, learning 0.112s)
             Mean action noise std: 2.96
          Mean value_function loss: 102.7071
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.3831
                       Mean reward: 471.26
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 92.3145
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.88s
                      Time elapsed: 00:14:50
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 910/1500 [0m                      

                       Computation: 106717 steps/s (collection: 0.801s, learning 0.120s)
             Mean action noise std: 2.96
          Mean value_function loss: 97.8139
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 18.3837
                       Mean reward: 419.34
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 91.1260
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.92s
                      Time elapsed: 00:14:51
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 911/1500 [0m                      

                       Computation: 109985 steps/s (collection: 0.791s, learning 0.103s)
             Mean action noise std: 2.96
          Mean value_function loss: 111.9385
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3806
                       Mean reward: 438.05
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.3927
    Episode_Reward/rotating_object: 91.4646
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.89s
                      Time elapsed: 00:14:52
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 912/1500 [0m                      

                       Computation: 111612 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 2.96
          Mean value_function loss: 105.7743
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.3799
                       Mean reward: 415.56
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 91.9399
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.88s
                      Time elapsed: 00:14:53
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 913/1500 [0m                      

                       Computation: 103007 steps/s (collection: 0.849s, learning 0.106s)
             Mean action noise std: 2.97
          Mean value_function loss: 108.3605
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 18.3900
                       Mean reward: 435.23
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.3973
    Episode_Reward/rotating_object: 92.8326
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.95s
                      Time elapsed: 00:14:54
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 914/1500 [0m                      

                       Computation: 103827 steps/s (collection: 0.829s, learning 0.118s)
             Mean action noise std: 2.97
          Mean value_function loss: 109.7707
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4009
                       Mean reward: 448.77
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.3893
    Episode_Reward/rotating_object: 88.5251
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.95s
                      Time elapsed: 00:14:55
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 915/1500 [0m                      

                       Computation: 110919 steps/s (collection: 0.798s, learning 0.089s)
             Mean action noise std: 2.97
          Mean value_function loss: 113.4919
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.4105
                       Mean reward: 470.60
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3972
    Episode_Reward/rotating_object: 90.6116
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.89s
                      Time elapsed: 00:14:56
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 916/1500 [0m                      

                       Computation: 108281 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 2.98
          Mean value_function loss: 106.0327
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.4187
                       Mean reward: 420.43
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 88.9050
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.91s
                      Time elapsed: 00:14:57
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 917/1500 [0m                      

                       Computation: 104374 steps/s (collection: 0.818s, learning 0.124s)
             Mean action noise std: 2.98
          Mean value_function loss: 101.0384
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4334
                       Mean reward: 413.52
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 91.5859
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.94s
                      Time elapsed: 00:14:58
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 918/1500 [0m                      

                       Computation: 102793 steps/s (collection: 0.811s, learning 0.146s)
             Mean action noise std: 2.98
          Mean value_function loss: 112.5419
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4428
                       Mean reward: 489.45
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3979
    Episode_Reward/rotating_object: 90.0944
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.96s
                      Time elapsed: 00:14:59
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 919/1500 [0m                      

                       Computation: 96617 steps/s (collection: 0.833s, learning 0.185s)
             Mean action noise std: 2.98
          Mean value_function loss: 111.3406
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4403
                       Mean reward: 428.85
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 87.9286
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 1.02s
                      Time elapsed: 00:15:00
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 920/1500 [0m                      

                       Computation: 109624 steps/s (collection: 0.793s, learning 0.104s)
             Mean action noise std: 2.99
          Mean value_function loss: 112.7802
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.4446
                       Mean reward: 477.39
               Mean episode length: 241.19
    Episode_Reward/reaching_object: 0.3987
    Episode_Reward/rotating_object: 87.6398
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.90s
                      Time elapsed: 00:15:00
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 921/1500 [0m                      

                       Computation: 103111 steps/s (collection: 0.761s, learning 0.192s)
             Mean action noise std: 2.99
          Mean value_function loss: 117.0882
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4512
                       Mean reward: 461.35
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 91.8056
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.95s
                      Time elapsed: 00:15:01
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 922/1500 [0m                      

                       Computation: 105789 steps/s (collection: 0.776s, learning 0.154s)
             Mean action noise std: 2.99
          Mean value_function loss: 119.2812
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4607
                       Mean reward: 426.09
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.4022
    Episode_Reward/rotating_object: 90.8479
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.93s
                      Time elapsed: 00:15:02
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 923/1500 [0m                      

                       Computation: 114365 steps/s (collection: 0.753s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 118.7879
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4668
                       Mean reward: 426.18
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 86.7526
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.86s
                      Time elapsed: 00:15:03
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 924/1500 [0m                      

                       Computation: 114066 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 2.99
          Mean value_function loss: 116.8386
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.4718
                       Mean reward: 466.69
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 94.1612
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.86s
                      Time elapsed: 00:15:04
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 925/1500 [0m                      

                       Computation: 110068 steps/s (collection: 0.783s, learning 0.110s)
             Mean action noise std: 3.00
          Mean value_function loss: 115.7973
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4742
                       Mean reward: 453.41
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.3976
    Episode_Reward/rotating_object: 85.0181
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.89s
                      Time elapsed: 00:15:05
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 926/1500 [0m                      

                       Computation: 113199 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 3.00
          Mean value_function loss: 118.4989
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.4751
                       Mean reward: 417.50
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 88.8740
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.87s
                      Time elapsed: 00:15:06
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 927/1500 [0m                      

                       Computation: 107457 steps/s (collection: 0.798s, learning 0.117s)
             Mean action noise std: 3.00
          Mean value_function loss: 128.0938
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4872
                       Mean reward: 427.32
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.3994
    Episode_Reward/rotating_object: 86.8203
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.91s
                      Time elapsed: 00:15:07
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 928/1500 [0m                      

                       Computation: 113468 steps/s (collection: 0.772s, learning 0.095s)
             Mean action noise std: 3.01
          Mean value_function loss: 130.0543
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4963
                       Mean reward: 458.96
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 92.6948
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.87s
                      Time elapsed: 00:15:08
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 929/1500 [0m                      

                       Computation: 103618 steps/s (collection: 0.786s, learning 0.163s)
             Mean action noise std: 3.01
          Mean value_function loss: 126.8542
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.4996
                       Mean reward: 435.77
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.4058
    Episode_Reward/rotating_object: 85.1372
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.95s
                      Time elapsed: 00:15:09
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 930/1500 [0m                      

                       Computation: 112567 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 3.01
          Mean value_function loss: 122.2853
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.4987
                       Mean reward: 464.00
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.4141
    Episode_Reward/rotating_object: 92.1591
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.87s
                      Time elapsed: 00:15:09
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 931/1500 [0m                      

                       Computation: 111078 steps/s (collection: 0.771s, learning 0.114s)
             Mean action noise std: 3.01
          Mean value_function loss: 122.0578
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.4988
                       Mean reward: 432.90
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.4071
    Episode_Reward/rotating_object: 90.9582
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.88s
                      Time elapsed: 00:15:10
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 932/1500 [0m                      

                       Computation: 110525 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 3.01
          Mean value_function loss: 121.8373
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.4996
                       Mean reward: 423.42
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.4105
    Episode_Reward/rotating_object: 89.3468
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.89s
                      Time elapsed: 00:15:11
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 933/1500 [0m                      

                       Computation: 112040 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 129.4889
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4984
                       Mean reward: 419.08
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 0.4054
    Episode_Reward/rotating_object: 88.3764
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.88s
                      Time elapsed: 00:15:12
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 934/1500 [0m                      

                       Computation: 107317 steps/s (collection: 0.824s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 119.8036
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.5022
                       Mean reward: 439.67
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 0.4055
    Episode_Reward/rotating_object: 87.9349
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.92s
                      Time elapsed: 00:15:13
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 935/1500 [0m                      

                       Computation: 104097 steps/s (collection: 0.821s, learning 0.123s)
             Mean action noise std: 3.02
          Mean value_function loss: 116.6123
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.5064
                       Mean reward: 466.01
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.4118
    Episode_Reward/rotating_object: 92.8404
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.94s
                      Time elapsed: 00:15:14
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 936/1500 [0m                      

                       Computation: 105702 steps/s (collection: 0.826s, learning 0.104s)
             Mean action noise std: 3.02
          Mean value_function loss: 123.7693
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.5119
                       Mean reward: 444.05
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 93.0365
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.93s
                      Time elapsed: 00:15:15
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 937/1500 [0m                      

                       Computation: 110725 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 3.02
          Mean value_function loss: 111.8428
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.5160
                       Mean reward: 457.85
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.4085
    Episode_Reward/rotating_object: 92.2291
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.89s
                      Time elapsed: 00:15:16
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 938/1500 [0m                      

                       Computation: 107399 steps/s (collection: 0.803s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 111.1951
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5156
                       Mean reward: 458.42
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.3932
    Episode_Reward/rotating_object: 88.4826
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.92s
                      Time elapsed: 00:15:17
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 939/1500 [0m                      

                       Computation: 114249 steps/s (collection: 0.759s, learning 0.102s)
             Mean action noise std: 3.03
          Mean value_function loss: 103.9977
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.5218
                       Mean reward: 430.86
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 88.9749
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.86s
                      Time elapsed: 00:15:18
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 940/1500 [0m                      

                       Computation: 111650 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 3.03
          Mean value_function loss: 106.6835
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.5159
                       Mean reward: 417.42
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 90.2503
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.88s
                      Time elapsed: 00:15:18
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 941/1500 [0m                      

                       Computation: 105872 steps/s (collection: 0.777s, learning 0.152s)
             Mean action noise std: 3.03
          Mean value_function loss: 98.9378
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 18.5183
                       Mean reward: 461.68
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 90.3824
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.93s
                      Time elapsed: 00:15:19
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 942/1500 [0m                      

                       Computation: 110484 steps/s (collection: 0.791s, learning 0.098s)
             Mean action noise std: 3.03
          Mean value_function loss: 103.9911
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.5186
                       Mean reward: 454.63
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 89.5453
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.89s
                      Time elapsed: 00:15:20
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 943/1500 [0m                      

                       Computation: 112258 steps/s (collection: 0.785s, learning 0.091s)
             Mean action noise std: 3.03
          Mean value_function loss: 117.2716
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.5245
                       Mean reward: 415.61
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 87.9075
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.88s
                      Time elapsed: 00:15:21
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 944/1500 [0m                      

                       Computation: 105629 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 3.04
          Mean value_function loss: 113.3351
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.5384
                       Mean reward: 407.13
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 85.5375
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.93s
                      Time elapsed: 00:15:22
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 945/1500 [0m                      

                       Computation: 113937 steps/s (collection: 0.770s, learning 0.093s)
             Mean action noise std: 3.04
          Mean value_function loss: 109.7499
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5454
                       Mean reward: 477.21
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 90.4210
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.86s
                      Time elapsed: 00:15:23
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 946/1500 [0m                      

                       Computation: 106519 steps/s (collection: 0.805s, learning 0.117s)
             Mean action noise std: 3.04
          Mean value_function loss: 102.3594
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5476
                       Mean reward: 437.51
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3932
    Episode_Reward/rotating_object: 87.9033
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.92s
                      Time elapsed: 00:15:24
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 947/1500 [0m                      

                       Computation: 109892 steps/s (collection: 0.783s, learning 0.112s)
             Mean action noise std: 3.04
          Mean value_function loss: 106.7349
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.5464
                       Mean reward: 443.54
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.3951
    Episode_Reward/rotating_object: 90.9535
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.89s
                      Time elapsed: 00:15:25
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 948/1500 [0m                      

                       Computation: 109483 steps/s (collection: 0.787s, learning 0.111s)
             Mean action noise std: 3.04
          Mean value_function loss: 106.6273
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5508
                       Mean reward: 432.81
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 91.6610
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.90s
                      Time elapsed: 00:15:26
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 949/1500 [0m                      

                       Computation: 104752 steps/s (collection: 0.796s, learning 0.143s)
             Mean action noise std: 3.05
          Mean value_function loss: 103.7082
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.5562
                       Mean reward: 459.45
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.4019
    Episode_Reward/rotating_object: 92.5222
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.94s
                      Time elapsed: 00:15:27
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 950/1500 [0m                      

                       Computation: 109015 steps/s (collection: 0.798s, learning 0.104s)
             Mean action noise std: 3.05
          Mean value_function loss: 120.3523
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.5535
                       Mean reward: 456.86
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 89.2004
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.90s
                      Time elapsed: 00:15:27
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 951/1500 [0m                      

                       Computation: 110239 steps/s (collection: 0.792s, learning 0.100s)
             Mean action noise std: 3.05
          Mean value_function loss: 107.7826
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.5558
                       Mean reward: 481.19
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.3911
    Episode_Reward/rotating_object: 91.8299
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.89s
                      Time elapsed: 00:15:28
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 952/1500 [0m                      

                       Computation: 108108 steps/s (collection: 0.777s, learning 0.132s)
             Mean action noise std: 3.05
          Mean value_function loss: 112.5149
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5558
                       Mean reward: 477.78
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.4034
    Episode_Reward/rotating_object: 92.0619
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.91s
                      Time elapsed: 00:15:29
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 953/1500 [0m                      

                       Computation: 109469 steps/s (collection: 0.791s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 110.4408
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.5615
                       Mean reward: 479.50
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 92.6139
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.90s
                      Time elapsed: 00:15:30
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 954/1500 [0m                      

                       Computation: 108683 steps/s (collection: 0.802s, learning 0.102s)
             Mean action noise std: 3.06
          Mean value_function loss: 106.8761
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5703
                       Mean reward: 493.80
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 95.5603
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.90s
                      Time elapsed: 00:15:31
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 955/1500 [0m                      

                       Computation: 108803 steps/s (collection: 0.784s, learning 0.119s)
             Mean action noise std: 3.06
          Mean value_function loss: 114.0023
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.5718
                       Mean reward: 455.09
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.4025
    Episode_Reward/rotating_object: 93.1597
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.90s
                      Time elapsed: 00:15:32
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 956/1500 [0m                      

                       Computation: 98768 steps/s (collection: 0.881s, learning 0.114s)
             Mean action noise std: 3.06
          Mean value_function loss: 112.4795
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 18.5750
                       Mean reward: 483.98
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.4032
    Episode_Reward/rotating_object: 94.9532
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 1.00s
                      Time elapsed: 00:15:33
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 957/1500 [0m                      

                       Computation: 104544 steps/s (collection: 0.848s, learning 0.092s)
             Mean action noise std: 3.06
          Mean value_function loss: 118.9716
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.5804
                       Mean reward: 440.07
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 93.5359
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.94s
                      Time elapsed: 00:15:34
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 958/1500 [0m                      

                       Computation: 99082 steps/s (collection: 0.871s, learning 0.121s)
             Mean action noise std: 3.07
          Mean value_function loss: 122.6382
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5835
                       Mean reward: 426.31
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 90.2128
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.99s
                      Time elapsed: 00:15:35
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 959/1500 [0m                      

                       Computation: 106412 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 3.07
          Mean value_function loss: 127.5510
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.5847
                       Mean reward: 470.19
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 0.4120
    Episode_Reward/rotating_object: 95.6239
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.92s
                      Time elapsed: 00:15:36
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 960/1500 [0m                      

                       Computation: 104399 steps/s (collection: 0.849s, learning 0.093s)
             Mean action noise std: 3.07
          Mean value_function loss: 116.4009
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.5864
                       Mean reward: 445.78
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 92.3152
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.94s
                      Time elapsed: 00:15:37
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 961/1500 [0m                      

                       Computation: 111282 steps/s (collection: 0.791s, learning 0.092s)
             Mean action noise std: 3.07
          Mean value_function loss: 118.7949
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5923
                       Mean reward: 475.56
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.4054
    Episode_Reward/rotating_object: 93.0743
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.88s
                      Time elapsed: 00:15:38
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 962/1500 [0m                      

                       Computation: 106077 steps/s (collection: 0.811s, learning 0.116s)
             Mean action noise std: 3.08
          Mean value_function loss: 117.9730
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.6038
                       Mean reward: 469.30
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.4154
    Episode_Reward/rotating_object: 92.4795
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.93s
                      Time elapsed: 00:15:39
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 963/1500 [0m                      

                       Computation: 104533 steps/s (collection: 0.806s, learning 0.134s)
             Mean action noise std: 3.08
          Mean value_function loss: 119.2288
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.6145
                       Mean reward: 456.28
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.4141
    Episode_Reward/rotating_object: 94.3674
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.94s
                      Time elapsed: 00:15:40
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 964/1500 [0m                      

                       Computation: 104724 steps/s (collection: 0.821s, learning 0.118s)
             Mean action noise std: 3.08
          Mean value_function loss: 135.5851
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.6211
                       Mean reward: 420.41
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 0.4089
    Episode_Reward/rotating_object: 86.8218
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.94s
                      Time elapsed: 00:15:40
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 965/1500 [0m                      

                       Computation: 91103 steps/s (collection: 0.836s, learning 0.243s)
             Mean action noise std: 3.08
          Mean value_function loss: 128.8034
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.6171
                       Mean reward: 436.65
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 0.4045
    Episode_Reward/rotating_object: 90.9169
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 1.08s
                      Time elapsed: 00:15:42
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 966/1500 [0m                      

                       Computation: 101830 steps/s (collection: 0.870s, learning 0.095s)
             Mean action noise std: 3.09
          Mean value_function loss: 125.0974
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.6245
                       Mean reward: 446.05
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.4136
    Episode_Reward/rotating_object: 90.9066
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.97s
                      Time elapsed: 00:15:42
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 967/1500 [0m                      

                       Computation: 109015 steps/s (collection: 0.777s, learning 0.125s)
             Mean action noise std: 3.09
          Mean value_function loss: 129.5196
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.6272
                       Mean reward: 445.49
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.4071
    Episode_Reward/rotating_object: 88.4026
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.90s
                      Time elapsed: 00:15:43
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 968/1500 [0m                      

                       Computation: 103999 steps/s (collection: 0.831s, learning 0.115s)
             Mean action noise std: 3.09
          Mean value_function loss: 127.8070
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.6297
                       Mean reward: 436.24
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.4136
    Episode_Reward/rotating_object: 92.5560
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.95s
                      Time elapsed: 00:15:44
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 969/1500 [0m                      

                       Computation: 104127 steps/s (collection: 0.794s, learning 0.150s)
             Mean action noise std: 3.09
          Mean value_function loss: 121.7238
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6380
                       Mean reward: 473.38
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.4084
    Episode_Reward/rotating_object: 89.7438
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.94s
                      Time elapsed: 00:15:45
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 970/1500 [0m                      

                       Computation: 104643 steps/s (collection: 0.839s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 125.3714
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.6561
                       Mean reward: 476.63
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 0.4012
    Episode_Reward/rotating_object: 89.5882
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.94s
                      Time elapsed: 00:15:46
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 971/1500 [0m                      

                       Computation: 106612 steps/s (collection: 0.826s, learning 0.096s)
             Mean action noise std: 3.10
          Mean value_function loss: 126.8895
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6590
                       Mean reward: 452.92
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.4096
    Episode_Reward/rotating_object: 88.2365
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.92s
                      Time elapsed: 00:15:47
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 972/1500 [0m                      

                       Computation: 96111 steps/s (collection: 0.919s, learning 0.104s)
             Mean action noise std: 3.10
          Mean value_function loss: 120.7187
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6607
                       Mean reward: 437.43
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.4052
    Episode_Reward/rotating_object: 88.6630
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 1.02s
                      Time elapsed: 00:15:48
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 973/1500 [0m                      

                       Computation: 104873 steps/s (collection: 0.825s, learning 0.113s)
             Mean action noise std: 3.10
          Mean value_function loss: 123.1165
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6560
                       Mean reward: 425.11
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.4055
    Episode_Reward/rotating_object: 88.9811
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.94s
                      Time elapsed: 00:15:49
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 974/1500 [0m                      

                       Computation: 103330 steps/s (collection: 0.855s, learning 0.096s)
             Mean action noise std: 3.10
          Mean value_function loss: 119.6605
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6608
                       Mean reward: 413.70
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.4104
    Episode_Reward/rotating_object: 87.2275
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.95s
                      Time elapsed: 00:15:50
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 975/1500 [0m                      

                       Computation: 109853 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 117.0455
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.6605
                       Mean reward: 391.18
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 89.3838
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.89s
                      Time elapsed: 00:15:51
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 976/1500 [0m                      

                       Computation: 112620 steps/s (collection: 0.779s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 114.0903
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.6589
                       Mean reward: 458.28
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 90.2744
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.87s
                      Time elapsed: 00:15:52
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 977/1500 [0m                      

                       Computation: 104444 steps/s (collection: 0.840s, learning 0.102s)
             Mean action noise std: 3.10
          Mean value_function loss: 106.8850
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.6505
                       Mean reward: 414.10
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.4065
    Episode_Reward/rotating_object: 91.0687
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.94s
                      Time elapsed: 00:15:53
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 978/1500 [0m                      

                       Computation: 109418 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 3.11
          Mean value_function loss: 112.7358
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.6595
                       Mean reward: 403.21
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.4026
    Episode_Reward/rotating_object: 87.1691
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.90s
                      Time elapsed: 00:15:54
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 979/1500 [0m                      

                       Computation: 106010 steps/s (collection: 0.788s, learning 0.139s)
             Mean action noise std: 3.11
          Mean value_function loss: 101.7441
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.6754
                       Mean reward: 499.20
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.4046
    Episode_Reward/rotating_object: 91.8914
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.93s
                      Time elapsed: 00:15:55
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 980/1500 [0m                      

                       Computation: 111316 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 95.8012
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.6827
                       Mean reward: 468.31
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 87.6088
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.88s
                      Time elapsed: 00:15:55
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 981/1500 [0m                      

                       Computation: 103232 steps/s (collection: 0.800s, learning 0.152s)
             Mean action noise std: 3.12
          Mean value_function loss: 105.8526
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.6876
                       Mean reward: 507.64
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.4015
    Episode_Reward/rotating_object: 94.6694
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.95s
                      Time elapsed: 00:15:56
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 982/1500 [0m                      

                       Computation: 96216 steps/s (collection: 0.857s, learning 0.165s)
             Mean action noise std: 3.12
          Mean value_function loss: 102.9272
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.6980
                       Mean reward: 454.58
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.3917
    Episode_Reward/rotating_object: 91.1171
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 1.02s
                      Time elapsed: 00:15:57
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 983/1500 [0m                      

                       Computation: 96201 steps/s (collection: 0.863s, learning 0.159s)
             Mean action noise std: 3.12
          Mean value_function loss: 105.6453
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.7079
                       Mean reward: 463.96
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.3866
    Episode_Reward/rotating_object: 89.7756
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 1.02s
                      Time elapsed: 00:15:58
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 984/1500 [0m                      

                       Computation: 102660 steps/s (collection: 0.860s, learning 0.098s)
             Mean action noise std: 3.12
          Mean value_function loss: 97.7365
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.7187
                       Mean reward: 467.21
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.3910
    Episode_Reward/rotating_object: 93.5130
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.96s
                      Time elapsed: 00:15:59
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 985/1500 [0m                      

                       Computation: 105673 steps/s (collection: 0.778s, learning 0.153s)
             Mean action noise std: 3.13
          Mean value_function loss: 103.8509
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7245
                       Mean reward: 500.93
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 0.3843
    Episode_Reward/rotating_object: 95.3531
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.93s
                      Time elapsed: 00:16:00
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 986/1500 [0m                      

                       Computation: 104469 steps/s (collection: 0.800s, learning 0.141s)
             Mean action noise std: 3.13
          Mean value_function loss: 102.0803
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.7300
                       Mean reward: 458.17
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.3861
    Episode_Reward/rotating_object: 88.3376
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.94s
                      Time elapsed: 00:16:01
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 987/1500 [0m                      

                       Computation: 111271 steps/s (collection: 0.772s, learning 0.112s)
             Mean action noise std: 3.13
          Mean value_function loss: 100.1771
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7321
                       Mean reward: 439.74
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 89.7165
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.88s
                      Time elapsed: 00:16:02
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 988/1500 [0m                      

                       Computation: 103018 steps/s (collection: 0.817s, learning 0.138s)
             Mean action noise std: 3.14
          Mean value_function loss: 95.6944
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7423
                       Mean reward: 481.97
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.3897
    Episode_Reward/rotating_object: 92.6460
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.95s
                      Time elapsed: 00:16:03
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 989/1500 [0m                      

                       Computation: 106367 steps/s (collection: 0.816s, learning 0.108s)
             Mean action noise std: 3.14
          Mean value_function loss: 100.8012
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7568
                       Mean reward: 458.50
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 92.8686
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.92s
                      Time elapsed: 00:16:04
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 990/1500 [0m                      

                       Computation: 104617 steps/s (collection: 0.817s, learning 0.123s)
             Mean action noise std: 3.14
          Mean value_function loss: 103.3999
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.7629
                       Mean reward: 455.43
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3853
    Episode_Reward/rotating_object: 91.4322
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.94s
                      Time elapsed: 00:16:05
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 991/1500 [0m                      

                       Computation: 104687 steps/s (collection: 0.816s, learning 0.123s)
             Mean action noise std: 3.14
          Mean value_function loss: 97.7336
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.7770
                       Mean reward: 454.58
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 0.3802
    Episode_Reward/rotating_object: 87.1514
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.94s
                      Time elapsed: 00:16:06
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 992/1500 [0m                      

                       Computation: 109291 steps/s (collection: 0.805s, learning 0.095s)
             Mean action noise std: 3.15
          Mean value_function loss: 101.4139
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7797
                       Mean reward: 440.77
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.3860
    Episode_Reward/rotating_object: 89.7754
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.90s
                      Time elapsed: 00:16:07
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 993/1500 [0m                      

                       Computation: 108048 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 3.15
          Mean value_function loss: 107.2502
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.7731
                       Mean reward: 496.92
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.3849
    Episode_Reward/rotating_object: 92.4625
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.91s
                      Time elapsed: 00:16:08
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 994/1500 [0m                      

                       Computation: 95941 steps/s (collection: 0.882s, learning 0.143s)
             Mean action noise std: 3.15
          Mean value_function loss: 106.8621
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.7731
                       Mean reward: 461.42
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.3808
    Episode_Reward/rotating_object: 87.6547
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 1.02s
                      Time elapsed: 00:16:09
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 995/1500 [0m                      

                       Computation: 98475 steps/s (collection: 0.895s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 106.2459
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.7756
                       Mean reward: 450.96
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.3838
    Episode_Reward/rotating_object: 91.5929
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 1.00s
                      Time elapsed: 00:16:10
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 996/1500 [0m                      

                       Computation: 103342 steps/s (collection: 0.799s, learning 0.152s)
             Mean action noise std: 3.15
          Mean value_function loss: 92.6829
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.7778
                       Mean reward: 466.08
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.3822
    Episode_Reward/rotating_object: 89.3357
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.95s
                      Time elapsed: 00:16:11
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 997/1500 [0m                      

                       Computation: 107071 steps/s (collection: 0.804s, learning 0.114s)
             Mean action noise std: 3.15
          Mean value_function loss: 102.4295
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.7803
                       Mean reward: 444.84
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.3891
    Episode_Reward/rotating_object: 89.1223
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.92s
                      Time elapsed: 00:16:12
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 998/1500 [0m                      

                       Computation: 111106 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 3.16
          Mean value_function loss: 100.5174
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 18.7770
                       Mean reward: 463.36
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.3928
    Episode_Reward/rotating_object: 94.2606
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.88s
                      Time elapsed: 00:16:13
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 999/1500 [0m                      

                       Computation: 89550 steps/s (collection: 0.872s, learning 0.225s)
             Mean action noise std: 3.16
          Mean value_function loss: 102.6379
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.7826
                       Mean reward: 439.72
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 93.4194
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 1.10s
                      Time elapsed: 00:16:14
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1000/1500 [0m                     

                       Computation: 33790 steps/s (collection: 2.795s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 109.1193
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.8009
                       Mean reward: 481.93
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.3927
    Episode_Reward/rotating_object: 94.2166
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.91s
                      Time elapsed: 00:16:17
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1001/1500 [0m                     

                       Computation: 30924 steps/s (collection: 2.979s, learning 0.200s)
             Mean action noise std: 3.17
          Mean value_function loss: 108.7934
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.8182
                       Mean reward: 500.76
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.3896
    Episode_Reward/rotating_object: 94.3452
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.18s
                      Time elapsed: 00:16:20
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1002/1500 [0m                     

                       Computation: 31721 steps/s (collection: 2.979s, learning 0.120s)
             Mean action noise std: 3.17
          Mean value_function loss: 115.6387
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.8274
                       Mean reward: 407.19
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3728
    Episode_Reward/rotating_object: 88.1626
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.10s
                      Time elapsed: 00:16:23
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1003/1500 [0m                     

                       Computation: 32370 steps/s (collection: 2.901s, learning 0.136s)
             Mean action noise std: 3.17
          Mean value_function loss: 104.5125
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.8290
                       Mean reward: 483.44
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.3870
    Episode_Reward/rotating_object: 91.8337
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.04s
                      Time elapsed: 00:16:26
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1004/1500 [0m                     

                       Computation: 31713 steps/s (collection: 2.975s, learning 0.125s)
             Mean action noise std: 3.17
          Mean value_function loss: 112.5101
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.8272
                       Mean reward: 452.64
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 89.5438
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.10s
                      Time elapsed: 00:16:29
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1005/1500 [0m                     

                       Computation: 31751 steps/s (collection: 2.974s, learning 0.122s)
             Mean action noise std: 3.17
          Mean value_function loss: 108.4172
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8332
                       Mean reward: 462.99
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3830
    Episode_Reward/rotating_object: 90.9719
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.10s
                      Time elapsed: 00:16:32
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1006/1500 [0m                     

                       Computation: 30854 steps/s (collection: 3.040s, learning 0.146s)
             Mean action noise std: 3.17
          Mean value_function loss: 109.9736
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.8311
                       Mean reward: 411.56
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 87.4904
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.19s
                      Time elapsed: 00:16:35
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1007/1500 [0m                     

                       Computation: 33967 steps/s (collection: 2.772s, learning 0.122s)
             Mean action noise std: 3.18
          Mean value_function loss: 112.9615
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8280
                       Mean reward: 470.00
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 92.8634
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 2.89s
                      Time elapsed: 00:16:38
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1008/1500 [0m                     

                       Computation: 23578 steps/s (collection: 4.058s, learning 0.112s)
             Mean action noise std: 3.18
          Mean value_function loss: 109.4899
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.8287
                       Mean reward: 474.35
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.3893
    Episode_Reward/rotating_object: 94.3702
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 4.17s
                      Time elapsed: 00:16:42
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1009/1500 [0m                     

                       Computation: 108841 steps/s (collection: 0.787s, learning 0.117s)
             Mean action noise std: 3.18
          Mean value_function loss: 108.1701
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 18.8376
                       Mean reward: 460.91
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.3987
    Episode_Reward/rotating_object: 91.8148
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.90s
                      Time elapsed: 00:16:43
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1010/1500 [0m                     

                       Computation: 110354 steps/s (collection: 0.791s, learning 0.100s)
             Mean action noise std: 3.18
          Mean value_function loss: 111.1425
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.8405
                       Mean reward: 504.61
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.3993
    Episode_Reward/rotating_object: 94.4238
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.89s
                      Time elapsed: 00:16:44
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1011/1500 [0m                     

                       Computation: 97937 steps/s (collection: 0.805s, learning 0.198s)
             Mean action noise std: 3.18
          Mean value_function loss: 111.2727
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.8392
                       Mean reward: 465.10
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.3939
    Episode_Reward/rotating_object: 92.2229
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.00s
                      Time elapsed: 00:16:45
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1012/1500 [0m                     

                       Computation: 85445 steps/s (collection: 0.960s, learning 0.190s)
             Mean action noise std: 3.19
          Mean value_function loss: 110.7381
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8368
                       Mean reward: 460.17
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.3864
    Episode_Reward/rotating_object: 86.6772
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 1.15s
                      Time elapsed: 00:16:46
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1013/1500 [0m                     

                       Computation: 98756 steps/s (collection: 0.843s, learning 0.152s)
             Mean action noise std: 3.19
          Mean value_function loss: 105.7846
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.8441
                       Mean reward: 425.28
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.3944
    Episode_Reward/rotating_object: 88.2392
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.00s
                      Time elapsed: 00:16:47
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1014/1500 [0m                     

                       Computation: 88921 steps/s (collection: 0.861s, learning 0.244s)
             Mean action noise std: 3.19
          Mean value_function loss: 107.3485
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.8461
                       Mean reward: 472.96
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.3922
    Episode_Reward/rotating_object: 92.2884
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.11s
                      Time elapsed: 00:16:48
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1015/1500 [0m                     

                       Computation: 84437 steps/s (collection: 0.926s, learning 0.238s)
             Mean action noise std: 3.19
          Mean value_function loss: 115.1732
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.8527
                       Mean reward: 468.76
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.3893
    Episode_Reward/rotating_object: 92.4657
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.16s
                      Time elapsed: 00:16:50
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1016/1500 [0m                     

                       Computation: 80890 steps/s (collection: 1.001s, learning 0.215s)
             Mean action noise std: 3.19
          Mean value_function loss: 110.1042
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.8527
                       Mean reward: 435.28
               Mean episode length: 237.55
    Episode_Reward/reaching_object: 0.3913
    Episode_Reward/rotating_object: 88.4321
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.22s
                      Time elapsed: 00:16:51
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1017/1500 [0m                     

                       Computation: 85161 steps/s (collection: 1.049s, learning 0.105s)
             Mean action noise std: 3.20
          Mean value_function loss: 109.8009
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.8590
                       Mean reward: 455.12
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.3914
    Episode_Reward/rotating_object: 91.9083
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.15s
                      Time elapsed: 00:16:52
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1018/1500 [0m                     

                       Computation: 110419 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 3.20
          Mean value_function loss: 105.7517
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.8660
                       Mean reward: 476.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3954
    Episode_Reward/rotating_object: 87.8552
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.89s
                      Time elapsed: 00:16:53
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1019/1500 [0m                     

                       Computation: 102817 steps/s (collection: 0.828s, learning 0.128s)
             Mean action noise std: 3.20
          Mean value_function loss: 103.1478
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.8692
                       Mean reward: 480.19
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 93.7884
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 0.96s
                      Time elapsed: 00:16:54
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1020/1500 [0m                     

                       Computation: 107107 steps/s (collection: 0.815s, learning 0.103s)
             Mean action noise std: 3.20
          Mean value_function loss: 91.4129
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.8679
                       Mean reward: 492.38
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.3798
    Episode_Reward/rotating_object: 93.2811
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.92s
                      Time elapsed: 00:16:55
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1021/1500 [0m                     

                       Computation: 104331 steps/s (collection: 0.822s, learning 0.121s)
             Mean action noise std: 3.20
          Mean value_function loss: 101.2626
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.8670
                       Mean reward: 449.76
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 92.4167
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.94s
                      Time elapsed: 00:16:56
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1022/1500 [0m                     

                       Computation: 99189 steps/s (collection: 0.889s, learning 0.102s)
             Mean action noise std: 3.21
          Mean value_function loss: 107.2481
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8755
                       Mean reward: 498.69
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 92.3954
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.99s
                      Time elapsed: 00:16:57
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1023/1500 [0m                     

                       Computation: 101066 steps/s (collection: 0.813s, learning 0.160s)
             Mean action noise std: 3.21
          Mean value_function loss: 102.8600
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.8867
                       Mean reward: 429.45
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.3851
    Episode_Reward/rotating_object: 90.8859
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.97s
                      Time elapsed: 00:16:58
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1024/1500 [0m                     

                       Computation: 102849 steps/s (collection: 0.795s, learning 0.161s)
             Mean action noise std: 3.21
          Mean value_function loss: 100.4512
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.8960
                       Mean reward: 470.98
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.3869
    Episode_Reward/rotating_object: 91.0424
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.96s
                      Time elapsed: 00:16:59
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1025/1500 [0m                     

                       Computation: 104990 steps/s (collection: 0.795s, learning 0.141s)
             Mean action noise std: 3.22
          Mean value_function loss: 102.4535
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.9021
                       Mean reward: 494.83
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.3888
    Episode_Reward/rotating_object: 94.3764
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.94s
                      Time elapsed: 00:16:59
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1026/1500 [0m                     

                       Computation: 97299 steps/s (collection: 0.810s, learning 0.200s)
             Mean action noise std: 3.22
          Mean value_function loss: 109.4848
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.9026
                       Mean reward: 492.60
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.3891
    Episode_Reward/rotating_object: 92.2978
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.01s
                      Time elapsed: 00:17:00
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1027/1500 [0m                     

                       Computation: 98421 steps/s (collection: 0.847s, learning 0.152s)
             Mean action noise std: 3.22
          Mean value_function loss: 119.4509
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.9064
                       Mean reward: 466.74
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.3926
    Episode_Reward/rotating_object: 98.2311
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.00s
                      Time elapsed: 00:17:01
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1028/1500 [0m                     

                       Computation: 99948 steps/s (collection: 0.814s, learning 0.169s)
             Mean action noise std: 3.23
          Mean value_function loss: 114.0247
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.9181
                       Mean reward: 462.73
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 0.3850
    Episode_Reward/rotating_object: 90.6002
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.98s
                      Time elapsed: 00:17:02
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1029/1500 [0m                     

                       Computation: 95996 steps/s (collection: 0.844s, learning 0.180s)
             Mean action noise std: 3.23
          Mean value_function loss: 114.5703
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.9260
                       Mean reward: 427.80
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 93.2367
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.02s
                      Time elapsed: 00:17:03
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1030/1500 [0m                     

                       Computation: 90196 steps/s (collection: 0.924s, learning 0.166s)
             Mean action noise std: 3.23
          Mean value_function loss: 113.4359
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.9330
                       Mean reward: 448.09
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 91.8532
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.09s
                      Time elapsed: 00:17:05
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1031/1500 [0m                     

                       Computation: 107505 steps/s (collection: 0.811s, learning 0.103s)
             Mean action noise std: 3.23
          Mean value_function loss: 115.4382
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.9340
                       Mean reward: 469.37
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.3992
    Episode_Reward/rotating_object: 91.9285
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.91s
                      Time elapsed: 00:17:05
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1032/1500 [0m                     

                       Computation: 103281 steps/s (collection: 0.805s, learning 0.147s)
             Mean action noise std: 3.23
          Mean value_function loss: 109.1799
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9288
                       Mean reward: 498.23
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.4038
    Episode_Reward/rotating_object: 97.7759
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.95s
                      Time elapsed: 00:17:06
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1033/1500 [0m                     

                       Computation: 108362 steps/s (collection: 0.808s, learning 0.100s)
             Mean action noise std: 3.24
          Mean value_function loss: 113.3390
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.9322
                       Mean reward: 448.34
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 90.9609
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.91s
                      Time elapsed: 00:17:07
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1034/1500 [0m                     

                       Computation: 109677 steps/s (collection: 0.789s, learning 0.108s)
             Mean action noise std: 3.24
          Mean value_function loss: 103.1128
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.9392
                       Mean reward: 457.54
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 0.4000
    Episode_Reward/rotating_object: 95.0606
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.90s
                      Time elapsed: 00:17:08
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1035/1500 [0m                     

                       Computation: 108932 steps/s (collection: 0.798s, learning 0.104s)
             Mean action noise std: 3.24
          Mean value_function loss: 105.6221
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.9485
                       Mean reward: 508.55
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3993
    Episode_Reward/rotating_object: 94.7255
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.90s
                      Time elapsed: 00:17:09
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1036/1500 [0m                     

                       Computation: 107461 steps/s (collection: 0.815s, learning 0.100s)
             Mean action noise std: 3.25
          Mean value_function loss: 112.2976
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9502
                       Mean reward: 454.66
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 96.0280
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.91s
                      Time elapsed: 00:17:10
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1037/1500 [0m                     

                       Computation: 106234 steps/s (collection: 0.830s, learning 0.096s)
             Mean action noise std: 3.25
          Mean value_function loss: 105.5750
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.9610
                       Mean reward: 455.12
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 93.2858
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.93s
                      Time elapsed: 00:17:11
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1038/1500 [0m                     

                       Computation: 105894 steps/s (collection: 0.835s, learning 0.094s)
             Mean action noise std: 3.26
          Mean value_function loss: 107.9984
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.9708
                       Mean reward: 486.58
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.3878
    Episode_Reward/rotating_object: 94.6829
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.93s
                      Time elapsed: 00:17:12
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1039/1500 [0m                     

                       Computation: 100107 steps/s (collection: 0.839s, learning 0.143s)
             Mean action noise std: 3.26
          Mean value_function loss: 104.2304
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.9881
                       Mean reward: 481.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3881
    Episode_Reward/rotating_object: 91.8142
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.98s
                      Time elapsed: 00:17:13
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1040/1500 [0m                     

                       Computation: 101136 steps/s (collection: 0.867s, learning 0.105s)
             Mean action noise std: 3.26
          Mean value_function loss: 113.5492
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.9899
                       Mean reward: 476.43
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3913
    Episode_Reward/rotating_object: 95.6543
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.97s
                      Time elapsed: 00:17:14
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1041/1500 [0m                     

                       Computation: 105315 steps/s (collection: 0.795s, learning 0.138s)
             Mean action noise std: 3.26
          Mean value_function loss: 112.6866
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9974
                       Mean reward: 501.55
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.3882
    Episode_Reward/rotating_object: 93.7126
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.93s
                      Time elapsed: 00:17:15
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1042/1500 [0m                     

                       Computation: 98626 steps/s (collection: 0.868s, learning 0.129s)
             Mean action noise std: 3.26
          Mean value_function loss: 104.0434
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.9953
                       Mean reward: 475.49
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 93.5202
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.00s
                      Time elapsed: 00:17:16
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1043/1500 [0m                     

                       Computation: 100479 steps/s (collection: 0.874s, learning 0.105s)
             Mean action noise std: 3.27
          Mean value_function loss: 114.5461
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.9978
                       Mean reward: 456.89
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.3820
    Episode_Reward/rotating_object: 91.7857
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.98s
                      Time elapsed: 00:17:17
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1044/1500 [0m                     

                       Computation: 101526 steps/s (collection: 0.843s, learning 0.126s)
             Mean action noise std: 3.27
          Mean value_function loss: 109.0133
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 19.0039
                       Mean reward: 485.74
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 94.1581
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.97s
                      Time elapsed: 00:17:18
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1045/1500 [0m                     

                       Computation: 90873 steps/s (collection: 0.880s, learning 0.202s)
             Mean action noise std: 3.27
          Mean value_function loss: 101.8574
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0096
                       Mean reward: 461.41
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.3885
    Episode_Reward/rotating_object: 92.6944
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.08s
                      Time elapsed: 00:17:19
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1046/1500 [0m                     

                       Computation: 99864 steps/s (collection: 0.867s, learning 0.118s)
             Mean action noise std: 3.27
          Mean value_function loss: 98.9802
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.0164
                       Mean reward: 464.08
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.3899
    Episode_Reward/rotating_object: 93.1783
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.98s
                      Time elapsed: 00:17:20
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1047/1500 [0m                     

                       Computation: 109278 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 91.0579
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.0330
                       Mean reward: 496.17
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.3886
    Episode_Reward/rotating_object: 94.0619
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.90s
                      Time elapsed: 00:17:21
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1048/1500 [0m                     

                       Computation: 104328 steps/s (collection: 0.816s, learning 0.126s)
             Mean action noise std: 3.28
          Mean value_function loss: 101.8124
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.0416
                       Mean reward: 464.52
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 94.1730
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.94s
                      Time elapsed: 00:17:22
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1049/1500 [0m                     

                       Computation: 106469 steps/s (collection: 0.804s, learning 0.120s)
             Mean action noise std: 3.29
          Mean value_function loss: 101.5083
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0429
                       Mean reward: 453.17
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.3879
    Episode_Reward/rotating_object: 90.2773
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.92s
                      Time elapsed: 00:17:23
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1050/1500 [0m                     

                       Computation: 101093 steps/s (collection: 0.802s, learning 0.170s)
             Mean action noise std: 3.29
          Mean value_function loss: 98.2301
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.0473
                       Mean reward: 442.48
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 0.3749
    Episode_Reward/rotating_object: 91.1291
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.97s
                      Time elapsed: 00:17:24
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1051/1500 [0m                     

                       Computation: 101186 steps/s (collection: 0.788s, learning 0.184s)
             Mean action noise std: 3.29
          Mean value_function loss: 102.4167
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.0513
                       Mean reward: 454.48
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.3847
    Episode_Reward/rotating_object: 90.2011
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.97s
                      Time elapsed: 00:17:24
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1052/1500 [0m                     

                       Computation: 105257 steps/s (collection: 0.801s, learning 0.133s)
             Mean action noise std: 3.29
          Mean value_function loss: 110.6217
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.0530
                       Mean reward: 482.42
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.3868
    Episode_Reward/rotating_object: 92.6619
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.93s
                      Time elapsed: 00:17:25
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1053/1500 [0m                     

                       Computation: 83685 steps/s (collection: 0.935s, learning 0.240s)
             Mean action noise std: 3.29
          Mean value_function loss: 113.0090
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.0547
                       Mean reward: 464.14
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.3912
    Episode_Reward/rotating_object: 96.7869
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 1.17s
                      Time elapsed: 00:17:27
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1054/1500 [0m                     

                       Computation: 92884 steps/s (collection: 0.961s, learning 0.097s)
             Mean action noise std: 3.29
          Mean value_function loss: 107.1183
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.0496
                       Mean reward: 476.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3840
    Episode_Reward/rotating_object: 89.2572
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 1.06s
                      Time elapsed: 00:17:28
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1055/1500 [0m                     

                       Computation: 101466 steps/s (collection: 0.856s, learning 0.113s)
             Mean action noise std: 3.29
          Mean value_function loss: 97.9201
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.0431
                       Mean reward: 452.09
               Mean episode length: 241.45
    Episode_Reward/reaching_object: 0.3858
    Episode_Reward/rotating_object: 93.1224
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.97s
                      Time elapsed: 00:17:29
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1056/1500 [0m                     

                       Computation: 99072 steps/s (collection: 0.869s, learning 0.123s)
             Mean action noise std: 3.29
          Mean value_function loss: 113.4930
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.0405
                       Mean reward: 476.05
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.3985
    Episode_Reward/rotating_object: 96.0927
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.99s
                      Time elapsed: 00:17:30
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1057/1500 [0m                     

                       Computation: 102609 steps/s (collection: 0.815s, learning 0.143s)
             Mean action noise std: 3.30
          Mean value_function loss: 105.0364
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0446
                       Mean reward: 460.74
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 91.7853
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.96s
                      Time elapsed: 00:17:31
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1058/1500 [0m                     

                       Computation: 91164 steps/s (collection: 0.878s, learning 0.201s)
             Mean action noise std: 3.30
          Mean value_function loss: 98.5120
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.0466
                       Mean reward: 481.64
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 92.3760
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.08s
                      Time elapsed: 00:17:32
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1059/1500 [0m                     

                       Computation: 88443 steps/s (collection: 0.916s, learning 0.196s)
             Mean action noise std: 3.30
          Mean value_function loss: 109.3791
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.0500
                       Mean reward: 449.76
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 92.9994
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 1.11s
                      Time elapsed: 00:17:33
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1060/1500 [0m                     

                       Computation: 97157 steps/s (collection: 0.859s, learning 0.153s)
             Mean action noise std: 3.31
          Mean value_function loss: 101.4790
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.0585
                       Mean reward: 425.32
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.3852
    Episode_Reward/rotating_object: 90.1099
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 1.01s
                      Time elapsed: 00:17:34
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1061/1500 [0m                     

                       Computation: 104346 steps/s (collection: 0.830s, learning 0.112s)
             Mean action noise std: 3.31
          Mean value_function loss: 103.6531
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.0791
                       Mean reward: 521.21
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.3975
    Episode_Reward/rotating_object: 93.9515
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.94s
                      Time elapsed: 00:17:35
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1062/1500 [0m                     

                       Computation: 107105 steps/s (collection: 0.819s, learning 0.099s)
             Mean action noise std: 3.32
          Mean value_function loss: 111.6474
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 19.0861
                       Mean reward: 506.53
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 0.3924
    Episode_Reward/rotating_object: 96.7994
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.92s
                      Time elapsed: 00:17:36
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1063/1500 [0m                     

                       Computation: 111800 steps/s (collection: 0.774s, learning 0.105s)
             Mean action noise std: 3.32
          Mean value_function loss: 102.9004
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.0875
                       Mean reward: 477.67
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 93.2023
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.88s
                      Time elapsed: 00:17:37
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1064/1500 [0m                     

                       Computation: 108816 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 3.32
          Mean value_function loss: 101.8059
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.0950
                       Mean reward: 466.09
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 0.3898
    Episode_Reward/rotating_object: 90.6345
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.90s
                      Time elapsed: 00:17:37
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1065/1500 [0m                     

                       Computation: 101623 steps/s (collection: 0.834s, learning 0.134s)
             Mean action noise std: 3.32
          Mean value_function loss: 111.1825
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.1127
                       Mean reward: 500.46
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.3996
    Episode_Reward/rotating_object: 97.9219
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.97s
                      Time elapsed: 00:17:38
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1066/1500 [0m                     

                       Computation: 104316 steps/s (collection: 0.799s, learning 0.143s)
             Mean action noise std: 3.33
          Mean value_function loss: 100.6111
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.1183
                       Mean reward: 466.82
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.3979
    Episode_Reward/rotating_object: 97.0069
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.94s
                      Time elapsed: 00:17:39
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1067/1500 [0m                     

                       Computation: 104393 steps/s (collection: 0.819s, learning 0.122s)
             Mean action noise std: 3.33
          Mean value_function loss: 100.6575
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.1238
                       Mean reward: 432.19
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.3967
    Episode_Reward/rotating_object: 92.6346
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.94s
                      Time elapsed: 00:17:40
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1068/1500 [0m                     

                       Computation: 108819 steps/s (collection: 0.786s, learning 0.118s)
             Mean action noise std: 3.33
          Mean value_function loss: 89.6574
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.1246
                       Mean reward: 466.31
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 95.2672
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.90s
                      Time elapsed: 00:17:41
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1069/1500 [0m                     

                       Computation: 105501 steps/s (collection: 0.808s, learning 0.124s)
             Mean action noise std: 3.33
          Mean value_function loss: 99.0465
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.1279
                       Mean reward: 453.65
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 94.5106
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.93s
                      Time elapsed: 00:17:42
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1070/1500 [0m                     

                       Computation: 111225 steps/s (collection: 0.776s, learning 0.108s)
             Mean action noise std: 3.34
          Mean value_function loss: 103.5348
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.1339
                       Mean reward: 428.60
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 0.3888
    Episode_Reward/rotating_object: 94.1371
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.88s
                      Time elapsed: 00:17:43
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1071/1500 [0m                     

                       Computation: 102990 steps/s (collection: 0.819s, learning 0.135s)
             Mean action noise std: 3.34
          Mean value_function loss: 108.9879
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1416
                       Mean reward: 508.36
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.4015
    Episode_Reward/rotating_object: 96.0158
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.95s
                      Time elapsed: 00:17:44
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1072/1500 [0m                     

                       Computation: 109739 steps/s (collection: 0.771s, learning 0.125s)
             Mean action noise std: 3.34
          Mean value_function loss: 102.0992
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.1488
                       Mean reward: 486.46
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 97.0076
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.90s
                      Time elapsed: 00:17:45
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1073/1500 [0m                     

                       Computation: 108534 steps/s (collection: 0.804s, learning 0.102s)
             Mean action noise std: 3.34
          Mean value_function loss: 106.4674
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.1465
                       Mean reward: 478.11
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 93.2261
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.91s
                      Time elapsed: 00:17:46
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1074/1500 [0m                     

                       Computation: 91214 steps/s (collection: 0.976s, learning 0.102s)
             Mean action noise std: 3.34
          Mean value_function loss: 103.5523
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.1520
                       Mean reward: 502.00
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 0.3919
    Episode_Reward/rotating_object: 96.7350
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.08s
                      Time elapsed: 00:17:47
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1075/1500 [0m                     

                       Computation: 108266 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 3.35
          Mean value_function loss: 113.8037
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.1618
                       Mean reward: 480.97
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.3989
    Episode_Reward/rotating_object: 97.3959
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.91s
                      Time elapsed: 00:17:48
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1076/1500 [0m                     

                       Computation: 107574 steps/s (collection: 0.791s, learning 0.123s)
             Mean action noise std: 3.35
          Mean value_function loss: 100.4587
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.1723
                       Mean reward: 506.90
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.3960
    Episode_Reward/rotating_object: 96.8661
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.91s
                      Time elapsed: 00:17:49
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1077/1500 [0m                     

                       Computation: 108388 steps/s (collection: 0.789s, learning 0.118s)
             Mean action noise std: 3.35
          Mean value_function loss: 103.1282
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.1730
                       Mean reward: 488.93
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3969
    Episode_Reward/rotating_object: 92.0342
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.91s
                      Time elapsed: 00:17:50
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1078/1500 [0m                     

                       Computation: 105273 steps/s (collection: 0.791s, learning 0.143s)
             Mean action noise std: 3.35
          Mean value_function loss: 97.0614
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.1727
                       Mean reward: 498.70
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.4000
    Episode_Reward/rotating_object: 98.9087
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.93s
                      Time elapsed: 00:17:50
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1079/1500 [0m                     

                       Computation: 104956 steps/s (collection: 0.789s, learning 0.148s)
             Mean action noise std: 3.36
          Mean value_function loss: 103.3399
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.1763
                       Mean reward: 464.01
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 96.7266
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.94s
                      Time elapsed: 00:17:51
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1080/1500 [0m                     

                       Computation: 108276 steps/s (collection: 0.773s, learning 0.135s)
             Mean action noise std: 3.36
          Mean value_function loss: 104.8319
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1783
                       Mean reward: 431.25
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.3905
    Episode_Reward/rotating_object: 91.0758
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.91s
                      Time elapsed: 00:17:52
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1081/1500 [0m                     

                       Computation: 107842 steps/s (collection: 0.791s, learning 0.121s)
             Mean action noise std: 3.36
          Mean value_function loss: 103.3555
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.1867
                       Mean reward: 528.09
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.3933
    Episode_Reward/rotating_object: 94.4252
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.91s
                      Time elapsed: 00:17:53
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1082/1500 [0m                     

                       Computation: 111169 steps/s (collection: 0.767s, learning 0.117s)
             Mean action noise std: 3.36
          Mean value_function loss: 101.0530
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.1935
                       Mean reward: 456.60
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3894
    Episode_Reward/rotating_object: 94.9068
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.88s
                      Time elapsed: 00:17:54
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1083/1500 [0m                     

                       Computation: 109154 steps/s (collection: 0.791s, learning 0.110s)
             Mean action noise std: 3.36
          Mean value_function loss: 100.8980
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.2011
                       Mean reward: 435.93
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 94.0085
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.90s
                      Time elapsed: 00:17:55
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1084/1500 [0m                     

                       Computation: 111145 steps/s (collection: 0.790s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 107.5263
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.2061
                       Mean reward: 459.66
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 0.3850
    Episode_Reward/rotating_object: 93.3291
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.88s
                      Time elapsed: 00:17:56
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1085/1500 [0m                     

                       Computation: 93515 steps/s (collection: 0.930s, learning 0.122s)
             Mean action noise std: 3.37
          Mean value_function loss: 107.8222
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.2070
                       Mean reward: 474.64
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3866
    Episode_Reward/rotating_object: 91.6000
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 1.05s
                      Time elapsed: 00:17:57
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1086/1500 [0m                     

                       Computation: 109596 steps/s (collection: 0.784s, learning 0.113s)
             Mean action noise std: 3.37
          Mean value_function loss: 97.1336
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.2107
                       Mean reward: 524.24
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 97.8345
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.90s
                      Time elapsed: 00:17:58
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1087/1500 [0m                     

                       Computation: 106226 steps/s (collection: 0.814s, learning 0.112s)
             Mean action noise std: 3.38
          Mean value_function loss: 109.0674
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.2121
                       Mean reward: 488.68
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3965
    Episode_Reward/rotating_object: 96.1981
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.93s
                      Time elapsed: 00:17:59
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1088/1500 [0m                     

                       Computation: 109350 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 3.38
          Mean value_function loss: 98.9660
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.2131
                       Mean reward: 513.75
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 95.7028
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.90s
                      Time elapsed: 00:18:00
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1089/1500 [0m                     

                       Computation: 105550 steps/s (collection: 0.814s, learning 0.118s)
             Mean action noise std: 3.38
          Mean value_function loss: 103.6459
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.2156
                       Mean reward: 471.68
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 91.1907
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.93s
                      Time elapsed: 00:18:01
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1090/1500 [0m                     

                       Computation: 95099 steps/s (collection: 0.868s, learning 0.165s)
             Mean action noise std: 3.38
          Mean value_function loss: 103.1683
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.2144
                       Mean reward: 482.10
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.3998
    Episode_Reward/rotating_object: 97.2274
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.03s
                      Time elapsed: 00:18:02
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1091/1500 [0m                     

                       Computation: 105603 steps/s (collection: 0.804s, learning 0.127s)
             Mean action noise std: 3.38
          Mean value_function loss: 112.2266
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 19.2185
                       Mean reward: 484.94
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 0.4035
    Episode_Reward/rotating_object: 99.8974
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.93s
                      Time elapsed: 00:18:03
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1092/1500 [0m                     

                       Computation: 98298 steps/s (collection: 0.813s, learning 0.187s)
             Mean action noise std: 3.39
          Mean value_function loss: 102.2427
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.2236
                       Mean reward: 503.93
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 100.5695
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.00s
                      Time elapsed: 00:18:04
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1093/1500 [0m                     

                       Computation: 102121 steps/s (collection: 0.818s, learning 0.144s)
             Mean action noise std: 3.39
          Mean value_function loss: 109.6881
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.2245
                       Mean reward: 527.95
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.3971
    Episode_Reward/rotating_object: 99.0542
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.96s
                      Time elapsed: 00:18:05
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1094/1500 [0m                     

                       Computation: 101378 steps/s (collection: 0.838s, learning 0.132s)
             Mean action noise std: 3.39
          Mean value_function loss: 115.2438
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.2282
                       Mean reward: 521.40
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.3901
    Episode_Reward/rotating_object: 99.6055
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.97s
                      Time elapsed: 00:18:06
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1095/1500 [0m                     

                       Computation: 90990 steps/s (collection: 0.895s, learning 0.185s)
             Mean action noise std: 3.40
          Mean value_function loss: 108.6405
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.2307
                       Mean reward: 457.78
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.4007
    Episode_Reward/rotating_object: 97.5953
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 1.08s
                      Time elapsed: 00:18:07
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1096/1500 [0m                     

                       Computation: 98970 steps/s (collection: 0.853s, learning 0.140s)
             Mean action noise std: 3.40
          Mean value_function loss: 117.0263
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 19.2299
                       Mean reward: 475.32
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.3943
    Episode_Reward/rotating_object: 92.5034
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.99s
                      Time elapsed: 00:18:08
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1097/1500 [0m                     

                       Computation: 107459 steps/s (collection: 0.812s, learning 0.103s)
             Mean action noise std: 3.40
          Mean value_function loss: 105.9971
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.2312
                       Mean reward: 442.35
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.3976
    Episode_Reward/rotating_object: 95.0571
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.91s
                      Time elapsed: 00:18:09
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1098/1500 [0m                     

                       Computation: 105954 steps/s (collection: 0.788s, learning 0.140s)
             Mean action noise std: 3.40
          Mean value_function loss: 111.4008
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.2273
                       Mean reward: 478.05
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 94.8560
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.93s
                      Time elapsed: 00:18:09
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1099/1500 [0m                     

                       Computation: 105082 steps/s (collection: 0.788s, learning 0.148s)
             Mean action noise std: 3.40
          Mean value_function loss: 107.1797
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2235
                       Mean reward: 488.22
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 93.1150
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.94s
                      Time elapsed: 00:18:10
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1100/1500 [0m                     

                       Computation: 107582 steps/s (collection: 0.801s, learning 0.113s)
             Mean action noise std: 3.40
          Mean value_function loss: 110.2870
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.2413
                       Mean reward: 484.25
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.4025
    Episode_Reward/rotating_object: 94.4381
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.91s
                      Time elapsed: 00:18:11
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1101/1500 [0m                     

                       Computation: 113499 steps/s (collection: 0.767s, learning 0.099s)
             Mean action noise std: 3.41
          Mean value_function loss: 106.5549
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.2429
                       Mean reward: 526.23
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.3979
    Episode_Reward/rotating_object: 96.3469
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.87s
                      Time elapsed: 00:18:12
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1102/1500 [0m                     

                       Computation: 98797 steps/s (collection: 0.862s, learning 0.133s)
             Mean action noise std: 3.41
          Mean value_function loss: 107.4869
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.2490
                       Mean reward: 470.54
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.3993
    Episode_Reward/rotating_object: 94.9665
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.00s
                      Time elapsed: 00:18:13
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1103/1500 [0m                     

                       Computation: 108388 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 3.41
          Mean value_function loss: 100.5978
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.2552
                       Mean reward: 490.92
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.3926
    Episode_Reward/rotating_object: 91.8609
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.91s
                      Time elapsed: 00:18:14
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1104/1500 [0m                     

                       Computation: 105151 steps/s (collection: 0.823s, learning 0.112s)
             Mean action noise std: 3.41
          Mean value_function loss: 107.0527
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.2538
                       Mean reward: 462.64
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 95.8307
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.93s
                      Time elapsed: 00:18:15
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1105/1500 [0m                     

                       Computation: 110238 steps/s (collection: 0.774s, learning 0.118s)
             Mean action noise std: 3.41
          Mean value_function loss: 103.3049
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.2548
                       Mean reward: 491.32
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.3956
    Episode_Reward/rotating_object: 97.8530
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.89s
                      Time elapsed: 00:18:16
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1106/1500 [0m                     

                       Computation: 93936 steps/s (collection: 0.887s, learning 0.160s)
             Mean action noise std: 3.42
          Mean value_function loss: 111.4075
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.2594
                       Mean reward: 472.45
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.3975
    Episode_Reward/rotating_object: 94.6040
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.05s
                      Time elapsed: 00:18:17
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1107/1500 [0m                     

                       Computation: 101460 steps/s (collection: 0.847s, learning 0.121s)
             Mean action noise std: 3.42
          Mean value_function loss: 104.4411
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.2626
                       Mean reward: 457.13
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 93.5236
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.97s
                      Time elapsed: 00:18:18
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1108/1500 [0m                     

                       Computation: 108306 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 3.43
          Mean value_function loss: 110.8867
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.2713
                       Mean reward: 502.93
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 98.3365
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.91s
                      Time elapsed: 00:18:19
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1109/1500 [0m                     

                       Computation: 104646 steps/s (collection: 0.817s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 107.5203
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.2691
                       Mean reward: 467.77
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 97.1113
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.94s
                      Time elapsed: 00:18:20
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1110/1500 [0m                     

                       Computation: 106898 steps/s (collection: 0.812s, learning 0.108s)
             Mean action noise std: 3.43
          Mean value_function loss: 104.3104
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.2724
                       Mean reward: 500.84
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.4019
    Episode_Reward/rotating_object: 97.5554
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.92s
                      Time elapsed: 00:18:21
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1111/1500 [0m                     

                       Computation: 105758 steps/s (collection: 0.813s, learning 0.116s)
             Mean action noise std: 3.44
          Mean value_function loss: 105.9878
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.2871
                       Mean reward: 493.39
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.4028
    Episode_Reward/rotating_object: 97.5956
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.93s
                      Time elapsed: 00:18:22
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1112/1500 [0m                     

                       Computation: 103468 steps/s (collection: 0.812s, learning 0.139s)
             Mean action noise std: 3.44
          Mean value_function loss: 107.8516
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.3009
                       Mean reward: 461.92
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 100.6124
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.95s
                      Time elapsed: 00:18:23
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1113/1500 [0m                     

                       Computation: 103910 steps/s (collection: 0.789s, learning 0.157s)
             Mean action noise std: 3.45
          Mean value_function loss: 105.6640
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3190
                       Mean reward: 485.28
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 97.5357
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.95s
                      Time elapsed: 00:18:23
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1114/1500 [0m                     

                       Computation: 110894 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 3.45
          Mean value_function loss: 113.2343
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3282
                       Mean reward: 478.67
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 94.7752
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.89s
                      Time elapsed: 00:18:24
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1115/1500 [0m                     

                       Computation: 102387 steps/s (collection: 0.812s, learning 0.149s)
             Mean action noise std: 3.45
          Mean value_function loss: 122.8957
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.3312
                       Mean reward: 477.82
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.3920
    Episode_Reward/rotating_object: 94.9733
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.96s
                      Time elapsed: 00:18:25
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1116/1500 [0m                     

                       Computation: 100887 steps/s (collection: 0.824s, learning 0.151s)
             Mean action noise std: 3.46
          Mean value_function loss: 100.5123
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.3393
                       Mean reward: 479.78
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 95.2869
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.97s
                      Time elapsed: 00:18:26
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1117/1500 [0m                     

                       Computation: 104366 steps/s (collection: 0.823s, learning 0.119s)
             Mean action noise std: 3.46
          Mean value_function loss: 102.3873
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.3456
                       Mean reward: 524.17
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 98.6291
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.94s
                      Time elapsed: 00:18:27
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1118/1500 [0m                     

                       Computation: 105255 steps/s (collection: 0.827s, learning 0.107s)
             Mean action noise std: 3.46
          Mean value_function loss: 106.7862
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 19.3628
                       Mean reward: 459.11
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.3870
    Episode_Reward/rotating_object: 91.4638
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.93s
                      Time elapsed: 00:18:28
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1119/1500 [0m                     

                       Computation: 107669 steps/s (collection: 0.809s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 109.3851
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.3758
                       Mean reward: 501.39
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.3930
    Episode_Reward/rotating_object: 96.1276
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.91s
                      Time elapsed: 00:18:29
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1120/1500 [0m                     

                       Computation: 106709 steps/s (collection: 0.784s, learning 0.137s)
             Mean action noise std: 3.47
          Mean value_function loss: 103.9614
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.3792
                       Mean reward: 475.72
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 0.3942
    Episode_Reward/rotating_object: 98.4144
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.92s
                      Time elapsed: 00:18:30
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1121/1500 [0m                     

                       Computation: 110401 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 108.3850
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.3814
                       Mean reward: 466.83
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.3836
    Episode_Reward/rotating_object: 95.9615
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.89s
                      Time elapsed: 00:18:31
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1122/1500 [0m                     

                       Computation: 105167 steps/s (collection: 0.816s, learning 0.119s)
             Mean action noise std: 3.47
          Mean value_function loss: 96.6897
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.3800
                       Mean reward: 460.34
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 96.1196
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.93s
                      Time elapsed: 00:18:32
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1123/1500 [0m                     

                       Computation: 86146 steps/s (collection: 1.025s, learning 0.116s)
             Mean action noise std: 3.47
          Mean value_function loss: 104.5151
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.3833
                       Mean reward: 445.37
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 0.3878
    Episode_Reward/rotating_object: 94.0157
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 1.14s
                      Time elapsed: 00:18:33
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1124/1500 [0m                     

                       Computation: 102579 steps/s (collection: 0.854s, learning 0.105s)
             Mean action noise std: 3.48
          Mean value_function loss: 111.1352
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.3926
                       Mean reward: 484.05
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.3950
    Episode_Reward/rotating_object: 101.5688
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.96s
                      Time elapsed: 00:18:34
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1125/1500 [0m                     

                       Computation: 108090 steps/s (collection: 0.816s, learning 0.093s)
             Mean action noise std: 3.48
          Mean value_function loss: 98.1161
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3997
                       Mean reward: 530.97
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 94.2333
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.91s
                      Time elapsed: 00:18:35
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1126/1500 [0m                     

                       Computation: 111832 steps/s (collection: 0.785s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 103.8106
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 19.4090
                       Mean reward: 469.64
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.3922
    Episode_Reward/rotating_object: 93.7654
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.88s
                      Time elapsed: 00:18:36
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1127/1500 [0m                     

                       Computation: 106203 steps/s (collection: 0.831s, learning 0.095s)
             Mean action noise std: 3.48
          Mean value_function loss: 101.7406
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.4064
                       Mean reward: 452.04
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.3926
    Episode_Reward/rotating_object: 95.8385
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.93s
                      Time elapsed: 00:18:37
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1128/1500 [0m                     

                       Computation: 112462 steps/s (collection: 0.773s, learning 0.102s)
             Mean action noise std: 3.48
          Mean value_function loss: 108.5921
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.3982
                       Mean reward: 494.05
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 99.3853
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.87s
                      Time elapsed: 00:18:38
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1129/1500 [0m                     

                       Computation: 99908 steps/s (collection: 0.815s, learning 0.168s)
             Mean action noise std: 3.49
          Mean value_function loss: 110.8091
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3909
                       Mean reward: 523.24
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.3923
    Episode_Reward/rotating_object: 102.0996
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.98s
                      Time elapsed: 00:18:39
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1130/1500 [0m                     

                       Computation: 103840 steps/s (collection: 0.803s, learning 0.144s)
             Mean action noise std: 3.49
          Mean value_function loss: 110.7969
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.3982
                       Mean reward: 486.81
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.3904
    Episode_Reward/rotating_object: 98.9889
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.95s
                      Time elapsed: 00:18:39
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1131/1500 [0m                     

                       Computation: 104639 steps/s (collection: 0.812s, learning 0.127s)
             Mean action noise std: 3.49
          Mean value_function loss: 110.1372
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.4001
                       Mean reward: 431.16
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.3870
    Episode_Reward/rotating_object: 91.4181
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.94s
                      Time elapsed: 00:18:40
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1132/1500 [0m                     

                       Computation: 99972 steps/s (collection: 0.841s, learning 0.143s)
             Mean action noise std: 3.49
          Mean value_function loss: 105.8685
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.3999
                       Mean reward: 477.78
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 0.3841
    Episode_Reward/rotating_object: 95.5258
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.98s
                      Time elapsed: 00:18:41
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1133/1500 [0m                     

                       Computation: 106847 steps/s (collection: 0.784s, learning 0.136s)
             Mean action noise std: 3.49
          Mean value_function loss: 102.6483
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.4044
                       Mean reward: 502.18
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.3896
    Episode_Reward/rotating_object: 94.8168
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.92s
                      Time elapsed: 00:18:42
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1134/1500 [0m                     

                       Computation: 102975 steps/s (collection: 0.814s, learning 0.141s)
             Mean action noise std: 3.50
          Mean value_function loss: 105.8279
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.4138
                       Mean reward: 498.43
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 102.1384
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.95s
                      Time elapsed: 00:18:43
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1135/1500 [0m                     

                       Computation: 113206 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 3.50
          Mean value_function loss: 107.4318
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.4259
                       Mean reward: 500.37
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.3976
    Episode_Reward/rotating_object: 98.2067
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.87s
                      Time elapsed: 00:18:44
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1136/1500 [0m                     

                       Computation: 110443 steps/s (collection: 0.791s, learning 0.100s)
             Mean action noise std: 3.51
          Mean value_function loss: 99.8513
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.4365
                       Mean reward: 508.54
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 97.4633
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.89s
                      Time elapsed: 00:18:45
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1137/1500 [0m                     

                       Computation: 109212 steps/s (collection: 0.800s, learning 0.100s)
             Mean action noise std: 3.51
          Mean value_function loss: 104.1914
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.4455
                       Mean reward: 501.25
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 94.9773
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.90s
                      Time elapsed: 00:18:46
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1138/1500 [0m                     

                       Computation: 106068 steps/s (collection: 0.802s, learning 0.125s)
             Mean action noise std: 3.51
          Mean value_function loss: 101.7761
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.4472
                       Mean reward: 498.71
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 96.6869
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.93s
                      Time elapsed: 00:18:47
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1139/1500 [0m                     

                       Computation: 98965 steps/s (collection: 0.886s, learning 0.108s)
             Mean action noise std: 3.51
          Mean value_function loss: 94.2777
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.4524
                       Mean reward: 492.05
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.3888
    Episode_Reward/rotating_object: 97.7993
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.99s
                      Time elapsed: 00:18:48
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1140/1500 [0m                     

                       Computation: 111914 steps/s (collection: 0.779s, learning 0.100s)
             Mean action noise std: 3.52
          Mean value_function loss: 101.4775
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 19.4632
                       Mean reward: 528.88
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.3888
    Episode_Reward/rotating_object: 97.7541
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.88s
                      Time elapsed: 00:18:49
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1141/1500 [0m                     

                       Computation: 97141 steps/s (collection: 0.854s, learning 0.158s)
             Mean action noise std: 3.52
          Mean value_function loss: 107.6765
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.4663
                       Mean reward: 475.62
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.3869
    Episode_Reward/rotating_object: 97.7012
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.01s
                      Time elapsed: 00:18:50
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1142/1500 [0m                     

                       Computation: 109216 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 3.52
          Mean value_function loss: 112.4317
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 19.4682
                       Mean reward: 462.06
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.3835
    Episode_Reward/rotating_object: 93.8223
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.90s
                      Time elapsed: 00:18:51
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1143/1500 [0m                     

                       Computation: 102214 steps/s (collection: 0.843s, learning 0.119s)
             Mean action noise std: 3.52
          Mean value_function loss: 102.3957
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.4700
                       Mean reward: 505.31
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.3907
    Episode_Reward/rotating_object: 96.7297
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.96s
                      Time elapsed: 00:18:52
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1144/1500 [0m                     

                       Computation: 109591 steps/s (collection: 0.782s, learning 0.115s)
             Mean action noise std: 3.52
          Mean value_function loss: 109.8444
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.4739
                       Mean reward: 490.41
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 100.8163
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.90s
                      Time elapsed: 00:18:52
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1145/1500 [0m                     

                       Computation: 101825 steps/s (collection: 0.775s, learning 0.190s)
             Mean action noise std: 3.53
          Mean value_function loss: 112.7543
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.4787
                       Mean reward: 462.84
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 95.0843
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.97s
                      Time elapsed: 00:18:53
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1146/1500 [0m                     

                       Computation: 111694 steps/s (collection: 0.771s, learning 0.109s)
             Mean action noise std: 3.53
          Mean value_function loss: 112.3692
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.4801
                       Mean reward: 475.30
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.3839
    Episode_Reward/rotating_object: 95.8705
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.88s
                      Time elapsed: 00:18:54
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1147/1500 [0m                     

                       Computation: 104085 steps/s (collection: 0.795s, learning 0.149s)
             Mean action noise std: 3.53
          Mean value_function loss: 108.2037
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.4822
                       Mean reward: 489.44
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.3932
    Episode_Reward/rotating_object: 99.5204
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.94s
                      Time elapsed: 00:18:55
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1148/1500 [0m                     

                       Computation: 100425 steps/s (collection: 0.804s, learning 0.175s)
             Mean action noise std: 3.53
          Mean value_function loss: 104.1084
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.4867
                       Mean reward: 431.44
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.3781
    Episode_Reward/rotating_object: 90.5237
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.98s
                      Time elapsed: 00:18:56
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1149/1500 [0m                     

                       Computation: 92311 steps/s (collection: 0.847s, learning 0.218s)
             Mean action noise std: 3.53
          Mean value_function loss: 107.2996
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.4816
                       Mean reward: 486.59
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.3866
    Episode_Reward/rotating_object: 95.6109
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.06s
                      Time elapsed: 00:18:57
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1150/1500 [0m                     

                       Computation: 100338 steps/s (collection: 0.825s, learning 0.155s)
             Mean action noise std: 3.54
          Mean value_function loss: 107.5463
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.4769
                       Mean reward: 434.08
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.3857
    Episode_Reward/rotating_object: 95.7966
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.98s
                      Time elapsed: 00:18:58
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1151/1500 [0m                     

                       Computation: 110994 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 3.54
          Mean value_function loss: 99.5626
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 19.4753
                       Mean reward: 475.94
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.3847
    Episode_Reward/rotating_object: 97.1166
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.89s
                      Time elapsed: 00:18:59
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1152/1500 [0m                     

                       Computation: 108155 steps/s (collection: 0.806s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 118.9023
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.4837
                       Mean reward: 472.92
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.3796
    Episode_Reward/rotating_object: 93.4754
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.91s
                      Time elapsed: 00:19:00
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1153/1500 [0m                     

                       Computation: 103319 steps/s (collection: 0.840s, learning 0.112s)
             Mean action noise std: 3.54
          Mean value_function loss: 119.1131
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.4820
                       Mean reward: 457.20
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 0.3785
    Episode_Reward/rotating_object: 94.4513
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.95s
                      Time elapsed: 00:19:01
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1154/1500 [0m                     

                       Computation: 102922 steps/s (collection: 0.786s, learning 0.169s)
             Mean action noise std: 3.54
          Mean value_function loss: 115.7628
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 19.4807
                       Mean reward: 484.07
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3924
    Episode_Reward/rotating_object: 94.7791
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.96s
                      Time elapsed: 00:19:02
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1155/1500 [0m                     

                       Computation: 99719 steps/s (collection: 0.858s, learning 0.128s)
             Mean action noise std: 3.55
          Mean value_function loss: 95.0001
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.4809
                       Mean reward: 435.78
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.3786
    Episode_Reward/rotating_object: 92.1728
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.99s
                      Time elapsed: 00:19:03
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1156/1500 [0m                     

                       Computation: 102032 steps/s (collection: 0.847s, learning 0.117s)
             Mean action noise std: 3.55
          Mean value_function loss: 111.7124
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.4798
                       Mean reward: 485.41
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3824
    Episode_Reward/rotating_object: 94.4018
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.96s
                      Time elapsed: 00:19:04
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1157/1500 [0m                     

                       Computation: 105306 steps/s (collection: 0.830s, learning 0.103s)
             Mean action noise std: 3.55
          Mean value_function loss: 115.3821
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.4904
                       Mean reward: 469.37
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.3909
    Episode_Reward/rotating_object: 96.6662
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.93s
                      Time elapsed: 00:19:05
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1158/1500 [0m                     

                       Computation: 98865 steps/s (collection: 0.818s, learning 0.176s)
             Mean action noise std: 3.55
          Mean value_function loss: 95.7874
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 19.5013
                       Mean reward: 493.10
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3906
    Episode_Reward/rotating_object: 96.1242
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.99s
                      Time elapsed: 00:19:06
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1159/1500 [0m                     

                       Computation: 94034 steps/s (collection: 0.902s, learning 0.143s)
             Mean action noise std: 3.56
          Mean value_function loss: 97.3024
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.5070
                       Mean reward: 434.23
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.3913
    Episode_Reward/rotating_object: 93.9257
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.05s
                      Time elapsed: 00:19:07
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1160/1500 [0m                     

                       Computation: 93710 steps/s (collection: 0.919s, learning 0.130s)
             Mean action noise std: 3.56
          Mean value_function loss: 99.7217
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5120
                       Mean reward: 458.98
               Mean episode length: 249.42
    Episode_Reward/reaching_object: 0.3872
    Episode_Reward/rotating_object: 90.7934
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.05s
                      Time elapsed: 00:19:08
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1161/1500 [0m                     

                       Computation: 105266 steps/s (collection: 0.832s, learning 0.102s)
             Mean action noise std: 3.56
          Mean value_function loss: 102.4632
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 19.5208
                       Mean reward: 508.08
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.3888
    Episode_Reward/rotating_object: 95.5538
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.93s
                      Time elapsed: 00:19:09
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1162/1500 [0m                     

                       Computation: 103037 steps/s (collection: 0.838s, learning 0.116s)
             Mean action noise std: 3.56
          Mean value_function loss: 108.2888
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.5291
                       Mean reward: 508.89
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.3816
    Episode_Reward/rotating_object: 92.3833
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.95s
                      Time elapsed: 00:19:10
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1163/1500 [0m                     

                       Computation: 105646 steps/s (collection: 0.803s, learning 0.127s)
             Mean action noise std: 3.57
          Mean value_function loss: 111.7565
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.5311
                       Mean reward: 477.93
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3793
    Episode_Reward/rotating_object: 93.8180
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.93s
                      Time elapsed: 00:19:11
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1164/1500 [0m                     

                       Computation: 102455 steps/s (collection: 0.816s, learning 0.144s)
             Mean action noise std: 3.57
          Mean value_function loss: 100.7282
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5353
                       Mean reward: 451.95
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3851
    Episode_Reward/rotating_object: 97.7688
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.96s
                      Time elapsed: 00:19:12
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1165/1500 [0m                     

                       Computation: 102295 steps/s (collection: 0.824s, learning 0.137s)
             Mean action noise std: 3.57
          Mean value_function loss: 116.5456
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5407
                       Mean reward: 511.80
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.3955
    Episode_Reward/rotating_object: 97.8154
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.96s
                      Time elapsed: 00:19:13
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1166/1500 [0m                     

                       Computation: 103689 steps/s (collection: 0.827s, learning 0.121s)
             Mean action noise std: 3.57
          Mean value_function loss: 109.8029
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.5464
                       Mean reward: 507.00
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.3826
    Episode_Reward/rotating_object: 94.7608
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.95s
                      Time elapsed: 00:19:14
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1167/1500 [0m                     

                       Computation: 110829 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 3.57
          Mean value_function loss: 109.4772
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.5440
                       Mean reward: 513.06
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 100.1012
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.89s
                      Time elapsed: 00:19:15
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1168/1500 [0m                     

                       Computation: 104135 steps/s (collection: 0.832s, learning 0.112s)
             Mean action noise std: 3.58
          Mean value_function loss: 118.2717
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.5419
                       Mean reward: 474.31
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.3964
    Episode_Reward/rotating_object: 100.1026
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.94s
                      Time elapsed: 00:19:15
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1169/1500 [0m                     

                       Computation: 98003 steps/s (collection: 0.835s, learning 0.169s)
             Mean action noise std: 3.58
          Mean value_function loss: 118.9654
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.5489
                       Mean reward: 468.16
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.3923
    Episode_Reward/rotating_object: 96.6311
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.00s
                      Time elapsed: 00:19:16
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1170/1500 [0m                     

                       Computation: 94613 steps/s (collection: 0.901s, learning 0.138s)
             Mean action noise std: 3.58
          Mean value_function loss: 109.8219
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.5557
                       Mean reward: 475.00
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.4005
    Episode_Reward/rotating_object: 98.8372
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.04s
                      Time elapsed: 00:19:18
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1171/1500 [0m                     

                       Computation: 105638 steps/s (collection: 0.837s, learning 0.094s)
             Mean action noise std: 3.58
          Mean value_function loss: 105.9230
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.5627
                       Mean reward: 504.25
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.3931
    Episode_Reward/rotating_object: 98.6473
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.93s
                      Time elapsed: 00:19:18
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1172/1500 [0m                     

                       Computation: 98889 steps/s (collection: 0.865s, learning 0.129s)
             Mean action noise std: 3.59
          Mean value_function loss: 108.5460
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.5767
                       Mean reward: 460.30
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 97.3330
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.99s
                      Time elapsed: 00:19:19
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1173/1500 [0m                     

                       Computation: 100197 steps/s (collection: 0.856s, learning 0.125s)
             Mean action noise std: 3.59
          Mean value_function loss: 125.6432
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 19.5787
                       Mean reward: 456.31
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 97.0820
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.98s
                      Time elapsed: 00:19:20
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1174/1500 [0m                     

                       Computation: 99636 steps/s (collection: 0.825s, learning 0.162s)
             Mean action noise std: 3.59
          Mean value_function loss: 123.0537
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 19.5748
                       Mean reward: 508.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3967
    Episode_Reward/rotating_object: 96.2899
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.99s
                      Time elapsed: 00:19:21
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1175/1500 [0m                     

                       Computation: 104473 steps/s (collection: 0.805s, learning 0.136s)
             Mean action noise std: 3.59
          Mean value_function loss: 121.6831
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5714
                       Mean reward: 466.93
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 90.9883
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.94s
                      Time elapsed: 00:19:22
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1176/1500 [0m                     

                       Computation: 110157 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 3.59
          Mean value_function loss: 114.4787
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5723
                       Mean reward: 489.36
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 96.2431
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.89s
                      Time elapsed: 00:19:23
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1177/1500 [0m                     

                       Computation: 108209 steps/s (collection: 0.792s, learning 0.116s)
             Mean action noise std: 3.60
          Mean value_function loss: 118.9280
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.5707
                       Mean reward: 476.29
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 95.4157
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.91s
                      Time elapsed: 00:19:24
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1178/1500 [0m                     

                       Computation: 100709 steps/s (collection: 0.811s, learning 0.165s)
             Mean action noise std: 3.60
          Mean value_function loss: 121.9914
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5697
                       Mean reward: 494.51
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 95.5727
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.98s
                      Time elapsed: 00:19:25
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1179/1500 [0m                     

                       Computation: 104472 steps/s (collection: 0.798s, learning 0.143s)
             Mean action noise std: 3.60
          Mean value_function loss: 118.7408
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.5706
                       Mean reward: 481.95
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 96.6039
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.94s
                      Time elapsed: 00:19:26
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1180/1500 [0m                     

                       Computation: 102604 steps/s (collection: 0.856s, learning 0.102s)
             Mean action noise std: 3.60
          Mean value_function loss: 131.6065
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5755
                       Mean reward: 530.62
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.4043
    Episode_Reward/rotating_object: 98.6487
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.96s
                      Time elapsed: 00:19:27
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1181/1500 [0m                     

                       Computation: 111661 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 3.61
          Mean value_function loss: 127.3908
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.5787
                       Mean reward: 462.42
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.4082
    Episode_Reward/rotating_object: 99.4690
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.88s
                      Time elapsed: 00:19:28
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1182/1500 [0m                     

                       Computation: 108600 steps/s (collection: 0.806s, learning 0.100s)
             Mean action noise std: 3.61
          Mean value_function loss: 126.4681
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.5809
                       Mean reward: 461.53
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 91.5774
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.91s
                      Time elapsed: 00:19:29
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1183/1500 [0m                     

                       Computation: 108704 steps/s (collection: 0.814s, learning 0.091s)
             Mean action noise std: 3.61
          Mean value_function loss: 127.6139
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.5776
                       Mean reward: 489.72
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.4043
    Episode_Reward/rotating_object: 95.6186
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.90s
                      Time elapsed: 00:19:30
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1184/1500 [0m                     

                       Computation: 110393 steps/s (collection: 0.794s, learning 0.096s)
             Mean action noise std: 3.61
          Mean value_function loss: 113.8355
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.5755
                       Mean reward: 499.72
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.3984
    Episode_Reward/rotating_object: 96.5303
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.89s
                      Time elapsed: 00:19:31
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1185/1500 [0m                     

                       Computation: 101285 steps/s (collection: 0.819s, learning 0.152s)
             Mean action noise std: 3.61
          Mean value_function loss: 111.7664
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.5766
                       Mean reward: 470.64
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.4134
    Episode_Reward/rotating_object: 99.8977
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.97s
                      Time elapsed: 00:19:32
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1186/1500 [0m                     

                       Computation: 101815 steps/s (collection: 0.866s, learning 0.100s)
             Mean action noise std: 3.62
          Mean value_function loss: 128.4080
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.5795
                       Mean reward: 472.23
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.3989
    Episode_Reward/rotating_object: 95.8378
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.97s
                      Time elapsed: 00:19:33
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1187/1500 [0m                     

                       Computation: 107741 steps/s (collection: 0.804s, learning 0.109s)
             Mean action noise std: 3.62
          Mean value_function loss: 131.4270
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.5791
                       Mean reward: 501.52
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.4026
    Episode_Reward/rotating_object: 96.6816
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.91s
                      Time elapsed: 00:19:33
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1188/1500 [0m                     

                       Computation: 110879 steps/s (collection: 0.780s, learning 0.107s)
             Mean action noise std: 3.62
          Mean value_function loss: 126.3184
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.5853
                       Mean reward: 497.23
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.4069
    Episode_Reward/rotating_object: 98.0110
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.89s
                      Time elapsed: 00:19:34
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1189/1500 [0m                     

                       Computation: 106943 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 3.62
          Mean value_function loss: 126.7456
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.5860
                       Mean reward: 474.76
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.3998
    Episode_Reward/rotating_object: 93.9471
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.92s
                      Time elapsed: 00:19:35
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1190/1500 [0m                     

                       Computation: 109384 steps/s (collection: 0.805s, learning 0.094s)
             Mean action noise std: 3.63
          Mean value_function loss: 110.2565
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.5896
                       Mean reward: 437.86
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 92.4016
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.90s
                      Time elapsed: 00:19:36
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1191/1500 [0m                     

                       Computation: 106268 steps/s (collection: 0.799s, learning 0.126s)
             Mean action noise std: 3.63
          Mean value_function loss: 110.5310
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.5938
                       Mean reward: 451.61
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 97.6395
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.93s
                      Time elapsed: 00:19:37
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1192/1500 [0m                     

                       Computation: 105503 steps/s (collection: 0.798s, learning 0.134s)
             Mean action noise std: 3.63
          Mean value_function loss: 108.8518
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.5940
                       Mean reward: 534.68
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.4072
    Episode_Reward/rotating_object: 101.5575
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.93s
                      Time elapsed: 00:19:38
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1193/1500 [0m                     

                       Computation: 109218 steps/s (collection: 0.773s, learning 0.127s)
             Mean action noise std: 3.63
          Mean value_function loss: 108.2158
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.5956
                       Mean reward: 454.19
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.3820
    Episode_Reward/rotating_object: 86.4708
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.90s
                      Time elapsed: 00:19:39
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1194/1500 [0m                     

                       Computation: 106303 steps/s (collection: 0.811s, learning 0.113s)
             Mean action noise std: 3.63
          Mean value_function loss: 108.3907
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.6036
                       Mean reward: 535.06
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.4039
    Episode_Reward/rotating_object: 102.6961
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.92s
                      Time elapsed: 00:19:40
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1195/1500 [0m                     

                       Computation: 107960 steps/s (collection: 0.803s, learning 0.108s)
             Mean action noise std: 3.63
          Mean value_function loss: 106.5496
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6042
                       Mean reward: 513.93
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.4031
    Episode_Reward/rotating_object: 99.8485
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.91s
                      Time elapsed: 00:19:41
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1196/1500 [0m                     

                       Computation: 107210 steps/s (collection: 0.798s, learning 0.119s)
             Mean action noise std: 3.63
          Mean value_function loss: 106.5827
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.6117
                       Mean reward: 504.93
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.3933
    Episode_Reward/rotating_object: 95.5326
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.92s
                      Time elapsed: 00:19:42
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1197/1500 [0m                     

                       Computation: 104390 steps/s (collection: 0.819s, learning 0.122s)
             Mean action noise std: 3.63
          Mean value_function loss: 106.4969
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.6092
                       Mean reward: 511.20
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 96.3657
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.94s
                      Time elapsed: 00:19:43
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1198/1500 [0m                     

                       Computation: 108350 steps/s (collection: 0.809s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 98.3993
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.6091
                       Mean reward: 513.21
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.3914
    Episode_Reward/rotating_object: 92.3852
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.91s
                      Time elapsed: 00:19:44
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1199/1500 [0m                     

                       Computation: 106010 steps/s (collection: 0.804s, learning 0.124s)
             Mean action noise std: 3.64
          Mean value_function loss: 107.3737
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6183
                       Mean reward: 473.48
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 95.7727
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.93s
                      Time elapsed: 00:19:44
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1200/1500 [0m                     

                       Computation: 106965 steps/s (collection: 0.786s, learning 0.133s)
             Mean action noise std: 3.64
          Mean value_function loss: 98.7768
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.6294
                       Mean reward: 518.10
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.4044
    Episode_Reward/rotating_object: 102.6275
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.92s
                      Time elapsed: 00:19:45
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1201/1500 [0m                     

                       Computation: 107037 steps/s (collection: 0.782s, learning 0.136s)
             Mean action noise std: 3.65
          Mean value_function loss: 94.2590
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.6452
                       Mean reward: 516.07
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 98.6296
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.92s
                      Time elapsed: 00:19:46
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1202/1500 [0m                     

                       Computation: 110249 steps/s (collection: 0.774s, learning 0.118s)
             Mean action noise std: 3.65
          Mean value_function loss: 95.9454
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 19.6552
                       Mean reward: 472.06
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.4022
    Episode_Reward/rotating_object: 100.7893
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.89s
                      Time elapsed: 00:19:47
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1203/1500 [0m                     

                       Computation: 113423 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 3.65
          Mean value_function loss: 98.2284
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.6569
                       Mean reward: 466.93
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 97.6366
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.87s
                      Time elapsed: 00:19:48
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1204/1500 [0m                     

                       Computation: 104687 steps/s (collection: 0.810s, learning 0.129s)
             Mean action noise std: 3.66
          Mean value_function loss: 102.6498
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.6552
                       Mean reward: 496.59
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 97.3273
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.94s
                      Time elapsed: 00:19:49
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1205/1500 [0m                     

                       Computation: 113663 steps/s (collection: 0.773s, learning 0.092s)
             Mean action noise std: 3.66
          Mean value_function loss: 96.0703
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.6552
                       Mean reward: 498.54
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.4004
    Episode_Reward/rotating_object: 98.2535
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.86s
                      Time elapsed: 00:19:50
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1206/1500 [0m                     

                       Computation: 110034 steps/s (collection: 0.800s, learning 0.094s)
             Mean action noise std: 3.66
          Mean value_function loss: 91.6184
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.6534
                       Mean reward: 528.06
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.3925
    Episode_Reward/rotating_object: 103.8103
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.89s
                      Time elapsed: 00:19:51
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1207/1500 [0m                     

                       Computation: 107990 steps/s (collection: 0.793s, learning 0.117s)
             Mean action noise std: 3.66
          Mean value_function loss: 103.2668
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.6518
                       Mean reward: 474.51
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.3967
    Episode_Reward/rotating_object: 99.8414
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.91s
                      Time elapsed: 00:19:52
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1208/1500 [0m                     

                       Computation: 111533 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 3.66
          Mean value_function loss: 114.0630
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.6500
                       Mean reward: 528.75
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.3975
    Episode_Reward/rotating_object: 100.4858
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.88s
                      Time elapsed: 00:19:53
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1209/1500 [0m                     

                       Computation: 110188 steps/s (collection: 0.771s, learning 0.121s)
             Mean action noise std: 3.66
          Mean value_function loss: 104.9345
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.6496
                       Mean reward: 484.67
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.3993
    Episode_Reward/rotating_object: 97.8756
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.89s
                      Time elapsed: 00:19:53
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1210/1500 [0m                     

                       Computation: 105728 steps/s (collection: 0.767s, learning 0.163s)
             Mean action noise std: 3.67
          Mean value_function loss: 95.7935
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.6539
                       Mean reward: 544.88
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.4049
    Episode_Reward/rotating_object: 103.4161
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.93s
                      Time elapsed: 00:19:54
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1211/1500 [0m                     

                       Computation: 105969 steps/s (collection: 0.817s, learning 0.111s)
             Mean action noise std: 3.67
          Mean value_function loss: 107.1358
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.6501
                       Mean reward: 521.83
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.3914
    Episode_Reward/rotating_object: 100.6342
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.93s
                      Time elapsed: 00:19:55
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1212/1500 [0m                     

                       Computation: 108538 steps/s (collection: 0.777s, learning 0.129s)
             Mean action noise std: 3.67
          Mean value_function loss: 100.9069
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.6569
                       Mean reward: 490.88
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.4085
    Episode_Reward/rotating_object: 103.5568
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.91s
                      Time elapsed: 00:19:56
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1213/1500 [0m                     

                       Computation: 98257 steps/s (collection: 0.829s, learning 0.171s)
             Mean action noise std: 3.68
          Mean value_function loss: 102.4214
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.6633
                       Mean reward: 526.65
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 99.5289
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 1.00s
                      Time elapsed: 00:19:57
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1214/1500 [0m                     

                       Computation: 106579 steps/s (collection: 0.783s, learning 0.139s)
             Mean action noise std: 3.68
          Mean value_function loss: 91.7155
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.6683
                       Mean reward: 476.54
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3980
    Episode_Reward/rotating_object: 98.8824
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.92s
                      Time elapsed: 00:19:58
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1215/1500 [0m                     

                       Computation: 111439 steps/s (collection: 0.787s, learning 0.096s)
             Mean action noise std: 3.68
          Mean value_function loss: 92.1229
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.6660
                       Mean reward: 500.59
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.4004
    Episode_Reward/rotating_object: 99.7505
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.88s
                      Time elapsed: 00:19:59
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1216/1500 [0m                     

                       Computation: 110623 steps/s (collection: 0.800s, learning 0.089s)
             Mean action noise std: 3.68
          Mean value_function loss: 95.5148
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.6740
                       Mean reward: 495.39
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.3918
    Episode_Reward/rotating_object: 102.2136
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.89s
                      Time elapsed: 00:20:00
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1217/1500 [0m                     

                       Computation: 109527 steps/s (collection: 0.794s, learning 0.104s)
             Mean action noise std: 3.69
          Mean value_function loss: 93.6783
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.6795
                       Mean reward: 543.69
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.3988
    Episode_Reward/rotating_object: 102.6864
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.90s
                      Time elapsed: 00:20:01
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1218/1500 [0m                     

                       Computation: 107253 steps/s (collection: 0.817s, learning 0.100s)
             Mean action noise std: 3.69
          Mean value_function loss: 94.2063
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.6874
                       Mean reward: 487.18
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.3971
    Episode_Reward/rotating_object: 104.7125
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.92s
                      Time elapsed: 00:20:02
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1219/1500 [0m                     

                       Computation: 110751 steps/s (collection: 0.777s, learning 0.111s)
             Mean action noise std: 3.69
          Mean value_function loss: 105.9605
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.6901
                       Mean reward: 500.04
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.3941
    Episode_Reward/rotating_object: 100.5022
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.89s
                      Time elapsed: 00:20:03
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1220/1500 [0m                     

                       Computation: 97670 steps/s (collection: 0.865s, learning 0.141s)
             Mean action noise std: 3.70
          Mean value_function loss: 103.3607
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.6932
                       Mean reward: 493.69
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3987
    Episode_Reward/rotating_object: 101.9660
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.01s
                      Time elapsed: 00:20:04
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1221/1500 [0m                     

                       Computation: 107008 steps/s (collection: 0.813s, learning 0.106s)
             Mean action noise std: 3.70
          Mean value_function loss: 102.8150
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.6924
                       Mean reward: 508.45
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.3930
    Episode_Reward/rotating_object: 101.4541
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.92s
                      Time elapsed: 00:20:05
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1222/1500 [0m                     

                       Computation: 106135 steps/s (collection: 0.797s, learning 0.129s)
             Mean action noise std: 3.70
          Mean value_function loss: 96.6782
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.6902
                       Mean reward: 519.39
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3934
    Episode_Reward/rotating_object: 102.6533
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.93s
                      Time elapsed: 00:20:05
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1223/1500 [0m                     

                       Computation: 107456 steps/s (collection: 0.799s, learning 0.116s)
             Mean action noise std: 3.70
          Mean value_function loss: 94.0810
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.6964
                       Mean reward: 476.88
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 102.3828
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.91s
                      Time elapsed: 00:20:06
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1224/1500 [0m                     

                       Computation: 109439 steps/s (collection: 0.781s, learning 0.117s)
             Mean action noise std: 3.70
          Mean value_function loss: 98.9413
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.6990
                       Mean reward: 511.62
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 103.1492
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.90s
                      Time elapsed: 00:20:07
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1225/1500 [0m                     

                       Computation: 105173 steps/s (collection: 0.809s, learning 0.126s)
             Mean action noise std: 3.70
          Mean value_function loss: 95.5149
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.6933
                       Mean reward: 488.15
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.3959
    Episode_Reward/rotating_object: 98.6276
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.93s
                      Time elapsed: 00:20:08
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1226/1500 [0m                     

                       Computation: 109718 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 3.71
          Mean value_function loss: 100.3577
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6929
                       Mean reward: 455.00
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.3965
    Episode_Reward/rotating_object: 97.3998
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.90s
                      Time elapsed: 00:20:09
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1227/1500 [0m                     

                       Computation: 109801 steps/s (collection: 0.779s, learning 0.117s)
             Mean action noise std: 3.71
          Mean value_function loss: 93.9870
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.6980
                       Mean reward: 497.51
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 0.3929
    Episode_Reward/rotating_object: 96.4646
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.90s
                      Time elapsed: 00:20:10
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1228/1500 [0m                     

                       Computation: 105210 steps/s (collection: 0.818s, learning 0.117s)
             Mean action noise std: 3.71
          Mean value_function loss: 92.9857
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.6986
                       Mean reward: 506.31
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.3939
    Episode_Reward/rotating_object: 98.4717
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.93s
                      Time elapsed: 00:20:11
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1229/1500 [0m                     

                       Computation: 104495 steps/s (collection: 0.790s, learning 0.151s)
             Mean action noise std: 3.71
          Mean value_function loss: 104.1571
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.6956
                       Mean reward: 467.48
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.3940
    Episode_Reward/rotating_object: 98.4698
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.94s
                      Time elapsed: 00:20:12
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1230/1500 [0m                     

                       Computation: 104149 steps/s (collection: 0.814s, learning 0.130s)
             Mean action noise std: 3.71
          Mean value_function loss: 90.1035
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.6975
                       Mean reward: 504.59
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.3969
    Episode_Reward/rotating_object: 98.9729
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.94s
                      Time elapsed: 00:20:13
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1231/1500 [0m                     

                       Computation: 95987 steps/s (collection: 0.838s, learning 0.187s)
             Mean action noise std: 3.72
          Mean value_function loss: 94.7053
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.7037
                       Mean reward: 509.64
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 106.2833
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 1.02s
                      Time elapsed: 00:20:14
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1232/1500 [0m                     

                       Computation: 102400 steps/s (collection: 0.844s, learning 0.116s)
             Mean action noise std: 3.72
          Mean value_function loss: 93.4189
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7111
                       Mean reward: 530.62
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 103.3594
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.96s
                      Time elapsed: 00:20:15
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1233/1500 [0m                     

                       Computation: 108223 steps/s (collection: 0.791s, learning 0.118s)
             Mean action noise std: 3.72
          Mean value_function loss: 93.0399
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7092
                       Mean reward: 485.33
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.3958
    Episode_Reward/rotating_object: 101.9010
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.91s
                      Time elapsed: 00:20:16
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1234/1500 [0m                     

                       Computation: 97641 steps/s (collection: 0.852s, learning 0.154s)
             Mean action noise std: 3.73
          Mean value_function loss: 98.9735
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.7160
                       Mean reward: 497.87
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.4002
    Episode_Reward/rotating_object: 104.3428
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 1.01s
                      Time elapsed: 00:20:17
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1235/1500 [0m                     

                       Computation: 111549 steps/s (collection: 0.790s, learning 0.091s)
             Mean action noise std: 3.73
          Mean value_function loss: 93.6872
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.7155
                       Mean reward: 499.44
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.3853
    Episode_Reward/rotating_object: 98.9503
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.88s
                      Time elapsed: 00:20:18
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1236/1500 [0m                     

                       Computation: 107497 steps/s (collection: 0.798s, learning 0.116s)
             Mean action noise std: 3.73
          Mean value_function loss: 94.6778
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.7184
                       Mean reward: 521.62
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.3968
    Episode_Reward/rotating_object: 104.3934
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.91s
                      Time elapsed: 00:20:19
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1237/1500 [0m                     

                       Computation: 104745 steps/s (collection: 0.806s, learning 0.132s)
             Mean action noise std: 3.73
          Mean value_function loss: 89.8546
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 19.7161
                       Mean reward: 496.22
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.3865
    Episode_Reward/rotating_object: 99.4926
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.94s
                      Time elapsed: 00:20:19
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1238/1500 [0m                     

                       Computation: 104709 steps/s (collection: 0.804s, learning 0.135s)
             Mean action noise std: 3.73
          Mean value_function loss: 86.2086
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 19.7208
                       Mean reward: 532.00
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.3880
    Episode_Reward/rotating_object: 100.9405
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.94s
                      Time elapsed: 00:20:20
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1239/1500 [0m                     

                       Computation: 107859 steps/s (collection: 0.794s, learning 0.118s)
             Mean action noise std: 3.73
          Mean value_function loss: 91.3535
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.7319
                       Mean reward: 502.05
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.3879
    Episode_Reward/rotating_object: 100.5938
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.91s
                      Time elapsed: 00:20:21
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1240/1500 [0m                     

                       Computation: 106939 steps/s (collection: 0.777s, learning 0.142s)
             Mean action noise std: 3.74
          Mean value_function loss: 96.6046
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.7401
                       Mean reward: 503.95
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3901
    Episode_Reward/rotating_object: 103.4809
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.92s
                      Time elapsed: 00:20:22
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1241/1500 [0m                     

                       Computation: 109611 steps/s (collection: 0.772s, learning 0.125s)
             Mean action noise std: 3.74
          Mean value_function loss: 96.6767
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7470
                       Mean reward: 526.41
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.3873
    Episode_Reward/rotating_object: 100.8246
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.90s
                      Time elapsed: 00:20:23
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1242/1500 [0m                     

                       Computation: 96750 steps/s (collection: 0.902s, learning 0.114s)
             Mean action noise std: 3.75
          Mean value_function loss: 99.8914
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.7533
                       Mean reward: 465.01
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.3939
    Episode_Reward/rotating_object: 100.1294
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.02s
                      Time elapsed: 00:20:24
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1243/1500 [0m                     

                       Computation: 112497 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 3.75
          Mean value_function loss: 95.1217
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.7687
                       Mean reward: 508.76
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3891
    Episode_Reward/rotating_object: 100.2414
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.87s
                      Time elapsed: 00:20:25
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1244/1500 [0m                     

                       Computation: 99384 steps/s (collection: 0.805s, learning 0.185s)
             Mean action noise std: 3.75
          Mean value_function loss: 92.7637
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7786
                       Mean reward: 556.39
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.3863
    Episode_Reward/rotating_object: 102.4023
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.99s
                      Time elapsed: 00:20:26
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1245/1500 [0m                     

                       Computation: 103463 steps/s (collection: 0.850s, learning 0.101s)
             Mean action noise std: 3.75
          Mean value_function loss: 104.8350
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.7810
                       Mean reward: 490.69
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.3889
    Episode_Reward/rotating_object: 101.3206
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.95s
                      Time elapsed: 00:20:27
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1246/1500 [0m                     

                       Computation: 103818 steps/s (collection: 0.851s, learning 0.096s)
             Mean action noise std: 3.76
          Mean value_function loss: 104.7187
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.7765
                       Mean reward: 472.60
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.3883
    Episode_Reward/rotating_object: 98.9456
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.95s
                      Time elapsed: 00:20:28
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1247/1500 [0m                     

                       Computation: 105357 steps/s (collection: 0.814s, learning 0.119s)
             Mean action noise std: 3.76
          Mean value_function loss: 99.9894
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.7808
                       Mean reward: 499.15
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.3896
    Episode_Reward/rotating_object: 99.6068
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.93s
                      Time elapsed: 00:20:29
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1248/1500 [0m                     

                       Computation: 107211 steps/s (collection: 0.804s, learning 0.113s)
             Mean action noise std: 3.76
          Mean value_function loss: 96.6638
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.7817
                       Mean reward: 556.73
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.3840
    Episode_Reward/rotating_object: 101.7869
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.92s
                      Time elapsed: 00:20:30
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1249/1500 [0m                     

                       Computation: 105640 steps/s (collection: 0.826s, learning 0.105s)
             Mean action noise std: 3.76
          Mean value_function loss: 96.9152
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.7878
                       Mean reward: 542.29
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.3876
    Episode_Reward/rotating_object: 99.3359
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.93s
                      Time elapsed: 00:20:31
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1250/1500 [0m                     

                       Computation: 105667 steps/s (collection: 0.814s, learning 0.117s)
             Mean action noise std: 3.76
          Mean value_function loss: 108.7566
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 19.7917
                       Mean reward: 472.28
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 97.9650
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.93s
                      Time elapsed: 00:20:32
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1251/1500 [0m                     

                       Computation: 109923 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 3.76
          Mean value_function loss: 91.8336
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.7908
                       Mean reward: 525.17
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.3907
    Episode_Reward/rotating_object: 102.5855
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.89s
                      Time elapsed: 00:20:32
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1252/1500 [0m                     

                       Computation: 102029 steps/s (collection: 0.843s, learning 0.120s)
             Mean action noise std: 3.76
          Mean value_function loss: 96.0090
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.7923
                       Mean reward: 492.30
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.3937
    Episode_Reward/rotating_object: 100.2454
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.96s
                      Time elapsed: 00:20:33
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1253/1500 [0m                     

                       Computation: 110820 steps/s (collection: 0.783s, learning 0.104s)
             Mean action noise std: 3.76
          Mean value_function loss: 95.9131
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7875
                       Mean reward: 504.02
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.3946
    Episode_Reward/rotating_object: 104.1380
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.89s
                      Time elapsed: 00:20:34
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1254/1500 [0m                     

                       Computation: 109167 steps/s (collection: 0.786s, learning 0.115s)
             Mean action noise std: 3.77
          Mean value_function loss: 95.5248
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7939
                       Mean reward: 461.98
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3901
    Episode_Reward/rotating_object: 97.8283
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.90s
                      Time elapsed: 00:20:35
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1255/1500 [0m                     

                       Computation: 103068 steps/s (collection: 0.778s, learning 0.176s)
             Mean action noise std: 3.77
          Mean value_function loss: 96.8372
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.7975
                       Mean reward: 551.67
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.4033
    Episode_Reward/rotating_object: 102.2257
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.95s
                      Time elapsed: 00:20:36
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1256/1500 [0m                     

                       Computation: 103254 steps/s (collection: 0.791s, learning 0.161s)
             Mean action noise std: 3.78
          Mean value_function loss: 104.7666
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.8100
                       Mean reward: 529.21
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3965
    Episode_Reward/rotating_object: 103.5306
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.95s
                      Time elapsed: 00:20:37
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1257/1500 [0m                     

                       Computation: 109811 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 3.78
          Mean value_function loss: 96.9418
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.8095
                       Mean reward: 514.01
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.3955
    Episode_Reward/rotating_object: 102.7228
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.90s
                      Time elapsed: 00:20:38
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1258/1500 [0m                     

                       Computation: 103573 steps/s (collection: 0.846s, learning 0.104s)
             Mean action noise std: 3.78
          Mean value_function loss: 93.6387
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8083
                       Mean reward: 483.42
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.3957
    Episode_Reward/rotating_object: 101.7664
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.95s
                      Time elapsed: 00:20:39
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1259/1500 [0m                     

                       Computation: 112563 steps/s (collection: 0.783s, learning 0.091s)
             Mean action noise std: 3.78
          Mean value_function loss: 94.1927
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.8187
                       Mean reward: 510.65
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 100.5616
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.87s
                      Time elapsed: 00:20:40
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1260/1500 [0m                     

                       Computation: 108428 steps/s (collection: 0.816s, learning 0.091s)
             Mean action noise std: 3.78
          Mean value_function loss: 95.5918
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.8199
                       Mean reward: 507.84
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.3947
    Episode_Reward/rotating_object: 102.4871
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.91s
                      Time elapsed: 00:20:41
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1261/1500 [0m                     

                       Computation: 110718 steps/s (collection: 0.772s, learning 0.116s)
             Mean action noise std: 3.78
          Mean value_function loss: 99.4768
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8216
                       Mean reward: 539.55
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 105.3172
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.89s
                      Time elapsed: 00:20:42
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1262/1500 [0m                     

                       Computation: 105437 steps/s (collection: 0.811s, learning 0.121s)
             Mean action noise std: 3.79
          Mean value_function loss: 102.5811
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.8227
                       Mean reward: 536.78
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.3973
    Episode_Reward/rotating_object: 105.4134
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.93s
                      Time elapsed: 00:20:43
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1263/1500 [0m                     

                       Computation: 110673 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 3.79
          Mean value_function loss: 95.2041
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8210
                       Mean reward: 504.57
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.3940
    Episode_Reward/rotating_object: 103.6521
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.89s
                      Time elapsed: 00:20:43
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1264/1500 [0m                     

                       Computation: 106842 steps/s (collection: 0.760s, learning 0.161s)
             Mean action noise std: 3.79
          Mean value_function loss: 107.2632
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.8233
                       Mean reward: 484.48
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.3808
    Episode_Reward/rotating_object: 97.3910
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.92s
                      Time elapsed: 00:20:44
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1265/1500 [0m                     

                       Computation: 103343 steps/s (collection: 0.791s, learning 0.160s)
             Mean action noise std: 3.79
          Mean value_function loss: 92.3808
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.8287
                       Mean reward: 524.46
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.3879
    Episode_Reward/rotating_object: 102.5797
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.95s
                      Time elapsed: 00:20:45
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1266/1500 [0m                     

                       Computation: 108988 steps/s (collection: 0.752s, learning 0.150s)
             Mean action noise std: 3.80
          Mean value_function loss: 94.3329
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 19.8310
                       Mean reward: 503.84
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.3975
    Episode_Reward/rotating_object: 97.7947
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.90s
                      Time elapsed: 00:20:46
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1267/1500 [0m                     

                       Computation: 105709 steps/s (collection: 0.755s, learning 0.175s)
             Mean action noise std: 3.80
          Mean value_function loss: 94.7181
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.8353
                       Mean reward: 557.89
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 104.6931
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.93s
                      Time elapsed: 00:20:47
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1268/1500 [0m                     

                       Computation: 99610 steps/s (collection: 0.831s, learning 0.156s)
             Mean action noise std: 3.80
          Mean value_function loss: 101.8538
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.8390
                       Mean reward: 533.58
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.3908
    Episode_Reward/rotating_object: 101.6049
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.99s
                      Time elapsed: 00:20:48
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1269/1500 [0m                     

                       Computation: 108156 steps/s (collection: 0.800s, learning 0.109s)
             Mean action noise std: 3.80
          Mean value_function loss: 101.3187
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.8379
                       Mean reward: 522.90
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3843
    Episode_Reward/rotating_object: 99.3113
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.91s
                      Time elapsed: 00:20:49
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1270/1500 [0m                     

                       Computation: 108806 steps/s (collection: 0.809s, learning 0.095s)
             Mean action noise std: 3.80
          Mean value_function loss: 98.6058
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.8455
                       Mean reward: 498.96
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.3895
    Episode_Reward/rotating_object: 101.3091
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.90s
                      Time elapsed: 00:20:50
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1271/1500 [0m                     

                       Computation: 112998 steps/s (collection: 0.766s, learning 0.104s)
             Mean action noise std: 3.81
          Mean value_function loss: 97.2507
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8523
                       Mean reward: 486.68
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.3853
    Episode_Reward/rotating_object: 97.7656
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.87s
                      Time elapsed: 00:20:51
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1272/1500 [0m                     

                       Computation: 108860 steps/s (collection: 0.794s, learning 0.109s)
             Mean action noise std: 3.81
          Mean value_function loss: 98.5295
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.8604
                       Mean reward: 503.86
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.3921
    Episode_Reward/rotating_object: 98.3715
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.90s
                      Time elapsed: 00:20:52
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1273/1500 [0m                     

                       Computation: 109522 steps/s (collection: 0.796s, learning 0.101s)
             Mean action noise std: 3.81
          Mean value_function loss: 100.9577
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.8731
                       Mean reward: 507.33
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.3971
    Episode_Reward/rotating_object: 100.9307
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.90s
                      Time elapsed: 00:20:53
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1274/1500 [0m                     

                       Computation: 107128 steps/s (collection: 0.808s, learning 0.109s)
             Mean action noise std: 3.82
          Mean value_function loss: 95.4363
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.8841
                       Mean reward: 489.62
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 0.3994
    Episode_Reward/rotating_object: 102.5303
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.92s
                      Time elapsed: 00:20:54
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1275/1500 [0m                     

                       Computation: 112037 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 3.82
          Mean value_function loss: 91.6792
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.8812
                       Mean reward: 527.32
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.4025
    Episode_Reward/rotating_object: 103.6971
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.88s
                      Time elapsed: 00:20:54
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1276/1500 [0m                     

                       Computation: 108812 steps/s (collection: 0.783s, learning 0.120s)
             Mean action noise std: 3.82
          Mean value_function loss: 98.6930
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.8784
                       Mean reward: 505.03
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.3960
    Episode_Reward/rotating_object: 103.6492
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.90s
                      Time elapsed: 00:20:55
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1277/1500 [0m                     

                       Computation: 108073 steps/s (collection: 0.780s, learning 0.130s)
             Mean action noise std: 3.82
          Mean value_function loss: 101.8806
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.8764
                       Mean reward: 506.29
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.3914
    Episode_Reward/rotating_object: 101.8796
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.91s
                      Time elapsed: 00:20:56
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1278/1500 [0m                     

                       Computation: 96820 steps/s (collection: 0.803s, learning 0.212s)
             Mean action noise std: 3.82
          Mean value_function loss: 105.1905
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8765
                       Mean reward: 546.65
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.3955
    Episode_Reward/rotating_object: 103.6366
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 1.02s
                      Time elapsed: 00:20:57
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1279/1500 [0m                     

                       Computation: 107438 steps/s (collection: 0.780s, learning 0.135s)
             Mean action noise std: 3.82
          Mean value_function loss: 100.0867
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.8704
                       Mean reward: 509.06
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.4038
    Episode_Reward/rotating_object: 102.6713
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.91s
                      Time elapsed: 00:20:58
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1280/1500 [0m                     

                       Computation: 100260 steps/s (collection: 0.831s, learning 0.150s)
             Mean action noise std: 3.83
          Mean value_function loss: 104.8818
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.8692
                       Mean reward: 520.59
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.3978
    Episode_Reward/rotating_object: 101.9894
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.98s
                      Time elapsed: 00:20:59
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1281/1500 [0m                     

                       Computation: 92105 steps/s (collection: 0.858s, learning 0.210s)
             Mean action noise std: 3.83
          Mean value_function loss: 105.5869
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.8744
                       Mean reward: 540.84
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.4044
    Episode_Reward/rotating_object: 103.1037
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 1.07s
                      Time elapsed: 00:21:00
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1282/1500 [0m                     

                       Computation: 102779 steps/s (collection: 0.862s, learning 0.094s)
             Mean action noise std: 3.83
          Mean value_function loss: 109.2062
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.8784
                       Mean reward: 477.56
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.4100
    Episode_Reward/rotating_object: 102.1551
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.96s
                      Time elapsed: 00:21:01
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1283/1500 [0m                     

                       Computation: 104371 steps/s (collection: 0.786s, learning 0.156s)
             Mean action noise std: 3.83
          Mean value_function loss: 102.1637
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8776
                       Mean reward: 534.21
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.4108
    Episode_Reward/rotating_object: 107.7469
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.94s
                      Time elapsed: 00:21:02
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1284/1500 [0m                     

                       Computation: 99463 steps/s (collection: 0.824s, learning 0.164s)
             Mean action noise std: 3.84
          Mean value_function loss: 101.1442
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8815
                       Mean reward: 474.39
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.3917
    Episode_Reward/rotating_object: 93.8343
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.99s
                      Time elapsed: 00:21:03
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1285/1500 [0m                     

                       Computation: 112158 steps/s (collection: 0.778s, learning 0.099s)
             Mean action noise std: 3.84
          Mean value_function loss: 107.7802
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8903
                       Mean reward: 532.48
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 100.6456
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.88s
                      Time elapsed: 00:21:04
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1286/1500 [0m                     

                       Computation: 107751 steps/s (collection: 0.805s, learning 0.108s)
             Mean action noise std: 3.84
          Mean value_function loss: 102.0974
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.8969
                       Mean reward: 548.52
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4082
    Episode_Reward/rotating_object: 104.4895
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.91s
                      Time elapsed: 00:21:05
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1287/1500 [0m                     

                       Computation: 95198 steps/s (collection: 0.863s, learning 0.170s)
             Mean action noise std: 3.85
          Mean value_function loss: 109.0747
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.8990
                       Mean reward: 506.28
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.4040
    Episode_Reward/rotating_object: 100.9431
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 1.03s
                      Time elapsed: 00:21:06
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1288/1500 [0m                     

                       Computation: 78143 steps/s (collection: 1.037s, learning 0.221s)
             Mean action noise std: 3.85
          Mean value_function loss: 104.9408
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.9004
                       Mean reward: 514.06
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.4106
    Episode_Reward/rotating_object: 102.4858
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 1.26s
                      Time elapsed: 00:21:07
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1289/1500 [0m                     

                       Computation: 102260 steps/s (collection: 0.780s, learning 0.181s)
             Mean action noise std: 3.85
          Mean value_function loss: 100.2067
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.9038
                       Mean reward: 495.55
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 0.4061
    Episode_Reward/rotating_object: 100.4669
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.96s
                      Time elapsed: 00:21:08
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1290/1500 [0m                     

                       Computation: 101050 steps/s (collection: 0.802s, learning 0.171s)
             Mean action noise std: 3.85
          Mean value_function loss: 102.1963
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9042
                       Mean reward: 500.59
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.4081
    Episode_Reward/rotating_object: 104.0302
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.97s
                      Time elapsed: 00:21:09
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1291/1500 [0m                     

                       Computation: 100752 steps/s (collection: 0.803s, learning 0.173s)
             Mean action noise std: 3.86
          Mean value_function loss: 102.7029
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9015
                       Mean reward: 557.46
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.4057
    Episode_Reward/rotating_object: 105.7043
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.98s
                      Time elapsed: 00:21:10
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1292/1500 [0m                     

                       Computation: 93806 steps/s (collection: 0.856s, learning 0.192s)
             Mean action noise std: 3.86
          Mean value_function loss: 111.8194
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.9050
                       Mean reward: 549.92
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4138
    Episode_Reward/rotating_object: 105.5244
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 1.05s
                      Time elapsed: 00:21:11
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1293/1500 [0m                     

                       Computation: 96458 steps/s (collection: 0.859s, learning 0.160s)
             Mean action noise std: 3.86
          Mean value_function loss: 102.2619
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9152
                       Mean reward: 515.38
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.4018
    Episode_Reward/rotating_object: 100.7759
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 1.02s
                      Time elapsed: 00:21:12
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1294/1500 [0m                     

                       Computation: 104169 steps/s (collection: 0.791s, learning 0.153s)
             Mean action noise std: 3.86
          Mean value_function loss: 101.9396
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 19.9245
                       Mean reward: 546.07
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.4072
    Episode_Reward/rotating_object: 105.6205
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.94s
                      Time elapsed: 00:21:13
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1295/1500 [0m                     

                       Computation: 100804 steps/s (collection: 0.824s, learning 0.151s)
             Mean action noise std: 3.86
          Mean value_function loss: 103.6953
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.9274
                       Mean reward: 523.01
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.4055
    Episode_Reward/rotating_object: 105.9810
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.98s
                      Time elapsed: 00:21:14
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1296/1500 [0m                     

                       Computation: 89330 steps/s (collection: 0.929s, learning 0.172s)
             Mean action noise std: 3.86
          Mean value_function loss: 95.1017
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.9277
                       Mean reward: 516.50
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.4124
    Episode_Reward/rotating_object: 104.6657
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 1.10s
                      Time elapsed: 00:21:15
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1297/1500 [0m                     

                       Computation: 101891 steps/s (collection: 0.818s, learning 0.147s)
             Mean action noise std: 3.87
          Mean value_function loss: 101.3342
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.9221
                       Mean reward: 514.99
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.4085
    Episode_Reward/rotating_object: 106.8504
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.96s
                      Time elapsed: 00:21:16
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1298/1500 [0m                     

                       Computation: 98006 steps/s (collection: 0.872s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 104.7525
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.9229
                       Mean reward: 553.41
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.4027
    Episode_Reward/rotating_object: 105.7654
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 1.00s
                      Time elapsed: 00:21:17
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1299/1500 [0m                     

                       Computation: 105175 steps/s (collection: 0.828s, learning 0.107s)
             Mean action noise std: 3.87
          Mean value_function loss: 104.6829
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9280
                       Mean reward: 486.08
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 103.1731
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.93s
                      Time elapsed: 00:21:18
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1300/1500 [0m                     

                       Computation: 112640 steps/s (collection: 0.770s, learning 0.103s)
             Mean action noise std: 3.88
          Mean value_function loss: 98.4213
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.9312
                       Mean reward: 524.66
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.3956
    Episode_Reward/rotating_object: 101.7106
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.87s
                      Time elapsed: 00:21:19
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1301/1500 [0m                     

                       Computation: 105367 steps/s (collection: 0.822s, learning 0.111s)
             Mean action noise std: 3.88
          Mean value_function loss: 100.5833
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.9299
                       Mean reward: 519.12
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.3996
    Episode_Reward/rotating_object: 100.9337
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.93s
                      Time elapsed: 00:21:20
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1302/1500 [0m                     

                       Computation: 94463 steps/s (collection: 0.884s, learning 0.157s)
             Mean action noise std: 3.88
          Mean value_function loss: 106.9567
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.9288
                       Mean reward: 502.73
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.3995
    Episode_Reward/rotating_object: 101.5504
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 1.04s
                      Time elapsed: 00:21:21
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1303/1500 [0m                     

                       Computation: 89806 steps/s (collection: 0.882s, learning 0.213s)
             Mean action noise std: 3.88
          Mean value_function loss: 103.3195
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9332
                       Mean reward: 528.34
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4021
    Episode_Reward/rotating_object: 105.4062
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 1.09s
                      Time elapsed: 00:21:22
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1304/1500 [0m                     

                       Computation: 111595 steps/s (collection: 0.768s, learning 0.113s)
             Mean action noise std: 3.89
          Mean value_function loss: 94.1978
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.9439
                       Mean reward: 520.55
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.3920
    Episode_Reward/rotating_object: 99.1171
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.88s
                      Time elapsed: 00:21:23
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1305/1500 [0m                     

                       Computation: 107219 steps/s (collection: 0.804s, learning 0.113s)
             Mean action noise std: 3.89
          Mean value_function loss: 96.7241
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.9454
                       Mean reward: 515.05
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 105.0772
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.92s
                      Time elapsed: 00:21:24
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1306/1500 [0m                     

                       Computation: 99253 steps/s (collection: 0.858s, learning 0.132s)
             Mean action noise std: 3.89
          Mean value_function loss: 101.3035
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9476
                       Mean reward: 477.75
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.4061
    Episode_Reward/rotating_object: 103.1223
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.99s
                      Time elapsed: 00:21:25
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1307/1500 [0m                     

                       Computation: 93272 steps/s (collection: 0.927s, learning 0.127s)
             Mean action noise std: 3.89
          Mean value_function loss: 109.8654
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.9439
                       Mean reward: 539.07
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 0.4017
    Episode_Reward/rotating_object: 102.6542
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 1.05s
                      Time elapsed: 00:21:26
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1308/1500 [0m                     

                       Computation: 107451 steps/s (collection: 0.823s, learning 0.092s)
             Mean action noise std: 3.89
          Mean value_function loss: 94.6648
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9449
                       Mean reward: 494.21
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.4003
    Episode_Reward/rotating_object: 102.1815
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.91s
                      Time elapsed: 00:21:27
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1309/1500 [0m                     

                       Computation: 94957 steps/s (collection: 0.881s, learning 0.155s)
             Mean action noise std: 3.90
          Mean value_function loss: 101.6311
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.9542
                       Mean reward: 525.09
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.4005
    Episode_Reward/rotating_object: 103.7077
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 1.04s
                      Time elapsed: 00:21:28
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1310/1500 [0m                     

                       Computation: 102643 steps/s (collection: 0.866s, learning 0.092s)
             Mean action noise std: 3.90
          Mean value_function loss: 91.5318
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.9560
                       Mean reward: 552.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 102.8417
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.96s
                      Time elapsed: 00:21:29
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1311/1500 [0m                     

                       Computation: 105497 steps/s (collection: 0.836s, learning 0.096s)
             Mean action noise std: 3.90
          Mean value_function loss: 90.4401
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.9673
                       Mean reward: 488.84
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4003
    Episode_Reward/rotating_object: 101.5190
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.93s
                      Time elapsed: 00:21:30
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1312/1500 [0m                     

                       Computation: 90991 steps/s (collection: 0.926s, learning 0.154s)
             Mean action noise std: 3.90
          Mean value_function loss: 103.3781
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.9734
                       Mean reward: 561.28
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.4020
    Episode_Reward/rotating_object: 104.0758
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 1.08s
                      Time elapsed: 00:21:31
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1313/1500 [0m                     

                       Computation: 88749 steps/s (collection: 0.930s, learning 0.178s)
             Mean action noise std: 3.91
          Mean value_function loss: 90.9602
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9733
                       Mean reward: 539.94
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.4053
    Episode_Reward/rotating_object: 107.9953
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 1.11s
                      Time elapsed: 00:21:32
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1314/1500 [0m                     

                       Computation: 105920 steps/s (collection: 0.802s, learning 0.126s)
             Mean action noise std: 3.91
          Mean value_function loss: 97.6500
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.9810
                       Mean reward: 512.46
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.4009
    Episode_Reward/rotating_object: 102.1980
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.93s
                      Time elapsed: 00:21:33
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1315/1500 [0m                     

                       Computation: 109670 steps/s (collection: 0.802s, learning 0.095s)
             Mean action noise std: 3.91
          Mean value_function loss: 95.0481
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.9801
                       Mean reward: 490.29
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.3925
    Episode_Reward/rotating_object: 101.9842
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.90s
                      Time elapsed: 00:21:34
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1316/1500 [0m                     

                       Computation: 105138 steps/s (collection: 0.816s, learning 0.119s)
             Mean action noise std: 3.91
          Mean value_function loss: 95.4255
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9838
                       Mean reward: 512.17
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3967
    Episode_Reward/rotating_object: 105.4459
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.93s
                      Time elapsed: 00:21:35
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1317/1500 [0m                     

                       Computation: 107508 steps/s (collection: 0.784s, learning 0.130s)
             Mean action noise std: 3.92
          Mean value_function loss: 93.3543
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.9875
                       Mean reward: 513.55
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.3986
    Episode_Reward/rotating_object: 104.5782
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.91s
                      Time elapsed: 00:21:36
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1318/1500 [0m                     

                       Computation: 95359 steps/s (collection: 0.871s, learning 0.160s)
             Mean action noise std: 3.92
          Mean value_function loss: 105.7491
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9876
                       Mean reward: 538.20
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.3963
    Episode_Reward/rotating_object: 103.9516
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 1.03s
                      Time elapsed: 00:21:37
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1319/1500 [0m                     

                       Computation: 98072 steps/s (collection: 0.912s, learning 0.090s)
             Mean action noise std: 3.92
          Mean value_function loss: 112.0310
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.9880
                       Mean reward: 483.56
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.3974
    Episode_Reward/rotating_object: 103.5257
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 1.00s
                      Time elapsed: 00:21:38
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1320/1500 [0m                     

                       Computation: 107589 steps/s (collection: 0.811s, learning 0.103s)
             Mean action noise std: 3.92
          Mean value_function loss: 84.6278
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.9816
                       Mean reward: 560.40
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.3961
    Episode_Reward/rotating_object: 106.0703
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.91s
                      Time elapsed: 00:21:39
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1321/1500 [0m                     

                       Computation: 103842 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 3.92
          Mean value_function loss: 109.7058
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.9751
                       Mean reward: 508.23
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.3945
    Episode_Reward/rotating_object: 100.9050
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.95s
                      Time elapsed: 00:21:39
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1322/1500 [0m                     

                       Computation: 107577 steps/s (collection: 0.793s, learning 0.121s)
             Mean action noise std: 3.92
          Mean value_function loss: 96.7970
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9700
                       Mean reward: 548.95
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 106.1166
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.91s
                      Time elapsed: 00:21:40
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1323/1500 [0m                     

                       Computation: 99845 steps/s (collection: 0.848s, learning 0.136s)
             Mean action noise std: 3.93
          Mean value_function loss: 100.4566
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.9752
                       Mean reward: 514.99
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.4013
    Episode_Reward/rotating_object: 105.1397
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.98s
                      Time elapsed: 00:21:41
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1324/1500 [0m                     

                       Computation: 108573 steps/s (collection: 0.815s, learning 0.090s)
             Mean action noise std: 3.93
          Mean value_function loss: 102.4669
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.9716
                       Mean reward: 526.45
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.3894
    Episode_Reward/rotating_object: 104.2755
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.91s
                      Time elapsed: 00:21:42
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1325/1500 [0m                     

                       Computation: 99449 steps/s (collection: 0.805s, learning 0.183s)
             Mean action noise std: 3.93
          Mean value_function loss: 104.8510
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.9669
                       Mean reward: 547.19
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 102.9199
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.99s
                      Time elapsed: 00:21:43
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1326/1500 [0m                     

                       Computation: 111973 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 3.93
          Mean value_function loss: 91.4960
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9738
                       Mean reward: 521.03
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.3905
    Episode_Reward/rotating_object: 102.4681
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.88s
                      Time elapsed: 00:21:44
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1327/1500 [0m                     

                       Computation: 101469 steps/s (collection: 0.806s, learning 0.163s)
             Mean action noise std: 3.94
          Mean value_function loss: 100.5788
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.9788
                       Mean reward: 546.09
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.3951
    Episode_Reward/rotating_object: 102.4846
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.97s
                      Time elapsed: 00:21:45
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1328/1500 [0m                     

                       Computation: 103248 steps/s (collection: 0.792s, learning 0.160s)
             Mean action noise std: 3.94
          Mean value_function loss: 103.0015
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.9824
                       Mean reward: 552.11
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.3992
    Episode_Reward/rotating_object: 104.6706
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.95s
                      Time elapsed: 00:21:46
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1329/1500 [0m                     

                       Computation: 107241 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 3.94
          Mean value_function loss: 110.6035
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.9830
                       Mean reward: 546.31
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.4002
    Episode_Reward/rotating_object: 107.1047
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.92s
                      Time elapsed: 00:21:47
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1330/1500 [0m                     

                       Computation: 89446 steps/s (collection: 0.947s, learning 0.152s)
             Mean action noise std: 3.94
          Mean value_function loss: 99.9316
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.9839
                       Mean reward: 515.19
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 0.4001
    Episode_Reward/rotating_object: 104.4260
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 1.10s
                      Time elapsed: 00:21:48
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1331/1500 [0m                     

                       Computation: 99085 steps/s (collection: 0.867s, learning 0.126s)
             Mean action noise std: 3.94
          Mean value_function loss: 97.8212
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.9864
                       Mean reward: 524.05
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 103.1261
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.99s
                      Time elapsed: 00:21:49
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1332/1500 [0m                     

                       Computation: 80502 steps/s (collection: 0.997s, learning 0.224s)
             Mean action noise std: 3.95
          Mean value_function loss: 93.7885
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.9915
                       Mean reward: 484.14
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.4026
    Episode_Reward/rotating_object: 107.6900
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 1.22s
                      Time elapsed: 00:21:50
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1333/1500 [0m                     

                       Computation: 42297 steps/s (collection: 2.214s, learning 0.111s)
             Mean action noise std: 3.95
          Mean value_function loss: 108.8530
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.9940
                       Mean reward: 517.13
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.3991
    Episode_Reward/rotating_object: 103.8726
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.32s
                      Time elapsed: 00:21:53
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1334/1500 [0m                     

                       Computation: 32816 steps/s (collection: 2.864s, learning 0.132s)
             Mean action noise std: 3.95
          Mean value_function loss: 107.3992
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.9907
                       Mean reward: 545.79
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 0.3996
    Episode_Reward/rotating_object: 105.1767
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.00s
                      Time elapsed: 00:21:56
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1335/1500 [0m                     

                       Computation: 32578 steps/s (collection: 2.882s, learning 0.135s)
             Mean action noise std: 3.95
          Mean value_function loss: 99.9054
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.9908
                       Mean reward: 542.69
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3916
    Episode_Reward/rotating_object: 103.1506
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.02s
                      Time elapsed: 00:21:59
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1336/1500 [0m                     

                       Computation: 31444 steps/s (collection: 2.999s, learning 0.127s)
             Mean action noise std: 3.95
          Mean value_function loss: 93.4025
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.9920
                       Mean reward: 522.82
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.3999
    Episode_Reward/rotating_object: 102.9380
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.13s
                      Time elapsed: 00:22:02
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1337/1500 [0m                     

                       Computation: 30340 steps/s (collection: 3.083s, learning 0.157s)
             Mean action noise std: 3.95
          Mean value_function loss: 100.3602
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.9979
                       Mean reward: 537.68
               Mean episode length: 247.66
    Episode_Reward/reaching_object: 0.3988
    Episode_Reward/rotating_object: 105.9686
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.24s
                      Time elapsed: 00:22:05
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1338/1500 [0m                     

                       Computation: 30739 steps/s (collection: 3.067s, learning 0.131s)
             Mean action noise std: 3.95
          Mean value_function loss: 103.4055
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9994
                       Mean reward: 538.46
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.4008
    Episode_Reward/rotating_object: 101.5375
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.20s
                      Time elapsed: 00:22:08
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1339/1500 [0m                     

                       Computation: 31706 steps/s (collection: 2.963s, learning 0.137s)
             Mean action noise std: 3.96
          Mean value_function loss: 106.2111
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.0046
                       Mean reward: 527.12
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 105.5682
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.10s
                      Time elapsed: 00:22:11
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1340/1500 [0m                     

                       Computation: 32333 steps/s (collection: 2.910s, learning 0.131s)
             Mean action noise std: 3.96
          Mean value_function loss: 98.9055
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.0220
                       Mean reward: 558.87
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 105.9950
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.04s
                      Time elapsed: 00:22:14
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1341/1500 [0m                     

                       Computation: 21017 steps/s (collection: 4.513s, learning 0.164s)
             Mean action noise std: 3.96
          Mean value_function loss: 97.0926
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.0273
                       Mean reward: 560.88
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.3990
    Episode_Reward/rotating_object: 107.4273
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.68s
                      Time elapsed: 00:22:19
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1342/1500 [0m                     

                       Computation: 93646 steps/s (collection: 0.911s, learning 0.139s)
             Mean action noise std: 3.96
          Mean value_function loss: 100.1387
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.0299
                       Mean reward: 518.53
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.3978
    Episode_Reward/rotating_object: 103.6738
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.05s
                      Time elapsed: 00:22:20
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1343/1500 [0m                     

                       Computation: 106128 steps/s (collection: 0.826s, learning 0.101s)
             Mean action noise std: 3.97
          Mean value_function loss: 106.0755
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.0344
                       Mean reward: 505.48
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 0.3962
    Episode_Reward/rotating_object: 99.6368
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.93s
                      Time elapsed: 00:22:21
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1344/1500 [0m                     

                       Computation: 116544 steps/s (collection: 0.754s, learning 0.089s)
             Mean action noise std: 3.97
          Mean value_function loss: 101.9758
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.0445
                       Mean reward: 515.03
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.3997
    Episode_Reward/rotating_object: 100.4619
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.84s
                      Time elapsed: 00:22:22
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1345/1500 [0m                     

                       Computation: 112405 steps/s (collection: 0.755s, learning 0.120s)
             Mean action noise std: 3.97
          Mean value_function loss: 103.0124
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.0453
                       Mean reward: 453.60
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 0.3891
    Episode_Reward/rotating_object: 100.2728
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.87s
                      Time elapsed: 00:22:23
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1346/1500 [0m                     

                       Computation: 110167 steps/s (collection: 0.778s, learning 0.115s)
             Mean action noise std: 3.97
          Mean value_function loss: 106.9781
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.0515
                       Mean reward: 508.42
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.4032
    Episode_Reward/rotating_object: 106.9950
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.89s
                      Time elapsed: 00:22:24
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1347/1500 [0m                     

                       Computation: 113404 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 3.98
          Mean value_function loss: 102.6601
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.0571
                       Mean reward: 549.48
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.4028
    Episode_Reward/rotating_object: 102.5312
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.87s
                      Time elapsed: 00:22:24
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1348/1500 [0m                     

                       Computation: 114443 steps/s (collection: 0.739s, learning 0.120s)
             Mean action noise std: 3.98
          Mean value_function loss: 97.0401
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.0666
                       Mean reward: 523.57
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.3982
    Episode_Reward/rotating_object: 103.7764
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.86s
                      Time elapsed: 00:22:25
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1349/1500 [0m                     

                       Computation: 111751 steps/s (collection: 0.757s, learning 0.123s)
             Mean action noise std: 3.98
          Mean value_function loss: 96.3953
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.0732
                       Mean reward: 503.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3995
    Episode_Reward/rotating_object: 100.5082
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.88s
                      Time elapsed: 00:22:26
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1350/1500 [0m                     

                       Computation: 118291 steps/s (collection: 0.741s, learning 0.090s)
             Mean action noise std: 3.99
          Mean value_function loss: 93.8690
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.0806
                       Mean reward: 551.39
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 104.1317
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.83s
                      Time elapsed: 00:22:27
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1351/1500 [0m                     

                       Computation: 111739 steps/s (collection: 0.792s, learning 0.088s)
             Mean action noise std: 3.99
          Mean value_function loss: 85.0051
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.0814
                       Mean reward: 541.34
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3981
    Episode_Reward/rotating_object: 104.7384
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.88s
                      Time elapsed: 00:22:28
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1352/1500 [0m                     

                       Computation: 113203 steps/s (collection: 0.774s, learning 0.095s)
             Mean action noise std: 3.99
          Mean value_function loss: 79.0841
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.0870
                       Mean reward: 520.96
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.4031
    Episode_Reward/rotating_object: 101.6221
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.87s
                      Time elapsed: 00:22:29
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1353/1500 [0m                     

                       Computation: 111606 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 3.99
          Mean value_function loss: 92.0753
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.0888
                       Mean reward: 572.80
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 105.7757
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.88s
                      Time elapsed: 00:22:30
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1354/1500 [0m                     

                       Computation: 94417 steps/s (collection: 0.894s, learning 0.148s)
             Mean action noise std: 4.00
          Mean value_function loss: 93.9375
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.0957
                       Mean reward: 541.85
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.4124
    Episode_Reward/rotating_object: 108.0459
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.04s
                      Time elapsed: 00:22:31
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1355/1500 [0m                     

                       Computation: 98258 steps/s (collection: 0.895s, learning 0.106s)
             Mean action noise std: 4.00
          Mean value_function loss: 100.0233
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.0990
                       Mean reward: 527.14
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.3996
    Episode_Reward/rotating_object: 104.1889
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.00s
                      Time elapsed: 00:22:32
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1356/1500 [0m                     

                       Computation: 109996 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 4.00
          Mean value_function loss: 96.8808
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 20.1107
                       Mean reward: 528.68
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.4023
    Episode_Reward/rotating_object: 107.1165
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.89s
                      Time elapsed: 00:22:33
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1357/1500 [0m                     

                       Computation: 110241 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 4.01
          Mean value_function loss: 103.9598
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.1170
                       Mean reward: 542.95
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.4089
    Episode_Reward/rotating_object: 108.8687
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.89s
                      Time elapsed: 00:22:33
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1358/1500 [0m                     

                       Computation: 109552 steps/s (collection: 0.754s, learning 0.143s)
             Mean action noise std: 4.01
          Mean value_function loss: 92.7726
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.1241
                       Mean reward: 535.87
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.4045
    Episode_Reward/rotating_object: 106.3412
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.90s
                      Time elapsed: 00:22:34
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1359/1500 [0m                     

                       Computation: 101904 steps/s (collection: 0.824s, learning 0.141s)
             Mean action noise std: 4.01
          Mean value_function loss: 103.5596
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.1200
                       Mean reward: 539.88
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.4119
    Episode_Reward/rotating_object: 107.0407
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.96s
                      Time elapsed: 00:22:35
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1360/1500 [0m                     

                       Computation: 113038 steps/s (collection: 0.765s, learning 0.105s)
             Mean action noise std: 4.01
          Mean value_function loss: 113.6543
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.1271
                       Mean reward: 512.97
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 0.4119
    Episode_Reward/rotating_object: 109.0443
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.87s
                      Time elapsed: 00:22:36
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1361/1500 [0m                     

                       Computation: 114141 steps/s (collection: 0.764s, learning 0.098s)
             Mean action noise std: 4.02
          Mean value_function loss: 97.9070
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.1269
                       Mean reward: 508.03
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.4067
    Episode_Reward/rotating_object: 105.2449
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.86s
                      Time elapsed: 00:22:37
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1362/1500 [0m                     

                       Computation: 113120 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 4.02
          Mean value_function loss: 100.2905
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.1285
                       Mean reward: 474.86
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.4063
    Episode_Reward/rotating_object: 98.9084
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.87s
                      Time elapsed: 00:22:38
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1363/1500 [0m                     

                       Computation: 108199 steps/s (collection: 0.801s, learning 0.108s)
             Mean action noise std: 4.02
          Mean value_function loss: 107.7621
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.1265
                       Mean reward: 556.23
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4165
    Episode_Reward/rotating_object: 108.0337
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.91s
                      Time elapsed: 00:22:39
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1364/1500 [0m                     

                       Computation: 117756 steps/s (collection: 0.744s, learning 0.091s)
             Mean action noise std: 4.02
          Mean value_function loss: 100.5191
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1275
                       Mean reward: 534.69
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.4113
    Episode_Reward/rotating_object: 104.8588
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.83s
                      Time elapsed: 00:22:40
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1365/1500 [0m                     

                       Computation: 114457 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 4.02
          Mean value_function loss: 96.1130
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.1254
                       Mean reward: 540.94
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 0.4123
    Episode_Reward/rotating_object: 106.0832
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.86s
                      Time elapsed: 00:22:41
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1366/1500 [0m                     

                       Computation: 113635 steps/s (collection: 0.770s, learning 0.095s)
             Mean action noise std: 4.03
          Mean value_function loss: 103.3846
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.1382
                       Mean reward: 521.36
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.4021
    Episode_Reward/rotating_object: 102.5252
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.87s
                      Time elapsed: 00:22:41
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1367/1500 [0m                     

                       Computation: 111221 steps/s (collection: 0.765s, learning 0.119s)
             Mean action noise std: 4.03
          Mean value_function loss: 100.0375
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.1507
                       Mean reward: 472.38
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.4120
    Episode_Reward/rotating_object: 103.6353
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.88s
                      Time elapsed: 00:22:42
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1368/1500 [0m                     

                       Computation: 109403 steps/s (collection: 0.777s, learning 0.122s)
             Mean action noise std: 4.03
          Mean value_function loss: 93.4612
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.1601
                       Mean reward: 546.89
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.4163
    Episode_Reward/rotating_object: 104.8557
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.90s
                      Time elapsed: 00:22:43
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1369/1500 [0m                     

                       Computation: 114771 steps/s (collection: 0.754s, learning 0.103s)
             Mean action noise std: 4.03
          Mean value_function loss: 94.1753
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.1603
                       Mean reward: 518.46
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.4104
    Episode_Reward/rotating_object: 102.8887
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.86s
                      Time elapsed: 00:22:44
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1370/1500 [0m                     

                       Computation: 110643 steps/s (collection: 0.764s, learning 0.124s)
             Mean action noise std: 4.03
          Mean value_function loss: 91.9321
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.1702
                       Mean reward: 549.36
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.4121
    Episode_Reward/rotating_object: 106.0877
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.89s
                      Time elapsed: 00:22:45
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1371/1500 [0m                     

                       Computation: 106578 steps/s (collection: 0.816s, learning 0.106s)
             Mean action noise std: 4.04
          Mean value_function loss: 91.8765
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.1766
                       Mean reward: 512.12
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.4029
    Episode_Reward/rotating_object: 104.0555
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.92s
                      Time elapsed: 00:22:46
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1372/1500 [0m                     

                       Computation: 115390 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 4.04
          Mean value_function loss: 84.1084
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 20.1911
                       Mean reward: 549.58
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 106.2040
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.85s
                      Time elapsed: 00:22:47
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1373/1500 [0m                     

                       Computation: 110262 steps/s (collection: 0.764s, learning 0.128s)
             Mean action noise std: 4.04
          Mean value_function loss: 93.7761
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.1928
                       Mean reward: 485.85
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.4021
    Episode_Reward/rotating_object: 103.6833
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.89s
                      Time elapsed: 00:22:48
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1374/1500 [0m                     

                       Computation: 114950 steps/s (collection: 0.753s, learning 0.103s)
             Mean action noise std: 4.05
          Mean value_function loss: 90.6187
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.1966
                       Mean reward: 482.03
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.4036
    Episode_Reward/rotating_object: 103.0325
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.86s
                      Time elapsed: 00:22:48
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1375/1500 [0m                     

                       Computation: 108153 steps/s (collection: 0.749s, learning 0.160s)
             Mean action noise std: 4.05
          Mean value_function loss: 92.0053
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.1980
                       Mean reward: 530.89
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.4015
    Episode_Reward/rotating_object: 106.3865
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.91s
                      Time elapsed: 00:22:49
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1376/1500 [0m                     

                       Computation: 102056 steps/s (collection: 0.795s, learning 0.169s)
             Mean action noise std: 4.05
          Mean value_function loss: 97.2449
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.1994
                       Mean reward: 522.19
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.3925
    Episode_Reward/rotating_object: 104.8461
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.96s
                      Time elapsed: 00:22:50
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1377/1500 [0m                     

                       Computation: 104201 steps/s (collection: 0.789s, learning 0.155s)
             Mean action noise std: 4.05
          Mean value_function loss: 85.9770
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.2055
                       Mean reward: 569.69
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.3956
    Episode_Reward/rotating_object: 104.7400
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.94s
                      Time elapsed: 00:22:51
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1378/1500 [0m                     

                       Computation: 107648 steps/s (collection: 0.777s, learning 0.137s)
             Mean action noise std: 4.05
          Mean value_function loss: 83.7161
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.2027
                       Mean reward: 547.82
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3952
    Episode_Reward/rotating_object: 107.8118
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.91s
                      Time elapsed: 00:22:52
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1379/1500 [0m                     

                       Computation: 118729 steps/s (collection: 0.742s, learning 0.086s)
             Mean action noise std: 4.06
          Mean value_function loss: 86.2074
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.2113
                       Mean reward: 529.77
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.3953
    Episode_Reward/rotating_object: 104.6111
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.83s
                      Time elapsed: 00:22:53
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1380/1500 [0m                     

                       Computation: 114135 steps/s (collection: 0.768s, learning 0.094s)
             Mean action noise std: 4.06
          Mean value_function loss: 95.3081
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.2101
                       Mean reward: 531.63
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.3969
    Episode_Reward/rotating_object: 103.2749
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.86s
                      Time elapsed: 00:22:54
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1381/1500 [0m                     

                       Computation: 101242 steps/s (collection: 0.786s, learning 0.185s)
             Mean action noise std: 4.06
          Mean value_function loss: 93.3649
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.2227
                       Mean reward: 521.06
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.3947
    Episode_Reward/rotating_object: 107.7530
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.97s
                      Time elapsed: 00:22:55
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1382/1500 [0m                     

                       Computation: 107656 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 4.06
          Mean value_function loss: 92.4803
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 20.2262
                       Mean reward: 539.59
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 0.4011
    Episode_Reward/rotating_object: 106.8043
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.91s
                      Time elapsed: 00:22:56
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1383/1500 [0m                     

                       Computation: 114780 steps/s (collection: 0.767s, learning 0.089s)
             Mean action noise std: 4.06
          Mean value_function loss: 94.4005
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.2264
                       Mean reward: 562.90
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.4054
    Episode_Reward/rotating_object: 106.7806
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.86s
                      Time elapsed: 00:22:57
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1384/1500 [0m                     

                       Computation: 115045 steps/s (collection: 0.764s, learning 0.091s)
             Mean action noise std: 4.06
          Mean value_function loss: 114.7447
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.2245
                       Mean reward: 546.38
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.4012
    Episode_Reward/rotating_object: 106.0848
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.85s
                      Time elapsed: 00:22:57
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1385/1500 [0m                     

                       Computation: 106746 steps/s (collection: 0.769s, learning 0.152s)
             Mean action noise std: 4.06
          Mean value_function loss: 112.4953
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2253
                       Mean reward: 522.27
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.4104
    Episode_Reward/rotating_object: 103.9947
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.92s
                      Time elapsed: 00:22:58
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1386/1500 [0m                     

                       Computation: 102800 steps/s (collection: 0.801s, learning 0.155s)
             Mean action noise std: 4.07
          Mean value_function loss: 109.2789
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.2288
                       Mean reward: 519.25
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.4144
    Episode_Reward/rotating_object: 107.5046
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.96s
                      Time elapsed: 00:22:59
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1387/1500 [0m                     

                       Computation: 107620 steps/s (collection: 0.757s, learning 0.156s)
             Mean action noise std: 4.07
          Mean value_function loss: 104.6928
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.2262
                       Mean reward: 526.62
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.4062
    Episode_Reward/rotating_object: 106.2619
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.91s
                      Time elapsed: 00:23:00
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1388/1500 [0m                     

                       Computation: 109489 steps/s (collection: 0.768s, learning 0.130s)
             Mean action noise std: 4.07
          Mean value_function loss: 110.4860
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.2259
                       Mean reward: 547.26
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.4069
    Episode_Reward/rotating_object: 105.5117
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.90s
                      Time elapsed: 00:23:01
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1389/1500 [0m                     

                       Computation: 116218 steps/s (collection: 0.762s, learning 0.084s)
             Mean action noise std: 4.07
          Mean value_function loss: 107.0896
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.2277
                       Mean reward: 530.16
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.4031
    Episode_Reward/rotating_object: 102.3035
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.85s
                      Time elapsed: 00:23:02
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1390/1500 [0m                     

                       Computation: 109753 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 4.07
          Mean value_function loss: 101.5931
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.2284
                       Mean reward: 532.62
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.4122
    Episode_Reward/rotating_object: 105.5928
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.90s
                      Time elapsed: 00:23:03
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1391/1500 [0m                     

                       Computation: 119027 steps/s (collection: 0.733s, learning 0.093s)
             Mean action noise std: 4.08
          Mean value_function loss: 101.2223
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 20.2371
                       Mean reward: 516.60
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.4126
    Episode_Reward/rotating_object: 101.0339
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.83s
                      Time elapsed: 00:23:04
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1392/1500 [0m                     

                       Computation: 99974 steps/s (collection: 0.797s, learning 0.187s)
             Mean action noise std: 4.08
          Mean value_function loss: 103.5405
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.2457
                       Mean reward: 524.44
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.4092
    Episode_Reward/rotating_object: 106.2689
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.98s
                      Time elapsed: 00:23:05
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1393/1500 [0m                     

                       Computation: 107099 steps/s (collection: 0.821s, learning 0.097s)
             Mean action noise std: 4.08
          Mean value_function loss: 120.3276
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.2532
                       Mean reward: 502.30
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.4100
    Episode_Reward/rotating_object: 105.3764
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.92s
                      Time elapsed: 00:23:06
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1394/1500 [0m                     

                       Computation: 100371 steps/s (collection: 0.869s, learning 0.110s)
             Mean action noise std: 4.08
          Mean value_function loss: 108.4155
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.2614
                       Mean reward: 551.58
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 105.9781
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.98s
                      Time elapsed: 00:23:07
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1395/1500 [0m                     

                       Computation: 105821 steps/s (collection: 0.814s, learning 0.115s)
             Mean action noise std: 4.09
          Mean value_function loss: 110.5336
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.2701
                       Mean reward: 553.03
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.4180
    Episode_Reward/rotating_object: 108.0872
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.93s
                      Time elapsed: 00:23:08
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1396/1500 [0m                     

                       Computation: 104381 steps/s (collection: 0.831s, learning 0.111s)
             Mean action noise std: 4.09
          Mean value_function loss: 108.3190
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.2699
                       Mean reward: 521.07
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.4151
    Episode_Reward/rotating_object: 107.9834
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.94s
                      Time elapsed: 00:23:08
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1397/1500 [0m                     

                       Computation: 84356 steps/s (collection: 0.878s, learning 0.287s)
             Mean action noise std: 4.09
          Mean value_function loss: 102.8475
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.2686
                       Mean reward: 565.57
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.4130
    Episode_Reward/rotating_object: 107.2328
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.17s
                      Time elapsed: 00:23:10
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1398/1500 [0m                     

                       Computation: 91619 steps/s (collection: 0.946s, learning 0.127s)
             Mean action noise std: 4.10
          Mean value_function loss: 101.8265
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.2709
                       Mean reward: 494.00
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.4162
    Episode_Reward/rotating_object: 105.1930
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.07s
                      Time elapsed: 00:23:11
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1399/1500 [0m                     

                       Computation: 111405 steps/s (collection: 0.792s, learning 0.091s)
             Mean action noise std: 4.10
          Mean value_function loss: 100.4496
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.2738
                       Mean reward: 541.41
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.4168
    Episode_Reward/rotating_object: 105.6777
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.88s
                      Time elapsed: 00:23:12
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1400/1500 [0m                     

                       Computation: 106287 steps/s (collection: 0.780s, learning 0.145s)
             Mean action noise std: 4.10
          Mean value_function loss: 101.5109
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.2780
                       Mean reward: 523.95
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.4188
    Episode_Reward/rotating_object: 105.9715
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.92s
                      Time elapsed: 00:23:13
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1401/1500 [0m                     

                       Computation: 95870 steps/s (collection: 0.881s, learning 0.145s)
             Mean action noise std: 4.10
          Mean value_function loss: 95.5563
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2815
                       Mean reward: 551.09
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 108.6276
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 1.03s
                      Time elapsed: 00:23:14
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1402/1500 [0m                     

                       Computation: 103322 steps/s (collection: 0.806s, learning 0.145s)
             Mean action noise std: 4.10
          Mean value_function loss: 95.6254
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.2823
                       Mean reward: 513.93
               Mean episode length: 242.03
    Episode_Reward/reaching_object: 0.4151
    Episode_Reward/rotating_object: 108.5277
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.95s
                      Time elapsed: 00:23:15
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1403/1500 [0m                     

                       Computation: 102243 steps/s (collection: 0.792s, learning 0.169s)
             Mean action noise std: 4.11
          Mean value_function loss: 101.3080
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.2778
                       Mean reward: 519.46
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 0.4099
    Episode_Reward/rotating_object: 103.0750
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.96s
                      Time elapsed: 00:23:15
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1404/1500 [0m                     

                       Computation: 102851 steps/s (collection: 0.825s, learning 0.131s)
             Mean action noise std: 4.11
          Mean value_function loss: 98.1326
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.2822
                       Mean reward: 526.00
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.4182
    Episode_Reward/rotating_object: 107.5730
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.96s
                      Time elapsed: 00:23:16
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1405/1500 [0m                     

                       Computation: 92818 steps/s (collection: 0.876s, learning 0.183s)
             Mean action noise std: 4.11
          Mean value_function loss: 94.4172
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2871
                       Mean reward: 517.52
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.4153
    Episode_Reward/rotating_object: 108.0280
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.06s
                      Time elapsed: 00:23:17
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1406/1500 [0m                     

                       Computation: 96567 steps/s (collection: 0.889s, learning 0.129s)
             Mean action noise std: 4.12
          Mean value_function loss: 100.7260
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.2958
                       Mean reward: 516.15
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.4142
    Episode_Reward/rotating_object: 108.7405
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.02s
                      Time elapsed: 00:23:19
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1407/1500 [0m                     

                       Computation: 96267 steps/s (collection: 0.838s, learning 0.183s)
             Mean action noise std: 4.12
          Mean value_function loss: 96.0977
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 20.3053
                       Mean reward: 489.94
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.4049
    Episode_Reward/rotating_object: 106.6297
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.02s
                      Time elapsed: 00:23:20
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1408/1500 [0m                     

                       Computation: 80741 steps/s (collection: 1.077s, learning 0.141s)
             Mean action noise std: 4.12
          Mean value_function loss: 98.5555
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.3208
                       Mean reward: 567.60
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.4142
    Episode_Reward/rotating_object: 107.0158
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.22s
                      Time elapsed: 00:23:21
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1409/1500 [0m                     

                       Computation: 86820 steps/s (collection: 0.951s, learning 0.182s)
             Mean action noise std: 4.12
          Mean value_function loss: 97.6629
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.3218
                       Mean reward: 519.04
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.4053
    Episode_Reward/rotating_object: 104.2224
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 1.13s
                      Time elapsed: 00:23:22
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1410/1500 [0m                     

                       Computation: 99794 steps/s (collection: 0.867s, learning 0.118s)
             Mean action noise std: 4.13
          Mean value_function loss: 100.0390
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.3248
                       Mean reward: 558.27
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4047
    Episode_Reward/rotating_object: 103.7298
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.99s
                      Time elapsed: 00:23:23
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1411/1500 [0m                     

                       Computation: 96857 steps/s (collection: 0.845s, learning 0.170s)
             Mean action noise std: 4.13
          Mean value_function loss: 98.3961
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.3295
                       Mean reward: 532.16
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.4119
    Episode_Reward/rotating_object: 108.7645
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.01s
                      Time elapsed: 00:23:24
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1412/1500 [0m                     

                       Computation: 77590 steps/s (collection: 1.065s, learning 0.202s)
             Mean action noise std: 4.13
          Mean value_function loss: 94.1762
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3304
                       Mean reward: 575.38
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.4106
    Episode_Reward/rotating_object: 110.0479
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.27s
                      Time elapsed: 00:23:25
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1413/1500 [0m                     

                       Computation: 93671 steps/s (collection: 0.848s, learning 0.202s)
             Mean action noise std: 4.13
          Mean value_function loss: 83.7982
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.3334
                       Mean reward: 498.49
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 103.0056
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 1.05s
                      Time elapsed: 00:23:26
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1414/1500 [0m                     

                       Computation: 105403 steps/s (collection: 0.787s, learning 0.146s)
             Mean action noise std: 4.13
          Mean value_function loss: 102.9239
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.3315
                       Mean reward: 559.68
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.4107
    Episode_Reward/rotating_object: 107.2041
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.93s
                      Time elapsed: 00:23:27
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1415/1500 [0m                     

                       Computation: 101561 steps/s (collection: 0.776s, learning 0.192s)
             Mean action noise std: 4.14
          Mean value_function loss: 92.2886
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.3333
                       Mean reward: 549.69
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.4098
    Episode_Reward/rotating_object: 107.5103
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.97s
                      Time elapsed: 00:23:28
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1416/1500 [0m                     

                       Computation: 108420 steps/s (collection: 0.813s, learning 0.093s)
             Mean action noise std: 4.14
          Mean value_function loss: 97.3617
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.3321
                       Mean reward: 588.94
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.4100
    Episode_Reward/rotating_object: 106.7495
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.91s
                      Time elapsed: 00:23:29
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1417/1500 [0m                     

                       Computation: 111140 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 4.14
          Mean value_function loss: 99.5378
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.3342
                       Mean reward: 536.88
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.4126
    Episode_Reward/rotating_object: 107.5655
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.88s
                      Time elapsed: 00:23:30
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1418/1500 [0m                     

                       Computation: 109636 steps/s (collection: 0.781s, learning 0.116s)
             Mean action noise std: 4.14
          Mean value_function loss: 93.3673
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.3369
                       Mean reward: 545.17
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.4066
    Episode_Reward/rotating_object: 104.0134
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.90s
                      Time elapsed: 00:23:31
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1419/1500 [0m                     

                       Computation: 104305 steps/s (collection: 0.821s, learning 0.122s)
             Mean action noise std: 4.14
          Mean value_function loss: 99.8972
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.3435
                       Mean reward: 523.35
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.4149
    Episode_Reward/rotating_object: 109.9577
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.94s
                      Time elapsed: 00:23:32
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1420/1500 [0m                     

                       Computation: 111795 steps/s (collection: 0.789s, learning 0.091s)
             Mean action noise std: 4.15
          Mean value_function loss: 101.7375
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.3512
                       Mean reward: 504.36
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 0.4084
    Episode_Reward/rotating_object: 105.3343
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.88s
                      Time elapsed: 00:23:33
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1421/1500 [0m                     

                       Computation: 107588 steps/s (collection: 0.807s, learning 0.106s)
             Mean action noise std: 4.15
          Mean value_function loss: 98.2609
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.3595
                       Mean reward: 532.05
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 0.4107
    Episode_Reward/rotating_object: 106.6265
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.91s
                      Time elapsed: 00:23:34
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1422/1500 [0m                     

                       Computation: 109690 steps/s (collection: 0.796s, learning 0.101s)
             Mean action noise std: 4.15
          Mean value_function loss: 99.6288
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 20.3678
                       Mean reward: 591.83
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.4177
    Episode_Reward/rotating_object: 112.2887
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.90s
                      Time elapsed: 00:23:34
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1423/1500 [0m                     

                       Computation: 111187 steps/s (collection: 0.777s, learning 0.107s)
             Mean action noise std: 4.15
          Mean value_function loss: 104.5889
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.3680
                       Mean reward: 520.60
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.4146
    Episode_Reward/rotating_object: 107.8331
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.88s
                      Time elapsed: 00:23:35
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1424/1500 [0m                     

                       Computation: 108301 steps/s (collection: 0.763s, learning 0.145s)
             Mean action noise std: 4.15
          Mean value_function loss: 104.8812
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.3692
                       Mean reward: 556.88
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.4141
    Episode_Reward/rotating_object: 108.9738
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.91s
                      Time elapsed: 00:23:36
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1425/1500 [0m                     

                       Computation: 109259 steps/s (collection: 0.761s, learning 0.139s)
             Mean action noise std: 4.15
          Mean value_function loss: 99.5933
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.3734
                       Mean reward: 542.70
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.4148
    Episode_Reward/rotating_object: 109.3677
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.90s
                      Time elapsed: 00:23:37
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1426/1500 [0m                     

                       Computation: 108333 steps/s (collection: 0.780s, learning 0.128s)
             Mean action noise std: 4.16
          Mean value_function loss: 98.8133
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.3739
                       Mean reward: 554.76
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.4088
    Episode_Reward/rotating_object: 108.2842
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.91s
                      Time elapsed: 00:23:38
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1427/1500 [0m                     

                       Computation: 113453 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 4.16
          Mean value_function loss: 92.7478
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3701
                       Mean reward: 521.89
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.4152
    Episode_Reward/rotating_object: 106.4017
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.87s
                      Time elapsed: 00:23:39
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1428/1500 [0m                     

                       Computation: 92177 steps/s (collection: 0.839s, learning 0.227s)
             Mean action noise std: 4.16
          Mean value_function loss: 87.5743
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.3786
                       Mean reward: 555.81
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.4160
    Episode_Reward/rotating_object: 112.2876
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.07s
                      Time elapsed: 00:23:40
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1429/1500 [0m                     

                       Computation: 110544 steps/s (collection: 0.799s, learning 0.090s)
             Mean action noise std: 4.16
          Mean value_function loss: 89.0474
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.3786
                       Mean reward: 512.06
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.3980
    Episode_Reward/rotating_object: 103.3502
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.89s
                      Time elapsed: 00:23:41
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1430/1500 [0m                     

                       Computation: 104833 steps/s (collection: 0.829s, learning 0.109s)
             Mean action noise std: 4.16
          Mean value_function loss: 93.6696
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.3810
                       Mean reward: 525.27
               Mean episode length: 243.65
    Episode_Reward/reaching_object: 0.4149
    Episode_Reward/rotating_object: 108.6853
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.94s
                      Time elapsed: 00:23:42
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1431/1500 [0m                     

                       Computation: 110434 steps/s (collection: 0.784s, learning 0.107s)
             Mean action noise std: 4.17
          Mean value_function loss: 93.3526
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.3913
                       Mean reward: 517.40
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 0.4031
    Episode_Reward/rotating_object: 106.7653
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.89s
                      Time elapsed: 00:23:43
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1432/1500 [0m                     

                       Computation: 112662 steps/s (collection: 0.785s, learning 0.087s)
             Mean action noise std: 4.17
          Mean value_function loss: 95.3923
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.4063
                       Mean reward: 551.20
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.4048
    Episode_Reward/rotating_object: 104.8079
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.87s
                      Time elapsed: 00:23:44
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1433/1500 [0m                     

                       Computation: 101816 steps/s (collection: 0.839s, learning 0.126s)
             Mean action noise std: 4.17
          Mean value_function loss: 97.2862
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.4125
                       Mean reward: 511.89
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.4066
    Episode_Reward/rotating_object: 103.5838
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.97s
                      Time elapsed: 00:23:45
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1434/1500 [0m                     

                       Computation: 109058 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 4.17
          Mean value_function loss: 96.8325
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.4192
                       Mean reward: 540.52
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.4112
    Episode_Reward/rotating_object: 104.1579
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.90s
                      Time elapsed: 00:23:45
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1435/1500 [0m                     

                       Computation: 104714 steps/s (collection: 0.799s, learning 0.140s)
             Mean action noise std: 4.18
          Mean value_function loss: 88.2921
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.4245
                       Mean reward: 551.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4128
    Episode_Reward/rotating_object: 108.9355
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.94s
                      Time elapsed: 00:23:46
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1436/1500 [0m                     

                       Computation: 115746 steps/s (collection: 0.759s, learning 0.090s)
             Mean action noise std: 4.18
          Mean value_function loss: 90.7280
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.4288
                       Mean reward: 536.79
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.4092
    Episode_Reward/rotating_object: 108.0988
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.85s
                      Time elapsed: 00:23:47
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1437/1500 [0m                     

                       Computation: 114781 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 4.18
          Mean value_function loss: 85.0484
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.4312
                       Mean reward: 519.86
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.4138
    Episode_Reward/rotating_object: 108.0705
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.86s
                      Time elapsed: 00:23:48
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1438/1500 [0m                     

                       Computation: 105865 steps/s (collection: 0.810s, learning 0.119s)
             Mean action noise std: 4.19
          Mean value_function loss: 86.7961
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.4507
                       Mean reward: 518.00
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.4037
    Episode_Reward/rotating_object: 108.1468
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.93s
                      Time elapsed: 00:23:49
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1439/1500 [0m                     

                       Computation: 95772 steps/s (collection: 0.851s, learning 0.176s)
             Mean action noise std: 4.19
          Mean value_function loss: 87.7842
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.4685
                       Mean reward: 557.94
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.4049
    Episode_Reward/rotating_object: 106.9165
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.03s
                      Time elapsed: 00:23:50
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1440/1500 [0m                     

                       Computation: 113430 steps/s (collection: 0.774s, learning 0.093s)
             Mean action noise std: 4.20
          Mean value_function loss: 78.3923
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.4794
                       Mean reward: 505.19
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.4113
    Episode_Reward/rotating_object: 107.4502
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.87s
                      Time elapsed: 00:23:51
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1441/1500 [0m                     

                       Computation: 115066 steps/s (collection: 0.767s, learning 0.087s)
             Mean action noise std: 4.20
          Mean value_function loss: 94.5122
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.4925
                       Mean reward: 497.82
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.4006
    Episode_Reward/rotating_object: 103.2297
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.85s
                      Time elapsed: 00:23:52
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1442/1500 [0m                     

                       Computation: 114345 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 4.20
          Mean value_function loss: 79.1508
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.5036
                       Mean reward: 526.01
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.3998
    Episode_Reward/rotating_object: 107.0041
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.86s
                      Time elapsed: 00:23:53
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1443/1500 [0m                     

                       Computation: 114438 steps/s (collection: 0.756s, learning 0.103s)
             Mean action noise std: 4.20
          Mean value_function loss: 98.6655
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.4913
                       Mean reward: 528.95
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.4072
    Episode_Reward/rotating_object: 108.8433
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.86s
                      Time elapsed: 00:23:53
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1444/1500 [0m                     

                       Computation: 111472 steps/s (collection: 0.747s, learning 0.135s)
             Mean action noise std: 4.21
          Mean value_function loss: 90.9936
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.4960
                       Mean reward: 552.26
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.4042
    Episode_Reward/rotating_object: 110.2634
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.88s
                      Time elapsed: 00:23:54
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1445/1500 [0m                     

                       Computation: 111516 steps/s (collection: 0.776s, learning 0.106s)
             Mean action noise std: 4.21
          Mean value_function loss: 91.6783
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.4938
                       Mean reward: 572.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4120
    Episode_Reward/rotating_object: 108.6976
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.88s
                      Time elapsed: 00:23:55
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1446/1500 [0m                     

                       Computation: 113195 steps/s (collection: 0.753s, learning 0.116s)
             Mean action noise std: 4.21
          Mean value_function loss: 99.3444
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.4994
                       Mean reward: 557.53
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.4081
    Episode_Reward/rotating_object: 107.3903
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.87s
                      Time elapsed: 00:23:56
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1447/1500 [0m                     

                       Computation: 109297 steps/s (collection: 0.789s, learning 0.111s)
             Mean action noise std: 4.22
          Mean value_function loss: 97.9962
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4987
                       Mean reward: 549.47
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.4150
    Episode_Reward/rotating_object: 109.9708
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.90s
                      Time elapsed: 00:23:57
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1448/1500 [0m                     

                       Computation: 118384 steps/s (collection: 0.744s, learning 0.087s)
             Mean action noise std: 4.22
          Mean value_function loss: 93.8541
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.5015
                       Mean reward: 547.94
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.4024
    Episode_Reward/rotating_object: 108.8691
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.83s
                      Time elapsed: 00:23:58
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1449/1500 [0m                     

                       Computation: 115274 steps/s (collection: 0.747s, learning 0.106s)
             Mean action noise std: 4.22
          Mean value_function loss: 89.2299
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.5169
                       Mean reward: 580.02
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.4134
    Episode_Reward/rotating_object: 111.3889
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.85s
                      Time elapsed: 00:23:59
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1450/1500 [0m                     

                       Computation: 110545 steps/s (collection: 0.768s, learning 0.122s)
             Mean action noise std: 4.22
          Mean value_function loss: 94.9222
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.5243
                       Mean reward: 538.89
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.4070
    Episode_Reward/rotating_object: 106.5270
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.89s
                      Time elapsed: 00:24:00
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1451/1500 [0m                     

                       Computation: 115856 steps/s (collection: 0.749s, learning 0.099s)
             Mean action noise std: 4.23
          Mean value_function loss: 90.9845
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.5318
                       Mean reward: 529.03
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.4124
    Episode_Reward/rotating_object: 107.5290
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.85s
                      Time elapsed: 00:24:00
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1452/1500 [0m                     

                       Computation: 108912 steps/s (collection: 0.779s, learning 0.123s)
             Mean action noise std: 4.23
          Mean value_function loss: 77.6799
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.5371
                       Mean reward: 542.46
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4069
    Episode_Reward/rotating_object: 109.4111
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.90s
                      Time elapsed: 00:24:01
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1453/1500 [0m                     

                       Computation: 111881 steps/s (collection: 0.762s, learning 0.116s)
             Mean action noise std: 4.23
          Mean value_function loss: 90.7856
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.5370
                       Mean reward: 511.00
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 0.4162
    Episode_Reward/rotating_object: 109.3920
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.88s
                      Time elapsed: 00:24:02
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1454/1500 [0m                     

                       Computation: 106743 steps/s (collection: 0.798s, learning 0.123s)
             Mean action noise std: 4.23
          Mean value_function loss: 101.1500
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.5429
                       Mean reward: 548.43
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.4186
    Episode_Reward/rotating_object: 111.4202
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.92s
                      Time elapsed: 00:24:03
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1455/1500 [0m                     

                       Computation: 109967 steps/s (collection: 0.788s, learning 0.106s)
             Mean action noise std: 4.24
          Mean value_function loss: 95.1611
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.5487
                       Mean reward: 545.79
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.4077
    Episode_Reward/rotating_object: 107.3531
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.89s
                      Time elapsed: 00:24:04
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1456/1500 [0m                     

                       Computation: 113157 steps/s (collection: 0.743s, learning 0.126s)
             Mean action noise std: 4.24
          Mean value_function loss: 92.2210
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.5483
                       Mean reward: 537.73
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.4197
    Episode_Reward/rotating_object: 110.6331
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.87s
                      Time elapsed: 00:24:05
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1457/1500 [0m                     

                       Computation: 111661 steps/s (collection: 0.771s, learning 0.109s)
             Mean action noise std: 4.24
          Mean value_function loss: 96.8307
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.5456
                       Mean reward: 583.22
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.4139
    Episode_Reward/rotating_object: 111.1964
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.88s
                      Time elapsed: 00:24:06
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1458/1500 [0m                     

                       Computation: 108465 steps/s (collection: 0.783s, learning 0.123s)
             Mean action noise std: 4.24
          Mean value_function loss: 91.7087
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.5386
                       Mean reward: 549.96
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.4155
    Episode_Reward/rotating_object: 109.0682
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.91s
                      Time elapsed: 00:24:07
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1459/1500 [0m                     

                       Computation: 98147 steps/s (collection: 0.900s, learning 0.102s)
             Mean action noise std: 4.25
          Mean value_function loss: 98.4858
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.5400
                       Mean reward: 534.96
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.4154
    Episode_Reward/rotating_object: 108.1122
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.00s
                      Time elapsed: 00:24:08
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1460/1500 [0m                     

                       Computation: 108114 steps/s (collection: 0.820s, learning 0.089s)
             Mean action noise std: 4.25
          Mean value_function loss: 103.4842
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.5422
                       Mean reward: 543.74
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.4114
    Episode_Reward/rotating_object: 109.5428
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.91s
                      Time elapsed: 00:24:09
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1461/1500 [0m                     

                       Computation: 107520 steps/s (collection: 0.768s, learning 0.146s)
             Mean action noise std: 4.25
          Mean value_function loss: 104.9394
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.5456
                       Mean reward: 574.25
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.4155
    Episode_Reward/rotating_object: 112.7127
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.91s
                      Time elapsed: 00:24:09
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1462/1500 [0m                     

                       Computation: 100432 steps/s (collection: 0.885s, learning 0.094s)
             Mean action noise std: 4.25
          Mean value_function loss: 110.9494
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.5492
                       Mean reward: 542.89
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.4129
    Episode_Reward/rotating_object: 106.8120
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.98s
                      Time elapsed: 00:24:10
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1463/1500 [0m                     

                       Computation: 108077 steps/s (collection: 0.819s, learning 0.091s)
             Mean action noise std: 4.25
          Mean value_function loss: 110.7424
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.5488
                       Mean reward: 580.92
               Mean episode length: 244.00
    Episode_Reward/reaching_object: 0.4181
    Episode_Reward/rotating_object: 110.3110
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.91s
                      Time elapsed: 00:24:11
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1464/1500 [0m                     

                       Computation: 102683 steps/s (collection: 0.778s, learning 0.180s)
             Mean action noise std: 4.25
          Mean value_function loss: 115.3842
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.5543
                       Mean reward: 528.57
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.4240
    Episode_Reward/rotating_object: 108.9216
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.96s
                      Time elapsed: 00:24:12
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1465/1500 [0m                     

                       Computation: 107442 steps/s (collection: 0.822s, learning 0.093s)
             Mean action noise std: 4.26
          Mean value_function loss: 111.9142
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.5476
                       Mean reward: 509.30
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 107.8348
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.91s
                      Time elapsed: 00:24:13
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1466/1500 [0m                     

                       Computation: 98464 steps/s (collection: 0.788s, learning 0.211s)
             Mean action noise std: 4.26
          Mean value_function loss: 107.0908
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.5440
                       Mean reward: 532.30
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 0.4227
    Episode_Reward/rotating_object: 108.8676
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.00s
                      Time elapsed: 00:24:14
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1467/1500 [0m                     

                       Computation: 103790 steps/s (collection: 0.850s, learning 0.097s)
             Mean action noise std: 4.26
          Mean value_function loss: 118.1559
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.5545
                       Mean reward: 554.90
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 108.1874
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.95s
                      Time elapsed: 00:24:15
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1468/1500 [0m                     

                       Computation: 93601 steps/s (collection: 0.830s, learning 0.220s)
             Mean action noise std: 4.26
          Mean value_function loss: 112.6580
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.5645
                       Mean reward: 552.08
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.4246
    Episode_Reward/rotating_object: 111.8812
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.05s
                      Time elapsed: 00:24:16
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1469/1500 [0m                     

                       Computation: 103810 steps/s (collection: 0.836s, learning 0.111s)
             Mean action noise std: 4.26
          Mean value_function loss: 115.2309
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 20.5675
                       Mean reward: 571.18
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.4217
    Episode_Reward/rotating_object: 107.3596
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.95s
                      Time elapsed: 00:24:17
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1470/1500 [0m                     

                       Computation: 106009 steps/s (collection: 0.769s, learning 0.159s)
             Mean action noise std: 4.26
          Mean value_function loss: 114.8275
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.5662
                       Mean reward: 553.91
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.4225
    Episode_Reward/rotating_object: 107.0938
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.93s
                      Time elapsed: 00:24:18
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1471/1500 [0m                     

                       Computation: 113131 steps/s (collection: 0.771s, learning 0.097s)
             Mean action noise std: 4.27
          Mean value_function loss: 102.5244
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.5668
                       Mean reward: 568.07
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.4232
    Episode_Reward/rotating_object: 107.1100
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.87s
                      Time elapsed: 00:24:19
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1472/1500 [0m                     

                       Computation: 115964 steps/s (collection: 0.756s, learning 0.092s)
             Mean action noise std: 4.27
          Mean value_function loss: 106.5778
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.5782
                       Mean reward: 597.19
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.4232
    Episode_Reward/rotating_object: 111.4511
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.85s
                      Time elapsed: 00:24:20
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1473/1500 [0m                     

                       Computation: 114041 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 4.27
          Mean value_function loss: 98.7959
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 20.5846
                       Mean reward: 576.51
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.4201
    Episode_Reward/rotating_object: 107.1619
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.86s
                      Time elapsed: 00:24:21
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1474/1500 [0m                     

                       Computation: 112604 steps/s (collection: 0.787s, learning 0.086s)
             Mean action noise std: 4.27
          Mean value_function loss: 110.9982
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.5841
                       Mean reward: 561.83
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.4213
    Episode_Reward/rotating_object: 111.6832
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.87s
                      Time elapsed: 00:24:22
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1475/1500 [0m                     

                       Computation: 111900 steps/s (collection: 0.771s, learning 0.108s)
             Mean action noise std: 4.27
          Mean value_function loss: 94.3152
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.5890
                       Mean reward: 540.39
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.4153
    Episode_Reward/rotating_object: 106.3336
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.88s
                      Time elapsed: 00:24:22
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1476/1500 [0m                     

                       Computation: 107502 steps/s (collection: 0.787s, learning 0.127s)
             Mean action noise std: 4.28
          Mean value_function loss: 93.4545
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.5851
                       Mean reward: 534.17
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.4093
    Episode_Reward/rotating_object: 107.3689
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.91s
                      Time elapsed: 00:24:23
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1477/1500 [0m                     

                       Computation: 109308 steps/s (collection: 0.777s, learning 0.122s)
             Mean action noise std: 4.28
          Mean value_function loss: 99.3629
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.5902
                       Mean reward: 538.16
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.4218
    Episode_Reward/rotating_object: 109.2826
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.90s
                      Time elapsed: 00:24:24
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1478/1500 [0m                     

                       Computation: 103876 steps/s (collection: 0.802s, learning 0.145s)
             Mean action noise std: 4.28
          Mean value_function loss: 98.8728
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.6028
                       Mean reward: 509.95
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 0.4150
    Episode_Reward/rotating_object: 105.4540
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.95s
                      Time elapsed: 00:24:25
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1479/1500 [0m                     

                       Computation: 94369 steps/s (collection: 0.892s, learning 0.150s)
             Mean action noise std: 4.29
          Mean value_function loss: 103.2988
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6081
                       Mean reward: 526.20
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.4156
    Episode_Reward/rotating_object: 108.6579
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.04s
                      Time elapsed: 00:24:26
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1480/1500 [0m                     

                       Computation: 91669 steps/s (collection: 0.857s, learning 0.216s)
             Mean action noise std: 4.29
          Mean value_function loss: 98.3462
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.6140
                       Mean reward: 546.43
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.4217
    Episode_Reward/rotating_object: 112.0747
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 1.07s
                      Time elapsed: 00:24:27
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1481/1500 [0m                     

                       Computation: 95943 steps/s (collection: 0.894s, learning 0.131s)
             Mean action noise std: 4.29
          Mean value_function loss: 95.7319
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 20.6154
                       Mean reward: 562.69
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.4217
    Episode_Reward/rotating_object: 110.4800
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 1.02s
                      Time elapsed: 00:24:28
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1482/1500 [0m                     

                       Computation: 99180 steps/s (collection: 0.864s, learning 0.127s)
             Mean action noise std: 4.29
          Mean value_function loss: 83.9600
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.6122
                       Mean reward: 522.66
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.4293
    Episode_Reward/rotating_object: 112.1818
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.99s
                      Time elapsed: 00:24:29
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1483/1500 [0m                     

                       Computation: 104564 steps/s (collection: 0.784s, learning 0.156s)
             Mean action noise std: 4.30
          Mean value_function loss: 82.5056
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.6103
                       Mean reward: 564.07
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.4192
    Episode_Reward/rotating_object: 110.5307
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.94s
                      Time elapsed: 00:24:30
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1484/1500 [0m                     

                       Computation: 106709 steps/s (collection: 0.771s, learning 0.150s)
             Mean action noise std: 4.30
          Mean value_function loss: 96.8068
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.6209
                       Mean reward: 534.31
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 0.4164
    Episode_Reward/rotating_object: 106.9700
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.92s
                      Time elapsed: 00:24:31
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1485/1500 [0m                     

                       Computation: 103488 steps/s (collection: 0.770s, learning 0.180s)
             Mean action noise std: 4.30
          Mean value_function loss: 99.9188
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.6261
                       Mean reward: 543.83
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.4174
    Episode_Reward/rotating_object: 106.7718
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.95s
                      Time elapsed: 00:24:32
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1486/1500 [0m                     

                       Computation: 110836 steps/s (collection: 0.753s, learning 0.134s)
             Mean action noise std: 4.30
          Mean value_function loss: 90.5658
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.6243
                       Mean reward: 574.87
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.4224
    Episode_Reward/rotating_object: 112.7829
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.89s
                      Time elapsed: 00:24:33
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1487/1500 [0m                     

                       Computation: 109520 steps/s (collection: 0.798s, learning 0.100s)
             Mean action noise std: 4.30
          Mean value_function loss: 90.2122
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.6287
                       Mean reward: 532.84
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.4151
    Episode_Reward/rotating_object: 106.5309
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.90s
                      Time elapsed: 00:24:34
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1488/1500 [0m                     

                       Computation: 114372 steps/s (collection: 0.760s, learning 0.099s)
             Mean action noise std: 4.31
          Mean value_function loss: 89.2637
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.6336
                       Mean reward: 548.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4206
    Episode_Reward/rotating_object: 109.2430
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.86s
                      Time elapsed: 00:24:35
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1489/1500 [0m                     

                       Computation: 110837 steps/s (collection: 0.771s, learning 0.116s)
             Mean action noise std: 4.31
          Mean value_function loss: 97.5424
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 20.6427
                       Mean reward: 539.59
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.4179
    Episode_Reward/rotating_object: 110.6058
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.89s
                      Time elapsed: 00:24:36
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1490/1500 [0m                     

                       Computation: 105748 steps/s (collection: 0.810s, learning 0.120s)
             Mean action noise std: 4.31
          Mean value_function loss: 91.9552
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 20.6467
                       Mean reward: 520.12
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.4156
    Episode_Reward/rotating_object: 108.7057
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.93s
                      Time elapsed: 00:24:37
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1491/1500 [0m                     

                       Computation: 108561 steps/s (collection: 0.818s, learning 0.087s)
             Mean action noise std: 4.32
          Mean value_function loss: 92.5023
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.6569
                       Mean reward: 550.57
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.4212
    Episode_Reward/rotating_object: 112.9138
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.91s
                      Time elapsed: 00:24:38
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1492/1500 [0m                     

                       Computation: 111448 steps/s (collection: 0.770s, learning 0.112s)
             Mean action noise std: 4.32
          Mean value_function loss: 84.7031
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.6642
                       Mean reward: 507.92
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.4105
    Episode_Reward/rotating_object: 107.8495
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.88s
                      Time elapsed: 00:24:38
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1493/1500 [0m                     

                       Computation: 116439 steps/s (collection: 0.752s, learning 0.092s)
             Mean action noise std: 4.32
          Mean value_function loss: 92.5813
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.6673
                       Mean reward: 570.62
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.4185
    Episode_Reward/rotating_object: 113.6619
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.84s
                      Time elapsed: 00:24:39
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1494/1500 [0m                     

                       Computation: 106627 steps/s (collection: 0.768s, learning 0.154s)
             Mean action noise std: 4.32
          Mean value_function loss: 99.1848
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 20.6728
                       Mean reward: 588.78
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.4125
    Episode_Reward/rotating_object: 110.2044
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.92s
                      Time elapsed: 00:24:40
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1495/1500 [0m                     

                       Computation: 108967 steps/s (collection: 0.778s, learning 0.125s)
             Mean action noise std: 4.32
          Mean value_function loss: 101.1095
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.6711
                       Mean reward: 518.76
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.4087
    Episode_Reward/rotating_object: 108.1359
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.90s
                      Time elapsed: 00:24:41
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1496/1500 [0m                     

                       Computation: 113803 steps/s (collection: 0.760s, learning 0.104s)
             Mean action noise std: 4.33
          Mean value_function loss: 90.5667
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.6747
                       Mean reward: 566.24
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.4169
    Episode_Reward/rotating_object: 109.8698
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.86s
                      Time elapsed: 00:24:42
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1497/1500 [0m                     

                       Computation: 105977 steps/s (collection: 0.816s, learning 0.112s)
             Mean action noise std: 4.33
          Mean value_function loss: 95.4246
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.6827
                       Mean reward: 511.06
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.4115
    Episode_Reward/rotating_object: 107.0144
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.93s
                      Time elapsed: 00:24:43
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1498/1500 [0m                     

                       Computation: 106580 steps/s (collection: 0.826s, learning 0.097s)
             Mean action noise std: 4.34
          Mean value_function loss: 97.5921
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6909
                       Mean reward: 559.06
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.4063
    Episode_Reward/rotating_object: 109.0030
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.92s
                      Time elapsed: 00:24:44
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1499/1500 [0m                     

                       Computation: 109539 steps/s (collection: 0.788s, learning 0.109s)
             Mean action noise std: 4.34
          Mean value_function loss: 106.0710
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.6996
                       Mean reward: 561.50
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.4132
    Episode_Reward/rotating_object: 107.7018
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.90s
                      Time elapsed: 00:24:45
                               ETA: 00:00:00

