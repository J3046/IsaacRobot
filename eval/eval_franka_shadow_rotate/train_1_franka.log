################################################################################
                      [1m Learning iteration 0/1500 [0m                       

                       Computation: 15804 steps/s (collection: 5.977s, learning 0.244s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0020
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 11.3601
                       Mean reward: 0.00
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0003
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 6.22s
                      Time elapsed: 00:00:06
                               ETA: 02:35:30

################################################################################
                      [1m Learning iteration 1/1500 [0m                       

                       Computation: 31759 steps/s (collection: 2.946s, learning 0.149s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 11.3786
                       Mean reward: 0.00
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0013
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.10s
                      Time elapsed: 00:00:09
                               ETA: 01:56:21

################################################################################
                      [1m Learning iteration 2/1500 [0m                       

                       Computation: 35106 steps/s (collection: 2.659s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 11.4084
                       Mean reward: 0.01
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0021
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.80s
                      Time elapsed: 00:00:12
                               ETA: 01:40:49

################################################################################
                      [1m Learning iteration 3/1500 [0m                       

                       Computation: 36315 steps/s (collection: 2.567s, learning 0.140s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 11.4146
                       Mean reward: 0.01
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0030
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.71s
                      Time elapsed: 00:00:14
                               ETA: 01:32:27

################################################################################
                      [1m Learning iteration 4/1500 [0m                       

                       Computation: 35129 steps/s (collection: 2.659s, learning 0.140s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 11.4063
                       Mean reward: 0.01
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0042
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 2.80s
                      Time elapsed: 00:00:17
                               ETA: 01:27:52

################################################################################
                      [1m Learning iteration 5/1500 [0m                       

                       Computation: 34922 steps/s (collection: 2.671s, learning 0.144s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 11.4149
                       Mean reward: 0.02
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0056
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 2.81s
                      Time elapsed: 00:00:20
                               ETA: 01:24:51

################################################################################
                      [1m Learning iteration 6/1500 [0m                       

                       Computation: 33674 steps/s (collection: 2.782s, learning 0.137s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 11.4115
                       Mean reward: 0.03
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0074
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 2.92s
                      Time elapsed: 00:00:23
                               ETA: 01:23:04

################################################################################
                      [1m Learning iteration 7/1500 [0m                       

                       Computation: 32590 steps/s (collection: 2.878s, learning 0.138s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 11.3914
                       Mean reward: 0.04
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0098
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.02s
                      Time elapsed: 00:00:26
                               ETA: 01:22:01

################################################################################
                      [1m Learning iteration 8/1500 [0m                       

                       Computation: 24597 steps/s (collection: 3.863s, learning 0.134s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 11.3734
                       Mean reward: 0.06
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0125
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 4.00s
                      Time elapsed: 00:00:30
                               ETA: 01:23:54

################################################################################
                      [1m Learning iteration 9/1500 [0m                       

                       Computation: 115201 steps/s (collection: 0.712s, learning 0.141s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 11.3589
                       Mean reward: 0.07
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0170
    Episode_Reward/rotating_object: 0.0000
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.85s
                      Time elapsed: 00:00:31
                               ETA: 01:17:35

################################################################################
                      [1m Learning iteration 10/1500 [0m                      

                       Computation: 116498 steps/s (collection: 0.696s, learning 0.148s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 11.3680
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0219
    Episode_Reward/rotating_object: 0.0001
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.84s
                      Time elapsed: 00:00:32
                               ETA: 01:12:23

################################################################################
                      [1m Learning iteration 11/1500 [0m                      

                       Computation: 120426 steps/s (collection: 0.720s, learning 0.097s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 11.3715
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0311
    Episode_Reward/rotating_object: 0.0007
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.82s
                      Time elapsed: 00:00:32
                               ETA: 01:08:00

################################################################################
                      [1m Learning iteration 12/1500 [0m                      

                       Computation: 125033 steps/s (collection: 0.700s, learning 0.086s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0442
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 11.3822
                       Mean reward: 0.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0362
    Episode_Reward/rotating_object: 0.0006
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.79s
                      Time elapsed: 00:00:33
                               ETA: 01:04:13

################################################################################
                      [1m Learning iteration 13/1500 [0m                      

                       Computation: 126115 steps/s (collection: 0.692s, learning 0.087s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0774
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 11.3993
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0581
    Episode_Reward/rotating_object: 0.0238
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.78s
                      Time elapsed: 00:00:34
                               ETA: 01:00:58

################################################################################
                      [1m Learning iteration 14/1500 [0m                      

                       Computation: 125015 steps/s (collection: 0.700s, learning 0.087s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0627
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 11.4373
                       Mean reward: 0.48
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.0686
    Episode_Reward/rotating_object: 0.0152
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.79s
                      Time elapsed: 00:00:35
                               ETA: 00:58:10

################################################################################
                      [1m Learning iteration 15/1500 [0m                      

                       Computation: 116694 steps/s (collection: 0.679s, learning 0.164s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1031
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 11.5134
                       Mean reward: 0.60
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.0879
    Episode_Reward/rotating_object: 0.0324
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.84s
                      Time elapsed: 00:00:36
                               ETA: 00:55:48

################################################################################
                      [1m Learning iteration 16/1500 [0m                      

                       Computation: 120346 steps/s (collection: 0.718s, learning 0.099s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1520
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.5613
                       Mean reward: 1.28
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.1053
    Episode_Reward/rotating_object: 0.0691
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.82s
                      Time elapsed: 00:00:36
                               ETA: 00:53:40

################################################################################
                      [1m Learning iteration 17/1500 [0m                      

                       Computation: 126053 steps/s (collection: 0.689s, learning 0.090s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2112
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 11.6066
                       Mean reward: 0.78
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.1178
    Episode_Reward/rotating_object: 0.0679
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.78s
                      Time elapsed: 00:00:37
                               ETA: 00:51:43

################################################################################
                      [1m Learning iteration 18/1500 [0m                      

                       Computation: 119644 steps/s (collection: 0.735s, learning 0.087s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.3481
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.6693
                       Mean reward: 1.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1359
    Episode_Reward/rotating_object: 0.2157
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.82s
                      Time elapsed: 00:00:38
                               ETA: 00:50:02

################################################################################
                      [1m Learning iteration 19/1500 [0m                      

                       Computation: 112225 steps/s (collection: 0.752s, learning 0.123s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.2054
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 11.7635
                       Mean reward: 2.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1467
    Episode_Reward/rotating_object: 0.1589
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.88s
                      Time elapsed: 00:00:39
                               ETA: 00:48:35

################################################################################
                      [1m Learning iteration 20/1500 [0m                      

                       Computation: 119513 steps/s (collection: 0.719s, learning 0.104s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.2877
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 11.8682
                       Mean reward: 2.15
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.1582
    Episode_Reward/rotating_object: 0.2059
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.82s
                      Time elapsed: 00:00:40
                               ETA: 00:47:12

################################################################################
                      [1m Learning iteration 21/1500 [0m                      

                       Computation: 113513 steps/s (collection: 0.731s, learning 0.135s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.2435
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 11.9499
                       Mean reward: 2.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1690
    Episode_Reward/rotating_object: 0.2651
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.87s
                      Time elapsed: 00:00:41
                               ETA: 00:46:00

################################################################################
                      [1m Learning iteration 22/1500 [0m                      

                       Computation: 112323 steps/s (collection: 0.716s, learning 0.159s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.2500
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 12.0216
                       Mean reward: 3.87
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.1746
    Episode_Reward/rotating_object: 0.4152
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.88s
                      Time elapsed: 00:00:41
                               ETA: 00:44:54

################################################################################
                      [1m Learning iteration 23/1500 [0m                      

                       Computation: 111255 steps/s (collection: 0.781s, learning 0.103s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.1307
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 12.0678
                       Mean reward: 2.75
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.1820
    Episode_Reward/rotating_object: 0.3598
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.88s
                      Time elapsed: 00:00:42
                               ETA: 00:43:55

################################################################################
                      [1m Learning iteration 24/1500 [0m                      

                       Computation: 119845 steps/s (collection: 0.731s, learning 0.089s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.2654
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.1114
                       Mean reward: 2.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1842
    Episode_Reward/rotating_object: 0.3153
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.82s
                      Time elapsed: 00:00:43
                               ETA: 00:42:56

################################################################################
                      [1m Learning iteration 25/1500 [0m                      

                       Computation: 113873 steps/s (collection: 0.747s, learning 0.116s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.1847
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.1975
                       Mean reward: 2.64
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.1933
    Episode_Reward/rotating_object: 0.5178
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.86s
                      Time elapsed: 00:00:44
                               ETA: 00:42:04

################################################################################
                      [1m Learning iteration 26/1500 [0m                      

                       Computation: 123390 steps/s (collection: 0.700s, learning 0.097s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.3338
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.2697
                       Mean reward: 4.71
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.1892
    Episode_Reward/rotating_object: 0.4839
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.80s
                      Time elapsed: 00:00:45
                               ETA: 00:41:12

################################################################################
                      [1m Learning iteration 27/1500 [0m                      

                       Computation: 121457 steps/s (collection: 0.687s, learning 0.123s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.3432
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 12.3431
                       Mean reward: 2.54
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.1965
    Episode_Reward/rotating_object: 0.3641
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.81s
                      Time elapsed: 00:00:46
                               ETA: 00:40:25

################################################################################
                      [1m Learning iteration 28/1500 [0m                      

                       Computation: 122827 steps/s (collection: 0.710s, learning 0.091s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.3221
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 12.3909
                       Mean reward: 3.38
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.2017
    Episode_Reward/rotating_object: 0.5218
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.80s
                      Time elapsed: 00:00:46
                               ETA: 00:39:40

################################################################################
                      [1m Learning iteration 29/1500 [0m                      

                       Computation: 119243 steps/s (collection: 0.709s, learning 0.116s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2125
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.4226
                       Mean reward: 4.93
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.2016
    Episode_Reward/rotating_object: 0.6023
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.82s
                      Time elapsed: 00:00:47
                               ETA: 00:39:00

################################################################################
                      [1m Learning iteration 30/1500 [0m                      

                       Computation: 122472 steps/s (collection: 0.708s, learning 0.095s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2219
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.4511
                       Mean reward: 3.84
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.2106
    Episode_Reward/rotating_object: 0.4371
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.80s
                      Time elapsed: 00:00:48
                               ETA: 00:38:21

################################################################################
                      [1m Learning iteration 31/1500 [0m                      

                       Computation: 116448 steps/s (collection: 0.736s, learning 0.109s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2298
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 12.4916
                       Mean reward: 4.13
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.2145
    Episode_Reward/rotating_object: 0.4761
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.84s
                      Time elapsed: 00:00:49
                               ETA: 00:37:46

################################################################################
                      [1m Learning iteration 32/1500 [0m                      

                       Computation: 121866 steps/s (collection: 0.714s, learning 0.093s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3625
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 12.5815
                       Mean reward: 4.59
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.2155
    Episode_Reward/rotating_object: 0.6327
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.81s
                      Time elapsed: 00:00:50
                               ETA: 00:37:12

################################################################################
                      [1m Learning iteration 33/1500 [0m                      

                       Computation: 110974 steps/s (collection: 0.739s, learning 0.147s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.2378
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 12.6346
                       Mean reward: 5.81
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.2288
    Episode_Reward/rotating_object: 0.6636
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.89s
                      Time elapsed: 00:00:51
                               ETA: 00:36:43

################################################################################
                      [1m Learning iteration 34/1500 [0m                      

                       Computation: 119126 steps/s (collection: 0.739s, learning 0.086s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.3560
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 12.6639
                       Mean reward: 4.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2441
    Episode_Reward/rotating_object: 0.5569
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.83s
                      Time elapsed: 00:00:51
                               ETA: 00:36:13

################################################################################
                      [1m Learning iteration 35/1500 [0m                      

                       Computation: 113897 steps/s (collection: 0.740s, learning 0.123s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.5170
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 12.6906
                       Mean reward: 4.20
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.2457
    Episode_Reward/rotating_object: 0.6121
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.86s
                      Time elapsed: 00:00:52
                               ETA: 00:35:47

################################################################################
                      [1m Learning iteration 36/1500 [0m                      

                       Computation: 116094 steps/s (collection: 0.755s, learning 0.092s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.6290
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.7517
                       Mean reward: 4.41
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.2509
    Episode_Reward/rotating_object: 0.8026
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.85s
                      Time elapsed: 00:00:53
                               ETA: 00:35:21

################################################################################
                      [1m Learning iteration 37/1500 [0m                      

                       Computation: 112708 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.8385
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 12.7978
                       Mean reward: 6.01
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 0.2639
    Episode_Reward/rotating_object: 0.7513
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.87s
                      Time elapsed: 00:00:54
                               ETA: 00:34:57

################################################################################
                      [1m Learning iteration 38/1500 [0m                      

                       Computation: 114664 steps/s (collection: 0.744s, learning 0.113s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.6493
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 12.8455
                       Mean reward: 5.06
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.2627
    Episode_Reward/rotating_object: 0.8036
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.86s
                      Time elapsed: 00:00:55
                               ETA: 00:34:34

################################################################################
                      [1m Learning iteration 39/1500 [0m                      

                       Computation: 114036 steps/s (collection: 0.755s, learning 0.107s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.7415
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 12.8874
                       Mean reward: 6.25
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.2685
    Episode_Reward/rotating_object: 1.1052
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.86s
                      Time elapsed: 00:00:56
                               ETA: 00:34:12

################################################################################
                      [1m Learning iteration 40/1500 [0m                      

                       Computation: 104995 steps/s (collection: 0.836s, learning 0.101s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.9321
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 12.9303
                       Mean reward: 5.99
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 0.2718
    Episode_Reward/rotating_object: 1.0858
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.94s
                      Time elapsed: 00:00:57
                               ETA: 00:33:54

################################################################################
                      [1m Learning iteration 41/1500 [0m                      

                       Computation: 102761 steps/s (collection: 0.789s, learning 0.168s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.7140
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 12.9710
                       Mean reward: 8.43
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 0.2774
    Episode_Reward/rotating_object: 1.1860
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.96s
                      Time elapsed: 00:00:58
                               ETA: 00:33:37

################################################################################
                      [1m Learning iteration 42/1500 [0m                      

                       Computation: 96681 steps/s (collection: 0.866s, learning 0.151s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7301
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.0274
                       Mean reward: 7.63
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.2864
    Episode_Reward/rotating_object: 1.0882
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.02s
                      Time elapsed: 00:00:59
                               ETA: 00:33:24

################################################################################
                      [1m Learning iteration 43/1500 [0m                      

                       Computation: 112937 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.6487
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.0932
                       Mean reward: 7.23
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.2864
    Episode_Reward/rotating_object: 1.3187
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.87s
                      Time elapsed: 00:00:59
                               ETA: 00:33:06

################################################################################
                      [1m Learning iteration 44/1500 [0m                      

                       Computation: 110477 steps/s (collection: 0.775s, learning 0.115s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.6300
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.1283
                       Mean reward: 9.23
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.2852
    Episode_Reward/rotating_object: 1.4591
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.89s
                      Time elapsed: 00:01:00
                               ETA: 00:32:49

################################################################################
                      [1m Learning iteration 45/1500 [0m                      

                       Computation: 115073 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.5683
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.1484
                       Mean reward: 6.60
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.2898
    Episode_Reward/rotating_object: 1.1401
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.85s
                      Time elapsed: 00:01:01
                               ETA: 00:32:32

################################################################################
                      [1m Learning iteration 46/1500 [0m                      

                       Computation: 109735 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.6453
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.1745
                       Mean reward: 6.53
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 0.2984
    Episode_Reward/rotating_object: 1.1541
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.90s
                      Time elapsed: 00:01:02
                               ETA: 00:32:17

################################################################################
                      [1m Learning iteration 47/1500 [0m                      

                       Computation: 116039 steps/s (collection: 0.756s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.7996
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 13.2073
                       Mean reward: 6.41
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.3022
    Episode_Reward/rotating_object: 1.2855
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.85s
                      Time elapsed: 00:01:03
                               ETA: 00:32:01

################################################################################
                      [1m Learning iteration 48/1500 [0m                      

                       Computation: 112185 steps/s (collection: 0.780s, learning 0.097s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.9050
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.2254
                       Mean reward: 8.08
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 1.3542
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.88s
                      Time elapsed: 00:01:04
                               ETA: 00:31:46

################################################################################
                      [1m Learning iteration 49/1500 [0m                      

                       Computation: 111545 steps/s (collection: 0.791s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.7534
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.2232
                       Mean reward: 10.46
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 1.3567
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.88s
                      Time elapsed: 00:01:05
                               ETA: 00:31:32

################################################################################
                      [1m Learning iteration 50/1500 [0m                      

                       Computation: 115599 steps/s (collection: 0.761s, learning 0.089s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7934
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.2459
                       Mean reward: 7.93
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 1.3416
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.85s
                      Time elapsed: 00:01:06
                               ETA: 00:31:18

################################################################################
                      [1m Learning iteration 51/1500 [0m                      

                       Computation: 112856 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7836
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 13.2951
                       Mean reward: 9.42
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.3224
    Episode_Reward/rotating_object: 1.3779
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.87s
                      Time elapsed: 00:01:06
                               ETA: 00:31:05

################################################################################
                      [1m Learning iteration 52/1500 [0m                      

                       Computation: 116536 steps/s (collection: 0.732s, learning 0.112s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.5972
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 13.3091
                       Mean reward: 7.81
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 1.4446
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.84s
                      Time elapsed: 00:01:07
                               ETA: 00:30:52

################################################################################
                      [1m Learning iteration 53/1500 [0m                      

                       Computation: 112474 steps/s (collection: 0.777s, learning 0.098s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.9396
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 13.3359
                       Mean reward: 9.60
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 1.8105
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.87s
                      Time elapsed: 00:01:08
                               ETA: 00:30:39

################################################################################
                      [1m Learning iteration 54/1500 [0m                      

                       Computation: 111482 steps/s (collection: 0.789s, learning 0.093s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.9152
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.3686
                       Mean reward: 7.55
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 1.5005
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.88s
                      Time elapsed: 00:01:09
                               ETA: 00:30:28

################################################################################
                      [1m Learning iteration 55/1500 [0m                      

                       Computation: 111078 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.9067
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 13.4211
                       Mean reward: 8.24
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 1.4807
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.88s
                      Time elapsed: 00:01:10
                               ETA: 00:30:17

################################################################################
                      [1m Learning iteration 56/1500 [0m                      

                       Computation: 113353 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.3116
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 13.4418
                       Mean reward: 10.34
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 1.4677
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.87s
                      Time elapsed: 00:01:11
                               ETA: 00:30:06

################################################################################
                      [1m Learning iteration 57/1500 [0m                      

                       Computation: 112610 steps/s (collection: 0.766s, learning 0.107s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.3737
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 13.4701
                       Mean reward: 8.36
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 1.5044
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.87s
                      Time elapsed: 00:01:12
                               ETA: 00:29:55

################################################################################
                      [1m Learning iteration 58/1500 [0m                      

                       Computation: 102239 steps/s (collection: 0.852s, learning 0.110s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.2210
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.5107
                       Mean reward: 10.50
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 1.5466
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.96s
                      Time elapsed: 00:01:13
                               ETA: 00:29:47

################################################################################
                      [1m Learning iteration 59/1500 [0m                      

                       Computation: 107847 steps/s (collection: 0.823s, learning 0.088s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.1069
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 13.5356
                       Mean reward: 8.99
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3158
    Episode_Reward/rotating_object: 1.5982
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.91s
                      Time elapsed: 00:01:14
                               ETA: 00:29:38

################################################################################
                      [1m Learning iteration 60/1500 [0m                      

                       Computation: 104774 steps/s (collection: 0.766s, learning 0.172s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.8550
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.5611
                       Mean reward: 11.44
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 1.7279
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.94s
                      Time elapsed: 00:01:14
                               ETA: 00:29:30

################################################################################
                      [1m Learning iteration 61/1500 [0m                      

                       Computation: 106481 steps/s (collection: 0.774s, learning 0.150s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.9042
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 13.5988
                       Mean reward: 8.48
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 1.6060
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.92s
                      Time elapsed: 00:01:15
                               ETA: 00:29:21

################################################################################
                      [1m Learning iteration 62/1500 [0m                      

                       Computation: 115813 steps/s (collection: 0.752s, learning 0.097s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.9608
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.6328
                       Mean reward: 8.93
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 1.5755
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.85s
                      Time elapsed: 00:01:16
                               ETA: 00:29:11

################################################################################
                      [1m Learning iteration 63/1500 [0m                      

                       Computation: 106006 steps/s (collection: 0.819s, learning 0.108s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.1966
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.6849
                       Mean reward: 11.11
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 1.4300
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.93s
                      Time elapsed: 00:01:17
                               ETA: 00:29:04

################################################################################
                      [1m Learning iteration 64/1500 [0m                      

                       Computation: 111078 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.9030
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 13.7484
                       Mean reward: 7.82
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 1.5168
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.88s
                      Time elapsed: 00:01:18
                               ETA: 00:28:55

################################################################################
                      [1m Learning iteration 65/1500 [0m                      

                       Computation: 111236 steps/s (collection: 0.796s, learning 0.088s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.8805
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 13.7807
                       Mean reward: 9.42
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 1.5828
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.88s
                      Time elapsed: 00:01:19
                               ETA: 00:28:47

################################################################################
                      [1m Learning iteration 66/1500 [0m                      

                       Computation: 108127 steps/s (collection: 0.794s, learning 0.116s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.2642
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 13.7923
                       Mean reward: 8.64
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 0.3228
    Episode_Reward/rotating_object: 1.6238
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.91s
                      Time elapsed: 00:01:20
                               ETA: 00:28:39

################################################################################
                      [1m Learning iteration 67/1500 [0m                      

                       Computation: 107417 steps/s (collection: 0.817s, learning 0.098s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.4763
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 13.7947
                       Mean reward: 12.38
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 1.6408
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.92s
                      Time elapsed: 00:01:21
                               ETA: 00:28:32

################################################################################
                      [1m Learning iteration 68/1500 [0m                      

                       Computation: 110928 steps/s (collection: 0.803s, learning 0.084s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.0147
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 13.8013
                       Mean reward: 9.01
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 1.9535
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.89s
                      Time elapsed: 00:01:22
                               ETA: 00:28:25

################################################################################
                      [1m Learning iteration 69/1500 [0m                      

                       Computation: 105131 steps/s (collection: 0.805s, learning 0.130s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.2506
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 13.8127
                       Mean reward: 7.91
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 1.5552
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.94s
                      Time elapsed: 00:01:23
                               ETA: 00:28:18

################################################################################
                      [1m Learning iteration 70/1500 [0m                      

                       Computation: 106713 steps/s (collection: 0.763s, learning 0.159s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.4471
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 13.8391
                       Mean reward: 13.08
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 1.8934
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.92s
                      Time elapsed: 00:01:24
                               ETA: 00:28:12

################################################################################
                      [1m Learning iteration 71/1500 [0m                      

                       Computation: 93464 steps/s (collection: 0.847s, learning 0.205s)
             Mean action noise std: 1.38
          Mean value_function loss: 1.3716
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.8648
                       Mean reward: 11.25
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 2.2423
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.05s
                      Time elapsed: 00:01:25
                               ETA: 00:28:08

################################################################################
                      [1m Learning iteration 72/1500 [0m                      

                       Computation: 103160 steps/s (collection: 0.783s, learning 0.170s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.5766
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 13.8985
                       Mean reward: 10.78
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 1.6899
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.95s
                      Time elapsed: 00:01:26
                               ETA: 00:28:02

################################################################################
                      [1m Learning iteration 73/1500 [0m                      

                       Computation: 113230 steps/s (collection: 0.761s, learning 0.107s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.4090
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 13.9295
                       Mean reward: 14.32
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 2.2097
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.87s
                      Time elapsed: 00:01:26
                               ETA: 00:27:55

################################################################################
                      [1m Learning iteration 74/1500 [0m                      

                       Computation: 111568 steps/s (collection: 0.765s, learning 0.116s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.5335
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 13.9474
                       Mean reward: 10.40
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 1.9488
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.88s
                      Time elapsed: 00:01:27
                               ETA: 00:27:48

################################################################################
                      [1m Learning iteration 75/1500 [0m                      

                       Computation: 111446 steps/s (collection: 0.795s, learning 0.087s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.6750
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.9647
                       Mean reward: 8.83
               Mean episode length: 237.47
    Episode_Reward/reaching_object: 0.3100
    Episode_Reward/rotating_object: 1.8116
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.88s
                      Time elapsed: 00:01:28
                               ETA: 00:27:42

################################################################################
                      [1m Learning iteration 76/1500 [0m                      

                       Computation: 109483 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.7055
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.9901
                       Mean reward: 8.79
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 1.6908
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.90s
                      Time elapsed: 00:01:29
                               ETA: 00:27:36

################################################################################
                      [1m Learning iteration 77/1500 [0m                      

                       Computation: 110840 steps/s (collection: 0.795s, learning 0.092s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.6588
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 14.0027
                       Mean reward: 10.22
               Mean episode length: 228.17
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 1.7481
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.89s
                      Time elapsed: 00:01:30
                               ETA: 00:27:29

################################################################################
                      [1m Learning iteration 78/1500 [0m                      

                       Computation: 110835 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.0152
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.0168
                       Mean reward: 10.56
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 1.7543
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.89s
                      Time elapsed: 00:01:31
                               ETA: 00:27:23

################################################################################
                      [1m Learning iteration 79/1500 [0m                      

                       Computation: 111630 steps/s (collection: 0.774s, learning 0.106s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.5841
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.0323
                       Mean reward: 15.52
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 1.8109
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.88s
                      Time elapsed: 00:01:32
                               ETA: 00:27:17

################################################################################
                      [1m Learning iteration 80/1500 [0m                      

                       Computation: 113709 steps/s (collection: 0.764s, learning 0.100s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.6749
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.0659
                       Mean reward: 12.32
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 2.2310
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.86s
                      Time elapsed: 00:01:33
                               ETA: 00:27:11

################################################################################
                      [1m Learning iteration 81/1500 [0m                      

                       Computation: 111965 steps/s (collection: 0.773s, learning 0.105s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.8382
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 14.0940
                       Mean reward: 11.82
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 1.7632
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.88s
                      Time elapsed: 00:01:33
                               ETA: 00:27:05

################################################################################
                      [1m Learning iteration 82/1500 [0m                      

                       Computation: 109123 steps/s (collection: 0.757s, learning 0.144s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.7184
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.1057
                       Mean reward: 9.42
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 0.3063
    Episode_Reward/rotating_object: 1.9560
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.90s
                      Time elapsed: 00:01:34
                               ETA: 00:27:00

################################################################################
                      [1m Learning iteration 83/1500 [0m                      

                       Computation: 112430 steps/s (collection: 0.788s, learning 0.086s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.6206
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.1312
                       Mean reward: 12.23
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.3089
    Episode_Reward/rotating_object: 1.8510
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.87s
                      Time elapsed: 00:01:35
                               ETA: 00:26:54

################################################################################
                      [1m Learning iteration 84/1500 [0m                      

                       Computation: 113201 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.3578
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.1588
                       Mean reward: 12.91
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 2.1972
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.87s
                      Time elapsed: 00:01:36
                               ETA: 00:26:49

################################################################################
                      [1m Learning iteration 85/1500 [0m                      

                       Computation: 110565 steps/s (collection: 0.780s, learning 0.110s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.5516
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.1647
                       Mean reward: 11.64
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 1.7962
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.89s
                      Time elapsed: 00:01:37
                               ETA: 00:26:43

################################################################################
                      [1m Learning iteration 86/1500 [0m                      

                       Computation: 114304 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.4085
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.1744
                       Mean reward: 8.85
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 2.1349
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.86s
                      Time elapsed: 00:01:38
                               ETA: 00:26:38

################################################################################
                      [1m Learning iteration 87/1500 [0m                      

                       Computation: 110562 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 1.45
          Mean value_function loss: 2.1265
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.1877
                       Mean reward: 13.60
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.3184
    Episode_Reward/rotating_object: 1.9977
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.89s
                      Time elapsed: 00:01:39
                               ETA: 00:26:33

################################################################################
                      [1m Learning iteration 88/1500 [0m                      

                       Computation: 113695 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 2.2877
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.2237
                       Mean reward: 9.79
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 1.6925
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.86s
                      Time elapsed: 00:01:40
                               ETA: 00:26:27

################################################################################
                      [1m Learning iteration 89/1500 [0m                      

                       Computation: 110263 steps/s (collection: 0.771s, learning 0.121s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.4171
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.2612
                       Mean reward: 11.39
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 2.0718
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.89s
                      Time elapsed: 00:01:40
                               ETA: 00:26:23

################################################################################
                      [1m Learning iteration 90/1500 [0m                      

                       Computation: 111902 steps/s (collection: 0.778s, learning 0.100s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.4257
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.2896
                       Mean reward: 12.96
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 2.2603
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.88s
                      Time elapsed: 00:01:41
                               ETA: 00:26:18

################################################################################
                      [1m Learning iteration 91/1500 [0m                      

                       Computation: 107873 steps/s (collection: 0.770s, learning 0.141s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.4724
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3119
                       Mean reward: 10.17
               Mean episode length: 231.42
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 2.0866
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.91s
                      Time elapsed: 00:01:42
                               ETA: 00:26:13

################################################################################
                      [1m Learning iteration 92/1500 [0m                      

                       Computation: 108107 steps/s (collection: 0.798s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.6904
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.3319
                       Mean reward: 12.04
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 2.1368
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.91s
                      Time elapsed: 00:01:43
                               ETA: 00:26:09

################################################################################
                      [1m Learning iteration 93/1500 [0m                      

                       Computation: 115550 steps/s (collection: 0.761s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.5282
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.3407
                       Mean reward: 13.69
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 2.2497
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.85s
                      Time elapsed: 00:01:44
                               ETA: 00:26:04

################################################################################
                      [1m Learning iteration 94/1500 [0m                      

                       Computation: 111505 steps/s (collection: 0.776s, learning 0.105s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.5427
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 14.3497
                       Mean reward: 17.89
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 2.3428
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.88s
                      Time elapsed: 00:01:45
                               ETA: 00:26:00

################################################################################
                      [1m Learning iteration 95/1500 [0m                      

                       Computation: 111767 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.4989
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.3530
                       Mean reward: 12.43
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 2.4394
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.88s
                      Time elapsed: 00:01:46
                               ETA: 00:25:55

################################################################################
                      [1m Learning iteration 96/1500 [0m                      

                       Computation: 106003 steps/s (collection: 0.834s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.2636
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 14.3594
                       Mean reward: 12.11
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 2.3205
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.93s
                      Time elapsed: 00:01:47
                               ETA: 00:25:51

################################################################################
                      [1m Learning iteration 97/1500 [0m                      

                       Computation: 104439 steps/s (collection: 0.833s, learning 0.108s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.4771
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.3615
                       Mean reward: 13.27
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 2.0947
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.94s
                      Time elapsed: 00:01:48
                               ETA: 00:25:48

################################################################################
                      [1m Learning iteration 98/1500 [0m                      

                       Computation: 113271 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.3673
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.3677
                       Mean reward: 13.83
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 0.3076
    Episode_Reward/rotating_object: 2.7682
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.87s
                      Time elapsed: 00:01:49
                               ETA: 00:25:44

################################################################################
                      [1m Learning iteration 99/1500 [0m                      

                       Computation: 107004 steps/s (collection: 0.774s, learning 0.145s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.7302
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.3872
                       Mean reward: 19.07
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 3.1699
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.92s
                      Time elapsed: 00:01:49
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 100/1500 [0m                      

                       Computation: 109945 steps/s (collection: 0.768s, learning 0.127s)
             Mean action noise std: 1.49
          Mean value_function loss: 3.8989
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.4148
                       Mean reward: 15.12
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 3.0032
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.89s
                      Time elapsed: 00:01:50
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 101/1500 [0m                      

                       Computation: 110098 steps/s (collection: 0.778s, learning 0.115s)
             Mean action noise std: 1.50
          Mean value_function loss: 3.8897
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.4240
                       Mean reward: 15.63
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.3037
    Episode_Reward/rotating_object: 2.7944
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.89s
                      Time elapsed: 00:01:51
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 102/1500 [0m                      

                       Computation: 112883 steps/s (collection: 0.786s, learning 0.085s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.2375
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.4453
                       Mean reward: 13.60
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.2950
    Episode_Reward/rotating_object: 3.2966
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.87s
                      Time elapsed: 00:01:52
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 103/1500 [0m                      

                       Computation: 117871 steps/s (collection: 0.746s, learning 0.088s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.6707
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.4622
                       Mean reward: 18.01
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.2962
    Episode_Reward/rotating_object: 3.3208
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.83s
                      Time elapsed: 00:01:53
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 104/1500 [0m                      

                       Computation: 107567 steps/s (collection: 0.824s, learning 0.090s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.3429
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.4843
                       Mean reward: 17.28
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 3.5150
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.91s
                      Time elapsed: 00:01:54
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 105/1500 [0m                      

                       Computation: 105396 steps/s (collection: 0.842s, learning 0.091s)
             Mean action noise std: 1.51
          Mean value_function loss: 5.9245
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.4938
                       Mean reward: 23.28
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.2952
    Episode_Reward/rotating_object: 3.7957
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.93s
                      Time elapsed: 00:01:55
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 106/1500 [0m                      

                       Computation: 96217 steps/s (collection: 0.901s, learning 0.121s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.2121
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 14.5029
                       Mean reward: 18.34
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.2978
    Episode_Reward/rotating_object: 3.4711
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.02s
                      Time elapsed: 00:01:56
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 107/1500 [0m                      

                       Computation: 107397 steps/s (collection: 0.819s, learning 0.096s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.8758
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.5042
                       Mean reward: 15.88
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 0.2940
    Episode_Reward/rotating_object: 3.7115
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.92s
                      Time elapsed: 00:01:57
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 108/1500 [0m                      

                       Computation: 92831 steps/s (collection: 0.920s, learning 0.139s)
             Mean action noise std: 1.51
          Mean value_function loss: 6.8928
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.5162
                       Mean reward: 17.83
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.2972
    Episode_Reward/rotating_object: 3.4576
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.06s
                      Time elapsed: 00:01:58
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 109/1500 [0m                      

                       Computation: 113718 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.8570
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.5257
                       Mean reward: 20.27
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 3.5455
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.86s
                      Time elapsed: 00:01:59
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 110/1500 [0m                      

                       Computation: 105392 steps/s (collection: 0.839s, learning 0.093s)
             Mean action noise std: 1.52
          Mean value_function loss: 8.2089
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.5326
                       Mean reward: 20.20
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 0.3001
    Episode_Reward/rotating_object: 4.1959
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.93s
                      Time elapsed: 00:02:00
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 111/1500 [0m                      

                       Computation: 105392 steps/s (collection: 0.805s, learning 0.128s)
             Mean action noise std: 1.52
          Mean value_function loss: 7.6104
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 14.5357
                       Mean reward: 37.08
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.3052
    Episode_Reward/rotating_object: 5.3602
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.93s
                      Time elapsed: 00:02:01
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 112/1500 [0m                      

                       Computation: 111588 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 1.52
          Mean value_function loss: 7.3439
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 14.5399
                       Mean reward: 16.29
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.2986
    Episode_Reward/rotating_object: 4.7978
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.88s
                      Time elapsed: 00:02:01
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 113/1500 [0m                      

                       Computation: 110255 steps/s (collection: 0.768s, learning 0.124s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.7943
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.5404
                       Mean reward: 26.92
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 5.8680
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.89s
                      Time elapsed: 00:02:02
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 114/1500 [0m                      

                       Computation: 105434 steps/s (collection: 0.816s, learning 0.117s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.3322
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.5431
                       Mean reward: 37.17
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 5.7656
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.93s
                      Time elapsed: 00:02:03
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 115/1500 [0m                      

                       Computation: 102509 steps/s (collection: 0.789s, learning 0.170s)
             Mean action noise std: 1.52
          Mean value_function loss: 6.6802
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.5414
                       Mean reward: 25.96
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 6.1339
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.96s
                      Time elapsed: 00:02:04
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 116/1500 [0m                      

                       Computation: 107745 steps/s (collection: 0.823s, learning 0.090s)
             Mean action noise std: 1.52
          Mean value_function loss: 7.9579
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 14.5404
                       Mean reward: 44.03
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 6.5495
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.91s
                      Time elapsed: 00:02:05
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 117/1500 [0m                      

                       Computation: 111928 steps/s (collection: 0.777s, learning 0.102s)
             Mean action noise std: 1.52
          Mean value_function loss: 9.1242
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.5412
                       Mean reward: 36.07
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 0.3065
    Episode_Reward/rotating_object: 6.3848
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.88s
                      Time elapsed: 00:02:06
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 118/1500 [0m                      

                       Computation: 114927 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 9.8258
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.5493
                       Mean reward: 29.59
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 5.1685
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.86s
                      Time elapsed: 00:02:07
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 119/1500 [0m                      

                       Computation: 109522 steps/s (collection: 0.810s, learning 0.087s)
             Mean action noise std: 1.53
          Mean value_function loss: 10.6919
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.5572
                       Mean reward: 35.55
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 0.3035
    Episode_Reward/rotating_object: 6.1369
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.90s
                      Time elapsed: 00:02:08
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 120/1500 [0m                      

                       Computation: 98021 steps/s (collection: 0.910s, learning 0.093s)
             Mean action noise std: 1.53
          Mean value_function loss: 11.3490
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.5579
                       Mean reward: 39.30
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 0.3038
    Episode_Reward/rotating_object: 5.9001
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.00s
                      Time elapsed: 00:02:09
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 121/1500 [0m                      

                       Computation: 101629 steps/s (collection: 0.876s, learning 0.092s)
             Mean action noise std: 1.53
          Mean value_function loss: 10.1801
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.5638
                       Mean reward: 31.82
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 0.3035
    Episode_Reward/rotating_object: 5.8926
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.97s
                      Time elapsed: 00:02:10
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 122/1500 [0m                      

                       Computation: 101735 steps/s (collection: 0.849s, learning 0.118s)
             Mean action noise std: 1.53
          Mean value_function loss: 12.2426
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.5709
                       Mean reward: 33.24
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 0.3013
    Episode_Reward/rotating_object: 5.9512
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.97s
                      Time elapsed: 00:02:11
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 123/1500 [0m                      

                       Computation: 106120 steps/s (collection: 0.835s, learning 0.092s)
             Mean action noise std: 1.53
          Mean value_function loss: 13.4368
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.5812
                       Mean reward: 18.78
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 0.3031
    Episode_Reward/rotating_object: 6.1532
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.93s
                      Time elapsed: 00:02:12
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 124/1500 [0m                      

                       Computation: 108500 steps/s (collection: 0.814s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 11.4322
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.6023
                       Mean reward: 24.19
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 0.3023
    Episode_Reward/rotating_object: 5.9999
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.91s
                      Time elapsed: 00:02:12
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 125/1500 [0m                      

                       Computation: 105714 steps/s (collection: 0.758s, learning 0.172s)
             Mean action noise std: 1.54
          Mean value_function loss: 14.2348
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.6100
                       Mean reward: 37.33
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 0.3002
    Episode_Reward/rotating_object: 6.3688
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.93s
                      Time elapsed: 00:02:13
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 126/1500 [0m                      

                       Computation: 110045 steps/s (collection: 0.787s, learning 0.107s)
             Mean action noise std: 1.54
          Mean value_function loss: 12.2694
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.6155
                       Mean reward: 29.23
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 0.3016
    Episode_Reward/rotating_object: 6.9344
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.89s
                      Time elapsed: 00:02:14
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 127/1500 [0m                      

                       Computation: 116354 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 1.54
          Mean value_function loss: 12.5332
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.6312
                       Mean reward: 42.22
               Mean episode length: 228.80
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 6.8574
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.84s
                      Time elapsed: 00:02:15
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 128/1500 [0m                      

                       Computation: 111638 steps/s (collection: 0.789s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 13.9244
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.6341
                       Mean reward: 38.94
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 0.3015
    Episode_Reward/rotating_object: 6.5062
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.88s
                      Time elapsed: 00:02:16
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 129/1500 [0m                      

                       Computation: 112903 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 1.55
          Mean value_function loss: 15.3414
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.6408
                       Mean reward: 34.81
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 7.9582
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.87s
                      Time elapsed: 00:02:17
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 130/1500 [0m                      

                       Computation: 112127 steps/s (collection: 0.790s, learning 0.086s)
             Mean action noise std: 1.55
          Mean value_function loss: 14.7895
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.6500
                       Mean reward: 45.51
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 7.8802
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.88s
                      Time elapsed: 00:02:18
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 131/1500 [0m                      

                       Computation: 104180 steps/s (collection: 0.823s, learning 0.120s)
             Mean action noise std: 1.55
          Mean value_function loss: 15.5917
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.6546
                       Mean reward: 32.08
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 7.7057
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.94s
                      Time elapsed: 00:02:19
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 132/1500 [0m                      

                       Computation: 103839 steps/s (collection: 0.856s, learning 0.090s)
             Mean action noise std: 1.55
          Mean value_function loss: 12.4484
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.6614
                       Mean reward: 54.42
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 9.4643
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.95s
                      Time elapsed: 00:02:20
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 133/1500 [0m                      

                       Computation: 107850 steps/s (collection: 0.800s, learning 0.112s)
             Mean action noise std: 1.55
          Mean value_function loss: 13.7844
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 14.6727
                       Mean reward: 56.88
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 8.7338
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.91s
                      Time elapsed: 00:02:21
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 134/1500 [0m                      

                       Computation: 105438 steps/s (collection: 0.780s, learning 0.152s)
             Mean action noise std: 1.55
          Mean value_function loss: 15.4343
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.6770
                       Mean reward: 47.55
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 0.3076
    Episode_Reward/rotating_object: 9.4074
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.93s
                      Time elapsed: 00:02:22
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 135/1500 [0m                      

                       Computation: 116292 steps/s (collection: 0.756s, learning 0.089s)
             Mean action noise std: 1.55
          Mean value_function loss: 19.6239
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 14.6779
                       Mean reward: 58.56
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 9.8425
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.85s
                      Time elapsed: 00:02:22
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 136/1500 [0m                      

                       Computation: 106473 steps/s (collection: 0.763s, learning 0.160s)
             Mean action noise std: 1.56
          Mean value_function loss: 20.6810
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 14.6805
                       Mean reward: 51.06
               Mean episode length: 218.41
    Episode_Reward/reaching_object: 0.2991
    Episode_Reward/rotating_object: 11.4386
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.92s
                      Time elapsed: 00:02:23
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 137/1500 [0m                      

                       Computation: 95921 steps/s (collection: 0.821s, learning 0.204s)
             Mean action noise std: 1.56
          Mean value_function loss: 18.0659
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 14.6826
                       Mean reward: 47.94
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 8.9939
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.02s
                      Time elapsed: 00:02:24
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 138/1500 [0m                      

                       Computation: 99706 steps/s (collection: 0.834s, learning 0.152s)
             Mean action noise std: 1.56
          Mean value_function loss: 18.1117
               Mean surrogate loss: 0.0244
                 Mean entropy loss: 14.6836
                       Mean reward: 53.34
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.2914
    Episode_Reward/rotating_object: 11.4258
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.99s
                      Time elapsed: 00:02:25
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 139/1500 [0m                      

                       Computation: 107772 steps/s (collection: 0.795s, learning 0.118s)
             Mean action noise std: 1.56
          Mean value_function loss: 15.0350
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.6842
                       Mean reward: 47.61
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 0.2943
    Episode_Reward/rotating_object: 9.7871
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.91s
                      Time elapsed: 00:02:26
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 140/1500 [0m                      

                       Computation: 113163 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.56
          Mean value_function loss: 14.8397
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 14.6856
                       Mean reward: 49.54
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 0.2910
    Episode_Reward/rotating_object: 9.8223
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.87s
                      Time elapsed: 00:02:27
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 141/1500 [0m                      

                       Computation: 112730 steps/s (collection: 0.772s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.0238
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.6860
                       Mean reward: 50.26
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 0.2936
    Episode_Reward/rotating_object: 9.5435
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.87s
                      Time elapsed: 00:02:28
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 142/1500 [0m                      

                       Computation: 113303 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.6343
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 14.6856
                       Mean reward: 41.65
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 0.2944
    Episode_Reward/rotating_object: 9.8741
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.87s
                      Time elapsed: 00:02:29
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 143/1500 [0m                      

                       Computation: 118335 steps/s (collection: 0.738s, learning 0.093s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.2008
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 14.6858
                       Mean reward: 40.78
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 0.2872
    Episode_Reward/rotating_object: 9.7473
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.83s
                      Time elapsed: 00:02:30
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 144/1500 [0m                      

                       Computation: 112056 steps/s (collection: 0.760s, learning 0.117s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.7850
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 14.6865
                       Mean reward: 48.61
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.2878
    Episode_Reward/rotating_object: 10.2594
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.88s
                      Time elapsed: 00:02:31
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 145/1500 [0m                      

                       Computation: 114586 steps/s (collection: 0.770s, learning 0.088s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.9975
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 14.6888
                       Mean reward: 63.63
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.2869
    Episode_Reward/rotating_object: 10.9086
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.86s
                      Time elapsed: 00:02:31
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 146/1500 [0m                      

                       Computation: 105061 steps/s (collection: 0.768s, learning 0.168s)
             Mean action noise std: 1.56
          Mean value_function loss: 14.2236
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.6913
                       Mean reward: 54.16
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.2846
    Episode_Reward/rotating_object: 9.3441
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.94s
                      Time elapsed: 00:02:32
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 147/1500 [0m                      

                       Computation: 110407 steps/s (collection: 0.756s, learning 0.134s)
             Mean action noise std: 1.56
          Mean value_function loss: 13.3542
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.6970
                       Mean reward: 37.64
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.2789
    Episode_Reward/rotating_object: 9.9236
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.89s
                      Time elapsed: 00:02:33
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 148/1500 [0m                      

                       Computation: 113999 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 1.56
          Mean value_function loss: 11.5489
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7092
                       Mean reward: 59.65
               Mean episode length: 233.40
    Episode_Reward/reaching_object: 0.2731
    Episode_Reward/rotating_object: 10.7081
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.86s
                      Time elapsed: 00:02:34
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 149/1500 [0m                      

                       Computation: 99635 steps/s (collection: 0.847s, learning 0.140s)
             Mean action noise std: 1.56
          Mean value_function loss: 12.1129
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7147
                       Mean reward: 55.30
               Mean episode length: 229.37
    Episode_Reward/reaching_object: 0.2684
    Episode_Reward/rotating_object: 11.2626
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.99s
                      Time elapsed: 00:02:35
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 150/1500 [0m                      

                       Computation: 111189 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 13.4317
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.7170
                       Mean reward: 58.89
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.2727
    Episode_Reward/rotating_object: 9.6626
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.88s
                      Time elapsed: 00:02:36
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 151/1500 [0m                      

                       Computation: 116367 steps/s (collection: 0.759s, learning 0.086s)
             Mean action noise std: 1.56
          Mean value_function loss: 14.5950
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.7167
                       Mean reward: 34.94
               Mean episode length: 219.79
    Episode_Reward/reaching_object: 0.2585
    Episode_Reward/rotating_object: 9.6318
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.84s
                      Time elapsed: 00:02:37
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 152/1500 [0m                      

                       Computation: 103814 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.1351
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.7142
                       Mean reward: 63.39
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 0.2619
    Episode_Reward/rotating_object: 11.3144
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.95s
                      Time elapsed: 00:02:38
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 153/1500 [0m                      

                       Computation: 111921 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 13.8813
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 14.7190
                       Mean reward: 33.91
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 0.2656
    Episode_Reward/rotating_object: 8.2220
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.88s
                      Time elapsed: 00:02:39
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 154/1500 [0m                      

                       Computation: 108260 steps/s (collection: 0.785s, learning 0.124s)
             Mean action noise std: 1.57
          Mean value_function loss: 18.0183
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 14.7229
                       Mean reward: 48.52
               Mean episode length: 224.00
    Episode_Reward/reaching_object: 0.2664
    Episode_Reward/rotating_object: 10.4854
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.91s
                      Time elapsed: 00:02:40
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 155/1500 [0m                      

                       Computation: 107808 steps/s (collection: 0.808s, learning 0.104s)
             Mean action noise std: 1.57
          Mean value_function loss: 16.3125
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 14.7281
                       Mean reward: 49.25
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 0.2526
    Episode_Reward/rotating_object: 9.8671
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.91s
                      Time elapsed: 00:02:40
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 156/1500 [0m                      

                       Computation: 111170 steps/s (collection: 0.742s, learning 0.143s)
             Mean action noise std: 1.57
          Mean value_function loss: 14.7422
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 14.7310
                       Mean reward: 29.10
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 0.2495
    Episode_Reward/rotating_object: 7.5089
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.88s
                      Time elapsed: 00:02:41
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 157/1500 [0m                      

                       Computation: 104602 steps/s (collection: 0.789s, learning 0.151s)
             Mean action noise std: 1.57
          Mean value_function loss: 14.6194
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 14.7320
                       Mean reward: 55.94
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 0.2577
    Episode_Reward/rotating_object: 9.9034
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.94s
                      Time elapsed: 00:02:42
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 158/1500 [0m                      

                       Computation: 105721 steps/s (collection: 0.825s, learning 0.105s)
             Mean action noise std: 1.57
          Mean value_function loss: 15.7802
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.7335
                       Mean reward: 45.55
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 0.2588
    Episode_Reward/rotating_object: 8.6151
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.93s
                      Time elapsed: 00:02:43
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 159/1500 [0m                      

                       Computation: 81444 steps/s (collection: 0.958s, learning 0.249s)
             Mean action noise std: 1.57
          Mean value_function loss: 18.3822
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.7382
                       Mean reward: 44.99
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 0.2514
    Episode_Reward/rotating_object: 8.4485
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.21s
                      Time elapsed: 00:02:44
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 160/1500 [0m                      

                       Computation: 99716 steps/s (collection: 0.783s, learning 0.203s)
             Mean action noise std: 1.57
          Mean value_function loss: 19.3797
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 14.7455
                       Mean reward: 37.15
               Mean episode length: 208.18
    Episode_Reward/reaching_object: 0.2493
    Episode_Reward/rotating_object: 8.9941
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.99s
                      Time elapsed: 00:02:45
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 161/1500 [0m                      

                       Computation: 105363 steps/s (collection: 0.821s, learning 0.112s)
             Mean action noise std: 1.57
          Mean value_function loss: 16.1732
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7492
                       Mean reward: 52.52
               Mean episode length: 208.08
    Episode_Reward/reaching_object: 0.2437
    Episode_Reward/rotating_object: 8.7208
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.93s
                      Time elapsed: 00:02:46
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 162/1500 [0m                      

                       Computation: 105089 steps/s (collection: 0.789s, learning 0.146s)
             Mean action noise std: 1.58
          Mean value_function loss: 17.9633
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7606
                       Mean reward: 47.34
               Mean episode length: 211.33
    Episode_Reward/reaching_object: 0.2531
    Episode_Reward/rotating_object: 7.7915
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.94s
                      Time elapsed: 00:02:47
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 163/1500 [0m                      

                       Computation: 113969 steps/s (collection: 0.765s, learning 0.097s)
             Mean action noise std: 1.58
          Mean value_function loss: 18.4223
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.7893
                       Mean reward: 34.34
               Mean episode length: 215.59
    Episode_Reward/reaching_object: 0.2520
    Episode_Reward/rotating_object: 8.1730
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.86s
                      Time elapsed: 00:02:48
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 164/1500 [0m                      

                       Computation: 111766 steps/s (collection: 0.786s, learning 0.094s)
             Mean action noise std: 1.59
          Mean value_function loss: 20.1417
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8177
                       Mean reward: 49.57
               Mean episode length: 201.24
    Episode_Reward/reaching_object: 0.2435
    Episode_Reward/rotating_object: 7.1079
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.88s
                      Time elapsed: 00:02:49
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 165/1500 [0m                      

                       Computation: 116177 steps/s (collection: 0.746s, learning 0.100s)
             Mean action noise std: 1.59
          Mean value_function loss: 16.7923
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8422
                       Mean reward: 41.51
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 0.2505
    Episode_Reward/rotating_object: 7.4413
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.85s
                      Time elapsed: 00:02:50
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 166/1500 [0m                      

                       Computation: 109072 steps/s (collection: 0.813s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 15.6008
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8661
                       Mean reward: 49.69
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.2527
    Episode_Reward/rotating_object: 8.1669
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.90s
                      Time elapsed: 00:02:51
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 167/1500 [0m                      

                       Computation: 112681 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 1.61
          Mean value_function loss: 15.1831
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.8951
                       Mean reward: 36.57
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.2541
    Episode_Reward/rotating_object: 7.3973
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.87s
                      Time elapsed: 00:02:52
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 168/1500 [0m                      

                       Computation: 111404 steps/s (collection: 0.777s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 16.8508
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.9279
                       Mean reward: 36.96
               Mean episode length: 219.05
    Episode_Reward/reaching_object: 0.2498
    Episode_Reward/rotating_object: 6.8671
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.88s
                      Time elapsed: 00:02:52
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 169/1500 [0m                      

                       Computation: 111796 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.5645
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.9448
                       Mean reward: 39.10
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 0.2495
    Episode_Reward/rotating_object: 7.1155
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.88s
                      Time elapsed: 00:02:53
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 170/1500 [0m                      

                       Computation: 109124 steps/s (collection: 0.746s, learning 0.155s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.1666
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9744
                       Mean reward: 32.96
               Mean episode length: 216.31
    Episode_Reward/reaching_object: 0.2502
    Episode_Reward/rotating_object: 6.9934
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.90s
                      Time elapsed: 00:02:54
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 171/1500 [0m                      

                       Computation: 94118 steps/s (collection: 0.827s, learning 0.217s)
             Mean action noise std: 1.62
          Mean value_function loss: 19.1120
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.9759
                       Mean reward: 46.72
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 0.2534
    Episode_Reward/rotating_object: 7.4717
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.04s
                      Time elapsed: 00:02:55
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 172/1500 [0m                      

                       Computation: 101066 steps/s (collection: 0.820s, learning 0.153s)
             Mean action noise std: 1.62
          Mean value_function loss: 17.9792
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.9828
                       Mean reward: 42.72
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 0.2532
    Episode_Reward/rotating_object: 7.9372
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.97s
                      Time elapsed: 00:02:56
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 173/1500 [0m                      

                       Computation: 91630 steps/s (collection: 0.895s, learning 0.178s)
             Mean action noise std: 1.63
          Mean value_function loss: 20.1836
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.9985
                       Mean reward: 46.37
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 0.2512
    Episode_Reward/rotating_object: 8.6969
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.07s
                      Time elapsed: 00:02:57
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 174/1500 [0m                      

                       Computation: 110726 steps/s (collection: 0.761s, learning 0.127s)
             Mean action noise std: 1.63
          Mean value_function loss: 20.6374
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.0100
                       Mean reward: 45.67
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 0.2523
    Episode_Reward/rotating_object: 8.1053
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.89s
                      Time elapsed: 00:02:58
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 175/1500 [0m                      

                       Computation: 104144 steps/s (collection: 0.805s, learning 0.139s)
             Mean action noise std: 1.64
          Mean value_function loss: 17.5814
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.0322
                       Mean reward: 50.64
               Mean episode length: 223.73
    Episode_Reward/reaching_object: 0.2532
    Episode_Reward/rotating_object: 8.4741
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.94s
                      Time elapsed: 00:02:59
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 176/1500 [0m                      

                       Computation: 110225 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 1.64
          Mean value_function loss: 21.5852
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.0583
                       Mean reward: 56.03
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 0.2574
    Episode_Reward/rotating_object: 9.5008
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.89s
                      Time elapsed: 00:03:00
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 177/1500 [0m                      

                       Computation: 114375 steps/s (collection: 0.759s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 21.7751
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.0746
                       Mean reward: 58.97
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 0.2617
    Episode_Reward/rotating_object: 10.5942
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.86s
                      Time elapsed: 00:03:01
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 178/1500 [0m                      

                       Computation: 108103 steps/s (collection: 0.808s, learning 0.101s)
             Mean action noise std: 1.65
          Mean value_function loss: 20.2521
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.0897
                       Mean reward: 54.42
               Mean episode length: 222.97
    Episode_Reward/reaching_object: 0.2685
    Episode_Reward/rotating_object: 10.6485
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.91s
                      Time elapsed: 00:03:02
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 179/1500 [0m                      

                       Computation: 108630 steps/s (collection: 0.813s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 22.1686
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.1010
                       Mean reward: 45.79
               Mean episode length: 226.40
    Episode_Reward/reaching_object: 0.2607
    Episode_Reward/rotating_object: 8.6621
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.90s
                      Time elapsed: 00:03:03
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 180/1500 [0m                      

                       Computation: 104253 steps/s (collection: 0.838s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 21.4474
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.1081
                       Mean reward: 66.29
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 0.2760
    Episode_Reward/rotating_object: 10.9599
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.94s
                      Time elapsed: 00:03:04
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 181/1500 [0m                      

                       Computation: 113465 steps/s (collection: 0.778s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 23.6159
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1107
                       Mean reward: 63.84
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.2692
    Episode_Reward/rotating_object: 11.3810
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.87s
                      Time elapsed: 00:03:05
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 182/1500 [0m                      

                       Computation: 110723 steps/s (collection: 0.790s, learning 0.098s)
             Mean action noise std: 1.66
          Mean value_function loss: 22.6939
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.1239
                       Mean reward: 58.08
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.2735
    Episode_Reward/rotating_object: 11.3110
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.89s
                      Time elapsed: 00:03:05
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 183/1500 [0m                      

                       Computation: 112432 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 28.1923
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1256
                       Mean reward: 57.34
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 0.2646
    Episode_Reward/rotating_object: 10.9057
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.87s
                      Time elapsed: 00:03:06
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 184/1500 [0m                      

                       Computation: 116499 steps/s (collection: 0.748s, learning 0.096s)
             Mean action noise std: 1.67
          Mean value_function loss: 28.3426
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.1396
                       Mean reward: 68.85
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 0.2720
    Episode_Reward/rotating_object: 11.3105
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.84s
                      Time elapsed: 00:03:07
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 185/1500 [0m                      

                       Computation: 111729 steps/s (collection: 0.785s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 26.4067
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.1594
                       Mean reward: 42.24
               Mean episode length: 221.72
    Episode_Reward/reaching_object: 0.2644
    Episode_Reward/rotating_object: 11.5227
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.88s
                      Time elapsed: 00:03:08
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 186/1500 [0m                      

                       Computation: 114925 steps/s (collection: 0.763s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 25.1638
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1789
                       Mean reward: 64.72
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 0.2718
    Episode_Reward/rotating_object: 11.5213
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.86s
                      Time elapsed: 00:03:09
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 187/1500 [0m                      

                       Computation: 112814 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 1.68
          Mean value_function loss: 26.4454
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.1837
                       Mean reward: 66.50
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 0.2726
    Episode_Reward/rotating_object: 10.9216
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.87s
                      Time elapsed: 00:03:10
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 188/1500 [0m                      

                       Computation: 109049 steps/s (collection: 0.808s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 24.9612
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.1938
                       Mean reward: 76.13
               Mean episode length: 220.60
    Episode_Reward/reaching_object: 0.2745
    Episode_Reward/rotating_object: 12.9633
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.90s
                      Time elapsed: 00:03:11
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 189/1500 [0m                      

                       Computation: 109966 steps/s (collection: 0.799s, learning 0.095s)
             Mean action noise std: 1.68
          Mean value_function loss: 27.3200
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.2013
                       Mean reward: 55.89
               Mean episode length: 230.55
    Episode_Reward/reaching_object: 0.2737
    Episode_Reward/rotating_object: 11.1489
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.89s
                      Time elapsed: 00:03:12
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 190/1500 [0m                      

                       Computation: 110274 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 29.2471
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.2149
                       Mean reward: 47.70
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.2642
    Episode_Reward/rotating_object: 10.8298
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.89s
                      Time elapsed: 00:03:12
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 191/1500 [0m                      

                       Computation: 106056 steps/s (collection: 0.831s, learning 0.096s)
             Mean action noise std: 1.69
          Mean value_function loss: 26.0167
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.2205
                       Mean reward: 50.20
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.2762
    Episode_Reward/rotating_object: 12.4611
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.93s
                      Time elapsed: 00:03:13
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 192/1500 [0m                      

                       Computation: 113788 steps/s (collection: 0.769s, learning 0.095s)
             Mean action noise std: 1.69
          Mean value_function loss: 28.1011
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.2187
                       Mean reward: 59.43
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.2699
    Episode_Reward/rotating_object: 12.5892
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.86s
                      Time elapsed: 00:03:14
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 193/1500 [0m                      

                       Computation: 108752 steps/s (collection: 0.809s, learning 0.095s)
             Mean action noise std: 1.69
          Mean value_function loss: 24.3784
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.2236
                       Mean reward: 57.56
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 0.2741
    Episode_Reward/rotating_object: 14.0615
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.90s
                      Time elapsed: 00:03:15
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 194/1500 [0m                      

                       Computation: 104634 steps/s (collection: 0.843s, learning 0.097s)
             Mean action noise std: 1.69
          Mean value_function loss: 25.4478
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2363
                       Mean reward: 58.65
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 0.2700
    Episode_Reward/rotating_object: 12.7450
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.94s
                      Time elapsed: 00:03:16
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 195/1500 [0m                      

                       Computation: 108865 steps/s (collection: 0.795s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 26.9389
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.2431
                       Mean reward: 53.67
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 0.2631
    Episode_Reward/rotating_object: 11.8991
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.90s
                      Time elapsed: 00:03:17
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 196/1500 [0m                      

                       Computation: 109901 steps/s (collection: 0.781s, learning 0.113s)
             Mean action noise std: 1.69
          Mean value_function loss: 29.4942
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.2494
                       Mean reward: 64.51
               Mean episode length: 232.84
    Episode_Reward/reaching_object: 0.2736
    Episode_Reward/rotating_object: 15.5077
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.89s
                      Time elapsed: 00:03:18
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 197/1500 [0m                      

                       Computation: 112236 steps/s (collection: 0.771s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 30.5419
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.2569
                       Mean reward: 74.67
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 0.2693
    Episode_Reward/rotating_object: 14.1062
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.88s
                      Time elapsed: 00:03:19
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 198/1500 [0m                      

                       Computation: 113437 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 30.8897
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.2568
                       Mean reward: 95.96
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 0.2792
    Episode_Reward/rotating_object: 16.4542
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.87s
                      Time elapsed: 00:03:20
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 199/1500 [0m                      

                       Computation: 112546 steps/s (collection: 0.764s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 26.8923
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.2554
                       Mean reward: 84.92
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.2791
    Episode_Reward/rotating_object: 16.6498
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.87s
                      Time elapsed: 00:03:21
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 200/1500 [0m                      

                       Computation: 114643 steps/s (collection: 0.752s, learning 0.106s)
             Mean action noise std: 1.70
          Mean value_function loss: 28.6348
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2657
                       Mean reward: 53.89
               Mean episode length: 223.47
    Episode_Reward/reaching_object: 0.2607
    Episode_Reward/rotating_object: 15.4626
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.86s
                      Time elapsed: 00:03:21
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 201/1500 [0m                      

                       Computation: 113294 steps/s (collection: 0.758s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 30.5697
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2756
                       Mean reward: 82.53
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 0.2714
    Episode_Reward/rotating_object: 15.7368
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.87s
                      Time elapsed: 00:03:22
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 202/1500 [0m                      

                       Computation: 109050 steps/s (collection: 0.792s, learning 0.110s)
             Mean action noise std: 1.70
          Mean value_function loss: 39.7233
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.2658
                       Mean reward: 89.03
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.2721
    Episode_Reward/rotating_object: 15.1687
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.90s
                      Time elapsed: 00:03:23
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 203/1500 [0m                      

                       Computation: 110405 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 1.70
          Mean value_function loss: 41.0478
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.2627
                       Mean reward: 89.06
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.2841
    Episode_Reward/rotating_object: 17.2481
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.89s
                      Time elapsed: 00:03:24
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 204/1500 [0m                      

                       Computation: 111220 steps/s (collection: 0.779s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 38.6399
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.2672
                       Mean reward: 86.21
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 0.2758
    Episode_Reward/rotating_object: 18.0704
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.88s
                      Time elapsed: 00:03:25
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 205/1500 [0m                      

                       Computation: 115972 steps/s (collection: 0.752s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 40.0487
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.2705
                       Mean reward: 90.01
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 0.2776
    Episode_Reward/rotating_object: 18.6499
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.85s
                      Time elapsed: 00:03:26
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 206/1500 [0m                      

                       Computation: 110889 steps/s (collection: 0.780s, learning 0.107s)
             Mean action noise std: 1.71
          Mean value_function loss: 40.1294
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.2792
                       Mean reward: 95.86
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 0.2675
    Episode_Reward/rotating_object: 18.3978
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.89s
                      Time elapsed: 00:03:27
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 207/1500 [0m                      

                       Computation: 111974 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 1.71
          Mean value_function loss: 32.6952
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2845
                       Mean reward: 104.27
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 0.2686
    Episode_Reward/rotating_object: 19.1792
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.88s
                      Time elapsed: 00:03:28
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 208/1500 [0m                      

                       Computation: 110723 steps/s (collection: 0.794s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 36.8849
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.2899
                       Mean reward: 91.97
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.2831
    Episode_Reward/rotating_object: 20.6053
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.89s
                      Time elapsed: 00:03:28
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 209/1500 [0m                      

                       Computation: 113662 steps/s (collection: 0.775s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 38.8458
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.2823
                       Mean reward: 121.75
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.2814
    Episode_Reward/rotating_object: 20.6994
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.86s
                      Time elapsed: 00:03:29
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 210/1500 [0m                      

                       Computation: 101218 steps/s (collection: 0.849s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 36.0560
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.2869
                       Mean reward: 118.27
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.2791
    Episode_Reward/rotating_object: 20.0008
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.97s
                      Time elapsed: 00:03:30
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 211/1500 [0m                      

                       Computation: 102809 steps/s (collection: 0.848s, learning 0.108s)
             Mean action noise std: 1.71
          Mean value_function loss: 35.1601
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.2837
                       Mean reward: 126.86
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 0.2774
    Episode_Reward/rotating_object: 22.7450
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.96s
                      Time elapsed: 00:03:31
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 212/1500 [0m                      

                       Computation: 109583 steps/s (collection: 0.805s, learning 0.092s)
             Mean action noise std: 1.71
          Mean value_function loss: 44.1443
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.2796
                       Mean reward: 78.26
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 0.2709
    Episode_Reward/rotating_object: 16.4123
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.90s
                      Time elapsed: 00:03:32
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 213/1500 [0m                      

                       Computation: 112033 steps/s (collection: 0.772s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 45.2136
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2882
                       Mean reward: 108.78
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 0.2732
    Episode_Reward/rotating_object: 20.7824
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.88s
                      Time elapsed: 00:03:33
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 214/1500 [0m                      

                       Computation: 113259 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 53.8262
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.2952
                       Mean reward: 98.62
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 0.2715
    Episode_Reward/rotating_object: 21.3177
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.87s
                      Time elapsed: 00:03:34
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 215/1500 [0m                      

                       Computation: 105712 steps/s (collection: 0.799s, learning 0.131s)
             Mean action noise std: 1.72
          Mean value_function loss: 51.7674
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.3001
                       Mean reward: 107.69
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 0.2740
    Episode_Reward/rotating_object: 21.0280
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.93s
                      Time elapsed: 00:03:35
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 216/1500 [0m                      

                       Computation: 88167 steps/s (collection: 0.963s, learning 0.152s)
             Mean action noise std: 1.72
          Mean value_function loss: 56.9349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.3052
                       Mean reward: 108.85
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 0.2689
    Episode_Reward/rotating_object: 20.2778
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.11s
                      Time elapsed: 00:03:36
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 217/1500 [0m                      

                       Computation: 88212 steps/s (collection: 0.924s, learning 0.190s)
             Mean action noise std: 1.72
          Mean value_function loss: 48.1153
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.3038
                       Mean reward: 129.96
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 0.2822
    Episode_Reward/rotating_object: 25.7021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.11s
                      Time elapsed: 00:03:37
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 218/1500 [0m                      

                       Computation: 90564 steps/s (collection: 0.934s, learning 0.151s)
             Mean action noise std: 1.72
          Mean value_function loss: 48.3762
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.3053
                       Mean reward: 140.53
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 0.2766
    Episode_Reward/rotating_object: 24.2900
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.09s
                      Time elapsed: 00:03:38
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 219/1500 [0m                      

                       Computation: 77568 steps/s (collection: 1.048s, learning 0.220s)
             Mean action noise std: 1.72
          Mean value_function loss: 39.3203
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.3092
                       Mean reward: 124.85
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.2791
    Episode_Reward/rotating_object: 24.6998
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.27s
                      Time elapsed: 00:03:39
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 220/1500 [0m                      

                       Computation: 89939 steps/s (collection: 0.890s, learning 0.203s)
             Mean action noise std: 1.72
          Mean value_function loss: 33.8747
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3144
                       Mean reward: 119.94
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 0.2840
    Episode_Reward/rotating_object: 28.3263
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.09s
                      Time elapsed: 00:03:40
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 221/1500 [0m                      

                       Computation: 90772 steps/s (collection: 0.873s, learning 0.210s)
             Mean action noise std: 1.73
          Mean value_function loss: 34.6431
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3218
                       Mean reward: 171.18
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.2859
    Episode_Reward/rotating_object: 27.3058
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.08s
                      Time elapsed: 00:03:42
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 222/1500 [0m                      

                       Computation: 90125 steps/s (collection: 0.924s, learning 0.166s)
             Mean action noise std: 1.73
          Mean value_function loss: 38.9430
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3232
                       Mean reward: 113.26
               Mean episode length: 233.06
    Episode_Reward/reaching_object: 0.2682
    Episode_Reward/rotating_object: 23.0346
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.09s
                      Time elapsed: 00:03:43
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 223/1500 [0m                      

                       Computation: 111072 steps/s (collection: 0.798s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 44.2295
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.3281
                       Mean reward: 158.03
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.2856
    Episode_Reward/rotating_object: 27.6905
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.89s
                      Time elapsed: 00:03:44
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 224/1500 [0m                      

                       Computation: 103275 steps/s (collection: 0.822s, learning 0.130s)
             Mean action noise std: 1.73
          Mean value_function loss: 45.9181
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.3334
                       Mean reward: 128.51
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 0.2762
    Episode_Reward/rotating_object: 27.0180
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.95s
                      Time elapsed: 00:03:44
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 225/1500 [0m                      

                       Computation: 110786 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 51.6462
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 15.3365
                       Mean reward: 173.53
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 0.2793
    Episode_Reward/rotating_object: 30.9425
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.89s
                      Time elapsed: 00:03:45
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 226/1500 [0m                      

                       Computation: 98370 steps/s (collection: 0.820s, learning 0.179s)
             Mean action noise std: 1.73
          Mean value_function loss: 56.4227
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3458
                       Mean reward: 98.50
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.2694
    Episode_Reward/rotating_object: 22.2070
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.00s
                      Time elapsed: 00:03:46
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 227/1500 [0m                      

                       Computation: 106066 steps/s (collection: 0.800s, learning 0.127s)
             Mean action noise std: 1.73
          Mean value_function loss: 61.4907
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 15.3418
                       Mean reward: 123.52
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 0.2775
    Episode_Reward/rotating_object: 25.8840
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.93s
                      Time elapsed: 00:03:47
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 228/1500 [0m                      

                       Computation: 112604 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 1.73
          Mean value_function loss: 53.1437
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.3386
                       Mean reward: 116.13
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 0.2665
    Episode_Reward/rotating_object: 22.8844
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.87s
                      Time elapsed: 00:03:48
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 229/1500 [0m                      

                       Computation: 107584 steps/s (collection: 0.816s, learning 0.098s)
             Mean action noise std: 1.73
          Mean value_function loss: 53.7108
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3363
                       Mean reward: 120.01
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 0.2711
    Episode_Reward/rotating_object: 25.1208
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.91s
                      Time elapsed: 00:03:49
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 230/1500 [0m                      

                       Computation: 99527 steps/s (collection: 0.852s, learning 0.136s)
             Mean action noise std: 1.73
          Mean value_function loss: 59.9746
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.3342
                       Mean reward: 111.52
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.2801
    Episode_Reward/rotating_object: 26.7213
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.99s
                      Time elapsed: 00:03:50
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 231/1500 [0m                      

                       Computation: 109995 steps/s (collection: 0.780s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 63.9830
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.3343
                       Mean reward: 120.15
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.2828
    Episode_Reward/rotating_object: 24.7784
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.89s
                      Time elapsed: 00:03:51
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 232/1500 [0m                      

                       Computation: 105825 steps/s (collection: 0.820s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 60.6510
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.3398
                       Mean reward: 147.00
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 0.2877
    Episode_Reward/rotating_object: 29.4536
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.93s
                      Time elapsed: 00:03:52
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 233/1500 [0m                      

                       Computation: 109914 steps/s (collection: 0.794s, learning 0.100s)
             Mean action noise std: 1.74
          Mean value_function loss: 61.1502
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.3456
                       Mean reward: 129.45
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.2874
    Episode_Reward/rotating_object: 29.0120
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.89s
                      Time elapsed: 00:03:53
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 234/1500 [0m                      

                       Computation: 104771 steps/s (collection: 0.826s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 54.3182
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.3510
                       Mean reward: 143.65
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 0.2886
    Episode_Reward/rotating_object: 26.4436
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.94s
                      Time elapsed: 00:03:54
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 235/1500 [0m                      

                       Computation: 105628 steps/s (collection: 0.838s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 56.6636
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.3505
                       Mean reward: 117.20
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 0.2836
    Episode_Reward/rotating_object: 27.7175
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.93s
                      Time elapsed: 00:03:55
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 236/1500 [0m                      

                       Computation: 110180 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 1.74
          Mean value_function loss: 62.3240
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.3526
                       Mean reward: 126.91
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 0.2853
    Episode_Reward/rotating_object: 27.2474
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.89s
                      Time elapsed: 00:03:56
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 237/1500 [0m                      

                       Computation: 93493 steps/s (collection: 0.862s, learning 0.190s)
             Mean action noise std: 1.74
          Mean value_function loss: 66.9832
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 15.3578
                       Mean reward: 160.31
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.2912
    Episode_Reward/rotating_object: 29.3341
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.05s
                      Time elapsed: 00:03:57
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 238/1500 [0m                      

                       Computation: 109592 steps/s (collection: 0.787s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 72.9571
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.3593
                       Mean reward: 144.13
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.2857
    Episode_Reward/rotating_object: 28.0635
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.90s
                      Time elapsed: 00:03:57
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 239/1500 [0m                      

                       Computation: 112731 steps/s (collection: 0.773s, learning 0.099s)
             Mean action noise std: 1.74
          Mean value_function loss: 78.3157
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.3590
                       Mean reward: 136.19
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 0.2761
    Episode_Reward/rotating_object: 27.5231
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.87s
                      Time elapsed: 00:03:58
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 240/1500 [0m                      

                       Computation: 108252 steps/s (collection: 0.776s, learning 0.133s)
             Mean action noise std: 1.74
          Mean value_function loss: 91.6447
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.3618
                       Mean reward: 129.64
               Mean episode length: 222.86
    Episode_Reward/reaching_object: 0.2720
    Episode_Reward/rotating_object: 27.4598
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.91s
                      Time elapsed: 00:03:59
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 241/1500 [0m                      

                       Computation: 110470 steps/s (collection: 0.773s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 92.0216
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 15.3663
                       Mean reward: 127.09
               Mean episode length: 226.31
    Episode_Reward/reaching_object: 0.2810
    Episode_Reward/rotating_object: 26.7196
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.89s
                      Time elapsed: 00:04:00
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 242/1500 [0m                      

                       Computation: 107744 steps/s (collection: 0.814s, learning 0.098s)
             Mean action noise std: 1.75
          Mean value_function loss: 83.0715
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3743
                       Mean reward: 111.96
               Mean episode length: 232.72
    Episode_Reward/reaching_object: 0.2727
    Episode_Reward/rotating_object: 26.2057
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.91s
                      Time elapsed: 00:04:01
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 243/1500 [0m                      

                       Computation: 111204 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 1.75
          Mean value_function loss: 85.7603
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.3870
                       Mean reward: 145.98
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 0.2753
    Episode_Reward/rotating_object: 25.6363
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.88s
                      Time elapsed: 00:04:02
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 244/1500 [0m                      

                       Computation: 104815 steps/s (collection: 0.823s, learning 0.115s)
             Mean action noise std: 1.75
          Mean value_function loss: 79.8878
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.3891
                       Mean reward: 155.59
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.2746
    Episode_Reward/rotating_object: 26.3553
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.94s
                      Time elapsed: 00:04:03
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 245/1500 [0m                      

                       Computation: 107654 steps/s (collection: 0.823s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 70.1448
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.3889
                       Mean reward: 120.22
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 0.2697
    Episode_Reward/rotating_object: 27.7424
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.91s
                      Time elapsed: 00:04:04
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 246/1500 [0m                      

                       Computation: 104242 steps/s (collection: 0.847s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 62.7726
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.3975
                       Mean reward: 130.59
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 0.2585
    Episode_Reward/rotating_object: 22.3333
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.94s
                      Time elapsed: 00:04:05
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 247/1500 [0m                      

                       Computation: 109328 steps/s (collection: 0.795s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 66.1317
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4091
                       Mean reward: 131.24
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 0.2657
    Episode_Reward/rotating_object: 24.5949
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.90s
                      Time elapsed: 00:04:06
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 248/1500 [0m                      

                       Computation: 107284 steps/s (collection: 0.814s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 56.3032
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4122
                       Mean reward: 111.79
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.2536
    Episode_Reward/rotating_object: 22.5099
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.92s
                      Time elapsed: 00:04:07
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 249/1500 [0m                      

                       Computation: 105612 steps/s (collection: 0.818s, learning 0.113s)
             Mean action noise std: 1.76
          Mean value_function loss: 58.5831
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.4114
                       Mean reward: 148.49
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 0.2617
    Episode_Reward/rotating_object: 25.3880
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.93s
                      Time elapsed: 00:04:07
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 250/1500 [0m                      

                       Computation: 102253 steps/s (collection: 0.829s, learning 0.133s)
             Mean action noise std: 1.76
          Mean value_function loss: 53.6726
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.4110
                       Mean reward: 143.44
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.2662
    Episode_Reward/rotating_object: 27.8186
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.96s
                      Time elapsed: 00:04:08
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 251/1500 [0m                      

                       Computation: 103766 steps/s (collection: 0.856s, learning 0.092s)
             Mean action noise std: 1.76
          Mean value_function loss: 58.0347
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.4137
                       Mean reward: 142.93
               Mean episode length: 235.26
    Episode_Reward/reaching_object: 0.2596
    Episode_Reward/rotating_object: 26.3737
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.95s
                      Time elapsed: 00:04:09
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 252/1500 [0m                      

                       Computation: 111624 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 59.7812
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.4169
                       Mean reward: 155.63
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 0.2576
    Episode_Reward/rotating_object: 26.6316
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.88s
                      Time elapsed: 00:04:10
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 253/1500 [0m                      

                       Computation: 102572 steps/s (collection: 0.790s, learning 0.168s)
             Mean action noise std: 1.76
          Mean value_function loss: 63.7861
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4215
                       Mean reward: 146.21
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.2537
    Episode_Reward/rotating_object: 27.2574
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.96s
                      Time elapsed: 00:04:11
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 254/1500 [0m                      

                       Computation: 100774 steps/s (collection: 0.826s, learning 0.150s)
             Mean action noise std: 1.76
          Mean value_function loss: 70.7495
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.4160
                       Mean reward: 148.11
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 0.2621
    Episode_Reward/rotating_object: 29.0167
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.98s
                      Time elapsed: 00:04:12
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 255/1500 [0m                      

                       Computation: 109510 steps/s (collection: 0.803s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 61.6337
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.4112
                       Mean reward: 168.24
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.2764
    Episode_Reward/rotating_object: 29.4582
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.90s
                      Time elapsed: 00:04:13
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 256/1500 [0m                      

                       Computation: 108143 steps/s (collection: 0.819s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 68.3158
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 15.4118
                       Mean reward: 168.78
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.2660
    Episode_Reward/rotating_object: 32.0446
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.91s
                      Time elapsed: 00:04:14
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 257/1500 [0m                      

                       Computation: 112280 steps/s (collection: 0.778s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 71.7323
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.4153
                       Mean reward: 169.29
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 0.2618
    Episode_Reward/rotating_object: 29.1401
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.88s
                      Time elapsed: 00:04:15
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 258/1500 [0m                      

                       Computation: 107846 steps/s (collection: 0.803s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 66.8747
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.4100
                       Mean reward: 159.18
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 0.2748
    Episode_Reward/rotating_object: 32.8675
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.91s
                      Time elapsed: 00:04:16
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 259/1500 [0m                      

                       Computation: 106926 steps/s (collection: 0.814s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 63.4658
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.4159
                       Mean reward: 146.87
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 0.2730
    Episode_Reward/rotating_object: 29.2584
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.92s
                      Time elapsed: 00:04:17
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 260/1500 [0m                      

                       Computation: 104425 steps/s (collection: 0.814s, learning 0.128s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.4651
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4213
                       Mean reward: 166.44
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 0.2681
    Episode_Reward/rotating_object: 32.2898
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.94s
                      Time elapsed: 00:04:18
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 261/1500 [0m                      

                       Computation: 108857 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.3869
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.4197
                       Mean reward: 127.64
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 0.2717
    Episode_Reward/rotating_object: 30.4371
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.90s
                      Time elapsed: 00:04:19
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 262/1500 [0m                      

                       Computation: 103779 steps/s (collection: 0.809s, learning 0.138s)
             Mean action noise std: 1.76
          Mean value_function loss: 64.1773
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.4217
                       Mean reward: 163.35
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 0.2680
    Episode_Reward/rotating_object: 31.1857
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.95s
                      Time elapsed: 00:04:20
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 263/1500 [0m                      

                       Computation: 112478 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 60.4674
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.4207
                       Mean reward: 128.38
               Mean episode length: 238.47
    Episode_Reward/reaching_object: 0.2669
    Episode_Reward/rotating_object: 32.8701
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.87s
                      Time elapsed: 00:04:20
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 264/1500 [0m                      

                       Computation: 113749 steps/s (collection: 0.762s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 63.9705
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.4201
                       Mean reward: 203.18
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 0.2699
    Episode_Reward/rotating_object: 35.7004
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.86s
                      Time elapsed: 00:04:21
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 265/1500 [0m                      

                       Computation: 101307 steps/s (collection: 0.812s, learning 0.159s)
             Mean action noise std: 1.77
          Mean value_function loss: 61.4917
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4243
                       Mean reward: 159.29
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 0.2681
    Episode_Reward/rotating_object: 33.7159
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.97s
                      Time elapsed: 00:04:22
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 266/1500 [0m                      

                       Computation: 102326 steps/s (collection: 0.843s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 60.9374
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.4370
                       Mean reward: 172.43
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 0.2693
    Episode_Reward/rotating_object: 35.0493
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.96s
                      Time elapsed: 00:04:23
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 267/1500 [0m                      

                       Computation: 102856 steps/s (collection: 0.846s, learning 0.110s)
             Mean action noise std: 1.77
          Mean value_function loss: 61.8194
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.4449
                       Mean reward: 202.11
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 0.2719
    Episode_Reward/rotating_object: 35.3406
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.96s
                      Time elapsed: 00:04:24
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 268/1500 [0m                      

                       Computation: 107123 steps/s (collection: 0.828s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 62.1527
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4453
                       Mean reward: 156.06
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 0.2608
    Episode_Reward/rotating_object: 34.0748
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.92s
                      Time elapsed: 00:04:25
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 269/1500 [0m                      

                       Computation: 109096 steps/s (collection: 0.794s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 64.2893
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4406
                       Mean reward: 170.76
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.2695
    Episode_Reward/rotating_object: 34.7995
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.90s
                      Time elapsed: 00:04:26
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 270/1500 [0m                      

                       Computation: 104509 steps/s (collection: 0.845s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 70.0301
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4371
                       Mean reward: 181.60
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.2650
    Episode_Reward/rotating_object: 33.9264
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.94s
                      Time elapsed: 00:04:27
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 271/1500 [0m                      

                       Computation: 109696 steps/s (collection: 0.793s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 63.1226
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.4392
                       Mean reward: 152.04
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 0.2698
    Episode_Reward/rotating_object: 33.3583
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.90s
                      Time elapsed: 00:04:28
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 272/1500 [0m                      

                       Computation: 107311 steps/s (collection: 0.801s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 54.5011
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.4442
                       Mean reward: 150.40
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.2740
    Episode_Reward/rotating_object: 32.6913
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.92s
                      Time elapsed: 00:04:29
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 273/1500 [0m                      

                       Computation: 115384 steps/s (collection: 0.764s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 58.0205
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.4481
                       Mean reward: 190.53
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.2715
    Episode_Reward/rotating_object: 34.7547
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.85s
                      Time elapsed: 00:04:30
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 274/1500 [0m                      

                       Computation: 105243 steps/s (collection: 0.803s, learning 0.131s)
             Mean action noise std: 1.78
          Mean value_function loss: 60.7808
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.4480
                       Mean reward: 193.86
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 0.2737
    Episode_Reward/rotating_object: 36.8521
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.93s
                      Time elapsed: 00:04:31
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 275/1500 [0m                      

                       Computation: 110551 steps/s (collection: 0.793s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 62.1981
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.4511
                       Mean reward: 189.81
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 0.2662
    Episode_Reward/rotating_object: 33.5320
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.89s
                      Time elapsed: 00:04:31
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 276/1500 [0m                      

                       Computation: 105535 steps/s (collection: 0.767s, learning 0.164s)
             Mean action noise std: 1.78
          Mean value_function loss: 61.4438
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.4566
                       Mean reward: 181.92
               Mean episode length: 230.39
    Episode_Reward/reaching_object: 0.2676
    Episode_Reward/rotating_object: 35.2014
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.93s
                      Time elapsed: 00:04:32
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 277/1500 [0m                      

                       Computation: 104896 steps/s (collection: 0.827s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 66.8969
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.4621
                       Mean reward: 214.56
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 0.2656
    Episode_Reward/rotating_object: 36.0006
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.94s
                      Time elapsed: 00:04:33
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 278/1500 [0m                      

                       Computation: 103915 steps/s (collection: 0.784s, learning 0.162s)
             Mean action noise std: 1.78
          Mean value_function loss: 67.1402
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.4664
                       Mean reward: 167.92
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 0.2756
    Episode_Reward/rotating_object: 34.2953
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.95s
                      Time elapsed: 00:04:34
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 279/1500 [0m                      

                       Computation: 103508 steps/s (collection: 0.806s, learning 0.143s)
             Mean action noise std: 1.78
          Mean value_function loss: 65.2347
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.4788
                       Mean reward: 214.64
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.2743
    Episode_Reward/rotating_object: 36.8082
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.95s
                      Time elapsed: 00:04:35
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 280/1500 [0m                      

                       Computation: 108403 steps/s (collection: 0.798s, learning 0.109s)
             Mean action noise std: 1.79
          Mean value_function loss: 67.7303
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.4843
                       Mean reward: 162.06
               Mean episode length: 226.68
    Episode_Reward/reaching_object: 0.2775
    Episode_Reward/rotating_object: 34.6535
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.91s
                      Time elapsed: 00:04:36
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 281/1500 [0m                      

                       Computation: 102337 steps/s (collection: 0.851s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 64.5935
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.4932
                       Mean reward: 190.96
               Mean episode length: 229.79
    Episode_Reward/reaching_object: 0.2844
    Episode_Reward/rotating_object: 35.3110
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.96s
                      Time elapsed: 00:04:37
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 282/1500 [0m                      

                       Computation: 102310 steps/s (collection: 0.867s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 63.5562
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.5052
                       Mean reward: 185.18
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 0.2848
    Episode_Reward/rotating_object: 34.9626
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.96s
                      Time elapsed: 00:04:38
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 283/1500 [0m                      

                       Computation: 87292 steps/s (collection: 0.907s, learning 0.220s)
             Mean action noise std: 1.79
          Mean value_function loss: 71.9028
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5074
                       Mean reward: 171.70
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 0.2914
    Episode_Reward/rotating_object: 35.9998
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.13s
                      Time elapsed: 00:04:39
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 284/1500 [0m                      

                       Computation: 102786 steps/s (collection: 0.860s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 63.4226
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5160
                       Mean reward: 173.49
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.2971
    Episode_Reward/rotating_object: 36.9686
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.96s
                      Time elapsed: 00:04:40
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 285/1500 [0m                      

                       Computation: 109858 steps/s (collection: 0.766s, learning 0.129s)
             Mean action noise std: 1.80
          Mean value_function loss: 67.5812
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5259
                       Mean reward: 188.44
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 0.2916
    Episode_Reward/rotating_object: 38.7623
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.89s
                      Time elapsed: 00:04:41
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 286/1500 [0m                      

                       Computation: 113261 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 76.4522
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5325
                       Mean reward: 167.10
               Mean episode length: 231.56
    Episode_Reward/reaching_object: 0.2751
    Episode_Reward/rotating_object: 32.6495
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.87s
                      Time elapsed: 00:04:42
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 287/1500 [0m                      

                       Computation: 105279 steps/s (collection: 0.838s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 81.3941
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.5408
                       Mean reward: 150.37
               Mean episode length: 221.79
    Episode_Reward/reaching_object: 0.2818
    Episode_Reward/rotating_object: 34.5068
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.93s
                      Time elapsed: 00:04:43
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 288/1500 [0m                      

                       Computation: 101579 steps/s (collection: 0.865s, learning 0.103s)
             Mean action noise std: 1.80
          Mean value_function loss: 79.6458
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5530
                       Mean reward: 204.24
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.2993
    Episode_Reward/rotating_object: 39.6980
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.97s
                      Time elapsed: 00:04:44
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 289/1500 [0m                      

                       Computation: 110310 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 80.5227
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5607
                       Mean reward: 187.47
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 0.2938
    Episode_Reward/rotating_object: 33.8609
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.89s
                      Time elapsed: 00:04:45
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 290/1500 [0m                      

                       Computation: 108753 steps/s (collection: 0.817s, learning 0.087s)
             Mean action noise std: 1.80
          Mean value_function loss: 78.5965
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.5639
                       Mean reward: 200.09
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 0.2876
    Episode_Reward/rotating_object: 35.0048
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.90s
                      Time elapsed: 00:04:46
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 291/1500 [0m                      

                       Computation: 104937 steps/s (collection: 0.822s, learning 0.115s)
             Mean action noise std: 1.81
          Mean value_function loss: 76.5991
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5725
                       Mean reward: 210.36
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3003
    Episode_Reward/rotating_object: 38.0706
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.94s
                      Time elapsed: 00:04:46
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 292/1500 [0m                      

                       Computation: 104419 steps/s (collection: 0.793s, learning 0.148s)
             Mean action noise std: 1.81
          Mean value_function loss: 78.1804
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5862
                       Mean reward: 172.24
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 0.2824
    Episode_Reward/rotating_object: 35.6496
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.94s
                      Time elapsed: 00:04:47
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 293/1500 [0m                      

                       Computation: 103666 steps/s (collection: 0.797s, learning 0.151s)
             Mean action noise std: 1.81
          Mean value_function loss: 78.3846
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5883
                       Mean reward: 190.31
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 0.2915
    Episode_Reward/rotating_object: 36.0748
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.95s
                      Time elapsed: 00:04:48
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 294/1500 [0m                      

                       Computation: 112785 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 76.6466
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.5913
                       Mean reward: 198.57
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.2926
    Episode_Reward/rotating_object: 37.0038
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.87s
                      Time elapsed: 00:04:49
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 295/1500 [0m                      

                       Computation: 101874 steps/s (collection: 0.811s, learning 0.154s)
             Mean action noise std: 1.81
          Mean value_function loss: 74.3097
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6010
                       Mean reward: 188.17
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.2890
    Episode_Reward/rotating_object: 37.8997
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.96s
                      Time elapsed: 00:04:50
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 296/1500 [0m                      

                       Computation: 102144 steps/s (collection: 0.771s, learning 0.192s)
             Mean action noise std: 1.81
          Mean value_function loss: 68.9679
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.6039
                       Mean reward: 196.55
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 0.2921
    Episode_Reward/rotating_object: 36.0671
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.96s
                      Time elapsed: 00:04:51
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 297/1500 [0m                      

                       Computation: 100434 steps/s (collection: 0.835s, learning 0.144s)
             Mean action noise std: 1.81
          Mean value_function loss: 67.2647
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.6018
                       Mean reward: 204.32
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.2926
    Episode_Reward/rotating_object: 38.2699
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.98s
                      Time elapsed: 00:04:52
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 298/1500 [0m                      

                       Computation: 95574 steps/s (collection: 0.883s, learning 0.146s)
             Mean action noise std: 1.82
          Mean value_function loss: 74.5217
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.6035
                       Mean reward: 148.06
               Mean episode length: 223.00
    Episode_Reward/reaching_object: 0.2875
    Episode_Reward/rotating_object: 39.6493
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.03s
                      Time elapsed: 00:04:53
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 299/1500 [0m                      

                       Computation: 105434 steps/s (collection: 0.814s, learning 0.119s)
             Mean action noise std: 1.82
          Mean value_function loss: 71.7341
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6093
                       Mean reward: 186.82
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.2867
    Episode_Reward/rotating_object: 36.7449
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.93s
                      Time elapsed: 00:04:54
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 300/1500 [0m                      

                       Computation: 106865 steps/s (collection: 0.822s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 69.5951
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6141
                       Mean reward: 201.20
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 0.2910
    Episode_Reward/rotating_object: 42.6275
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.92s
                      Time elapsed: 00:04:55
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 301/1500 [0m                      

                       Computation: 99977 steps/s (collection: 0.847s, learning 0.136s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.3741
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6144
                       Mean reward: 192.48
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 0.2956
    Episode_Reward/rotating_object: 41.0494
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.98s
                      Time elapsed: 00:04:56
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 302/1500 [0m                      

                       Computation: 101035 steps/s (collection: 0.870s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.0373
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.6126
                       Mean reward: 229.57
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.3026
    Episode_Reward/rotating_object: 40.5939
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.97s
                      Time elapsed: 00:04:57
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 303/1500 [0m                      

                       Computation: 113282 steps/s (collection: 0.765s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 72.8698
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6155
                       Mean reward: 189.49
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.2941
    Episode_Reward/rotating_object: 40.3256
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.87s
                      Time elapsed: 00:04:58
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 304/1500 [0m                      

                       Computation: 107234 steps/s (collection: 0.830s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.4849
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.6173
                       Mean reward: 219.71
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 0.2908
    Episode_Reward/rotating_object: 38.8198
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.92s
                      Time elapsed: 00:04:59
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 305/1500 [0m                      

                       Computation: 109268 steps/s (collection: 0.801s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 80.8002
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.6138
                       Mean reward: 228.99
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 0.2964
    Episode_Reward/rotating_object: 46.2153
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.90s
                      Time elapsed: 00:05:00
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 306/1500 [0m                      

                       Computation: 108717 steps/s (collection: 0.813s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 84.4299
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6115
                       Mean reward: 236.52
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.2888
    Episode_Reward/rotating_object: 42.8271
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.90s
                      Time elapsed: 00:05:01
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 307/1500 [0m                      

                       Computation: 104271 steps/s (collection: 0.851s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 88.0157
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6116
                       Mean reward: 220.81
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 0.3009
    Episode_Reward/rotating_object: 44.4546
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.94s
                      Time elapsed: 00:05:01
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 308/1500 [0m                      

                       Computation: 103346 steps/s (collection: 0.842s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 80.0580
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6271
                       Mean reward: 233.40
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.2965
    Episode_Reward/rotating_object: 41.8477
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.95s
                      Time elapsed: 00:05:02
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 309/1500 [0m                      

                       Computation: 91135 steps/s (collection: 0.902s, learning 0.177s)
             Mean action noise std: 1.83
          Mean value_function loss: 86.6928
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.6387
                       Mean reward: 211.22
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 43.8956
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 1.08s
                      Time elapsed: 00:05:04
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 310/1500 [0m                      

                       Computation: 95617 steps/s (collection: 0.919s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 79.5261
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6474
                       Mean reward: 217.61
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.3065
    Episode_Reward/rotating_object: 42.3725
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 1.03s
                      Time elapsed: 00:05:05
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 311/1500 [0m                      

                       Computation: 97929 steps/s (collection: 0.885s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 77.8669
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.6486
                       Mean reward: 209.95
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 45.5306
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 1.00s
                      Time elapsed: 00:05:06
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 312/1500 [0m                      

                       Computation: 110487 steps/s (collection: 0.792s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 75.3114
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.6570
                       Mean reward: 222.86
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 45.6200
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.89s
                      Time elapsed: 00:05:06
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 313/1500 [0m                      

                       Computation: 104007 steps/s (collection: 0.789s, learning 0.156s)
             Mean action noise std: 1.83
          Mean value_function loss: 73.1096
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6593
                       Mean reward: 228.65
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.2982
    Episode_Reward/rotating_object: 41.0705
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.95s
                      Time elapsed: 00:05:07
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 314/1500 [0m                      

                       Computation: 110652 steps/s (collection: 0.792s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 65.8337
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.6625
                       Mean reward: 231.95
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 42.2570
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.89s
                      Time elapsed: 00:05:08
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 315/1500 [0m                      

                       Computation: 106319 steps/s (collection: 0.800s, learning 0.125s)
             Mean action noise std: 1.83
          Mean value_function loss: 83.6875
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 15.6662
                       Mean reward: 230.16
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 44.6190
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.92s
                      Time elapsed: 00:05:09
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 316/1500 [0m                      

                       Computation: 103711 steps/s (collection: 0.812s, learning 0.136s)
             Mean action noise std: 1.83
          Mean value_function loss: 88.2400
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6681
                       Mean reward: 242.66
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 45.4944
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.95s
                      Time elapsed: 00:05:10
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 317/1500 [0m                      

                       Computation: 99562 steps/s (collection: 0.824s, learning 0.164s)
             Mean action noise std: 1.84
          Mean value_function loss: 67.7178
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6702
                       Mean reward: 223.08
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 45.3197
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.99s
                      Time elapsed: 00:05:11
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 318/1500 [0m                      

                       Computation: 106604 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 1.84
          Mean value_function loss: 68.9718
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6819
                       Mean reward: 239.32
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 0.3046
    Episode_Reward/rotating_object: 44.1504
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.92s
                      Time elapsed: 00:05:12
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 319/1500 [0m                      

                       Computation: 108945 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 68.4971
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.6930
                       Mean reward: 232.25
               Mean episode length: 234.93
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 45.7699
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.90s
                      Time elapsed: 00:05:13
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 320/1500 [0m                      

                       Computation: 105636 steps/s (collection: 0.821s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 68.3354
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6967
                       Mean reward: 250.17
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3011
    Episode_Reward/rotating_object: 46.1437
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.93s
                      Time elapsed: 00:05:14
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 321/1500 [0m                      

                       Computation: 102203 steps/s (collection: 0.858s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 71.9042
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6940
                       Mean reward: 221.25
               Mean episode length: 235.94
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 42.5995
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.96s
                      Time elapsed: 00:05:15
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 322/1500 [0m                      

                       Computation: 109631 steps/s (collection: 0.792s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 60.5061
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.6952
                       Mean reward: 202.09
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 45.0209
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.90s
                      Time elapsed: 00:05:16
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 323/1500 [0m                      

                       Computation: 103783 steps/s (collection: 0.850s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 89.4012
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7001
                       Mean reward: 207.94
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 45.3588
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.95s
                      Time elapsed: 00:05:17
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 324/1500 [0m                      

                       Computation: 112045 steps/s (collection: 0.780s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 72.9282
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.7021
                       Mean reward: 251.62
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 46.5101
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.88s
                      Time elapsed: 00:05:18
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 325/1500 [0m                      

                       Computation: 109655 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.9683
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7070
                       Mean reward: 215.76
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 0.3082
    Episode_Reward/rotating_object: 43.0529
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.90s
                      Time elapsed: 00:05:18
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 326/1500 [0m                      

                       Computation: 110242 steps/s (collection: 0.772s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 95.5531
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7201
                       Mean reward: 201.70
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 0.3037
    Episode_Reward/rotating_object: 41.2806
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.89s
                      Time elapsed: 00:05:19
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 327/1500 [0m                      

                       Computation: 103123 steps/s (collection: 0.781s, learning 0.173s)
             Mean action noise std: 1.85
          Mean value_function loss: 86.9990
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.7260
                       Mean reward: 253.93
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 44.1432
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.95s
                      Time elapsed: 00:05:20
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 328/1500 [0m                      

                       Computation: 107106 steps/s (collection: 0.805s, learning 0.113s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.2336
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7244
                       Mean reward: 211.95
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 45.5154
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.92s
                      Time elapsed: 00:05:21
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 329/1500 [0m                      

                       Computation: 108333 steps/s (collection: 0.781s, learning 0.126s)
             Mean action noise std: 1.85
          Mean value_function loss: 84.8496
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.7193
                       Mean reward: 247.61
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 45.3829
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.91s
                      Time elapsed: 00:05:22
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 330/1500 [0m                      

                       Computation: 97669 steps/s (collection: 0.846s, learning 0.160s)
             Mean action noise std: 1.85
          Mean value_function loss: 74.0347
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7215
                       Mean reward: 193.31
               Mean episode length: 236.85
    Episode_Reward/reaching_object: 0.3035
    Episode_Reward/rotating_object: 41.7286
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 1.01s
                      Time elapsed: 00:05:23
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 331/1500 [0m                      

                       Computation: 104560 steps/s (collection: 0.813s, learning 0.128s)
             Mean action noise std: 1.85
          Mean value_function loss: 83.2001
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7301
                       Mean reward: 247.99
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 44.7940
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.94s
                      Time elapsed: 00:05:24
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 332/1500 [0m                      

                       Computation: 113493 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 82.0725
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.7379
                       Mean reward: 244.40
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 48.2153
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.87s
                      Time elapsed: 00:05:25
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 333/1500 [0m                      

                       Computation: 46067 steps/s (collection: 2.018s, learning 0.116s)
             Mean action noise std: 1.86
          Mean value_function loss: 88.0946
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7451
                       Mean reward: 244.60
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.3075
    Episode_Reward/rotating_object: 44.5354
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.13s
                      Time elapsed: 00:05:27
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 334/1500 [0m                      

                       Computation: 34355 steps/s (collection: 2.742s, learning 0.119s)
             Mean action noise std: 1.86
          Mean value_function loss: 87.0946
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7478
                       Mean reward: 261.36
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.3042
    Episode_Reward/rotating_object: 45.3479
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.86s
                      Time elapsed: 00:05:30
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 335/1500 [0m                      

                       Computation: 33625 steps/s (collection: 2.800s, learning 0.124s)
             Mean action noise std: 1.86
          Mean value_function loss: 86.7200
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7491
                       Mean reward: 241.15
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 44.7085
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 2.92s
                      Time elapsed: 00:05:33
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 336/1500 [0m                      

                       Computation: 33816 steps/s (collection: 2.789s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 84.3836
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7493
                       Mean reward: 244.60
               Mean episode length: 232.27
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 46.4489
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 2.91s
                      Time elapsed: 00:05:36
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 337/1500 [0m                      

                       Computation: 31157 steps/s (collection: 2.982s, learning 0.173s)
             Mean action noise std: 1.86
          Mean value_function loss: 89.8795
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7534
                       Mean reward: 253.47
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 48.8622
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.16s
                      Time elapsed: 00:05:39
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 338/1500 [0m                      

                       Computation: 27945 steps/s (collection: 3.336s, learning 0.182s)
             Mean action noise std: 1.86
          Mean value_function loss: 82.0532
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7573
                       Mean reward: 234.29
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 46.0361
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.52s
                      Time elapsed: 00:05:42
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 339/1500 [0m                      

                       Computation: 28896 steps/s (collection: 3.259s, learning 0.143s)
             Mean action noise std: 1.86
          Mean value_function loss: 91.9874
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7614
                       Mean reward: 221.53
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 45.3277
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.40s
                      Time elapsed: 00:05:46
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 340/1500 [0m                      

                       Computation: 29583 steps/s (collection: 3.186s, learning 0.137s)
             Mean action noise std: 1.86
          Mean value_function loss: 90.1277
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.7669
                       Mean reward: 238.47
               Mean episode length: 241.92
    Episode_Reward/reaching_object: 0.3070
    Episode_Reward/rotating_object: 49.5555
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.32s
                      Time elapsed: 00:05:49
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 341/1500 [0m                      

                       Computation: 21808 steps/s (collection: 4.349s, learning 0.159s)
             Mean action noise std: 1.86
          Mean value_function loss: 88.4730
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7667
                       Mean reward: 246.91
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 0.3039
    Episode_Reward/rotating_object: 46.2070
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.51s
                      Time elapsed: 00:05:54
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 342/1500 [0m                      

                       Computation: 103919 steps/s (collection: 0.854s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 85.4831
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7698
                       Mean reward: 230.08
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.3006
    Episode_Reward/rotating_object: 42.8411
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.95s
                      Time elapsed: 00:05:55
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 343/1500 [0m                      

                       Computation: 104053 steps/s (collection: 0.790s, learning 0.154s)
             Mean action noise std: 1.87
          Mean value_function loss: 83.4969
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7742
                       Mean reward: 221.90
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 44.1653
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.94s
                      Time elapsed: 00:05:56
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 344/1500 [0m                      

                       Computation: 107074 steps/s (collection: 0.820s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 74.9454
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.7742
                       Mean reward: 261.61
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 47.6247
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.92s
                      Time elapsed: 00:05:56
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 345/1500 [0m                      

                       Computation: 112594 steps/s (collection: 0.758s, learning 0.115s)
             Mean action noise std: 1.87
          Mean value_function loss: 86.2607
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.7768
                       Mean reward: 224.06
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 49.6262
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.87s
                      Time elapsed: 00:05:57
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 346/1500 [0m                      

                       Computation: 103143 steps/s (collection: 0.812s, learning 0.141s)
             Mean action noise std: 1.87
          Mean value_function loss: 96.8462
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7811
                       Mean reward: 240.61
               Mean episode length: 231.91
    Episode_Reward/reaching_object: 0.3081
    Episode_Reward/rotating_object: 51.0053
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.95s
                      Time elapsed: 00:05:58
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 347/1500 [0m                      

                       Computation: 112639 steps/s (collection: 0.767s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 92.0248
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7906
                       Mean reward: 221.40
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 47.3504
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.87s
                      Time elapsed: 00:05:59
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 348/1500 [0m                      

                       Computation: 107284 steps/s (collection: 0.817s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 83.2708
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.7970
                       Mean reward: 203.27
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 49.3800
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.92s
                      Time elapsed: 00:06:00
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 349/1500 [0m                      

                       Computation: 108322 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 94.2306
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.8067
                       Mean reward: 245.22
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 49.4309
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.91s
                      Time elapsed: 00:06:01
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 350/1500 [0m                      

                       Computation: 113452 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 85.8699
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.8113
                       Mean reward: 252.42
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 47.2425
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.87s
                      Time elapsed: 00:06:02
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 351/1500 [0m                      

                       Computation: 98248 steps/s (collection: 0.897s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 85.6998
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.8096
                       Mean reward: 227.55
               Mean episode length: 229.37
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 45.5909
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.00s
                      Time elapsed: 00:06:03
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 352/1500 [0m                      

                       Computation: 109561 steps/s (collection: 0.763s, learning 0.134s)
             Mean action noise std: 1.88
          Mean value_function loss: 78.6870
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8144
                       Mean reward: 246.54
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.3064
    Episode_Reward/rotating_object: 45.1557
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.90s
                      Time elapsed: 00:06:04
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 353/1500 [0m                      

                       Computation: 105334 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 1.88
          Mean value_function loss: 77.4007
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8185
                       Mean reward: 238.31
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.3099
    Episode_Reward/rotating_object: 49.3618
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.93s
                      Time elapsed: 00:06:05
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 354/1500 [0m                      

                       Computation: 107239 steps/s (collection: 0.816s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 74.4078
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.8253
                       Mean reward: 258.93
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 51.1197
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.92s
                      Time elapsed: 00:06:06
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 355/1500 [0m                      

                       Computation: 95321 steps/s (collection: 0.835s, learning 0.197s)
             Mean action noise std: 1.88
          Mean value_function loss: 87.2903
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8301
                       Mean reward: 287.59
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 51.8585
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 1.03s
                      Time elapsed: 00:06:07
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 356/1500 [0m                      

                       Computation: 109717 steps/s (collection: 0.806s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 87.1419
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8407
                       Mean reward: 251.41
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 52.0349
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.90s
                      Time elapsed: 00:06:08
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 357/1500 [0m                      

                       Computation: 102970 steps/s (collection: 0.820s, learning 0.135s)
             Mean action noise std: 1.89
          Mean value_function loss: 81.1815
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8497
                       Mean reward: 289.92
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 51.6674
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.95s
                      Time elapsed: 00:06:09
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 358/1500 [0m                      

                       Computation: 106562 steps/s (collection: 0.791s, learning 0.131s)
             Mean action noise std: 1.89
          Mean value_function loss: 76.8199
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8573
                       Mean reward: 243.80
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 52.9667
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.92s
                      Time elapsed: 00:06:09
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 359/1500 [0m                      

                       Computation: 104636 steps/s (collection: 0.809s, learning 0.130s)
             Mean action noise std: 1.89
          Mean value_function loss: 84.6764
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8718
                       Mean reward: 280.95
               Mean episode length: 234.30
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 55.0096
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.94s
                      Time elapsed: 00:06:10
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 360/1500 [0m                      

                       Computation: 113863 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 73.8367
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.8851
                       Mean reward: 243.43
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 48.7843
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.86s
                      Time elapsed: 00:06:11
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 361/1500 [0m                      

                       Computation: 114593 steps/s (collection: 0.766s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 80.6613
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8933
                       Mean reward: 238.54
               Mean episode length: 227.41
    Episode_Reward/reaching_object: 0.3099
    Episode_Reward/rotating_object: 47.8410
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.86s
                      Time elapsed: 00:06:12
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 362/1500 [0m                      

                       Computation: 104931 steps/s (collection: 0.843s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 82.2677
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9030
                       Mean reward: 226.22
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 0.3235
    Episode_Reward/rotating_object: 51.9942
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.94s
                      Time elapsed: 00:06:13
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 363/1500 [0m                      

                       Computation: 111603 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 1.90
          Mean value_function loss: 85.3103
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.9176
                       Mean reward: 254.92
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 53.8725
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.88s
                      Time elapsed: 00:06:14
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 364/1500 [0m                      

                       Computation: 108736 steps/s (collection: 0.793s, learning 0.111s)
             Mean action noise std: 1.90
          Mean value_function loss: 87.1297
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9242
                       Mean reward: 265.08
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 53.0874
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.90s
                      Time elapsed: 00:06:15
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 365/1500 [0m                      

                       Computation: 110336 steps/s (collection: 0.785s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 82.1967
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9289
                       Mean reward: 237.69
               Mean episode length: 225.37
    Episode_Reward/reaching_object: 0.3087
    Episode_Reward/rotating_object: 50.7481
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.89s
                      Time elapsed: 00:06:16
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 366/1500 [0m                      

                       Computation: 113872 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 1.90
          Mean value_function loss: 87.9479
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.9341
                       Mean reward: 248.61
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 49.8266
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.86s
                      Time elapsed: 00:06:17
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 367/1500 [0m                      

                       Computation: 107350 steps/s (collection: 0.792s, learning 0.124s)
             Mean action noise std: 1.91
          Mean value_function loss: 89.9354
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.9332
                       Mean reward: 254.37
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 51.9571
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.92s
                      Time elapsed: 00:06:17
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 368/1500 [0m                      

                       Computation: 106806 steps/s (collection: 0.811s, learning 0.109s)
             Mean action noise std: 1.91
          Mean value_function loss: 91.7599
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.9345
                       Mean reward: 248.15
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 51.7331
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.92s
                      Time elapsed: 00:06:18
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 369/1500 [0m                      

                       Computation: 105496 steps/s (collection: 0.764s, learning 0.168s)
             Mean action noise std: 1.91
          Mean value_function loss: 96.6012
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9361
                       Mean reward: 278.12
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 54.3632
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.93s
                      Time elapsed: 00:06:19
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 370/1500 [0m                      

                       Computation: 107641 steps/s (collection: 0.821s, learning 0.092s)
             Mean action noise std: 1.91
          Mean value_function loss: 98.8369
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9447
                       Mean reward: 257.47
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 53.4969
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.91s
                      Time elapsed: 00:06:20
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 371/1500 [0m                      

                       Computation: 111337 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 97.6023
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9530
                       Mean reward: 275.21
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.3345
    Episode_Reward/rotating_object: 55.7401
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.88s
                      Time elapsed: 00:06:21
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 372/1500 [0m                      

                       Computation: 99453 steps/s (collection: 0.850s, learning 0.139s)
             Mean action noise std: 1.91
          Mean value_function loss: 96.9756
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9620
                       Mean reward: 245.32
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 52.9547
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.99s
                      Time elapsed: 00:06:22
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 373/1500 [0m                      

                       Computation: 106541 steps/s (collection: 0.834s, learning 0.089s)
             Mean action noise std: 1.92
          Mean value_function loss: 93.0315
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.9744
                       Mean reward: 227.71
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 48.6505
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.92s
                      Time elapsed: 00:06:23
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 374/1500 [0m                      

                       Computation: 111030 steps/s (collection: 0.796s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 82.8020
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.9802
                       Mean reward: 265.91
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 50.9654
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.89s
                      Time elapsed: 00:06:24
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 375/1500 [0m                      

                       Computation: 115812 steps/s (collection: 0.754s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 90.4940
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.9892
                       Mean reward: 292.05
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.3280
    Episode_Reward/rotating_object: 57.0402
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.85s
                      Time elapsed: 00:06:25
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 376/1500 [0m                      

                       Computation: 107791 steps/s (collection: 0.820s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 75.2203
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 15.9904
                       Mean reward: 286.97
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.3300
    Episode_Reward/rotating_object: 57.1690
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.91s
                      Time elapsed: 00:06:26
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 377/1500 [0m                      

                       Computation: 110394 steps/s (collection: 0.789s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 96.6272
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9904
                       Mean reward: 274.50
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 56.6667
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.89s
                      Time elapsed: 00:06:27
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 378/1500 [0m                      

                       Computation: 115094 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 90.7224
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9966
                       Mean reward: 302.84
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 55.1207
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.85s
                      Time elapsed: 00:06:27
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 379/1500 [0m                      

                       Computation: 109020 steps/s (collection: 0.770s, learning 0.132s)
             Mean action noise std: 1.92
          Mean value_function loss: 91.3679
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.0054
                       Mean reward: 277.85
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 53.4499
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.90s
                      Time elapsed: 00:06:28
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 380/1500 [0m                      

                       Computation: 111822 steps/s (collection: 0.767s, learning 0.112s)
             Mean action noise std: 1.92
          Mean value_function loss: 80.8747
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 16.0059
                       Mean reward: 264.77
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 53.4384
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.88s
                      Time elapsed: 00:06:29
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 381/1500 [0m                      

                       Computation: 116805 steps/s (collection: 0.744s, learning 0.098s)
             Mean action noise std: 1.92
          Mean value_function loss: 84.3388
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.0082
                       Mean reward: 281.07
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 54.7212
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.84s
                      Time elapsed: 00:06:30
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 382/1500 [0m                      

                       Computation: 109751 steps/s (collection: 0.800s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 84.2042
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.0108
                       Mean reward: 289.06
               Mean episode length: 234.97
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 57.3544
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.90s
                      Time elapsed: 00:06:31
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 383/1500 [0m                      

                       Computation: 118135 steps/s (collection: 0.745s, learning 0.087s)
             Mean action noise std: 1.93
          Mean value_function loss: 93.5899
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.0190
                       Mean reward: 247.52
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 51.5781
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.83s
                      Time elapsed: 00:06:32
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 384/1500 [0m                      

                       Computation: 109630 steps/s (collection: 0.800s, learning 0.097s)
             Mean action noise std: 1.93
          Mean value_function loss: 86.8971
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0209
                       Mean reward: 283.93
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 55.7752
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.90s
                      Time elapsed: 00:06:33
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 385/1500 [0m                      

                       Computation: 111427 steps/s (collection: 0.774s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 84.5657
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0262
                       Mean reward: 299.81
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 56.1182
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.88s
                      Time elapsed: 00:06:34
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 386/1500 [0m                      

                       Computation: 107025 steps/s (collection: 0.765s, learning 0.153s)
             Mean action noise std: 1.93
          Mean value_function loss: 83.3265
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0306
                       Mean reward: 258.83
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.3295
    Episode_Reward/rotating_object: 59.5207
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.92s
                      Time elapsed: 00:06:34
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 387/1500 [0m                      

                       Computation: 113583 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 95.0248
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 16.0382
                       Mean reward: 275.39
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 55.6968
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.87s
                      Time elapsed: 00:06:35
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 388/1500 [0m                      

                       Computation: 111722 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 1.93
          Mean value_function loss: 76.0661
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.0398
                       Mean reward: 266.16
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 57.7792
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.88s
                      Time elapsed: 00:06:36
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 389/1500 [0m                      

                       Computation: 110150 steps/s (collection: 0.779s, learning 0.114s)
             Mean action noise std: 1.93
          Mean value_function loss: 84.6726
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.0406
                       Mean reward: 291.33
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 57.9038
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.89s
                      Time elapsed: 00:06:37
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 390/1500 [0m                      

                       Computation: 110905 steps/s (collection: 0.787s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 90.9074
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0494
                       Mean reward: 248.79
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 57.0033
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.89s
                      Time elapsed: 00:06:38
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 391/1500 [0m                      

                       Computation: 118222 steps/s (collection: 0.738s, learning 0.093s)
             Mean action noise std: 1.94
          Mean value_function loss: 88.6150
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.0484
                       Mean reward: 288.32
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.3331
    Episode_Reward/rotating_object: 58.7046
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.83s
                      Time elapsed: 00:06:39
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 392/1500 [0m                      

                       Computation: 115592 steps/s (collection: 0.760s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 93.4211
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0488
                       Mean reward: 272.32
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 56.4976
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.85s
                      Time elapsed: 00:06:40
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 393/1500 [0m                      

                       Computation: 113203 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 1.94
          Mean value_function loss: 84.5157
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 16.0487
                       Mean reward: 292.85
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 55.7338
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.87s
                      Time elapsed: 00:06:41
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 394/1500 [0m                      

                       Computation: 107094 steps/s (collection: 0.799s, learning 0.119s)
             Mean action noise std: 1.94
          Mean value_function loss: 95.8377
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.0511
                       Mean reward: 319.60
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 57.8573
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.92s
                      Time elapsed: 00:06:41
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 395/1500 [0m                      

                       Computation: 111803 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 90.1311
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0561
                       Mean reward: 281.00
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 57.1423
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.88s
                      Time elapsed: 00:06:42
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 396/1500 [0m                      

                       Computation: 111683 steps/s (collection: 0.748s, learning 0.132s)
             Mean action noise std: 1.94
          Mean value_function loss: 94.6731
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0594
                       Mean reward: 309.94
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 55.1793
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.88s
                      Time elapsed: 00:06:43
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 397/1500 [0m                      

                       Computation: 112713 steps/s (collection: 0.780s, learning 0.092s)
             Mean action noise std: 1.94
          Mean value_function loss: 86.7795
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.0659
                       Mean reward: 243.90
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 54.2283
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.87s
                      Time elapsed: 00:06:44
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 398/1500 [0m                      

                       Computation: 113179 steps/s (collection: 0.756s, learning 0.113s)
             Mean action noise std: 1.94
          Mean value_function loss: 92.4035
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0636
                       Mean reward: 283.06
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 59.1937
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.87s
                      Time elapsed: 00:06:45
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 399/1500 [0m                      

                       Computation: 114460 steps/s (collection: 0.765s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 96.1537
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0678
                       Mean reward: 332.08
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.3267
    Episode_Reward/rotating_object: 58.4036
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.86s
                      Time elapsed: 00:06:46
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 400/1500 [0m                      

                       Computation: 113857 steps/s (collection: 0.770s, learning 0.093s)
             Mean action noise std: 1.95
          Mean value_function loss: 81.3816
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0730
                       Mean reward: 309.64
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.3316
    Episode_Reward/rotating_object: 59.0025
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.86s
                      Time elapsed: 00:06:47
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 401/1500 [0m                      

                       Computation: 113021 steps/s (collection: 0.775s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 80.5404
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.0821
                       Mean reward: 301.06
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.3330
    Episode_Reward/rotating_object: 60.0368
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.87s
                      Time elapsed: 00:06:48
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 402/1500 [0m                      

                       Computation: 106227 steps/s (collection: 0.808s, learning 0.118s)
             Mean action noise std: 1.95
          Mean value_function loss: 86.8921
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0914
                       Mean reward: 321.15
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.3299
    Episode_Reward/rotating_object: 62.3607
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.93s
                      Time elapsed: 00:06:49
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 403/1500 [0m                      

                       Computation: 107542 steps/s (collection: 0.749s, learning 0.166s)
             Mean action noise std: 1.96
          Mean value_function loss: 99.6798
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1066
                       Mean reward: 300.11
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 58.8006
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.91s
                      Time elapsed: 00:06:49
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 404/1500 [0m                      

                       Computation: 113621 steps/s (collection: 0.760s, learning 0.105s)
             Mean action noise std: 1.96
          Mean value_function loss: 96.3623
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.1188
                       Mean reward: 301.95
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 55.7647
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.87s
                      Time elapsed: 00:06:50
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 405/1500 [0m                      

                       Computation: 114720 steps/s (collection: 0.761s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 90.6953
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.1209
                       Mean reward: 282.52
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 58.4130
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.86s
                      Time elapsed: 00:06:51
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 406/1500 [0m                      

                       Computation: 113426 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 1.96
          Mean value_function loss: 98.4061
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1209
                       Mean reward: 326.22
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.3297
    Episode_Reward/rotating_object: 62.7500
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.87s
                      Time elapsed: 00:06:52
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 407/1500 [0m                      

                       Computation: 101022 steps/s (collection: 0.857s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 95.0543
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.1258
                       Mean reward: 286.67
               Mean episode length: 243.18
    Episode_Reward/reaching_object: 0.3267
    Episode_Reward/rotating_object: 58.3460
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.97s
                      Time elapsed: 00:06:53
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 408/1500 [0m                      

                       Computation: 107121 steps/s (collection: 0.806s, learning 0.112s)
             Mean action noise std: 1.96
          Mean value_function loss: 85.6576
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1284
                       Mean reward: 290.31
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 61.7908
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.92s
                      Time elapsed: 00:06:54
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 409/1500 [0m                      

                       Computation: 98068 steps/s (collection: 0.832s, learning 0.170s)
             Mean action noise std: 1.96
          Mean value_function loss: 88.3581
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.1344
                       Mean reward: 302.89
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 0.3221
    Episode_Reward/rotating_object: 58.8781
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.00s
                      Time elapsed: 00:06:55
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 410/1500 [0m                      

                       Computation: 107757 steps/s (collection: 0.826s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 90.4522
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.1384
                       Mean reward: 291.16
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 59.2081
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.91s
                      Time elapsed: 00:06:56
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 411/1500 [0m                      

                       Computation: 110736 steps/s (collection: 0.770s, learning 0.118s)
             Mean action noise std: 1.96
          Mean value_function loss: 86.6296
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1400
                       Mean reward: 281.35
               Mean episode length: 233.73
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 56.3748
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.89s
                      Time elapsed: 00:06:57
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 412/1500 [0m                      

                       Computation: 108890 steps/s (collection: 0.789s, learning 0.114s)
             Mean action noise std: 1.97
          Mean value_function loss: 89.9737
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1463
                       Mean reward: 307.92
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 59.2332
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.90s
                      Time elapsed: 00:06:58
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 413/1500 [0m                      

                       Computation: 110377 steps/s (collection: 0.772s, learning 0.119s)
             Mean action noise std: 1.97
          Mean value_function loss: 86.7092
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 16.1488
                       Mean reward: 319.20
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 61.6769
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.89s
                      Time elapsed: 00:06:58
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 414/1500 [0m                      

                       Computation: 111814 steps/s (collection: 0.783s, learning 0.096s)
             Mean action noise std: 1.97
          Mean value_function loss: 75.1420
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.1453
                       Mean reward: 302.02
               Mean episode length: 234.52
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 60.6906
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.88s
                      Time elapsed: 00:06:59
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 415/1500 [0m                      

                       Computation: 102955 steps/s (collection: 0.804s, learning 0.151s)
             Mean action noise std: 1.97
          Mean value_function loss: 75.3806
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.1495
                       Mean reward: 316.21
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.3221
    Episode_Reward/rotating_object: 60.5786
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.95s
                      Time elapsed: 00:07:00
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 416/1500 [0m                      

                       Computation: 108637 steps/s (collection: 0.768s, learning 0.137s)
             Mean action noise std: 1.97
          Mean value_function loss: 82.4081
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.1535
                       Mean reward: 253.73
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 54.0959
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.90s
                      Time elapsed: 00:07:01
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 417/1500 [0m                      

                       Computation: 103537 steps/s (collection: 0.815s, learning 0.135s)
             Mean action noise std: 1.97
          Mean value_function loss: 86.2303
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.1661
                       Mean reward: 312.96
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 58.0381
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.95s
                      Time elapsed: 00:07:02
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 418/1500 [0m                      

                       Computation: 111355 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 1.97
          Mean value_function loss: 82.0371
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.1667
                       Mean reward: 258.10
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 59.7419
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.88s
                      Time elapsed: 00:07:03
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 419/1500 [0m                      

                       Computation: 113023 steps/s (collection: 0.766s, learning 0.104s)
             Mean action noise std: 1.97
          Mean value_function loss: 89.4864
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.1593
                       Mean reward: 296.02
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 0.3195
    Episode_Reward/rotating_object: 58.4492
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.87s
                      Time elapsed: 00:07:04
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 420/1500 [0m                      

                       Computation: 114333 steps/s (collection: 0.769s, learning 0.091s)
             Mean action noise std: 1.97
          Mean value_function loss: 95.2140
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.1605
                       Mean reward: 333.42
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 59.3354
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.86s
                      Time elapsed: 00:07:05
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 421/1500 [0m                      

                       Computation: 109784 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 1.97
          Mean value_function loss: 90.2608
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1685
                       Mean reward: 302.35
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 59.5747
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.90s
                      Time elapsed: 00:07:06
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 422/1500 [0m                      

                       Computation: 107836 steps/s (collection: 0.812s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 89.1605
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.1808
                       Mean reward: 305.61
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 57.2677
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.91s
                      Time elapsed: 00:07:07
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 423/1500 [0m                      

                       Computation: 95179 steps/s (collection: 0.922s, learning 0.111s)
             Mean action noise std: 1.98
          Mean value_function loss: 94.6209
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1957
                       Mean reward: 335.84
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 62.4493
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 1.03s
                      Time elapsed: 00:07:08
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 424/1500 [0m                      

                       Computation: 97565 steps/s (collection: 0.855s, learning 0.152s)
             Mean action noise std: 1.98
          Mean value_function loss: 94.2673
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.2055
                       Mean reward: 287.78
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 58.2507
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 1.01s
                      Time elapsed: 00:07:09
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 425/1500 [0m                      

                       Computation: 107110 steps/s (collection: 0.821s, learning 0.097s)
             Mean action noise std: 1.98
          Mean value_function loss: 96.9077
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.2058
                       Mean reward: 321.42
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 0.3125
    Episode_Reward/rotating_object: 61.8171
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.92s
                      Time elapsed: 00:07:10
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 426/1500 [0m                      

                       Computation: 105554 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 1.99
          Mean value_function loss: 96.2308
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2182
                       Mean reward: 290.15
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 59.1865
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.93s
                      Time elapsed: 00:07:10
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 427/1500 [0m                      

                       Computation: 96923 steps/s (collection: 0.887s, learning 0.128s)
             Mean action noise std: 1.99
          Mean value_function loss: 86.9300
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2349
                       Mean reward: 298.87
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 60.4028
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 1.01s
                      Time elapsed: 00:07:12
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 428/1500 [0m                      

                       Computation: 103081 steps/s (collection: 0.824s, learning 0.130s)
             Mean action noise std: 1.99
          Mean value_function loss: 94.3941
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.2311
                       Mean reward: 284.79
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.3240
    Episode_Reward/rotating_object: 59.7119
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.95s
                      Time elapsed: 00:07:12
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 429/1500 [0m                      

                       Computation: 91481 steps/s (collection: 0.902s, learning 0.172s)
             Mean action noise std: 1.99
          Mean value_function loss: 90.7860
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 16.2319
                       Mean reward: 302.77
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.3272
    Episode_Reward/rotating_object: 63.2236
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 1.07s
                      Time elapsed: 00:07:14
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 430/1500 [0m                      

                       Computation: 97189 steps/s (collection: 0.878s, learning 0.133s)
             Mean action noise std: 1.99
          Mean value_function loss: 104.9643
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2403
                       Mean reward: 241.99
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 55.9342
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 1.01s
                      Time elapsed: 00:07:15
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 431/1500 [0m                      

                       Computation: 106524 steps/s (collection: 0.815s, learning 0.108s)
             Mean action noise std: 1.99
          Mean value_function loss: 102.3819
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.2512
                       Mean reward: 296.53
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 60.7763
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.92s
                      Time elapsed: 00:07:15
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 432/1500 [0m                      

                       Computation: 75736 steps/s (collection: 1.123s, learning 0.175s)
             Mean action noise std: 2.00
          Mean value_function loss: 99.1240
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2635
                       Mean reward: 307.76
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 58.4626
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.30s
                      Time elapsed: 00:07:17
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 433/1500 [0m                      

                       Computation: 89139 steps/s (collection: 0.974s, learning 0.129s)
             Mean action noise std: 2.00
          Mean value_function loss: 100.0930
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2790
                       Mean reward: 284.03
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 57.2292
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 1.10s
                      Time elapsed: 00:07:18
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 434/1500 [0m                      

                       Computation: 99190 steps/s (collection: 0.845s, learning 0.146s)
             Mean action noise std: 2.00
          Mean value_function loss: 96.6997
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.2923
                       Mean reward: 333.53
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 58.4973
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.99s
                      Time elapsed: 00:07:19
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 435/1500 [0m                      

                       Computation: 104219 steps/s (collection: 0.822s, learning 0.121s)
             Mean action noise std: 2.01
          Mean value_function loss: 96.8943
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.3029
                       Mean reward: 327.74
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 61.9638
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.94s
                      Time elapsed: 00:07:20
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 436/1500 [0m                      

                       Computation: 105916 steps/s (collection: 0.842s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 106.2923
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3141
                       Mean reward: 274.67
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 60.7033
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.93s
                      Time elapsed: 00:07:21
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 437/1500 [0m                      

                       Computation: 111927 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 107.6096
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.3236
                       Mean reward: 312.36
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 60.4889
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.88s
                      Time elapsed: 00:07:22
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 438/1500 [0m                      

                       Computation: 109608 steps/s (collection: 0.781s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 88.0708
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.3289
                       Mean reward: 289.94
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 59.0696
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.90s
                      Time elapsed: 00:07:23
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 439/1500 [0m                      

                       Computation: 108833 steps/s (collection: 0.766s, learning 0.138s)
             Mean action noise std: 2.02
          Mean value_function loss: 92.6799
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.3348
                       Mean reward: 315.65
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 60.5826
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.90s
                      Time elapsed: 00:07:23
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 440/1500 [0m                      

                       Computation: 106354 steps/s (collection: 0.765s, learning 0.159s)
             Mean action noise std: 2.02
          Mean value_function loss: 100.5861
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.3370
                       Mean reward: 310.52
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 62.7388
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.92s
                      Time elapsed: 00:07:24
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 441/1500 [0m                      

                       Computation: 106441 steps/s (collection: 0.779s, learning 0.144s)
             Mean action noise std: 2.02
          Mean value_function loss: 98.7236
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 16.3393
                       Mean reward: 302.99
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 0.3079
    Episode_Reward/rotating_object: 58.0587
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.92s
                      Time elapsed: 00:07:25
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 442/1500 [0m                      

                       Computation: 111931 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 2.02
          Mean value_function loss: 95.3925
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3425
                       Mean reward: 281.44
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 61.2338
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.88s
                      Time elapsed: 00:07:26
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 443/1500 [0m                      

                       Computation: 114464 steps/s (collection: 0.767s, learning 0.092s)
             Mean action noise std: 2.02
          Mean value_function loss: 96.6984
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.3459
                       Mean reward: 328.16
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 63.5974
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.86s
                      Time elapsed: 00:07:27
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 444/1500 [0m                      

                       Computation: 108095 steps/s (collection: 0.807s, learning 0.103s)
             Mean action noise std: 2.02
          Mean value_function loss: 99.9451
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.3509
                       Mean reward: 299.19
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 59.4685
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.91s
                      Time elapsed: 00:07:28
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 445/1500 [0m                      

                       Computation: 104517 steps/s (collection: 0.809s, learning 0.131s)
             Mean action noise std: 2.03
          Mean value_function loss: 92.1640
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.3549
                       Mean reward: 306.61
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 61.1152
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.94s
                      Time elapsed: 00:07:29
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 446/1500 [0m                      

                       Computation: 102773 steps/s (collection: 0.844s, learning 0.113s)
             Mean action noise std: 2.03
          Mean value_function loss: 98.4836
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.3588
                       Mean reward: 306.55
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 63.0287
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.96s
                      Time elapsed: 00:07:30
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 447/1500 [0m                      

                       Computation: 113533 steps/s (collection: 0.764s, learning 0.102s)
             Mean action noise std: 2.03
          Mean value_function loss: 111.2329
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.3650
                       Mean reward: 346.64
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 61.3108
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.87s
                      Time elapsed: 00:07:31
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 448/1500 [0m                      

                       Computation: 107621 steps/s (collection: 0.791s, learning 0.122s)
             Mean action noise std: 2.03
          Mean value_function loss: 97.7101
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.3665
                       Mean reward: 340.41
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 59.8039
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.91s
                      Time elapsed: 00:07:32
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 449/1500 [0m                      

                       Computation: 110518 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 107.0442
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.3785
                       Mean reward: 298.69
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.3268
    Episode_Reward/rotating_object: 61.7061
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.89s
                      Time elapsed: 00:07:32
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 450/1500 [0m                      

                       Computation: 108087 steps/s (collection: 0.767s, learning 0.142s)
             Mean action noise std: 2.03
          Mean value_function loss: 112.9542
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3877
                       Mean reward: 266.62
               Mean episode length: 241.45
    Episode_Reward/reaching_object: 0.3235
    Episode_Reward/rotating_object: 60.8153
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.91s
                      Time elapsed: 00:07:33
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 451/1500 [0m                      

                       Computation: 109377 steps/s (collection: 0.801s, learning 0.098s)
             Mean action noise std: 2.04
          Mean value_function loss: 96.0893
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.3962
                       Mean reward: 294.73
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 60.1360
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.90s
                      Time elapsed: 00:07:34
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 452/1500 [0m                      

                       Computation: 110027 steps/s (collection: 0.758s, learning 0.135s)
             Mean action noise std: 2.04
          Mean value_function loss: 97.7377
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.4084
                       Mean reward: 330.28
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3250
    Episode_Reward/rotating_object: 59.1141
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.89s
                      Time elapsed: 00:07:35
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 453/1500 [0m                      

                       Computation: 112431 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 99.8686
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.4147
                       Mean reward: 319.17
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 0.3260
    Episode_Reward/rotating_object: 65.1373
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.87s
                      Time elapsed: 00:07:36
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 454/1500 [0m                      

                       Computation: 110072 steps/s (collection: 0.780s, learning 0.113s)
             Mean action noise std: 2.04
          Mean value_function loss: 94.5129
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.4174
                       Mean reward: 323.24
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 62.2848
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.89s
                      Time elapsed: 00:07:37
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 455/1500 [0m                      

                       Computation: 106619 steps/s (collection: 0.817s, learning 0.105s)
             Mean action noise std: 2.04
          Mean value_function loss: 93.1156
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.4214
                       Mean reward: 261.42
               Mean episode length: 235.25
    Episode_Reward/reaching_object: 0.3316
    Episode_Reward/rotating_object: 62.8342
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.92s
                      Time elapsed: 00:07:38
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 456/1500 [0m                      

                       Computation: 106182 steps/s (collection: 0.812s, learning 0.114s)
             Mean action noise std: 2.05
          Mean value_function loss: 86.1253
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 16.4312
                       Mean reward: 331.73
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 0.3259
    Episode_Reward/rotating_object: 64.0559
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.93s
                      Time elapsed: 00:07:39
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 457/1500 [0m                      

                       Computation: 111037 steps/s (collection: 0.781s, learning 0.105s)
             Mean action noise std: 2.05
          Mean value_function loss: 102.8982
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4520
                       Mean reward: 311.41
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 63.0071
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.89s
                      Time elapsed: 00:07:40
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 458/1500 [0m                      

                       Computation: 108342 steps/s (collection: 0.815s, learning 0.092s)
             Mean action noise std: 2.05
          Mean value_function loss: 97.4644
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4546
                       Mean reward: 302.89
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 61.2445
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.91s
                      Time elapsed: 00:07:41
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 459/1500 [0m                      

                       Computation: 117328 steps/s (collection: 0.752s, learning 0.086s)
             Mean action noise std: 2.05
          Mean value_function loss: 96.7132
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.4525
                       Mean reward: 335.89
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.3237
    Episode_Reward/rotating_object: 66.9333
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.84s
                      Time elapsed: 00:07:41
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 460/1500 [0m                      

                       Computation: 111524 steps/s (collection: 0.757s, learning 0.124s)
             Mean action noise std: 2.06
          Mean value_function loss: 99.9292
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4629
                       Mean reward: 314.47
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 65.0930
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.88s
                      Time elapsed: 00:07:42
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 461/1500 [0m                      

                       Computation: 111239 steps/s (collection: 0.779s, learning 0.105s)
             Mean action noise std: 2.06
          Mean value_function loss: 99.8550
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 16.4636
                       Mean reward: 328.29
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 62.4368
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.88s
                      Time elapsed: 00:07:43
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 462/1500 [0m                      

                       Computation: 102145 steps/s (collection: 0.823s, learning 0.139s)
             Mean action noise std: 2.06
          Mean value_function loss: 100.5565
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.4699
                       Mean reward: 340.76
               Mean episode length: 239.19
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 68.3873
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.96s
                      Time elapsed: 00:07:44
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 463/1500 [0m                      

                       Computation: 110507 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 98.9718
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.4724
                       Mean reward: 319.69
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 63.1800
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.89s
                      Time elapsed: 00:07:45
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 464/1500 [0m                      

                       Computation: 115138 steps/s (collection: 0.765s, learning 0.089s)
             Mean action noise std: 2.06
          Mean value_function loss: 92.9094
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4875
                       Mean reward: 293.55
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 59.6763
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.85s
                      Time elapsed: 00:07:46
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 465/1500 [0m                      

                       Computation: 110788 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 2.07
          Mean value_function loss: 94.8423
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.5069
                       Mean reward: 359.88
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.3258
    Episode_Reward/rotating_object: 64.7340
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.89s
                      Time elapsed: 00:07:47
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 466/1500 [0m                      

                       Computation: 109549 steps/s (collection: 0.807s, learning 0.091s)
             Mean action noise std: 2.07
          Mean value_function loss: 94.1433
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.5132
                       Mean reward: 333.02
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 66.1128
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.90s
                      Time elapsed: 00:07:48
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 467/1500 [0m                      

                       Computation: 110227 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 2.07
          Mean value_function loss: 103.7183
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.5269
                       Mean reward: 320.88
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 64.1728
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.89s
                      Time elapsed: 00:07:49
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 468/1500 [0m                      

                       Computation: 114468 steps/s (collection: 0.754s, learning 0.105s)
             Mean action noise std: 2.07
          Mean value_function loss: 92.6639
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 16.5400
                       Mean reward: 326.26
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 61.9205
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.86s
                      Time elapsed: 00:07:49
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 469/1500 [0m                      

                       Computation: 109819 steps/s (collection: 0.751s, learning 0.145s)
             Mean action noise std: 2.07
          Mean value_function loss: 98.6531
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.5435
                       Mean reward: 333.95
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 61.4217
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.90s
                      Time elapsed: 00:07:50
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 470/1500 [0m                      

                       Computation: 111405 steps/s (collection: 0.752s, learning 0.130s)
             Mean action noise std: 2.08
          Mean value_function loss: 90.6762
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.5489
                       Mean reward: 262.09
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 60.1057
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.88s
                      Time elapsed: 00:07:51
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 471/1500 [0m                      

                       Computation: 110477 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 99.7803
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5589
                       Mean reward: 321.14
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 61.8542
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.89s
                      Time elapsed: 00:07:52
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 472/1500 [0m                      

                       Computation: 113833 steps/s (collection: 0.763s, learning 0.101s)
             Mean action noise std: 2.08
          Mean value_function loss: 98.6625
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.5640
                       Mean reward: 336.88
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.3236
    Episode_Reward/rotating_object: 68.5831
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.86s
                      Time elapsed: 00:07:53
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 473/1500 [0m                      

                       Computation: 109444 steps/s (collection: 0.800s, learning 0.098s)
             Mean action noise std: 2.08
          Mean value_function loss: 94.0666
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.5692
                       Mean reward: 332.72
               Mean episode length: 233.70
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 65.8731
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.90s
                      Time elapsed: 00:07:54
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 474/1500 [0m                      

                       Computation: 109472 steps/s (collection: 0.770s, learning 0.128s)
             Mean action noise std: 2.08
          Mean value_function loss: 97.2351
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.5819
                       Mean reward: 347.37
               Mean episode length: 240.12
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 71.1900
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.90s
                      Time elapsed: 00:07:55
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 475/1500 [0m                      

                       Computation: 107992 steps/s (collection: 0.809s, learning 0.102s)
             Mean action noise std: 2.09
          Mean value_function loss: 100.7041
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.5854
                       Mean reward: 294.49
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 61.0783
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.91s
                      Time elapsed: 00:07:56
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 476/1500 [0m                      

                       Computation: 114895 steps/s (collection: 0.769s, learning 0.086s)
             Mean action noise std: 2.09
          Mean value_function loss: 99.8248
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.5876
                       Mean reward: 331.34
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 65.7869
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.86s
                      Time elapsed: 00:07:57
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 477/1500 [0m                      

                       Computation: 111525 steps/s (collection: 0.760s, learning 0.122s)
             Mean action noise std: 2.09
          Mean value_function loss: 100.7781
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.5976
                       Mean reward: 317.15
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.3267
    Episode_Reward/rotating_object: 62.7532
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.88s
                      Time elapsed: 00:07:57
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 478/1500 [0m                      

                       Computation: 109073 steps/s (collection: 0.765s, learning 0.137s)
             Mean action noise std: 2.09
          Mean value_function loss: 97.1613
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6021
                       Mean reward: 323.15
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 63.8674
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.90s
                      Time elapsed: 00:07:58
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 479/1500 [0m                      

                       Computation: 95032 steps/s (collection: 0.846s, learning 0.188s)
             Mean action noise std: 2.10
          Mean value_function loss: 109.1961
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6098
                       Mean reward: 286.46
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 0.3179
    Episode_Reward/rotating_object: 64.7298
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 1.03s
                      Time elapsed: 00:07:59
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 480/1500 [0m                      

                       Computation: 105329 steps/s (collection: 0.841s, learning 0.093s)
             Mean action noise std: 2.10
          Mean value_function loss: 107.5375
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6303
                       Mean reward: 317.29
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 0.3272
    Episode_Reward/rotating_object: 64.3413
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.93s
                      Time elapsed: 00:08:00
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 481/1500 [0m                      

                       Computation: 109303 steps/s (collection: 0.771s, learning 0.129s)
             Mean action noise std: 2.10
          Mean value_function loss: 118.4057
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.6405
                       Mean reward: 313.19
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 66.6242
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.90s
                      Time elapsed: 00:08:01
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 482/1500 [0m                      

                       Computation: 111432 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 2.10
          Mean value_function loss: 98.8616
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6492
                       Mean reward: 284.69
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 63.3868
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.88s
                      Time elapsed: 00:08:02
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 483/1500 [0m                      

                       Computation: 113595 steps/s (collection: 0.768s, learning 0.097s)
             Mean action noise std: 2.11
          Mean value_function loss: 99.8685
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.6558
                       Mean reward: 339.43
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 64.2627
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.87s
                      Time elapsed: 00:08:03
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 484/1500 [0m                      

                       Computation: 109487 steps/s (collection: 0.792s, learning 0.105s)
             Mean action noise std: 2.11
          Mean value_function loss: 102.5517
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.6651
                       Mean reward: 338.14
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 0.3284
    Episode_Reward/rotating_object: 68.0064
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.90s
                      Time elapsed: 00:08:04
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 485/1500 [0m                      

                       Computation: 94222 steps/s (collection: 0.879s, learning 0.165s)
             Mean action noise std: 2.11
          Mean value_function loss: 102.2260
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.6733
                       Mean reward: 323.75
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 63.1829
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 1.04s
                      Time elapsed: 00:08:05
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 486/1500 [0m                      

                       Computation: 105894 steps/s (collection: 0.814s, learning 0.114s)
             Mean action noise std: 2.11
          Mean value_function loss: 98.1157
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.6861
                       Mean reward: 344.12
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.3236
    Episode_Reward/rotating_object: 66.0815
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.93s
                      Time elapsed: 00:08:06
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 487/1500 [0m                      

                       Computation: 112581 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 99.4585
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.7019
                       Mean reward: 348.14
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 67.7735
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.87s
                      Time elapsed: 00:08:07
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 488/1500 [0m                      

                       Computation: 104019 steps/s (collection: 0.825s, learning 0.120s)
             Mean action noise std: 2.12
          Mean value_function loss: 102.9104
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.7098
                       Mean reward: 291.05
               Mean episode length: 234.22
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 67.3772
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.95s
                      Time elapsed: 00:08:08
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 489/1500 [0m                      

                       Computation: 103378 steps/s (collection: 0.838s, learning 0.113s)
             Mean action noise std: 2.12
          Mean value_function loss: 101.9286
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.7118
                       Mean reward: 337.37
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 62.6530
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.95s
                      Time elapsed: 00:08:09
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 490/1500 [0m                      

                       Computation: 111608 steps/s (collection: 0.786s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 102.2143
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.7320
                       Mean reward: 348.62
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 68.0464
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.88s
                      Time elapsed: 00:08:09
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 491/1500 [0m                      

                       Computation: 104557 steps/s (collection: 0.835s, learning 0.105s)
             Mean action noise std: 2.13
          Mean value_function loss: 114.4191
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7476
                       Mean reward: 364.75
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 65.5159
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.94s
                      Time elapsed: 00:08:10
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 492/1500 [0m                      

                       Computation: 107308 steps/s (collection: 0.793s, learning 0.123s)
             Mean action noise std: 2.13
          Mean value_function loss: 102.4339
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.7629
                       Mean reward: 358.19
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 67.9151
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.92s
                      Time elapsed: 00:08:11
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 493/1500 [0m                      

                       Computation: 101057 steps/s (collection: 0.808s, learning 0.165s)
             Mean action noise std: 2.13
          Mean value_function loss: 111.4218
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.7638
                       Mean reward: 337.74
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 65.8464
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.97s
                      Time elapsed: 00:08:12
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 494/1500 [0m                      

                       Computation: 108880 steps/s (collection: 0.797s, learning 0.106s)
             Mean action noise std: 2.13
          Mean value_function loss: 115.2611
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.7654
                       Mean reward: 366.01
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.3215
    Episode_Reward/rotating_object: 66.3324
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.90s
                      Time elapsed: 00:08:13
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 495/1500 [0m                      

                       Computation: 112970 steps/s (collection: 0.782s, learning 0.089s)
             Mean action noise std: 2.14
          Mean value_function loss: 115.7259
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.7759
                       Mean reward: 312.96
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 65.4362
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.87s
                      Time elapsed: 00:08:14
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 496/1500 [0m                      

                       Computation: 107402 steps/s (collection: 0.806s, learning 0.110s)
             Mean action noise std: 2.14
          Mean value_function loss: 112.8455
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.7726
                       Mean reward: 302.86
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 65.1254
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.92s
                      Time elapsed: 00:08:15
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 497/1500 [0m                      

                       Computation: 110111 steps/s (collection: 0.805s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 110.5127
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.7732
                       Mean reward: 322.56
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 70.3139
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.89s
                      Time elapsed: 00:08:16
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 498/1500 [0m                      

                       Computation: 111956 steps/s (collection: 0.763s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 105.1128
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7776
                       Mean reward: 309.27
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 64.5640
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.88s
                      Time elapsed: 00:08:17
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 499/1500 [0m                      

                       Computation: 103650 steps/s (collection: 0.851s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 100.1796
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.7837
                       Mean reward: 282.27
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 0.3224
    Episode_Reward/rotating_object: 65.3656
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.95s
                      Time elapsed: 00:08:18
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 500/1500 [0m                      

                       Computation: 112106 steps/s (collection: 0.790s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 99.3493
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.7890
                       Mean reward: 344.82
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3234
    Episode_Reward/rotating_object: 68.7285
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.88s
                      Time elapsed: 00:08:19
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 501/1500 [0m                      

                       Computation: 110707 steps/s (collection: 0.795s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 102.9987
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.7904
                       Mean reward: 332.07
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 68.7424
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.89s
                      Time elapsed: 00:08:19
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 502/1500 [0m                      

                       Computation: 112872 steps/s (collection: 0.781s, learning 0.090s)
             Mean action noise std: 2.15
          Mean value_function loss: 98.5990
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.7928
                       Mean reward: 347.33
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 65.2002
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.87s
                      Time elapsed: 00:08:20
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 503/1500 [0m                      

                       Computation: 112546 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 2.15
          Mean value_function loss: 98.0443
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.7897
                       Mean reward: 324.61
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 69.7603
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.87s
                      Time elapsed: 00:08:21
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 504/1500 [0m                      

                       Computation: 109369 steps/s (collection: 0.789s, learning 0.110s)
             Mean action noise std: 2.15
          Mean value_function loss: 102.4739
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.7887
                       Mean reward: 329.46
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 66.7375
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.90s
                      Time elapsed: 00:08:22
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 505/1500 [0m                      

                       Computation: 113434 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 104.9603
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.7890
                       Mean reward: 330.54
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 0.3101
    Episode_Reward/rotating_object: 71.7450
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.87s
                      Time elapsed: 00:08:23
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 506/1500 [0m                      

                       Computation: 109824 steps/s (collection: 0.746s, learning 0.150s)
             Mean action noise std: 2.15
          Mean value_function loss: 94.6820
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.7928
                       Mean reward: 334.46
               Mean episode length: 242.33
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 65.1282
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.90s
                      Time elapsed: 00:08:24
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 507/1500 [0m                      

                       Computation: 108098 steps/s (collection: 0.818s, learning 0.091s)
             Mean action noise std: 2.15
          Mean value_function loss: 105.7933
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8019
                       Mean reward: 344.10
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 66.7830
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.91s
                      Time elapsed: 00:08:25
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 508/1500 [0m                      

                       Computation: 108850 steps/s (collection: 0.804s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 102.1733
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8165
                       Mean reward: 318.78
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 68.2986
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.90s
                      Time elapsed: 00:08:26
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 509/1500 [0m                      

                       Computation: 106272 steps/s (collection: 0.823s, learning 0.102s)
             Mean action noise std: 2.16
          Mean value_function loss: 109.8390
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8273
                       Mean reward: 352.50
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 69.3750
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.93s
                      Time elapsed: 00:08:27
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 510/1500 [0m                      

                       Computation: 95516 steps/s (collection: 0.901s, learning 0.129s)
             Mean action noise std: 2.16
          Mean value_function loss: 95.3124
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8285
                       Mean reward: 306.25
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 67.4021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.03s
                      Time elapsed: 00:08:28
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 511/1500 [0m                      

                       Computation: 105819 steps/s (collection: 0.839s, learning 0.090s)
             Mean action noise std: 2.16
          Mean value_function loss: 109.0717
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.8401
                       Mean reward: 335.19
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 70.3278
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.93s
                      Time elapsed: 00:08:29
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 512/1500 [0m                      

                       Computation: 108325 steps/s (collection: 0.814s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 100.4338
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.8433
                       Mean reward: 370.24
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 67.5301
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.91s
                      Time elapsed: 00:08:29
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 513/1500 [0m                      

                       Computation: 107603 steps/s (collection: 0.789s, learning 0.125s)
             Mean action noise std: 2.17
          Mean value_function loss: 95.0565
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.8499
                       Mean reward: 312.86
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 64.5432
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.91s
                      Time elapsed: 00:08:30
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 514/1500 [0m                      

                       Computation: 112816 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 103.7814
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.8503
                       Mean reward: 329.98
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 0.2990
    Episode_Reward/rotating_object: 65.6101
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.87s
                      Time elapsed: 00:08:31
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 515/1500 [0m                      

                       Computation: 100715 steps/s (collection: 0.790s, learning 0.186s)
             Mean action noise std: 2.17
          Mean value_function loss: 110.8004
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.8598
                       Mean reward: 347.31
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 68.8195
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.98s
                      Time elapsed: 00:08:32
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 516/1500 [0m                      

                       Computation: 109606 steps/s (collection: 0.783s, learning 0.114s)
             Mean action noise std: 2.17
          Mean value_function loss: 101.3274
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 16.8763
                       Mean reward: 350.40
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 67.1839
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.90s
                      Time elapsed: 00:08:33
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 517/1500 [0m                      

                       Computation: 114497 steps/s (collection: 0.762s, learning 0.097s)
             Mean action noise std: 2.18
          Mean value_function loss: 92.4909
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.8910
                       Mean reward: 383.20
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 70.9831
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.86s
                      Time elapsed: 00:08:34
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 518/1500 [0m                      

                       Computation: 105455 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 2.18
          Mean value_function loss: 100.8902
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8953
                       Mean reward: 340.82
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 69.0562
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.93s
                      Time elapsed: 00:08:35
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 519/1500 [0m                      

                       Computation: 104858 steps/s (collection: 0.826s, learning 0.112s)
             Mean action noise std: 2.18
          Mean value_function loss: 99.1886
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.9011
                       Mean reward: 304.85
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 64.1452
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.94s
                      Time elapsed: 00:08:36
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 520/1500 [0m                      

                       Computation: 102439 steps/s (collection: 0.853s, learning 0.107s)
             Mean action noise std: 2.18
          Mean value_function loss: 113.4987
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9080
                       Mean reward: 342.37
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 69.5142
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.96s
                      Time elapsed: 00:08:37
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 521/1500 [0m                      

                       Computation: 109145 steps/s (collection: 0.809s, learning 0.092s)
             Mean action noise std: 2.18
          Mean value_function loss: 102.5161
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.9117
                       Mean reward: 295.50
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 66.6099
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.90s
                      Time elapsed: 00:08:38
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 522/1500 [0m                      

                       Computation: 112905 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 95.2010
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.9155
                       Mean reward: 346.91
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 70.7594
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.87s
                      Time elapsed: 00:08:39
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 523/1500 [0m                      

                       Computation: 104221 steps/s (collection: 0.851s, learning 0.092s)
             Mean action noise std: 2.19
          Mean value_function loss: 99.7944
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.9174
                       Mean reward: 296.74
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 0.2989
    Episode_Reward/rotating_object: 66.1811
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.94s
                      Time elapsed: 00:08:40
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 524/1500 [0m                      

                       Computation: 93724 steps/s (collection: 0.940s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 91.4308
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.9242
                       Mean reward: 346.58
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 0.3075
    Episode_Reward/rotating_object: 65.1443
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 1.05s
                      Time elapsed: 00:08:41
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 525/1500 [0m                      

                       Computation: 99684 steps/s (collection: 0.887s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 101.3177
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.9298
                       Mean reward: 353.77
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.3057
    Episode_Reward/rotating_object: 67.6668
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.99s
                      Time elapsed: 00:08:42
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 526/1500 [0m                      

                       Computation: 105163 steps/s (collection: 0.837s, learning 0.098s)
             Mean action noise std: 2.19
          Mean value_function loss: 93.2055
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.9327
                       Mean reward: 357.62
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 0.3034
    Episode_Reward/rotating_object: 66.1710
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.93s
                      Time elapsed: 00:08:42
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 527/1500 [0m                      

                       Computation: 86274 steps/s (collection: 0.910s, learning 0.230s)
             Mean action noise std: 2.20
          Mean value_function loss: 89.4336
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 16.9411
                       Mean reward: 305.99
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 69.5846
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 1.14s
                      Time elapsed: 00:08:44
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 528/1500 [0m                      

                       Computation: 103974 steps/s (collection: 0.857s, learning 0.088s)
             Mean action noise std: 2.20
          Mean value_function loss: 93.4544
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9537
                       Mean reward: 373.39
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 71.0165
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.95s
                      Time elapsed: 00:08:45
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 529/1500 [0m                      

                       Computation: 100896 steps/s (collection: 0.824s, learning 0.150s)
             Mean action noise std: 2.20
          Mean value_function loss: 88.1677
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.9681
                       Mean reward: 323.49
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 66.2733
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.97s
                      Time elapsed: 00:08:46
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 530/1500 [0m                      

                       Computation: 103765 steps/s (collection: 0.842s, learning 0.105s)
             Mean action noise std: 2.21
          Mean value_function loss: 95.8265
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 16.9797
                       Mean reward: 384.15
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 66.9050
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.95s
                      Time elapsed: 00:08:46
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 531/1500 [0m                      

                       Computation: 101645 steps/s (collection: 0.798s, learning 0.169s)
             Mean action noise std: 2.21
          Mean value_function loss: 98.2310
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.9861
                       Mean reward: 352.24
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 71.1328
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.97s
                      Time elapsed: 00:08:47
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 532/1500 [0m                      

                       Computation: 113925 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 2.21
          Mean value_function loss: 98.5000
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 16.9912
                       Mean reward: 345.13
               Mean episode length: 233.79
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 70.2784
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.86s
                      Time elapsed: 00:08:48
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 533/1500 [0m                      

                       Computation: 111090 steps/s (collection: 0.759s, learning 0.126s)
             Mean action noise std: 2.21
          Mean value_function loss: 90.6743
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 17.0005
                       Mean reward: 327.28
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 69.0839
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.88s
                      Time elapsed: 00:08:49
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 534/1500 [0m                      

                       Computation: 103968 steps/s (collection: 0.792s, learning 0.153s)
             Mean action noise std: 2.22
          Mean value_function loss: 100.2389
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 17.0163
                       Mean reward: 360.27
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 67.7585
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.95s
                      Time elapsed: 00:08:50
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 535/1500 [0m                      

                       Computation: 108895 steps/s (collection: 0.794s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 98.1676
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 17.0191
                       Mean reward: 370.33
               Mean episode length: 238.33
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 70.3009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.90s
                      Time elapsed: 00:08:51
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 536/1500 [0m                      

                       Computation: 111470 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 2.22
          Mean value_function loss: 102.3445
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 17.0282
                       Mean reward: 367.96
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 70.7033
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.88s
                      Time elapsed: 00:08:52
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 537/1500 [0m                      

                       Computation: 105543 steps/s (collection: 0.835s, learning 0.096s)
             Mean action noise std: 2.22
          Mean value_function loss: 108.6120
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.0381
                       Mean reward: 320.18
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 69.5134
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.93s
                      Time elapsed: 00:08:53
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 538/1500 [0m                      

                       Computation: 97871 steps/s (collection: 0.847s, learning 0.158s)
             Mean action noise std: 2.23
          Mean value_function loss: 97.0021
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.0509
                       Mean reward: 350.38
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 72.1344
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 1.00s
                      Time elapsed: 00:08:54
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 539/1500 [0m                      

                       Computation: 103586 steps/s (collection: 0.850s, learning 0.099s)
             Mean action noise std: 2.23
          Mean value_function loss: 103.7032
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.0695
                       Mean reward: 385.82
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 73.0416
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.95s
                      Time elapsed: 00:08:55
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 540/1500 [0m                      

                       Computation: 109068 steps/s (collection: 0.801s, learning 0.100s)
             Mean action noise std: 2.23
          Mean value_function loss: 101.1279
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.0822
                       Mean reward: 316.86
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 71.3352
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.90s
                      Time elapsed: 00:08:56
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 541/1500 [0m                      

                       Computation: 106287 steps/s (collection: 0.816s, learning 0.109s)
             Mean action noise std: 2.24
          Mean value_function loss: 103.6474
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.0952
                       Mean reward: 366.20
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 73.6387
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.92s
                      Time elapsed: 00:08:57
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 542/1500 [0m                      

                       Computation: 107327 steps/s (collection: 0.777s, learning 0.139s)
             Mean action noise std: 2.24
          Mean value_function loss: 99.9516
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.1031
                       Mean reward: 361.14
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 73.2063
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.92s
                      Time elapsed: 00:08:58
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 543/1500 [0m                      

                       Computation: 105015 steps/s (collection: 0.818s, learning 0.118s)
             Mean action noise std: 2.24
          Mean value_function loss: 93.1829
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1084
                       Mean reward: 403.72
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.3246
    Episode_Reward/rotating_object: 76.1659
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.94s
                      Time elapsed: 00:08:58
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 544/1500 [0m                      

                       Computation: 105458 steps/s (collection: 0.815s, learning 0.117s)
             Mean action noise std: 2.25
          Mean value_function loss: 91.1972
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.1195
                       Mean reward: 364.68
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 72.0346
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.93s
                      Time elapsed: 00:08:59
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 545/1500 [0m                      

                       Computation: 96140 steps/s (collection: 0.891s, learning 0.132s)
             Mean action noise std: 2.25
          Mean value_function loss: 99.3775
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1321
                       Mean reward: 366.57
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 74.9978
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 1.02s
                      Time elapsed: 00:09:00
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 546/1500 [0m                      

                       Computation: 100515 steps/s (collection: 0.831s, learning 0.147s)
             Mean action noise std: 2.25
          Mean value_function loss: 102.9176
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.1454
                       Mean reward: 366.25
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 74.6062
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.98s
                      Time elapsed: 00:09:01
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 547/1500 [0m                      

                       Computation: 103304 steps/s (collection: 0.846s, learning 0.106s)
             Mean action noise std: 2.26
          Mean value_function loss: 102.3497
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.1551
                       Mean reward: 408.98
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 75.6558
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.95s
                      Time elapsed: 00:09:02
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 548/1500 [0m                      

                       Computation: 106166 steps/s (collection: 0.829s, learning 0.097s)
             Mean action noise std: 2.26
          Mean value_function loss: 103.3303
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.1625
                       Mean reward: 393.44
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 75.5704
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.93s
                      Time elapsed: 00:09:03
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 549/1500 [0m                      

                       Computation: 93861 steps/s (collection: 0.956s, learning 0.091s)
             Mean action noise std: 2.26
          Mean value_function loss: 95.3159
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.1706
                       Mean reward: 368.20
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 71.0158
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 1.05s
                      Time elapsed: 00:09:04
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 550/1500 [0m                      

                       Computation: 101077 steps/s (collection: 0.810s, learning 0.163s)
             Mean action noise std: 2.27
          Mean value_function loss: 97.4177
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.1824
                       Mean reward: 354.48
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 73.3984
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.97s
                      Time elapsed: 00:09:05
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 551/1500 [0m                      

                       Computation: 108684 steps/s (collection: 0.800s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 103.1343
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.2078
                       Mean reward: 360.36
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 68.2556
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.90s
                      Time elapsed: 00:09:06
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 552/1500 [0m                      

                       Computation: 104809 steps/s (collection: 0.804s, learning 0.134s)
             Mean action noise std: 2.28
          Mean value_function loss: 98.9348
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.2174
                       Mean reward: 390.38
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 72.1745
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.94s
                      Time elapsed: 00:09:07
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 553/1500 [0m                      

                       Computation: 96016 steps/s (collection: 0.836s, learning 0.188s)
             Mean action noise std: 2.28
          Mean value_function loss: 100.0442
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 17.2203
                       Mean reward: 374.65
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 74.4501
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 1.02s
                      Time elapsed: 00:09:08
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 554/1500 [0m                      

                       Computation: 103384 steps/s (collection: 0.764s, learning 0.187s)
             Mean action noise std: 2.29
          Mean value_function loss: 93.1172
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.2322
                       Mean reward: 357.64
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 73.0125
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.95s
                      Time elapsed: 00:09:09
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 555/1500 [0m                      

                       Computation: 105772 steps/s (collection: 0.825s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 91.1000
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.2540
                       Mean reward: 389.10
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 77.7330
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.93s
                      Time elapsed: 00:09:10
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 556/1500 [0m                      

                       Computation: 110339 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 90.0371
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 17.2633
                       Mean reward: 365.03
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 72.8342
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.89s
                      Time elapsed: 00:09:11
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 557/1500 [0m                      

                       Computation: 109898 steps/s (collection: 0.795s, learning 0.100s)
             Mean action noise std: 2.30
          Mean value_function loss: 98.5340
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.2717
                       Mean reward: 362.77
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 72.8388
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.89s
                      Time elapsed: 00:09:12
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 558/1500 [0m                      

                       Computation: 104126 steps/s (collection: 0.846s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 94.7713
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.2849
                       Mean reward: 409.29
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 75.5666
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.94s
                      Time elapsed: 00:09:13
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 559/1500 [0m                      

                       Computation: 108273 steps/s (collection: 0.818s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 98.0403
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.2914
                       Mean reward: 347.79
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 71.3024
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.91s
                      Time elapsed: 00:09:14
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 560/1500 [0m                      

                       Computation: 106909 steps/s (collection: 0.813s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 91.3645
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.2994
                       Mean reward: 333.83
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 69.0921
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.92s
                      Time elapsed: 00:09:15
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 561/1500 [0m                      

                       Computation: 110835 steps/s (collection: 0.780s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 92.4366
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2981
                       Mean reward: 382.37
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 78.5984
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.89s
                      Time elapsed: 00:09:16
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 562/1500 [0m                      

                       Computation: 105774 steps/s (collection: 0.824s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 100.6335
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 17.2995
                       Mean reward: 382.18
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 76.2607
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.93s
                      Time elapsed: 00:09:16
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 563/1500 [0m                      

                       Computation: 106427 steps/s (collection: 0.815s, learning 0.109s)
             Mean action noise std: 2.31
          Mean value_function loss: 99.1648
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.3080
                       Mean reward: 394.51
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 74.2543
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.92s
                      Time elapsed: 00:09:17
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 564/1500 [0m                      

                       Computation: 102839 steps/s (collection: 0.793s, learning 0.163s)
             Mean action noise std: 2.32
          Mean value_function loss: 82.2765
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3220
                       Mean reward: 385.12
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 76.8732
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.96s
                      Time elapsed: 00:09:18
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 565/1500 [0m                      

                       Computation: 112949 steps/s (collection: 0.779s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 90.4311
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3296
                       Mean reward: 402.05
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.3106
    Episode_Reward/rotating_object: 73.8942
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.87s
                      Time elapsed: 00:09:19
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 566/1500 [0m                      

                       Computation: 99009 steps/s (collection: 0.800s, learning 0.193s)
             Mean action noise std: 2.32
          Mean value_function loss: 93.5035
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.3239
                       Mean reward: 364.85
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 77.5552
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.99s
                      Time elapsed: 00:09:20
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 567/1500 [0m                      

                       Computation: 104675 steps/s (collection: 0.794s, learning 0.146s)
             Mean action noise std: 2.32
          Mean value_function loss: 88.6275
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 17.3327
                       Mean reward: 417.69
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.3125
    Episode_Reward/rotating_object: 76.0126
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.94s
                      Time elapsed: 00:09:21
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 568/1500 [0m                      

                       Computation: 99717 steps/s (collection: 0.824s, learning 0.162s)
             Mean action noise std: 2.33
          Mean value_function loss: 93.0605
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.3388
                       Mean reward: 380.07
               Mean episode length: 242.75
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 74.9545
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.99s
                      Time elapsed: 00:09:22
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 569/1500 [0m                      

                       Computation: 105895 steps/s (collection: 0.810s, learning 0.119s)
             Mean action noise std: 2.33
          Mean value_function loss: 90.2699
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 17.3496
                       Mean reward: 407.57
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 75.2648
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.93s
                      Time elapsed: 00:09:23
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 570/1500 [0m                      

                       Computation: 113541 steps/s (collection: 0.778s, learning 0.088s)
             Mean action noise std: 2.33
          Mean value_function loss: 95.7037
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.3596
                       Mean reward: 359.86
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 73.1678
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.87s
                      Time elapsed: 00:09:24
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 571/1500 [0m                      

                       Computation: 103031 steps/s (collection: 0.846s, learning 0.109s)
             Mean action noise std: 2.33
          Mean value_function loss: 101.4863
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.3646
                       Mean reward: 315.58
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 71.1428
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.95s
                      Time elapsed: 00:09:25
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 572/1500 [0m                      

                       Computation: 110824 steps/s (collection: 0.785s, learning 0.102s)
             Mean action noise std: 2.34
          Mean value_function loss: 103.4110
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 17.3672
                       Mean reward: 355.07
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 75.5342
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.89s
                      Time elapsed: 00:09:26
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 573/1500 [0m                      

                       Computation: 111862 steps/s (collection: 0.789s, learning 0.090s)
             Mean action noise std: 2.34
          Mean value_function loss: 100.0516
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.3694
                       Mean reward: 388.98
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 71.5080
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.88s
                      Time elapsed: 00:09:27
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 574/1500 [0m                      

                       Computation: 99944 steps/s (collection: 0.867s, learning 0.117s)
             Mean action noise std: 2.34
          Mean value_function loss: 101.8066
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.3740
                       Mean reward: 379.60
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 72.3937
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.98s
                      Time elapsed: 00:09:28
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 575/1500 [0m                      

                       Computation: 115125 steps/s (collection: 0.761s, learning 0.092s)
             Mean action noise std: 2.34
          Mean value_function loss: 111.2724
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 17.3719
                       Mean reward: 412.28
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.3271
    Episode_Reward/rotating_object: 78.5436
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.85s
                      Time elapsed: 00:09:28
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 576/1500 [0m                      

                       Computation: 107194 steps/s (collection: 0.811s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 99.6407
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.3702
                       Mean reward: 352.87
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 71.9770
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.92s
                      Time elapsed: 00:09:29
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 577/1500 [0m                      

                       Computation: 103130 steps/s (collection: 0.830s, learning 0.123s)
             Mean action noise std: 2.34
          Mean value_function loss: 106.0924
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.3657
                       Mean reward: 382.69
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 0.3323
    Episode_Reward/rotating_object: 75.5728
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.95s
                      Time elapsed: 00:09:30
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 578/1500 [0m                      

                       Computation: 97441 steps/s (collection: 0.839s, learning 0.170s)
             Mean action noise std: 2.35
          Mean value_function loss: 93.1742
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.3818
                       Mean reward: 370.14
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.3277
    Episode_Reward/rotating_object: 75.7470
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 1.01s
                      Time elapsed: 00:09:31
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 579/1500 [0m                      

                       Computation: 105833 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 2.35
          Mean value_function loss: 91.3007
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4003
                       Mean reward: 377.25
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3318
    Episode_Reward/rotating_object: 76.8928
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.93s
                      Time elapsed: 00:09:32
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 580/1500 [0m                      

                       Computation: 96858 steps/s (collection: 0.838s, learning 0.177s)
             Mean action noise std: 2.36
          Mean value_function loss: 85.7609
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.4034
                       Mean reward: 354.46
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 74.0989
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.01s
                      Time elapsed: 00:09:33
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 581/1500 [0m                      

                       Computation: 96799 steps/s (collection: 0.819s, learning 0.196s)
             Mean action noise std: 2.36
          Mean value_function loss: 89.4610
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.4123
                       Mean reward: 407.57
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3224
    Episode_Reward/rotating_object: 75.5825
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 1.02s
                      Time elapsed: 00:09:34
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 582/1500 [0m                      

                       Computation: 97769 steps/s (collection: 0.884s, learning 0.121s)
             Mean action noise std: 2.37
          Mean value_function loss: 99.0951
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4277
                       Mean reward: 384.45
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 73.1360
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 1.01s
                      Time elapsed: 00:09:35
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 583/1500 [0m                      

                       Computation: 97581 steps/s (collection: 0.860s, learning 0.148s)
             Mean action noise std: 2.37
          Mean value_function loss: 103.1739
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.4411
                       Mean reward: 355.36
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 72.6567
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 1.01s
                      Time elapsed: 00:09:36
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 584/1500 [0m                      

                       Computation: 98812 steps/s (collection: 0.850s, learning 0.145s)
             Mean action noise std: 2.37
          Mean value_function loss: 96.7701
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.4391
                       Mean reward: 367.48
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 75.8260
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.99s
                      Time elapsed: 00:09:37
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 585/1500 [0m                      

                       Computation: 99508 steps/s (collection: 0.874s, learning 0.114s)
             Mean action noise std: 2.38
          Mean value_function loss: 95.4816
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4475
                       Mean reward: 374.26
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 76.7348
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.99s
                      Time elapsed: 00:09:38
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 586/1500 [0m                      

                       Computation: 108193 steps/s (collection: 0.808s, learning 0.101s)
             Mean action noise std: 2.38
          Mean value_function loss: 90.3755
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.4488
                       Mean reward: 353.38
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 0.3089
    Episode_Reward/rotating_object: 76.3095
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.91s
                      Time elapsed: 00:09:39
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 587/1500 [0m                      

                       Computation: 100116 steps/s (collection: 0.788s, learning 0.194s)
             Mean action noise std: 2.38
          Mean value_function loss: 100.1695
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.4554
                       Mean reward: 396.94
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 74.8468
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.98s
                      Time elapsed: 00:09:40
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 588/1500 [0m                      

                       Computation: 105208 steps/s (collection: 0.805s, learning 0.130s)
             Mean action noise std: 2.38
          Mean value_function loss: 104.3271
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 17.4626
                       Mean reward: 354.91
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 71.6887
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.93s
                      Time elapsed: 00:09:41
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 589/1500 [0m                      

                       Computation: 111609 steps/s (collection: 0.771s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 100.3453
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.4696
                       Mean reward: 345.42
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 71.4051
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.88s
                      Time elapsed: 00:09:42
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 590/1500 [0m                      

                       Computation: 109687 steps/s (collection: 0.806s, learning 0.090s)
             Mean action noise std: 2.39
          Mean value_function loss: 110.0180
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.4726
                       Mean reward: 400.50
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 78.1280
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.90s
                      Time elapsed: 00:09:43
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 591/1500 [0m                      

                       Computation: 106272 steps/s (collection: 0.808s, learning 0.117s)
             Mean action noise std: 2.39
          Mean value_function loss: 105.4025
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.4798
                       Mean reward: 406.29
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 78.0093
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.93s
                      Time elapsed: 00:09:44
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 592/1500 [0m                      

                       Computation: 107648 steps/s (collection: 0.806s, learning 0.108s)
             Mean action noise std: 2.39
          Mean value_function loss: 104.9046
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4983
                       Mean reward: 416.36
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.3158
    Episode_Reward/rotating_object: 78.2629
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 0.91s
                      Time elapsed: 00:09:45
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 593/1500 [0m                      

                       Computation: 106680 steps/s (collection: 0.823s, learning 0.098s)
             Mean action noise std: 2.40
          Mean value_function loss: 101.9979
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.5129
                       Mean reward: 372.62
               Mean episode length: 240.12
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 73.9278
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.92s
                      Time elapsed: 00:09:46
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 594/1500 [0m                      

                       Computation: 107600 steps/s (collection: 0.808s, learning 0.105s)
             Mean action noise std: 2.40
          Mean value_function loss: 108.0856
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.5264
                       Mean reward: 403.58
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 76.4870
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.91s
                      Time elapsed: 00:09:47
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 595/1500 [0m                      

                       Computation: 107155 steps/s (collection: 0.801s, learning 0.117s)
             Mean action noise std: 2.41
          Mean value_function loss: 110.1527
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.5397
                       Mean reward: 404.15
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 80.0262
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.92s
                      Time elapsed: 00:09:47
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 596/1500 [0m                      

                       Computation: 106130 steps/s (collection: 0.815s, learning 0.111s)
             Mean action noise std: 2.41
          Mean value_function loss: 120.2997
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.5517
                       Mean reward: 338.64
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 71.6442
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 0.93s
                      Time elapsed: 00:09:48
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 597/1500 [0m                      

                       Computation: 107269 steps/s (collection: 0.804s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 100.8105
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.5603
                       Mean reward: 379.55
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 72.5401
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.92s
                      Time elapsed: 00:09:49
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 598/1500 [0m                      

                       Computation: 107504 steps/s (collection: 0.793s, learning 0.121s)
             Mean action noise std: 2.42
          Mean value_function loss: 110.1291
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.5795
                       Mean reward: 429.14
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 77.2548
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.91s
                      Time elapsed: 00:09:50
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 599/1500 [0m                      

                       Computation: 110130 steps/s (collection: 0.772s, learning 0.121s)
             Mean action noise std: 2.42
          Mean value_function loss: 112.6402
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.5965
                       Mean reward: 347.97
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 74.2388
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.89s
                      Time elapsed: 00:09:51
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 600/1500 [0m                      

                       Computation: 99114 steps/s (collection: 0.863s, learning 0.129s)
             Mean action noise std: 2.43
          Mean value_function loss: 106.0814
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.6038
                       Mean reward: 450.60
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 75.3891
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.99s
                      Time elapsed: 00:09:52
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 601/1500 [0m                      

                       Computation: 102173 steps/s (collection: 0.817s, learning 0.145s)
             Mean action noise std: 2.43
          Mean value_function loss: 105.1449
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.6234
                       Mean reward: 355.55
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 75.6977
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.96s
                      Time elapsed: 00:09:53
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 602/1500 [0m                      

                       Computation: 85499 steps/s (collection: 1.035s, learning 0.115s)
             Mean action noise std: 2.44
          Mean value_function loss: 92.6965
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6477
                       Mean reward: 373.37
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.3095
    Episode_Reward/rotating_object: 73.9264
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 1.15s
                      Time elapsed: 00:09:54
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 603/1500 [0m                      

                       Computation: 98965 steps/s (collection: 0.804s, learning 0.189s)
             Mean action noise std: 2.44
          Mean value_function loss: 104.2542
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6680
                       Mean reward: 344.28
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 73.6657
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.99s
                      Time elapsed: 00:09:55
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 604/1500 [0m                      

                       Computation: 105556 steps/s (collection: 0.790s, learning 0.142s)
             Mean action noise std: 2.45
          Mean value_function loss: 105.5563
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.6874
                       Mean reward: 383.17
               Mean episode length: 238.09
    Episode_Reward/reaching_object: 0.3265
    Episode_Reward/rotating_object: 76.7168
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.93s
                      Time elapsed: 00:09:56
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 605/1500 [0m                      

                       Computation: 99138 steps/s (collection: 0.799s, learning 0.192s)
             Mean action noise std: 2.45
          Mean value_function loss: 95.4843
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7025
                       Mean reward: 364.44
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 0.3093
    Episode_Reward/rotating_object: 75.9789
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.99s
                      Time elapsed: 00:09:57
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 606/1500 [0m                      

                       Computation: 105658 steps/s (collection: 0.806s, learning 0.124s)
             Mean action noise std: 2.45
          Mean value_function loss: 102.3905
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 17.7067
                       Mean reward: 354.55
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 77.7143
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.93s
                      Time elapsed: 00:09:58
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 607/1500 [0m                      

                       Computation: 103215 steps/s (collection: 0.796s, learning 0.157s)
             Mean action noise std: 2.46
          Mean value_function loss: 102.0152
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7138
                       Mean reward: 362.56
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 74.8856
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.95s
                      Time elapsed: 00:09:59
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 608/1500 [0m                      

                       Computation: 99944 steps/s (collection: 0.820s, learning 0.164s)
             Mean action noise std: 2.46
          Mean value_function loss: 98.1768
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7291
                       Mean reward: 437.75
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 82.4267
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.98s
                      Time elapsed: 00:10:00
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 609/1500 [0m                      

                       Computation: 104212 steps/s (collection: 0.823s, learning 0.120s)
             Mean action noise std: 2.46
          Mean value_function loss: 105.0598
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7391
                       Mean reward: 332.20
               Mean episode length: 229.52
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 74.9433
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.94s
                      Time elapsed: 00:10:01
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 610/1500 [0m                      

                       Computation: 100015 steps/s (collection: 0.851s, learning 0.132s)
             Mean action noise std: 2.47
          Mean value_function loss: 103.2641
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.7553
                       Mean reward: 429.29
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.3011
    Episode_Reward/rotating_object: 79.3078
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.98s
                      Time elapsed: 00:10:02
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 611/1500 [0m                      

                       Computation: 112893 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 2.47
          Mean value_function loss: 98.1986
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.7564
                       Mean reward: 365.19
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 76.9900
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.87s
                      Time elapsed: 00:10:03
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 612/1500 [0m                      

                       Computation: 107778 steps/s (collection: 0.818s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 96.1488
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.7607
                       Mean reward: 394.58
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 81.4389
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.91s
                      Time elapsed: 00:10:04
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 613/1500 [0m                      

                       Computation: 99675 steps/s (collection: 0.882s, learning 0.104s)
             Mean action noise std: 2.48
          Mean value_function loss: 104.4683
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 17.7681
                       Mean reward: 370.31
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 0.3058
    Episode_Reward/rotating_object: 74.2452
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.99s
                      Time elapsed: 00:10:05
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 614/1500 [0m                      

                       Computation: 105743 steps/s (collection: 0.802s, learning 0.128s)
             Mean action noise std: 2.48
          Mean value_function loss: 101.0378
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7818
                       Mean reward: 378.82
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.3041
    Episode_Reward/rotating_object: 76.2548
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.93s
                      Time elapsed: 00:10:06
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 615/1500 [0m                      

                       Computation: 94066 steps/s (collection: 0.903s, learning 0.142s)
             Mean action noise std: 2.49
          Mean value_function loss: 107.7716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7987
                       Mean reward: 439.57
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 86.5585
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 1.05s
                      Time elapsed: 00:10:07
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 616/1500 [0m                      

                       Computation: 92673 steps/s (collection: 0.943s, learning 0.118s)
             Mean action noise std: 2.49
          Mean value_function loss: 98.9420
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8060
                       Mean reward: 373.40
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 74.1860
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 1.06s
                      Time elapsed: 00:10:08
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 617/1500 [0m                      

                       Computation: 104415 steps/s (collection: 0.832s, learning 0.109s)
             Mean action noise std: 2.50
          Mean value_function loss: 92.8870
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.8146
                       Mean reward: 405.55
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 80.8676
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.94s
                      Time elapsed: 00:10:09
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 618/1500 [0m                      

                       Computation: 104608 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 2.50
          Mean value_function loss: 112.2860
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.8257
                       Mean reward: 383.09
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 73.9298
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.94s
                      Time elapsed: 00:10:10
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 619/1500 [0m                      

                       Computation: 106814 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 2.50
          Mean value_function loss: 100.1214
               Mean surrogate loss: 0.0206
                 Mean entropy loss: 17.8312
                       Mean reward: 395.65
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.3100
    Episode_Reward/rotating_object: 73.8278
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.92s
                      Time elapsed: 00:10:11
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 620/1500 [0m                      

                       Computation: 109334 steps/s (collection: 0.808s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 106.6794
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8334
                       Mean reward: 402.60
               Mean episode length: 235.17
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 77.7715
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.90s
                      Time elapsed: 00:10:11
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 621/1500 [0m                      

                       Computation: 113556 steps/s (collection: 0.771s, learning 0.095s)
             Mean action noise std: 2.50
          Mean value_function loss: 110.0292
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.8329
                       Mean reward: 415.42
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 0.3221
    Episode_Reward/rotating_object: 78.6217
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.87s
                      Time elapsed: 00:10:12
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 622/1500 [0m                      

                       Computation: 105251 steps/s (collection: 0.817s, learning 0.117s)
             Mean action noise std: 2.51
          Mean value_function loss: 118.0848
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 17.8303
                       Mean reward: 407.21
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 76.4609
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.93s
                      Time elapsed: 00:10:13
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 623/1500 [0m                      

                       Computation: 101348 steps/s (collection: 0.845s, learning 0.125s)
             Mean action noise std: 2.51
          Mean value_function loss: 114.5764
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8364
                       Mean reward: 399.80
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 0.3241
    Episode_Reward/rotating_object: 78.8576
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.97s
                      Time elapsed: 00:10:14
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 624/1500 [0m                      

                       Computation: 105488 steps/s (collection: 0.841s, learning 0.091s)
             Mean action noise std: 2.52
          Mean value_function loss: 103.4506
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.8520
                       Mean reward: 363.53
               Mean episode length: 233.38
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 77.2528
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.93s
                      Time elapsed: 00:10:15
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 625/1500 [0m                      

                       Computation: 106120 steps/s (collection: 0.808s, learning 0.118s)
             Mean action noise std: 2.52
          Mean value_function loss: 115.8778
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.8693
                       Mean reward: 410.60
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 81.7676
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.93s
                      Time elapsed: 00:10:16
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 626/1500 [0m                      

                       Computation: 109168 steps/s (collection: 0.803s, learning 0.097s)
             Mean action noise std: 2.52
          Mean value_function loss: 117.6787
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8741
                       Mean reward: 415.97
               Mean episode length: 233.67
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 77.9390
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 0.90s
                      Time elapsed: 00:10:17
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 627/1500 [0m                      

                       Computation: 104986 steps/s (collection: 0.833s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 114.4464
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 17.8745
                       Mean reward: 411.90
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 80.4498
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 0.94s
                      Time elapsed: 00:10:18
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 628/1500 [0m                      

                       Computation: 106587 steps/s (collection: 0.804s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 108.7598
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8776
                       Mean reward: 361.68
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.3158
    Episode_Reward/rotating_object: 71.8082
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.92s
                      Time elapsed: 00:10:19
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 629/1500 [0m                      

                       Computation: 107568 steps/s (collection: 0.809s, learning 0.105s)
             Mean action noise std: 2.53
          Mean value_function loss: 113.4642
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.8945
                       Mean reward: 377.57
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 0.3125
    Episode_Reward/rotating_object: 75.6166
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.91s
                      Time elapsed: 00:10:20
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 630/1500 [0m                      

                       Computation: 101315 steps/s (collection: 0.856s, learning 0.115s)
             Mean action noise std: 2.54
          Mean value_function loss: 112.9845
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.9074
                       Mean reward: 368.12
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3289
    Episode_Reward/rotating_object: 75.7901
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.97s
                      Time elapsed: 00:10:21
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 631/1500 [0m                      

                       Computation: 106945 steps/s (collection: 0.814s, learning 0.105s)
             Mean action noise std: 2.54
          Mean value_function loss: 111.8629
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.9146
                       Mean reward: 423.21
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 74.9717
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.92s
                      Time elapsed: 00:10:22
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 632/1500 [0m                      

                       Computation: 102923 steps/s (collection: 0.855s, learning 0.100s)
             Mean action noise std: 2.54
          Mean value_function loss: 107.8791
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.9214
                       Mean reward: 413.12
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 81.3389
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.96s
                      Time elapsed: 00:10:23
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 633/1500 [0m                      

                       Computation: 97101 steps/s (collection: 0.913s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 108.3811
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9353
                       Mean reward: 379.49
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 76.7142
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 1.01s
                      Time elapsed: 00:10:24
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 634/1500 [0m                      

                       Computation: 103715 steps/s (collection: 0.841s, learning 0.107s)
             Mean action noise std: 2.55
          Mean value_function loss: 116.3735
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 17.9478
                       Mean reward: 402.26
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 78.8713
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.95s
                      Time elapsed: 00:10:25
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 635/1500 [0m                      

                       Computation: 97770 steps/s (collection: 0.850s, learning 0.156s)
             Mean action noise std: 2.55
          Mean value_function loss: 98.7926
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.9533
                       Mean reward: 373.17
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 75.3859
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 1.01s
                      Time elapsed: 00:10:26
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 636/1500 [0m                      

                       Computation: 102352 steps/s (collection: 0.840s, learning 0.120s)
             Mean action noise std: 2.55
          Mean value_function loss: 100.9342
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.9589
                       Mean reward: 361.70
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.3203
    Episode_Reward/rotating_object: 73.4665
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.96s
                      Time elapsed: 00:10:27
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 637/1500 [0m                      

                       Computation: 101148 steps/s (collection: 0.873s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 103.7035
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.9666
                       Mean reward: 358.70
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 75.3415
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.97s
                      Time elapsed: 00:10:27
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 638/1500 [0m                      

                       Computation: 100480 steps/s (collection: 0.856s, learning 0.122s)
             Mean action noise std: 2.56
          Mean value_function loss: 107.0961
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.9721
                       Mean reward: 399.20
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 77.1198
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.98s
                      Time elapsed: 00:10:28
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 639/1500 [0m                      

                       Computation: 98756 steps/s (collection: 0.894s, learning 0.102s)
             Mean action noise std: 2.56
          Mean value_function loss: 111.5828
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.9724
                       Mean reward: 410.85
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 73.9917
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 1.00s
                      Time elapsed: 00:10:29
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 640/1500 [0m                      

                       Computation: 98592 steps/s (collection: 0.884s, learning 0.113s)
             Mean action noise std: 2.56
          Mean value_function loss: 99.1553
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.9775
                       Mean reward: 359.79
               Mean episode length: 236.70
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 75.1096
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 1.00s
                      Time elapsed: 00:10:30
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 641/1500 [0m                      

                       Computation: 106163 steps/s (collection: 0.828s, learning 0.098s)
             Mean action noise std: 2.56
          Mean value_function loss: 100.3130
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.9857
                       Mean reward: 355.28
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 74.1679
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 0.93s
                      Time elapsed: 00:10:31
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 642/1500 [0m                      

                       Computation: 103481 steps/s (collection: 0.854s, learning 0.096s)
             Mean action noise std: 2.57
          Mean value_function loss: 108.6132
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.9941
                       Mean reward: 361.05
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 71.6492
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.95s
                      Time elapsed: 00:10:32
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 643/1500 [0m                      

                       Computation: 104396 steps/s (collection: 0.835s, learning 0.107s)
             Mean action noise std: 2.57
          Mean value_function loss: 116.5765
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0106
                       Mean reward: 414.70
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 81.0833
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.94s
                      Time elapsed: 00:10:33
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 644/1500 [0m                      

                       Computation: 86673 steps/s (collection: 0.937s, learning 0.197s)
             Mean action noise std: 2.58
          Mean value_function loss: 100.6218
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.0244
                       Mean reward: 423.43
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 76.6139
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.13s
                      Time elapsed: 00:10:34
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 645/1500 [0m                      

                       Computation: 105161 steps/s (collection: 0.827s, learning 0.108s)
             Mean action noise std: 2.58
          Mean value_function loss: 110.2630
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0394
                       Mean reward: 377.04
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 77.8939
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 0.93s
                      Time elapsed: 00:10:35
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 646/1500 [0m                      

                       Computation: 103907 steps/s (collection: 0.841s, learning 0.105s)
             Mean action noise std: 2.58
          Mean value_function loss: 106.0149
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.0454
                       Mean reward: 416.63
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.3072
    Episode_Reward/rotating_object: 78.9032
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.95s
                      Time elapsed: 00:10:36
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 647/1500 [0m                      

                       Computation: 108599 steps/s (collection: 0.802s, learning 0.103s)
             Mean action noise std: 2.59
          Mean value_function loss: 104.9556
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.0452
                       Mean reward: 393.84
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 76.9655
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.91s
                      Time elapsed: 00:10:37
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 648/1500 [0m                      

                       Computation: 99256 steps/s (collection: 0.829s, learning 0.161s)
             Mean action noise std: 2.59
          Mean value_function loss: 95.9893
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.0561
                       Mean reward: 368.24
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 0.3036
    Episode_Reward/rotating_object: 74.5848
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 0.99s
                      Time elapsed: 00:10:38
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 649/1500 [0m                      

                       Computation: 102118 steps/s (collection: 0.794s, learning 0.168s)
             Mean action noise std: 2.59
          Mean value_function loss: 106.0373
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0665
                       Mean reward: 348.87
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 0.3061
    Episode_Reward/rotating_object: 75.5966
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 0.96s
                      Time elapsed: 00:10:39
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 650/1500 [0m                      

                       Computation: 110025 steps/s (collection: 0.792s, learning 0.102s)
             Mean action noise std: 2.60
          Mean value_function loss: 106.5431
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.0814
                       Mean reward: 359.53
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 0.3057
    Episode_Reward/rotating_object: 75.6177
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.89s
                      Time elapsed: 00:10:40
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 651/1500 [0m                      

                       Computation: 106000 steps/s (collection: 0.777s, learning 0.151s)
             Mean action noise std: 2.61
          Mean value_function loss: 101.7683
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.1018
                       Mean reward: 431.64
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 0.3032
    Episode_Reward/rotating_object: 78.4746
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 0.93s
                      Time elapsed: 00:10:41
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 652/1500 [0m                      

                       Computation: 100950 steps/s (collection: 0.838s, learning 0.136s)
             Mean action noise std: 2.61
          Mean value_function loss: 101.2260
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.1154
                       Mean reward: 386.11
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 76.9265
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 0.97s
                      Time elapsed: 00:10:42
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 653/1500 [0m                      

                       Computation: 110425 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 2.61
          Mean value_function loss: 105.6268
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.1216
                       Mean reward: 381.23
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 76.4213
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.89s
                      Time elapsed: 00:10:43
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 654/1500 [0m                      

                       Computation: 106224 steps/s (collection: 0.825s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 105.9457
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1378
                       Mean reward: 402.74
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.3008
    Episode_Reward/rotating_object: 72.5928
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.93s
                      Time elapsed: 00:10:44
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 655/1500 [0m                      

                       Computation: 112084 steps/s (collection: 0.782s, learning 0.095s)
             Mean action noise std: 2.62
          Mean value_function loss: 108.8870
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.1596
                       Mean reward: 387.77
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 0.3050
    Episode_Reward/rotating_object: 74.5542
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.88s
                      Time elapsed: 00:10:45
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 656/1500 [0m                      

                       Computation: 108499 steps/s (collection: 0.810s, learning 0.096s)
             Mean action noise std: 2.63
          Mean value_function loss: 105.7903
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1720
                       Mean reward: 408.94
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 77.2613
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 0.91s
                      Time elapsed: 00:10:46
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 657/1500 [0m                      

                       Computation: 108505 steps/s (collection: 0.803s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 112.1376
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.1748
                       Mean reward: 404.00
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.3123
    Episode_Reward/rotating_object: 78.1271
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 0.91s
                      Time elapsed: 00:10:46
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 658/1500 [0m                      

                       Computation: 107363 steps/s (collection: 0.820s, learning 0.096s)
             Mean action noise std: 2.63
          Mean value_function loss: 109.6924
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.1733
                       Mean reward: 398.49
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 79.5262
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.92s
                      Time elapsed: 00:10:47
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 659/1500 [0m                      

                       Computation: 107343 steps/s (collection: 0.807s, learning 0.108s)
             Mean action noise std: 2.63
          Mean value_function loss: 107.0677
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.1758
                       Mean reward: 349.08
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.3040
    Episode_Reward/rotating_object: 74.7479
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.92s
                      Time elapsed: 00:10:48
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 660/1500 [0m                      

                       Computation: 104476 steps/s (collection: 0.806s, learning 0.135s)
             Mean action noise std: 2.64
          Mean value_function loss: 97.8851
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.1821
                       Mean reward: 392.81
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 77.5473
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.94s
                      Time elapsed: 00:10:49
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 661/1500 [0m                      

                       Computation: 98881 steps/s (collection: 0.822s, learning 0.172s)
             Mean action noise std: 2.64
          Mean value_function loss: 104.1251
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.1894
                       Mean reward: 397.08
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 81.1285
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.99s
                      Time elapsed: 00:10:50
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 662/1500 [0m                      

                       Computation: 106656 steps/s (collection: 0.826s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 107.6138
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.1936
                       Mean reward: 370.30
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 77.9848
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.92s
                      Time elapsed: 00:10:51
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 663/1500 [0m                      

                       Computation: 93359 steps/s (collection: 0.863s, learning 0.190s)
             Mean action noise std: 2.65
          Mean value_function loss: 100.9854
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.2061
                       Mean reward: 371.14
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.3123
    Episode_Reward/rotating_object: 81.9621
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 1.05s
                      Time elapsed: 00:10:52
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 664/1500 [0m                      

                       Computation: 101849 steps/s (collection: 0.833s, learning 0.132s)
             Mean action noise std: 2.65
          Mean value_function loss: 103.1697
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.2195
                       Mean reward: 401.49
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.3054
    Episode_Reward/rotating_object: 73.4020
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.97s
                      Time elapsed: 00:10:53
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 665/1500 [0m                      

                       Computation: 95501 steps/s (collection: 0.883s, learning 0.146s)
             Mean action noise std: 2.65
          Mean value_function loss: 107.0609
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.2305
                       Mean reward: 393.93
               Mean episode length: 235.83
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 74.5289
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 1.03s
                      Time elapsed: 00:10:54
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 666/1500 [0m                      

                       Computation: 61467 steps/s (collection: 1.496s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 106.4351
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.2471
                       Mean reward: 374.95
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.2999
    Episode_Reward/rotating_object: 72.4413
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.60s
                      Time elapsed: 00:10:56
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 667/1500 [0m                      

                       Computation: 34534 steps/s (collection: 2.720s, learning 0.127s)
             Mean action noise std: 2.66
          Mean value_function loss: 104.3123
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.2529
                       Mean reward: 402.41
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 80.9920
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 2.85s
                      Time elapsed: 00:10:59
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 668/1500 [0m                      

                       Computation: 33400 steps/s (collection: 2.824s, learning 0.119s)
             Mean action noise std: 2.66
          Mean value_function loss: 103.6634
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2529
                       Mean reward: 372.87
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 74.4122
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 2.94s
                      Time elapsed: 00:11:02
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 669/1500 [0m                      

                       Computation: 32065 steps/s (collection: 2.948s, learning 0.118s)
             Mean action noise std: 2.66
          Mean value_function loss: 105.3575
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 18.2493
                       Mean reward: 352.18
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 76.3729
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.07s
                      Time elapsed: 00:11:05
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 670/1500 [0m                      

                       Computation: 30702 steps/s (collection: 3.063s, learning 0.139s)
             Mean action noise std: 2.66
          Mean value_function loss: 107.8523
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.2466
                       Mean reward: 424.26
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 78.3778
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.20s
                      Time elapsed: 00:11:08
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 671/1500 [0m                      

                       Computation: 31646 steps/s (collection: 2.970s, learning 0.137s)
             Mean action noise std: 2.66
          Mean value_function loss: 96.9685
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.2511
                       Mean reward: 385.00
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 78.4287
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.11s
                      Time elapsed: 00:11:11
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 672/1500 [0m                      

                       Computation: 33773 steps/s (collection: 2.785s, learning 0.126s)
             Mean action noise std: 2.66
          Mean value_function loss: 107.7487
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2500
                       Mean reward: 393.13
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.3230
    Episode_Reward/rotating_object: 82.0751
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 2.91s
                      Time elapsed: 00:11:14
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 673/1500 [0m                      

                       Computation: 32632 steps/s (collection: 2.876s, learning 0.137s)
             Mean action noise std: 2.67
          Mean value_function loss: 102.9301
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.2482
                       Mean reward: 363.22
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 78.2599
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.01s
                      Time elapsed: 00:11:17
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 674/1500 [0m                      

                       Computation: 30921 steps/s (collection: 3.035s, learning 0.145s)
             Mean action noise std: 2.67
          Mean value_function loss: 100.1213
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2517
                       Mean reward: 374.22
               Mean episode length: 236.70
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 80.9825
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.18s
                      Time elapsed: 00:11:20
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 675/1500 [0m                      

                       Computation: 30635 steps/s (collection: 3.060s, learning 0.149s)
             Mean action noise std: 2.67
          Mean value_function loss: 108.5616
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2647
                       Mean reward: 391.59
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 0.3239
    Episode_Reward/rotating_object: 78.5617
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.21s
                      Time elapsed: 00:11:23
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 676/1500 [0m                      

                       Computation: 107107 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 2.68
          Mean value_function loss: 114.1925
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2757
                       Mean reward: 422.82
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.3304
    Episode_Reward/rotating_object: 81.6780
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.92s
                      Time elapsed: 00:11:24
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 677/1500 [0m                      

                       Computation: 102786 steps/s (collection: 0.792s, learning 0.165s)
             Mean action noise std: 2.68
          Mean value_function loss: 115.1683
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.2833
                       Mean reward: 402.77
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 75.5102
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.96s
                      Time elapsed: 00:11:25
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 678/1500 [0m                      

                       Computation: 98811 steps/s (collection: 0.882s, learning 0.113s)
             Mean action noise std: 2.68
          Mean value_function loss: 103.5693
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.2858
                       Mean reward: 351.92
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 78.1338
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.99s
                      Time elapsed: 00:11:26
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 679/1500 [0m                      

                       Computation: 99019 steps/s (collection: 0.810s, learning 0.183s)
             Mean action noise std: 2.69
          Mean value_function loss: 106.0278
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.2916
                       Mean reward: 418.58
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 79.9709
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.99s
                      Time elapsed: 00:11:27
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 680/1500 [0m                      

                       Computation: 94646 steps/s (collection: 0.872s, learning 0.167s)
             Mean action noise std: 2.69
          Mean value_function loss: 101.4259
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3097
                       Mean reward: 369.32
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 0.3203
    Episode_Reward/rotating_object: 74.9713
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 1.04s
                      Time elapsed: 00:11:28
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 681/1500 [0m                      

                       Computation: 85728 steps/s (collection: 0.877s, learning 0.270s)
             Mean action noise std: 2.70
          Mean value_function loss: 101.1183
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3253
                       Mean reward: 421.32
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 75.9474
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 1.15s
                      Time elapsed: 00:11:29
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 682/1500 [0m                      

                       Computation: 100456 steps/s (collection: 0.830s, learning 0.149s)
             Mean action noise std: 2.70
          Mean value_function loss: 100.8168
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3376
                       Mean reward: 411.05
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 82.2476
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.98s
                      Time elapsed: 00:11:30
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 683/1500 [0m                      

                       Computation: 112223 steps/s (collection: 0.769s, learning 0.107s)
             Mean action noise std: 2.71
          Mean value_function loss: 108.7650
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3537
                       Mean reward: 399.52
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 79.9881
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.88s
                      Time elapsed: 00:11:31
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 684/1500 [0m                      

                       Computation: 103110 steps/s (collection: 0.804s, learning 0.149s)
             Mean action noise std: 2.71
          Mean value_function loss: 106.2284
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3632
                       Mean reward: 417.09
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 79.1151
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.95s
                      Time elapsed: 00:11:32
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 685/1500 [0m                      

                       Computation: 107810 steps/s (collection: 0.786s, learning 0.126s)
             Mean action noise std: 2.71
          Mean value_function loss: 109.2079
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3658
                       Mean reward: 398.97
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 78.7631
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.91s
                      Time elapsed: 00:11:33
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 686/1500 [0m                      

                       Computation: 94704 steps/s (collection: 0.877s, learning 0.161s)
             Mean action noise std: 2.71
          Mean value_function loss: 102.2906
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 18.3716
                       Mean reward: 402.56
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 0.3100
    Episode_Reward/rotating_object: 79.4440
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 1.04s
                      Time elapsed: 00:11:34
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 687/1500 [0m                      

                       Computation: 102235 steps/s (collection: 0.822s, learning 0.140s)
             Mean action noise std: 2.72
          Mean value_function loss: 104.1411
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.3813
                       Mean reward: 413.52
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 79.4304
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.96s
                      Time elapsed: 00:11:35
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 688/1500 [0m                      

                       Computation: 109231 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 109.5710
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.3936
                       Mean reward: 367.19
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 80.5118
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.90s
                      Time elapsed: 00:11:36
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 689/1500 [0m                      

                       Computation: 104256 steps/s (collection: 0.820s, learning 0.123s)
             Mean action noise std: 2.73
          Mean value_function loss: 98.6695
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.4052
                       Mean reward: 393.29
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 82.6031
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.94s
                      Time elapsed: 00:11:37
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 690/1500 [0m                      

                       Computation: 109604 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 2.73
          Mean value_function loss: 96.0648
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.4142
                       Mean reward: 423.43
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 85.4409
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.90s
                      Time elapsed: 00:11:38
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 691/1500 [0m                      

                       Computation: 105286 steps/s (collection: 0.830s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 96.0206
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4264
                       Mean reward: 378.83
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.3053
    Episode_Reward/rotating_object: 77.8134
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.93s
                      Time elapsed: 00:11:39
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 692/1500 [0m                      

                       Computation: 108094 steps/s (collection: 0.812s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 99.2401
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.4373
                       Mean reward: 424.96
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.3179
    Episode_Reward/rotating_object: 84.0389
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.91s
                      Time elapsed: 00:11:40
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 693/1500 [0m                      

                       Computation: 106412 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 2.74
          Mean value_function loss: 99.5745
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4473
                       Mean reward: 361.26
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 0.3224
    Episode_Reward/rotating_object: 81.6891
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.92s
                      Time elapsed: 00:11:41
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 694/1500 [0m                      

                       Computation: 106290 steps/s (collection: 0.820s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 102.8509
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4558
                       Mean reward: 421.68
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 80.4312
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.92s
                      Time elapsed: 00:11:41
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 695/1500 [0m                      

                       Computation: 103919 steps/s (collection: 0.811s, learning 0.135s)
             Mean action noise std: 2.75
          Mean value_function loss: 104.2803
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.4717
                       Mean reward: 398.91
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 81.0884
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.95s
                      Time elapsed: 00:11:42
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 696/1500 [0m                      

                       Computation: 102622 steps/s (collection: 0.816s, learning 0.142s)
             Mean action noise std: 2.76
          Mean value_function loss: 106.3332
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.4850
                       Mean reward: 416.63
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 82.5591
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.96s
                      Time elapsed: 00:11:43
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 697/1500 [0m                      

                       Computation: 106979 steps/s (collection: 0.804s, learning 0.115s)
             Mean action noise std: 2.76
          Mean value_function loss: 97.7799
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.4882
                       Mean reward: 403.31
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 79.6482
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.92s
                      Time elapsed: 00:11:44
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 698/1500 [0m                      

                       Computation: 105192 steps/s (collection: 0.800s, learning 0.135s)
             Mean action noise std: 2.76
          Mean value_function loss: 101.5864
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.4924
                       Mean reward: 456.55
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 84.6904
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.93s
                      Time elapsed: 00:11:45
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 699/1500 [0m                      

                       Computation: 104533 steps/s (collection: 0.800s, learning 0.140s)
             Mean action noise std: 2.77
          Mean value_function loss: 92.5681
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5101
                       Mean reward: 443.74
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.3267
    Episode_Reward/rotating_object: 87.2693
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.94s
                      Time elapsed: 00:11:46
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 700/1500 [0m                      

                       Computation: 106238 steps/s (collection: 0.795s, learning 0.130s)
             Mean action noise std: 2.77
          Mean value_function loss: 108.3209
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.5215
                       Mean reward: 416.77
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 83.9958
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.93s
                      Time elapsed: 00:11:47
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 701/1500 [0m                      

                       Computation: 104153 steps/s (collection: 0.849s, learning 0.095s)
             Mean action noise std: 2.77
          Mean value_function loss: 99.6862
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 18.5247
                       Mean reward: 445.28
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 80.3608
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.94s
                      Time elapsed: 00:11:48
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 702/1500 [0m                      

                       Computation: 108558 steps/s (collection: 0.796s, learning 0.109s)
             Mean action noise std: 2.77
          Mean value_function loss: 97.1278
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.5257
                       Mean reward: 404.63
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 78.7339
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.91s
                      Time elapsed: 00:11:49
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 703/1500 [0m                      

                       Computation: 105318 steps/s (collection: 0.836s, learning 0.097s)
             Mean action noise std: 2.78
          Mean value_function loss: 100.1841
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5327
                       Mean reward: 389.38
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 78.5473
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.93s
                      Time elapsed: 00:11:50
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 704/1500 [0m                      

                       Computation: 112003 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 94.7538
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.5436
                       Mean reward: 399.48
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 80.9429
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.88s
                      Time elapsed: 00:11:51
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 705/1500 [0m                      

                       Computation: 106149 steps/s (collection: 0.810s, learning 0.116s)
             Mean action noise std: 2.78
          Mean value_function loss: 91.7986
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.5502
                       Mean reward: 357.82
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 76.8507
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.93s
                      Time elapsed: 00:11:52
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 706/1500 [0m                      

                       Computation: 102141 steps/s (collection: 0.813s, learning 0.150s)
             Mean action noise std: 2.78
          Mean value_function loss: 94.8897
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5504
                       Mean reward: 403.00
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 82.3107
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.96s
                      Time elapsed: 00:11:53
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 707/1500 [0m                      

                       Computation: 101674 steps/s (collection: 0.862s, learning 0.105s)
             Mean action noise std: 2.79
          Mean value_function loss: 94.4677
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.5608
                       Mean reward: 449.74
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3292
    Episode_Reward/rotating_object: 86.3673
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.97s
                      Time elapsed: 00:11:54
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 708/1500 [0m                      

                       Computation: 108229 steps/s (collection: 0.777s, learning 0.131s)
             Mean action noise std: 2.79
          Mean value_function loss: 96.7207
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.5680
                       Mean reward: 402.61
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 0.3179
    Episode_Reward/rotating_object: 83.8849
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.91s
                      Time elapsed: 00:11:55
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 709/1500 [0m                      

                       Computation: 106870 steps/s (collection: 0.831s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 109.6660
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.5773
                       Mean reward: 399.11
               Mean episode length: 241.72
    Episode_Reward/reaching_object: 0.3237
    Episode_Reward/rotating_object: 87.4993
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.92s
                      Time elapsed: 00:11:55
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 710/1500 [0m                      

                       Computation: 112260 steps/s (collection: 0.787s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 94.8875
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.5827
                       Mean reward: 350.90
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 78.3046
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.88s
                      Time elapsed: 00:11:56
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 711/1500 [0m                      

                       Computation: 112736 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 95.6455
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.5922
                       Mean reward: 423.77
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 81.8131
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.87s
                      Time elapsed: 00:11:57
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 712/1500 [0m                      

                       Computation: 107021 steps/s (collection: 0.769s, learning 0.150s)
             Mean action noise std: 2.80
          Mean value_function loss: 97.9233
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.6124
                       Mean reward: 435.25
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.3122
    Episode_Reward/rotating_object: 83.5725
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.92s
                      Time elapsed: 00:11:58
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 713/1500 [0m                      

                       Computation: 110647 steps/s (collection: 0.787s, learning 0.102s)
             Mean action noise std: 2.81
          Mean value_function loss: 91.0624
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6182
                       Mean reward: 422.71
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 82.3321
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.89s
                      Time elapsed: 00:11:59
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 714/1500 [0m                      

                       Computation: 114292 steps/s (collection: 0.768s, learning 0.092s)
             Mean action noise std: 2.81
          Mean value_function loss: 103.4102
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.6183
                       Mean reward: 436.72
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 88.3858
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.86s
                      Time elapsed: 00:12:00
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 715/1500 [0m                      

                       Computation: 96492 steps/s (collection: 0.891s, learning 0.128s)
             Mean action noise std: 2.81
          Mean value_function loss: 91.7721
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6223
                       Mean reward: 418.74
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 81.6062
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.02s
                      Time elapsed: 00:12:01
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 716/1500 [0m                      

                       Computation: 103662 steps/s (collection: 0.806s, learning 0.143s)
             Mean action noise std: 2.81
          Mean value_function loss: 89.5865
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.6326
                       Mean reward: 428.80
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 86.7888
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.95s
                      Time elapsed: 00:12:02
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 717/1500 [0m                      

                       Computation: 112379 steps/s (collection: 0.764s, learning 0.111s)
             Mean action noise std: 2.82
          Mean value_function loss: 97.0608
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6386
                       Mean reward: 478.27
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 87.7852
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.87s
                      Time elapsed: 00:12:03
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 718/1500 [0m                      

                       Computation: 112680 steps/s (collection: 0.779s, learning 0.093s)
             Mean action noise std: 2.82
          Mean value_function loss: 95.5155
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.6390
                       Mean reward: 436.21
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 83.4095
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.87s
                      Time elapsed: 00:12:04
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 719/1500 [0m                      

                       Computation: 98281 steps/s (collection: 0.827s, learning 0.173s)
             Mean action noise std: 2.82
          Mean value_function loss: 91.9540
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.6399
                       Mean reward: 391.98
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 84.3818
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.00s
                      Time elapsed: 00:12:05
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 720/1500 [0m                      

                       Computation: 104741 steps/s (collection: 0.826s, learning 0.113s)
             Mean action noise std: 2.82
          Mean value_function loss: 87.8831
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6489
                       Mean reward: 426.95
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 81.9853
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.94s
                      Time elapsed: 00:12:05
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 721/1500 [0m                      

                       Computation: 107075 steps/s (collection: 0.804s, learning 0.114s)
             Mean action noise std: 2.83
          Mean value_function loss: 91.5173
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.6587
                       Mean reward: 434.00
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 89.4023
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.92s
                      Time elapsed: 00:12:06
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 722/1500 [0m                      

                       Computation: 111127 steps/s (collection: 0.767s, learning 0.118s)
             Mean action noise std: 2.83
          Mean value_function loss: 87.8680
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.6704
                       Mean reward: 413.90
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 82.2106
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.88s
                      Time elapsed: 00:12:07
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 723/1500 [0m                      

                       Computation: 107045 steps/s (collection: 0.788s, learning 0.131s)
             Mean action noise std: 2.84
          Mean value_function loss: 89.9214
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6831
                       Mean reward: 419.63
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 0.3224
    Episode_Reward/rotating_object: 81.3935
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.92s
                      Time elapsed: 00:12:08
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 724/1500 [0m                      

                       Computation: 107415 steps/s (collection: 0.786s, learning 0.129s)
             Mean action noise std: 2.84
          Mean value_function loss: 93.0156
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6943
                       Mean reward: 411.67
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 82.6399
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.92s
                      Time elapsed: 00:12:09
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 725/1500 [0m                      

                       Computation: 112641 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 2.84
          Mean value_function loss: 89.3181
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.7007
                       Mean reward: 393.66
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 83.6424
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.87s
                      Time elapsed: 00:12:10
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 726/1500 [0m                      

                       Computation: 114186 steps/s (collection: 0.760s, learning 0.101s)
             Mean action noise std: 2.85
          Mean value_function loss: 97.9493
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7165
                       Mean reward: 428.81
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 0.3250
    Episode_Reward/rotating_object: 86.0408
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.86s
                      Time elapsed: 00:12:11
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 727/1500 [0m                      

                       Computation: 106881 steps/s (collection: 0.813s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 94.7202
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.7282
                       Mean reward: 440.54
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 89.3807
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.92s
                      Time elapsed: 00:12:12
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 728/1500 [0m                      

                       Computation: 117235 steps/s (collection: 0.749s, learning 0.089s)
             Mean action noise std: 2.86
          Mean value_function loss: 102.0082
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7301
                       Mean reward: 430.76
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 85.0156
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.84s
                      Time elapsed: 00:12:13
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 729/1500 [0m                      

                       Computation: 111235 steps/s (collection: 0.797s, learning 0.087s)
             Mean action noise std: 2.86
          Mean value_function loss: 88.7438
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.7372
                       Mean reward: 393.51
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 83.9529
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.88s
                      Time elapsed: 00:12:14
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 730/1500 [0m                      

                       Computation: 108714 steps/s (collection: 0.792s, learning 0.112s)
             Mean action noise std: 2.87
          Mean value_function loss: 93.3692
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.7605
                       Mean reward: 465.43
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 85.4364
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.90s
                      Time elapsed: 00:12:14
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 731/1500 [0m                      

                       Computation: 105848 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 2.87
          Mean value_function loss: 83.3499
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.7812
                       Mean reward: 494.38
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 88.6013
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.93s
                      Time elapsed: 00:12:15
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 732/1500 [0m                      

                       Computation: 102594 steps/s (collection: 0.827s, learning 0.131s)
             Mean action noise std: 2.87
          Mean value_function loss: 84.5315
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.7887
                       Mean reward: 445.41
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 87.3219
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.96s
                      Time elapsed: 00:12:16
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 733/1500 [0m                      

                       Computation: 103890 steps/s (collection: 0.783s, learning 0.163s)
             Mean action noise std: 2.88
          Mean value_function loss: 83.8270
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.8073
                       Mean reward: 462.59
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 87.7083
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.95s
                      Time elapsed: 00:12:17
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 734/1500 [0m                      

                       Computation: 106424 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 2.89
          Mean value_function loss: 83.3478
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.8295
                       Mean reward: 402.45
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 84.8833
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.92s
                      Time elapsed: 00:12:18
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 735/1500 [0m                      

                       Computation: 106061 steps/s (collection: 0.775s, learning 0.152s)
             Mean action noise std: 2.89
          Mean value_function loss: 82.3119
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.8412
                       Mean reward: 488.80
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 95.9032
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.93s
                      Time elapsed: 00:12:19
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 736/1500 [0m                      

                       Computation: 108150 steps/s (collection: 0.763s, learning 0.146s)
             Mean action noise std: 2.89
          Mean value_function loss: 84.2029
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 18.8449
                       Mean reward: 428.57
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 82.7897
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.91s
                      Time elapsed: 00:12:20
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 737/1500 [0m                      

                       Computation: 111224 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 2.89
          Mean value_function loss: 76.3759
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.8443
                       Mean reward: 397.96
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.3227
    Episode_Reward/rotating_object: 87.6405
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.88s
                      Time elapsed: 00:12:21
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 738/1500 [0m                      

                       Computation: 108113 steps/s (collection: 0.812s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 80.4995
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.8515
                       Mean reward: 415.38
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 82.4512
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.91s
                      Time elapsed: 00:12:22
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 739/1500 [0m                      

                       Computation: 105595 steps/s (collection: 0.794s, learning 0.137s)
             Mean action noise std: 2.90
          Mean value_function loss: 90.5745
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.8676
                       Mean reward: 481.76
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 90.2893
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.93s
                      Time elapsed: 00:12:23
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 740/1500 [0m                      

                       Computation: 111085 steps/s (collection: 0.786s, learning 0.099s)
             Mean action noise std: 2.91
          Mean value_function loss: 88.7269
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.8831
                       Mean reward: 433.70
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 89.0536
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.88s
                      Time elapsed: 00:12:24
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 741/1500 [0m                      

                       Computation: 113909 steps/s (collection: 0.756s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 82.2910
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 18.8928
                       Mean reward: 408.43
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 85.8534
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.86s
                      Time elapsed: 00:12:24
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 742/1500 [0m                      

                       Computation: 116153 steps/s (collection: 0.758s, learning 0.089s)
             Mean action noise std: 2.92
          Mean value_function loss: 76.8465
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.8975
                       Mean reward: 423.32
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 86.2510
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.85s
                      Time elapsed: 00:12:25
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 743/1500 [0m                      

                       Computation: 114391 steps/s (collection: 0.747s, learning 0.112s)
             Mean action noise std: 2.92
          Mean value_function loss: 87.6496
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.9021
                       Mean reward: 426.46
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 84.8697
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.86s
                      Time elapsed: 00:12:26
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 744/1500 [0m                      

                       Computation: 116075 steps/s (collection: 0.742s, learning 0.105s)
             Mean action noise std: 2.92
          Mean value_function loss: 86.7327
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9165
                       Mean reward: 351.14
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 82.4310
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.85s
                      Time elapsed: 00:12:27
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 745/1500 [0m                      

                       Computation: 104080 steps/s (collection: 0.816s, learning 0.128s)
             Mean action noise std: 2.93
          Mean value_function loss: 93.1888
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.9401
                       Mean reward: 417.22
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 83.3057
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.94s
                      Time elapsed: 00:12:28
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 746/1500 [0m                      

                       Computation: 105200 steps/s (collection: 0.841s, learning 0.093s)
             Mean action noise std: 2.93
          Mean value_function loss: 89.1840
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.9427
                       Mean reward: 441.32
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.3064
    Episode_Reward/rotating_object: 83.2121
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.93s
                      Time elapsed: 00:12:29
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 747/1500 [0m                      

                       Computation: 100853 steps/s (collection: 0.881s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 98.9847
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.9521
                       Mean reward: 405.57
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.3240
    Episode_Reward/rotating_object: 81.7882
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.97s
                      Time elapsed: 00:12:30
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 748/1500 [0m                      

                       Computation: 101234 steps/s (collection: 0.877s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 99.5118
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.9630
                       Mean reward: 425.55
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 85.1702
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.97s
                      Time elapsed: 00:12:31
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 749/1500 [0m                      

                       Computation: 116642 steps/s (collection: 0.749s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 98.5138
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 18.9757
                       Mean reward: 415.03
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 85.2921
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.84s
                      Time elapsed: 00:12:32
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 750/1500 [0m                      

                       Computation: 110134 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 2.94
          Mean value_function loss: 98.7023
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 18.9799
                       Mean reward: 356.56
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 83.3743
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.89s
                      Time elapsed: 00:12:33
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 751/1500 [0m                      

                       Computation: 95875 steps/s (collection: 0.864s, learning 0.161s)
             Mean action noise std: 2.95
          Mean value_function loss: 96.0685
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.9847
                       Mean reward: 405.18
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 83.8464
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 1.03s
                      Time elapsed: 00:12:34
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 752/1500 [0m                      

                       Computation: 102577 steps/s (collection: 0.870s, learning 0.088s)
             Mean action noise std: 2.95
          Mean value_function loss: 96.9027
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.9961
                       Mean reward: 428.75
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 85.4155
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.96s
                      Time elapsed: 00:12:35
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 753/1500 [0m                      

                       Computation: 103488 steps/s (collection: 0.852s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 99.1163
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.0121
                       Mean reward: 436.75
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 87.5625
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.95s
                      Time elapsed: 00:12:36
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 754/1500 [0m                      

                       Computation: 109548 steps/s (collection: 0.793s, learning 0.104s)
             Mean action noise std: 2.96
          Mean value_function loss: 89.3882
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.0226
                       Mean reward: 410.56
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 85.7681
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.90s
                      Time elapsed: 00:12:36
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 755/1500 [0m                      

                       Computation: 109515 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 2.96
          Mean value_function loss: 82.1273
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.0287
                       Mean reward: 401.43
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 87.5117
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.90s
                      Time elapsed: 00:12:37
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 756/1500 [0m                      

                       Computation: 109375 steps/s (collection: 0.777s, learning 0.122s)
             Mean action noise std: 2.97
          Mean value_function loss: 93.1196
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.0374
                       Mean reward: 417.85
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 85.1841
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.90s
                      Time elapsed: 00:12:38
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 757/1500 [0m                      

                       Computation: 109796 steps/s (collection: 0.731s, learning 0.165s)
             Mean action noise std: 2.97
          Mean value_function loss: 92.7888
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.0565
                       Mean reward: 424.45
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 85.2196
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.90s
                      Time elapsed: 00:12:39
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 758/1500 [0m                      

                       Computation: 109878 steps/s (collection: 0.808s, learning 0.087s)
             Mean action noise std: 2.97
          Mean value_function loss: 90.3318
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.0678
                       Mean reward: 452.93
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 87.4421
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.89s
                      Time elapsed: 00:12:40
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 759/1500 [0m                      

                       Computation: 116830 steps/s (collection: 0.729s, learning 0.113s)
             Mean action noise std: 2.98
          Mean value_function loss: 98.1367
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.0771
                       Mean reward: 433.56
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 86.4928
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.84s
                      Time elapsed: 00:12:41
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 760/1500 [0m                      

                       Computation: 105914 steps/s (collection: 0.832s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 106.3729
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.0867
                       Mean reward: 453.00
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 89.9942
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.93s
                      Time elapsed: 00:12:42
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 761/1500 [0m                      

                       Computation: 102214 steps/s (collection: 0.863s, learning 0.099s)
             Mean action noise std: 2.99
          Mean value_function loss: 101.2220
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.0979
                       Mean reward: 406.08
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 81.8491
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.96s
                      Time elapsed: 00:12:43
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 762/1500 [0m                      

                       Computation: 104806 steps/s (collection: 0.811s, learning 0.127s)
             Mean action noise std: 2.99
          Mean value_function loss: 96.1627
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.1094
                       Mean reward: 419.83
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 87.3258
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.94s
                      Time elapsed: 00:12:44
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 763/1500 [0m                      

                       Computation: 101024 steps/s (collection: 0.789s, learning 0.184s)
             Mean action noise std: 3.00
          Mean value_function loss: 81.5514
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 19.1204
                       Mean reward: 421.37
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 84.2788
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.97s
                      Time elapsed: 00:12:45
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 764/1500 [0m                      

                       Computation: 90736 steps/s (collection: 0.991s, learning 0.093s)
             Mean action noise std: 3.00
          Mean value_function loss: 91.8458
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.1202
                       Mean reward: 459.49
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 85.4982
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 1.08s
                      Time elapsed: 00:12:46
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 765/1500 [0m                      

                       Computation: 113714 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 3.00
          Mean value_function loss: 93.6334
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.1129
                       Mean reward: 392.07
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 81.7202
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.86s
                      Time elapsed: 00:12:47
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 766/1500 [0m                      

                       Computation: 111278 steps/s (collection: 0.786s, learning 0.097s)
             Mean action noise std: 3.00
          Mean value_function loss: 96.8359
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.1167
                       Mean reward: 411.38
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 85.2514
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.88s
                      Time elapsed: 00:12:47
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 767/1500 [0m                      

                       Computation: 109905 steps/s (collection: 0.789s, learning 0.105s)
             Mean action noise std: 3.00
          Mean value_function loss: 94.4846
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.1177
                       Mean reward: 418.47
               Mean episode length: 246.44
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 81.4429
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 0.89s
                      Time elapsed: 00:12:48
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 768/1500 [0m                      

                       Computation: 113318 steps/s (collection: 0.754s, learning 0.114s)
             Mean action noise std: 3.01
          Mean value_function loss: 84.9011
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.1206
                       Mean reward: 450.51
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 82.7893
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.87s
                      Time elapsed: 00:12:49
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 769/1500 [0m                      

                       Computation: 113683 steps/s (collection: 0.761s, learning 0.104s)
             Mean action noise std: 3.01
          Mean value_function loss: 87.2462
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.1328
                       Mean reward: 450.35
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 88.1215
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.86s
                      Time elapsed: 00:12:50
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 770/1500 [0m                      

                       Computation: 112111 steps/s (collection: 0.765s, learning 0.112s)
             Mean action noise std: 3.01
          Mean value_function loss: 86.5494
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.1397
                       Mean reward: 464.28
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 89.2087
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.88s
                      Time elapsed: 00:12:51
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 771/1500 [0m                      

                       Computation: 115292 steps/s (collection: 0.746s, learning 0.107s)
             Mean action noise std: 3.01
          Mean value_function loss: 99.0180
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.1408
                       Mean reward: 432.43
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 87.3045
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.85s
                      Time elapsed: 00:12:52
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 772/1500 [0m                      

                       Computation: 112583 steps/s (collection: 0.762s, learning 0.111s)
             Mean action noise std: 3.02
          Mean value_function loss: 99.0458
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.1471
                       Mean reward: 472.01
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3184
    Episode_Reward/rotating_object: 87.9228
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.87s
                      Time elapsed: 00:12:53
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 773/1500 [0m                      

                       Computation: 114428 steps/s (collection: 0.748s, learning 0.111s)
             Mean action noise std: 3.02
          Mean value_function loss: 89.8616
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.1536
                       Mean reward: 429.25
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 89.0566
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.86s
                      Time elapsed: 00:12:54
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 774/1500 [0m                      

                       Computation: 112529 steps/s (collection: 0.770s, learning 0.104s)
             Mean action noise std: 3.03
          Mean value_function loss: 99.5184
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.1611
                       Mean reward: 447.03
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 84.8587
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.87s
                      Time elapsed: 00:12:54
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 775/1500 [0m                      

                       Computation: 113426 steps/s (collection: 0.764s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 85.3197
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.1738
                       Mean reward: 493.10
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 90.8751
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.87s
                      Time elapsed: 00:12:55
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 776/1500 [0m                      

                       Computation: 113501 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 3.04
          Mean value_function loss: 99.8207
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1846
                       Mean reward: 408.38
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.3276
    Episode_Reward/rotating_object: 85.6730
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.87s
                      Time elapsed: 00:12:56
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 777/1500 [0m                      

                       Computation: 115191 steps/s (collection: 0.763s, learning 0.091s)
             Mean action noise std: 3.04
          Mean value_function loss: 105.9475
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.1934
                       Mean reward: 429.96
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.3322
    Episode_Reward/rotating_object: 89.9514
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.85s
                      Time elapsed: 00:12:57
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 778/1500 [0m                      

                       Computation: 111548 steps/s (collection: 0.764s, learning 0.117s)
             Mean action noise std: 3.04
          Mean value_function loss: 89.2483
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 19.2018
                       Mean reward: 443.51
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3270
    Episode_Reward/rotating_object: 87.7069
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.88s
                      Time elapsed: 00:12:58
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 779/1500 [0m                      

                       Computation: 112310 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 3.04
          Mean value_function loss: 85.5496
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2019
                       Mean reward: 437.74
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 89.0759
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.88s
                      Time elapsed: 00:12:59
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 780/1500 [0m                      

                       Computation: 112230 steps/s (collection: 0.780s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 109.5793
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.2056
                       Mean reward: 391.61
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 84.5162
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.88s
                      Time elapsed: 00:13:00
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 781/1500 [0m                      

                       Computation: 117739 steps/s (collection: 0.747s, learning 0.088s)
             Mean action noise std: 3.05
          Mean value_function loss: 105.4052
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.2149
                       Mean reward: 426.53
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 85.6227
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.83s
                      Time elapsed: 00:13:00
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 782/1500 [0m                      

                       Computation: 117405 steps/s (collection: 0.745s, learning 0.093s)
             Mean action noise std: 3.06
          Mean value_function loss: 91.9732
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.2336
                       Mean reward: 436.69
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 0.3262
    Episode_Reward/rotating_object: 86.8885
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.84s
                      Time elapsed: 00:13:01
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 783/1500 [0m                      

                       Computation: 114549 steps/s (collection: 0.764s, learning 0.094s)
             Mean action noise std: 3.06
          Mean value_function loss: 95.5918
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.2368
                       Mean reward: 454.93
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 85.8619
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.86s
                      Time elapsed: 00:13:02
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 784/1500 [0m                      

                       Computation: 114596 steps/s (collection: 0.757s, learning 0.101s)
             Mean action noise std: 3.06
          Mean value_function loss: 97.6557
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.2461
                       Mean reward: 424.26
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.3204
    Episode_Reward/rotating_object: 87.8124
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.86s
                      Time elapsed: 00:13:03
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 785/1500 [0m                      

                       Computation: 113729 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 3.07
          Mean value_function loss: 99.1754
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.2586
                       Mean reward: 420.67
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.3305
    Episode_Reward/rotating_object: 91.8880
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.86s
                      Time elapsed: 00:13:04
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 786/1500 [0m                      

                       Computation: 117353 steps/s (collection: 0.745s, learning 0.093s)
             Mean action noise std: 3.07
          Mean value_function loss: 106.4343
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 19.2732
                       Mean reward: 451.11
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.3281
    Episode_Reward/rotating_object: 87.5631
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.84s
                      Time elapsed: 00:13:05
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 787/1500 [0m                      

                       Computation: 115762 steps/s (collection: 0.756s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 93.5849
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.2770
                       Mean reward: 440.66
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 89.7457
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.85s
                      Time elapsed: 00:13:06
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 788/1500 [0m                      

                       Computation: 115716 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 3.08
          Mean value_function loss: 97.7295
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.2879
                       Mean reward: 414.85
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 85.0349
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.85s
                      Time elapsed: 00:13:06
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 789/1500 [0m                      

                       Computation: 116729 steps/s (collection: 0.754s, learning 0.089s)
             Mean action noise std: 3.08
          Mean value_function loss: 96.2344
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 19.2956
                       Mean reward: 442.38
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 83.8286
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.84s
                      Time elapsed: 00:13:07
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 790/1500 [0m                      

                       Computation: 117535 steps/s (collection: 0.747s, learning 0.090s)
             Mean action noise std: 3.09
          Mean value_function loss: 91.7669
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.2977
                       Mean reward: 433.75
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 0.3163
    Episode_Reward/rotating_object: 87.8972
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.84s
                      Time elapsed: 00:13:08
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 791/1500 [0m                      

                       Computation: 112542 steps/s (collection: 0.757s, learning 0.116s)
             Mean action noise std: 3.09
          Mean value_function loss: 107.1157
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.3047
                       Mean reward: 442.47
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 86.2828
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.87s
                      Time elapsed: 00:13:09
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 792/1500 [0m                      

                       Computation: 112729 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 3.09
          Mean value_function loss: 97.5894
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.3134
                       Mean reward: 429.24
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 85.7691
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.87s
                      Time elapsed: 00:13:10
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 793/1500 [0m                      

                       Computation: 111457 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 3.10
          Mean value_function loss: 102.4806
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.3163
                       Mean reward: 436.79
               Mean episode length: 243.67
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 89.3173
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.88s
                      Time elapsed: 00:13:11
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 794/1500 [0m                      

                       Computation: 114402 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 3.09
          Mean value_function loss: 105.3780
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.3045
                       Mean reward: 493.96
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 90.4101
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.86s
                      Time elapsed: 00:13:12
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 795/1500 [0m                      

                       Computation: 109677 steps/s (collection: 0.794s, learning 0.102s)
             Mean action noise std: 3.10
          Mean value_function loss: 107.4608
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.3006
                       Mean reward: 456.42
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 88.5668
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.90s
                      Time elapsed: 00:13:13
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 796/1500 [0m                      

                       Computation: 115547 steps/s (collection: 0.756s, learning 0.095s)
             Mean action noise std: 3.10
          Mean value_function loss: 103.7547
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 19.3078
                       Mean reward: 401.37
               Mean episode length: 239.77
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 85.3404
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.85s
                      Time elapsed: 00:13:13
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 797/1500 [0m                      

                       Computation: 111066 steps/s (collection: 0.775s, learning 0.111s)
             Mean action noise std: 3.10
          Mean value_function loss: 107.1102
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.3154
                       Mean reward: 453.44
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 84.3142
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.89s
                      Time elapsed: 00:13:14
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 798/1500 [0m                      

                       Computation: 105833 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 100.3233
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.3238
                       Mean reward: 463.12
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 86.4606
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.93s
                      Time elapsed: 00:13:15
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 799/1500 [0m                      

                       Computation: 112881 steps/s (collection: 0.760s, learning 0.110s)
             Mean action noise std: 3.11
          Mean value_function loss: 101.3872
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.3320
                       Mean reward: 453.64
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 86.5048
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.87s
                      Time elapsed: 00:13:16
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 800/1500 [0m                      

                       Computation: 111760 steps/s (collection: 0.780s, learning 0.100s)
             Mean action noise std: 3.11
          Mean value_function loss: 99.1039
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.3342
                       Mean reward: 409.26
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 0.3282
    Episode_Reward/rotating_object: 90.1548
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.88s
                      Time elapsed: 00:13:17
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 801/1500 [0m                      

                       Computation: 115133 steps/s (collection: 0.763s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 94.3642
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.3349
                       Mean reward: 414.60
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.3220
    Episode_Reward/rotating_object: 85.6857
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.85s
                      Time elapsed: 00:13:18
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 802/1500 [0m                      

                       Computation: 108747 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 3.12
          Mean value_function loss: 93.6206
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.3481
                       Mean reward: 472.11
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 88.0288
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.90s
                      Time elapsed: 00:13:19
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 803/1500 [0m                      

                       Computation: 107825 steps/s (collection: 0.808s, learning 0.104s)
             Mean action noise std: 3.12
          Mean value_function loss: 101.4945
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.3577
                       Mean reward: 430.45
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 88.3021
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.91s
                      Time elapsed: 00:13:20
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 804/1500 [0m                      

                       Computation: 105543 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 96.8813
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.3693
                       Mean reward: 467.62
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 90.1020
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.93s
                      Time elapsed: 00:13:21
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 805/1500 [0m                      

                       Computation: 109821 steps/s (collection: 0.806s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 97.2607
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.3800
                       Mean reward: 441.67
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 88.3986
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.90s
                      Time elapsed: 00:13:21
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 806/1500 [0m                      

                       Computation: 107103 steps/s (collection: 0.809s, learning 0.109s)
             Mean action noise std: 3.14
          Mean value_function loss: 94.1580
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.3917
                       Mean reward: 456.45
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.3236
    Episode_Reward/rotating_object: 87.0009
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.92s
                      Time elapsed: 00:13:22
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 807/1500 [0m                      

                       Computation: 108792 steps/s (collection: 0.799s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 97.4265
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.3992
                       Mean reward: 393.37
               Mean episode length: 241.68
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 83.0307
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.90s
                      Time elapsed: 00:13:23
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 808/1500 [0m                      

                       Computation: 93824 steps/s (collection: 0.922s, learning 0.126s)
             Mean action noise std: 3.15
          Mean value_function loss: 102.0785
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.4058
                       Mean reward: 469.06
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.3289
    Episode_Reward/rotating_object: 89.9747
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 1.05s
                      Time elapsed: 00:13:24
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 809/1500 [0m                      

                       Computation: 76855 steps/s (collection: 0.974s, learning 0.306s)
             Mean action noise std: 3.15
          Mean value_function loss: 92.4109
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.4188
                       Mean reward: 441.52
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 88.3559
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 1.28s
                      Time elapsed: 00:13:26
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 810/1500 [0m                      

                       Computation: 84856 steps/s (collection: 0.963s, learning 0.196s)
             Mean action noise std: 3.15
          Mean value_function loss: 100.9644
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.4272
                       Mean reward: 433.35
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.3280
    Episode_Reward/rotating_object: 92.9222
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.16s
                      Time elapsed: 00:13:27
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 811/1500 [0m                      

                       Computation: 87526 steps/s (collection: 0.933s, learning 0.190s)
             Mean action noise std: 3.16
          Mean value_function loss: 95.6369
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.4440
                       Mean reward: 449.76
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 85.9322
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 1.12s
                      Time elapsed: 00:13:28
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 812/1500 [0m                      

                       Computation: 66995 steps/s (collection: 1.132s, learning 0.336s)
             Mean action noise std: 3.17
          Mean value_function loss: 99.9897
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.4550
                       Mean reward: 400.44
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 0.3213
    Episode_Reward/rotating_object: 87.0154
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.47s
                      Time elapsed: 00:13:29
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 813/1500 [0m                      

                       Computation: 94114 steps/s (collection: 0.901s, learning 0.143s)
             Mean action noise std: 3.17
          Mean value_function loss: 98.7163
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.4669
                       Mean reward: 451.46
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 0.3282
    Episode_Reward/rotating_object: 87.6997
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.04s
                      Time elapsed: 00:13:30
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 814/1500 [0m                      

                       Computation: 87705 steps/s (collection: 0.950s, learning 0.171s)
             Mean action noise std: 3.17
          Mean value_function loss: 93.0569
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.4773
                       Mean reward: 416.58
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 85.8530
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 1.12s
                      Time elapsed: 00:13:31
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 815/1500 [0m                      

                       Computation: 85663 steps/s (collection: 0.992s, learning 0.156s)
             Mean action noise std: 3.18
          Mean value_function loss: 102.5554
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.4888
                       Mean reward: 445.76
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 85.8210
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 1.15s
                      Time elapsed: 00:13:33
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 816/1500 [0m                      

                       Computation: 94728 steps/s (collection: 0.931s, learning 0.107s)
             Mean action noise std: 3.18
          Mean value_function loss: 98.2306
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.5004
                       Mean reward: 477.50
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 92.3084
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 1.04s
                      Time elapsed: 00:13:34
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 817/1500 [0m                      

                       Computation: 108067 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 3.18
          Mean value_function loss: 97.6357
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.5025
                       Mean reward: 417.91
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 0.3252
    Episode_Reward/rotating_object: 87.3067
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.91s
                      Time elapsed: 00:13:35
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 818/1500 [0m                      

                       Computation: 103692 steps/s (collection: 0.807s, learning 0.141s)
             Mean action noise std: 3.19
          Mean value_function loss: 98.7918
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.5079
                       Mean reward: 429.13
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 87.0495
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.95s
                      Time elapsed: 00:13:36
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 819/1500 [0m                      

                       Computation: 100171 steps/s (collection: 0.862s, learning 0.119s)
             Mean action noise std: 3.19
          Mean value_function loss: 95.7058
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5272
                       Mean reward: 429.45
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 88.7131
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.98s
                      Time elapsed: 00:13:37
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 820/1500 [0m                      

                       Computation: 95795 steps/s (collection: 0.842s, learning 0.184s)
             Mean action noise std: 3.20
          Mean value_function loss: 94.3179
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.5398
                       Mean reward: 477.40
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 90.2835
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.03s
                      Time elapsed: 00:13:38
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 821/1500 [0m                      

                       Computation: 102437 steps/s (collection: 0.841s, learning 0.119s)
             Mean action noise std: 3.21
          Mean value_function loss: 100.2361
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.5498
                       Mean reward: 466.61
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 85.0131
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.96s
                      Time elapsed: 00:13:38
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 822/1500 [0m                      

                       Computation: 106104 steps/s (collection: 0.791s, learning 0.136s)
             Mean action noise std: 3.21
          Mean value_function loss: 99.4248
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.5594
                       Mean reward: 448.23
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 92.1215
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.93s
                      Time elapsed: 00:13:39
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 823/1500 [0m                      

                       Computation: 101334 steps/s (collection: 0.813s, learning 0.158s)
             Mean action noise std: 3.21
          Mean value_function loss: 98.4347
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.5628
                       Mean reward: 425.57
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 84.4100
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.97s
                      Time elapsed: 00:13:40
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 824/1500 [0m                      

                       Computation: 82763 steps/s (collection: 1.028s, learning 0.159s)
             Mean action noise std: 3.22
          Mean value_function loss: 97.7638
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.5611
                       Mean reward: 447.37
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 90.5799
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 1.19s
                      Time elapsed: 00:13:42
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 825/1500 [0m                      

                       Computation: 86008 steps/s (collection: 0.950s, learning 0.193s)
             Mean action noise std: 3.22
          Mean value_function loss: 90.3567
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.5691
                       Mean reward: 460.08
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 87.5012
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 1.14s
                      Time elapsed: 00:13:43
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 826/1500 [0m                      

                       Computation: 80569 steps/s (collection: 1.043s, learning 0.177s)
             Mean action noise std: 3.23
          Mean value_function loss: 98.6568
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.5871
                       Mean reward: 452.41
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 86.4343
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.22s
                      Time elapsed: 00:13:44
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 827/1500 [0m                      

                       Computation: 87613 steps/s (collection: 0.953s, learning 0.169s)
             Mean action noise std: 3.23
          Mean value_function loss: 94.9967
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6054
                       Mean reward: 420.23
               Mean episode length: 236.07
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 85.4814
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.12s
                      Time elapsed: 00:13:45
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 828/1500 [0m                      

                       Computation: 101823 steps/s (collection: 0.847s, learning 0.119s)
             Mean action noise std: 3.24
          Mean value_function loss: 99.4560
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.6102
                       Mean reward: 477.15
               Mean episode length: 241.05
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 89.4084
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.97s
                      Time elapsed: 00:13:46
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 829/1500 [0m                      

                       Computation: 101557 steps/s (collection: 0.816s, learning 0.152s)
             Mean action noise std: 3.24
          Mean value_function loss: 98.9511
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.6167
                       Mean reward: 442.63
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 89.3396
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.97s
                      Time elapsed: 00:13:47
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 830/1500 [0m                      

                       Computation: 113431 steps/s (collection: 0.778s, learning 0.089s)
             Mean action noise std: 3.25
          Mean value_function loss: 97.5778
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.6332
                       Mean reward: 443.89
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.3254
    Episode_Reward/rotating_object: 88.3725
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.87s
                      Time elapsed: 00:13:48
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 831/1500 [0m                      

                       Computation: 115353 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 3.26
          Mean value_function loss: 109.4216
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.6515
                       Mean reward: 480.73
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 89.7578
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.85s
                      Time elapsed: 00:13:49
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 832/1500 [0m                      

                       Computation: 115592 steps/s (collection: 0.757s, learning 0.093s)
             Mean action noise std: 3.26
          Mean value_function loss: 99.0012
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.6648
                       Mean reward: 425.21
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 88.0161
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.85s
                      Time elapsed: 00:13:50
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 833/1500 [0m                      

                       Computation: 117398 steps/s (collection: 0.747s, learning 0.091s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.2980
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6711
                       Mean reward: 460.80
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.3286
    Episode_Reward/rotating_object: 93.2377
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.84s
                      Time elapsed: 00:13:50
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 834/1500 [0m                      

                       Computation: 111547 steps/s (collection: 0.774s, learning 0.107s)
             Mean action noise std: 3.27
          Mean value_function loss: 97.4918
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 19.6748
                       Mean reward: 424.01
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 89.2325
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.88s
                      Time elapsed: 00:13:51
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 835/1500 [0m                      

                       Computation: 112625 steps/s (collection: 0.779s, learning 0.094s)
             Mean action noise std: 3.27
          Mean value_function loss: 99.9293
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.6803
                       Mean reward: 434.18
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 86.9038
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.87s
                      Time elapsed: 00:13:52
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 836/1500 [0m                      

                       Computation: 113397 steps/s (collection: 0.759s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 87.2405
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.6842
                       Mean reward: 426.34
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 0.3265
    Episode_Reward/rotating_object: 89.4775
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.87s
                      Time elapsed: 00:13:53
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 837/1500 [0m                      

                       Computation: 113009 steps/s (collection: 0.766s, learning 0.104s)
             Mean action noise std: 3.28
          Mean value_function loss: 88.2631
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.6839
                       Mean reward: 458.15
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 87.6358
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.87s
                      Time elapsed: 00:13:54
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 838/1500 [0m                      

                       Computation: 116693 steps/s (collection: 0.756s, learning 0.086s)
             Mean action noise std: 3.28
          Mean value_function loss: 92.7557
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.6886
                       Mean reward: 412.92
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 87.0565
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.84s
                      Time elapsed: 00:13:55
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 839/1500 [0m                      

                       Computation: 104209 steps/s (collection: 0.827s, learning 0.116s)
             Mean action noise std: 3.28
          Mean value_function loss: 89.5536
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.6928
                       Mean reward: 445.43
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 83.7765
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.94s
                      Time elapsed: 00:13:56
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 840/1500 [0m                      

                       Computation: 103295 steps/s (collection: 0.843s, learning 0.109s)
             Mean action noise std: 3.28
          Mean value_function loss: 84.7847
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.6885
                       Mean reward: 440.55
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 87.1501
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.95s
                      Time elapsed: 00:13:57
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 841/1500 [0m                      

                       Computation: 109803 steps/s (collection: 0.783s, learning 0.112s)
             Mean action noise std: 3.28
          Mean value_function loss: 98.4489
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.6876
                       Mean reward: 461.62
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 93.1781
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.90s
                      Time elapsed: 00:13:58
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 842/1500 [0m                      

                       Computation: 93764 steps/s (collection: 0.939s, learning 0.110s)
             Mean action noise std: 3.28
          Mean value_function loss: 107.2800
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.6931
                       Mean reward: 436.22
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 87.9580
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 1.05s
                      Time elapsed: 00:13:59
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 843/1500 [0m                      

                       Computation: 101786 steps/s (collection: 0.848s, learning 0.118s)
             Mean action noise std: 3.29
          Mean value_function loss: 95.6431
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.7097
                       Mean reward: 448.83
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.3134
    Episode_Reward/rotating_object: 89.8833
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.97s
                      Time elapsed: 00:14:00
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 844/1500 [0m                      

                       Computation: 98385 steps/s (collection: 0.886s, learning 0.113s)
             Mean action noise std: 3.29
          Mean value_function loss: 91.4495
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 19.7232
                       Mean reward: 387.71
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 88.1699
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 1.00s
                      Time elapsed: 00:14:01
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 845/1500 [0m                      

                       Computation: 110858 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 91.9685
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7326
                       Mean reward: 466.60
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 91.5031
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.89s
                      Time elapsed: 00:14:01
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 846/1500 [0m                      

                       Computation: 110435 steps/s (collection: 0.782s, learning 0.108s)
             Mean action noise std: 3.30
          Mean value_function loss: 88.5517
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.7411
                       Mean reward: 491.44
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 92.5991
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.89s
                      Time elapsed: 00:14:02
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 847/1500 [0m                      

                       Computation: 112022 steps/s (collection: 0.776s, learning 0.102s)
             Mean action noise std: 3.31
          Mean value_function loss: 84.1340
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.7495
                       Mean reward: 444.33
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 89.6145
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.88s
                      Time elapsed: 00:14:03
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 848/1500 [0m                      

                       Computation: 107188 steps/s (collection: 0.786s, learning 0.131s)
             Mean action noise std: 3.31
          Mean value_function loss: 81.9156
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.7638
                       Mean reward: 470.53
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 93.4275
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.92s
                      Time elapsed: 00:14:04
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 849/1500 [0m                      

                       Computation: 106075 steps/s (collection: 0.830s, learning 0.097s)
             Mean action noise std: 3.32
          Mean value_function loss: 94.1425
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.7782
                       Mean reward: 474.96
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 90.9330
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.93s
                      Time elapsed: 00:14:05
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 850/1500 [0m                      

                       Computation: 107927 steps/s (collection: 0.805s, learning 0.106s)
             Mean action noise std: 3.32
          Mean value_function loss: 80.7584
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.7827
                       Mean reward: 444.59
               Mean episode length: 241.79
    Episode_Reward/reaching_object: 0.3178
    Episode_Reward/rotating_object: 88.0053
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.91s
                      Time elapsed: 00:14:06
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 851/1500 [0m                      

                       Computation: 106298 steps/s (collection: 0.829s, learning 0.096s)
             Mean action noise std: 3.32
          Mean value_function loss: 91.4410
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.7864
                       Mean reward: 430.43
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 88.1427
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.92s
                      Time elapsed: 00:14:07
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 852/1500 [0m                      

                       Computation: 110935 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 3.32
          Mean value_function loss: 91.4095
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 19.7909
                       Mean reward: 464.57
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 89.9607
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.89s
                      Time elapsed: 00:14:08
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 853/1500 [0m                      

                       Computation: 112345 steps/s (collection: 0.777s, learning 0.098s)
             Mean action noise std: 3.33
          Mean value_function loss: 103.7388
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.7980
                       Mean reward: 453.05
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 89.5103
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.88s
                      Time elapsed: 00:14:09
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 854/1500 [0m                      

                       Computation: 114683 steps/s (collection: 0.765s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 92.2822
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.8018
                       Mean reward: 453.77
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 90.3780
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.86s
                      Time elapsed: 00:14:09
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 855/1500 [0m                      

                       Computation: 111922 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 93.5710
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.8023
                       Mean reward: 425.38
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 89.6684
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.88s
                      Time elapsed: 00:14:10
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 856/1500 [0m                      

                       Computation: 110625 steps/s (collection: 0.800s, learning 0.089s)
             Mean action noise std: 3.33
          Mean value_function loss: 95.8709
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.8045
                       Mean reward: 413.04
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.3179
    Episode_Reward/rotating_object: 89.9740
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.89s
                      Time elapsed: 00:14:11
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 857/1500 [0m                      

                       Computation: 110006 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 90.4972
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.8057
                       Mean reward: 470.70
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 91.3127
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.89s
                      Time elapsed: 00:14:12
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 858/1500 [0m                      

                       Computation: 112688 steps/s (collection: 0.771s, learning 0.101s)
             Mean action noise std: 3.34
          Mean value_function loss: 80.3780
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8117
                       Mean reward: 476.38
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 88.4759
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.87s
                      Time elapsed: 00:14:13
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 859/1500 [0m                      

                       Computation: 114941 steps/s (collection: 0.762s, learning 0.094s)
             Mean action noise std: 3.34
          Mean value_function loss: 88.6501
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.8158
                       Mean reward: 456.87
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.3203
    Episode_Reward/rotating_object: 89.7896
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.86s
                      Time elapsed: 00:14:14
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 860/1500 [0m                      

                       Computation: 114882 steps/s (collection: 0.766s, learning 0.090s)
             Mean action noise std: 3.35
          Mean value_function loss: 94.5890
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.8217
                       Mean reward: 436.11
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 89.6518
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.86s
                      Time elapsed: 00:14:15
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 861/1500 [0m                      

                       Computation: 105081 steps/s (collection: 0.837s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 83.8473
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 19.8226
                       Mean reward: 436.51
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 85.8187
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.94s
                      Time elapsed: 00:14:16
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 862/1500 [0m                      

                       Computation: 108722 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 3.35
          Mean value_function loss: 90.9030
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.8266
                       Mean reward: 432.38
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 90.5378
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.90s
                      Time elapsed: 00:14:17
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 863/1500 [0m                      

                       Computation: 111765 steps/s (collection: 0.782s, learning 0.098s)
             Mean action noise std: 3.36
          Mean value_function loss: 87.1396
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.8270
                       Mean reward: 478.61
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 90.8375
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.88s
                      Time elapsed: 00:14:17
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 864/1500 [0m                      

                       Computation: 116901 steps/s (collection: 0.749s, learning 0.092s)
             Mean action noise std: 3.36
          Mean value_function loss: 95.5952
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.8372
                       Mean reward: 461.37
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.3283
    Episode_Reward/rotating_object: 89.0112
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.84s
                      Time elapsed: 00:14:18
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 865/1500 [0m                      

                       Computation: 113153 steps/s (collection: 0.760s, learning 0.109s)
             Mean action noise std: 3.36
          Mean value_function loss: 101.2221
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.8477
                       Mean reward: 499.15
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 0.3209
    Episode_Reward/rotating_object: 92.6181
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.87s
                      Time elapsed: 00:14:19
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 866/1500 [0m                      

                       Computation: 107819 steps/s (collection: 0.810s, learning 0.102s)
             Mean action noise std: 3.37
          Mean value_function loss: 100.8036
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.8605
                       Mean reward: 429.74
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 90.4537
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.91s
                      Time elapsed: 00:14:20
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 867/1500 [0m                      

                       Computation: 110598 steps/s (collection: 0.777s, learning 0.112s)
             Mean action noise std: 3.38
          Mean value_function loss: 92.7676
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.8724
                       Mean reward: 443.05
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 87.3800
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.89s
                      Time elapsed: 00:14:21
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 868/1500 [0m                      

                       Computation: 108419 steps/s (collection: 0.812s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 76.2814
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.8771
                       Mean reward: 464.71
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 88.4642
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.91s
                      Time elapsed: 00:14:22
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 869/1500 [0m                      

                       Computation: 109002 steps/s (collection: 0.805s, learning 0.097s)
             Mean action noise std: 3.38
          Mean value_function loss: 93.5101
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.8794
                       Mean reward: 429.14
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 89.6516
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.90s
                      Time elapsed: 00:14:23
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 870/1500 [0m                      

                       Computation: 110384 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 91.0096
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 19.8871
                       Mean reward: 467.75
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 90.8551
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.89s
                      Time elapsed: 00:14:24
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 871/1500 [0m                      

                       Computation: 106972 steps/s (collection: 0.805s, learning 0.113s)
             Mean action noise std: 3.39
          Mean value_function loss: 89.6531
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8923
                       Mean reward: 444.04
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 87.3693
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.92s
                      Time elapsed: 00:14:25
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 872/1500 [0m                      

                       Computation: 99331 steps/s (collection: 0.857s, learning 0.132s)
             Mean action noise std: 3.39
          Mean value_function loss: 96.6153
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.8996
                       Mean reward: 463.84
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 87.7201
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.99s
                      Time elapsed: 00:14:26
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 873/1500 [0m                      

                       Computation: 104089 steps/s (collection: 0.841s, learning 0.103s)
             Mean action noise std: 3.39
          Mean value_function loss: 101.8464
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8985
                       Mean reward: 457.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 90.0794
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.94s
                      Time elapsed: 00:14:27
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 874/1500 [0m                      

                       Computation: 111335 steps/s (collection: 0.787s, learning 0.096s)
             Mean action noise std: 3.39
          Mean value_function loss: 93.1855
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.9033
                       Mean reward: 475.16
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 88.6752
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.88s
                      Time elapsed: 00:14:27
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 875/1500 [0m                      

                       Computation: 108280 steps/s (collection: 0.797s, learning 0.110s)
             Mean action noise std: 3.40
          Mean value_function loss: 84.9462
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9108
                       Mean reward: 436.30
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 85.5004
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.91s
                      Time elapsed: 00:14:28
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 876/1500 [0m                      

                       Computation: 107283 steps/s (collection: 0.798s, learning 0.118s)
             Mean action noise std: 3.40
          Mean value_function loss: 85.0522
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.9247
                       Mean reward: 466.78
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 92.8535
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.92s
                      Time elapsed: 00:14:29
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 877/1500 [0m                      

                       Computation: 110954 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 3.41
          Mean value_function loss: 84.1254
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.9364
                       Mean reward: 471.62
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 91.1032
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.89s
                      Time elapsed: 00:14:30
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 878/1500 [0m                      

                       Computation: 104645 steps/s (collection: 0.846s, learning 0.094s)
             Mean action noise std: 3.42
          Mean value_function loss: 88.9341
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.9531
                       Mean reward: 441.55
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 86.8063
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.94s
                      Time elapsed: 00:14:31
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 879/1500 [0m                      

                       Computation: 111428 steps/s (collection: 0.781s, learning 0.101s)
             Mean action noise std: 3.42
          Mean value_function loss: 79.7435
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 19.9696
                       Mean reward: 395.73
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 90.9464
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.88s
                      Time elapsed: 00:14:32
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 880/1500 [0m                      

                       Computation: 111726 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 3.43
          Mean value_function loss: 88.3160
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.9822
                       Mean reward: 480.68
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 92.7849
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.88s
                      Time elapsed: 00:14:33
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 881/1500 [0m                      

                       Computation: 109296 steps/s (collection: 0.804s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 81.3572
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9982
                       Mean reward: 486.65
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 91.6636
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.90s
                      Time elapsed: 00:14:34
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 882/1500 [0m                      

                       Computation: 111485 steps/s (collection: 0.771s, learning 0.111s)
             Mean action noise std: 3.44
          Mean value_function loss: 80.4934
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.0070
                       Mean reward: 459.33
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.3120
    Episode_Reward/rotating_object: 93.5059
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.88s
                      Time elapsed: 00:14:35
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 883/1500 [0m                      

                       Computation: 110201 steps/s (collection: 0.785s, learning 0.107s)
             Mean action noise std: 3.44
          Mean value_function loss: 93.3546
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.0107
                       Mean reward: 435.33
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 93.0103
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.89s
                      Time elapsed: 00:14:35
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 884/1500 [0m                      

                       Computation: 102797 steps/s (collection: 0.847s, learning 0.109s)
             Mean action noise std: 3.44
          Mean value_function loss: 85.3447
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.0194
                       Mean reward: 452.21
               Mean episode length: 241.09
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 91.1320
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.96s
                      Time elapsed: 00:14:36
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 885/1500 [0m                      

                       Computation: 108866 steps/s (collection: 0.795s, learning 0.108s)
             Mean action noise std: 3.45
          Mean value_function loss: 85.4712
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.0238
                       Mean reward: 455.72
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.3115
    Episode_Reward/rotating_object: 90.1826
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.90s
                      Time elapsed: 00:14:37
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 886/1500 [0m                      

                       Computation: 106977 steps/s (collection: 0.814s, learning 0.105s)
             Mean action noise std: 3.45
          Mean value_function loss: 81.8455
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.0287
                       Mean reward: 477.04
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 90.7913
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.92s
                      Time elapsed: 00:14:38
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 887/1500 [0m                      

                       Computation: 105342 steps/s (collection: 0.805s, learning 0.128s)
             Mean action noise std: 3.46
          Mean value_function loss: 90.1873
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.0467
                       Mean reward: 450.13
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 89.0520
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.93s
                      Time elapsed: 00:14:39
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 888/1500 [0m                      

                       Computation: 110344 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 3.46
          Mean value_function loss: 82.0067
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.0601
                       Mean reward: 468.62
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 86.3464
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.89s
                      Time elapsed: 00:14:40
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 889/1500 [0m                      

                       Computation: 102762 steps/s (collection: 0.818s, learning 0.139s)
             Mean action noise std: 3.46
          Mean value_function loss: 82.1332
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.0702
                       Mean reward: 454.72
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 92.1589
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.96s
                      Time elapsed: 00:14:41
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 890/1500 [0m                      

                       Computation: 109772 steps/s (collection: 0.799s, learning 0.097s)
             Mean action noise std: 3.47
          Mean value_function loss: 92.2870
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.0930
                       Mean reward: 440.11
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 92.5342
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.90s
                      Time elapsed: 00:14:42
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 891/1500 [0m                      

                       Computation: 114498 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 3.48
          Mean value_function loss: 91.1422
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.1161
                       Mean reward: 446.64
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 94.7948
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.86s
                      Time elapsed: 00:14:43
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 892/1500 [0m                      

                       Computation: 114596 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 3.48
          Mean value_function loss: 96.2656
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 20.1248
                       Mean reward: 498.72
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 94.2994
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.86s
                      Time elapsed: 00:14:44
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 893/1500 [0m                      

                       Computation: 110674 steps/s (collection: 0.782s, learning 0.106s)
             Mean action noise std: 3.48
          Mean value_function loss: 87.2679
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.1319
                       Mean reward: 427.43
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 89.4840
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.89s
                      Time elapsed: 00:14:45
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 894/1500 [0m                      

                       Computation: 105452 steps/s (collection: 0.828s, learning 0.105s)
             Mean action noise std: 3.49
          Mean value_function loss: 81.9141
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.1513
                       Mean reward: 432.93
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 92.9640
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.93s
                      Time elapsed: 00:14:45
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 895/1500 [0m                      

                       Computation: 101835 steps/s (collection: 0.862s, learning 0.103s)
             Mean action noise std: 3.50
          Mean value_function loss: 86.1227
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.1749
                       Mean reward: 464.70
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 90.6210
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.97s
                      Time elapsed: 00:14:46
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 896/1500 [0m                      

                       Computation: 109861 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 3.51
          Mean value_function loss: 91.4918
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.1901
                       Mean reward: 445.35
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 91.2418
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.89s
                      Time elapsed: 00:14:47
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 897/1500 [0m                      

                       Computation: 114468 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 3.51
          Mean value_function loss: 96.3956
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 20.1935
                       Mean reward: 467.09
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 90.3202
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.86s
                      Time elapsed: 00:14:48
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 898/1500 [0m                      

                       Computation: 115879 steps/s (collection: 0.756s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 94.0115
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.1978
                       Mean reward: 451.68
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 0.3002
    Episode_Reward/rotating_object: 89.3550
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.85s
                      Time elapsed: 00:14:49
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 899/1500 [0m                      

                       Computation: 110789 steps/s (collection: 0.765s, learning 0.123s)
             Mean action noise std: 3.51
          Mean value_function loss: 89.9755
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.2049
                       Mean reward: 431.86
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.3034
    Episode_Reward/rotating_object: 89.1102
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.89s
                      Time elapsed: 00:14:50
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 900/1500 [0m                      

                       Computation: 106869 steps/s (collection: 0.815s, learning 0.105s)
             Mean action noise std: 3.52
          Mean value_function loss: 86.2184
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.2086
                       Mean reward: 445.56
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.3063
    Episode_Reward/rotating_object: 91.5930
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.92s
                      Time elapsed: 00:14:51
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 901/1500 [0m                      

                       Computation: 109424 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 3.52
          Mean value_function loss: 90.0511
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.2178
                       Mean reward: 469.73
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 89.2178
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.90s
                      Time elapsed: 00:14:52
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 902/1500 [0m                      

                       Computation: 109856 steps/s (collection: 0.791s, learning 0.104s)
             Mean action noise std: 3.53
          Mean value_function loss: 84.8998
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2292
                       Mean reward: 450.76
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.3051
    Episode_Reward/rotating_object: 94.5052
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.89s
                      Time elapsed: 00:14:53
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 903/1500 [0m                      

                       Computation: 115461 steps/s (collection: 0.754s, learning 0.097s)
             Mean action noise std: 3.53
          Mean value_function loss: 90.3083
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 20.2324
                       Mean reward: 477.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3050
    Episode_Reward/rotating_object: 92.4232
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.85s
                      Time elapsed: 00:14:53
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 904/1500 [0m                      

                       Computation: 113923 steps/s (collection: 0.762s, learning 0.101s)
             Mean action noise std: 3.53
          Mean value_function loss: 83.2555
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.2415
                       Mean reward: 450.66
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 87.1432
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.86s
                      Time elapsed: 00:14:54
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 905/1500 [0m                      

                       Computation: 113522 steps/s (collection: 0.760s, learning 0.106s)
             Mean action noise std: 3.54
          Mean value_function loss: 81.2077
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.2512
                       Mean reward: 453.42
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 88.5946
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.87s
                      Time elapsed: 00:14:55
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 906/1500 [0m                      

                       Computation: 111642 steps/s (collection: 0.768s, learning 0.113s)
             Mean action noise std: 3.54
          Mean value_function loss: 95.8171
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.2559
                       Mean reward: 401.00
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 85.8952
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.88s
                      Time elapsed: 00:14:56
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 907/1500 [0m                      

                       Computation: 113580 steps/s (collection: 0.753s, learning 0.112s)
             Mean action noise std: 3.55
          Mean value_function loss: 93.5604
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.2700
                       Mean reward: 460.25
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.3100
    Episode_Reward/rotating_object: 91.9613
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.87s
                      Time elapsed: 00:14:57
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 908/1500 [0m                      

                       Computation: 116911 steps/s (collection: 0.746s, learning 0.095s)
             Mean action noise std: 3.55
          Mean value_function loss: 89.0550
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.2777
                       Mean reward: 436.04
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 89.5784
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.84s
                      Time elapsed: 00:14:58
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 909/1500 [0m                      

                       Computation: 111037 steps/s (collection: 0.776s, learning 0.110s)
             Mean action noise std: 3.56
          Mean value_function loss: 86.2365
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.2815
                       Mean reward: 494.95
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 94.3930
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.89s
                      Time elapsed: 00:14:59
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 910/1500 [0m                      

                       Computation: 105140 steps/s (collection: 0.827s, learning 0.108s)
             Mean action noise std: 3.56
          Mean value_function loss: 88.6455
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.2814
                       Mean reward: 455.55
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 87.7158
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.93s
                      Time elapsed: 00:15:00
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 911/1500 [0m                      

                       Computation: 110050 steps/s (collection: 0.787s, learning 0.107s)
             Mean action noise std: 3.56
          Mean value_function loss: 105.8971
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.2808
                       Mean reward: 422.93
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 87.0028
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.89s
                      Time elapsed: 00:15:01
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 912/1500 [0m                      

                       Computation: 105213 steps/s (collection: 0.820s, learning 0.115s)
             Mean action noise std: 3.56
          Mean value_function loss: 98.7944
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.2863
                       Mean reward: 467.39
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 93.0738
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.93s
                      Time elapsed: 00:15:01
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 913/1500 [0m                      

                       Computation: 105276 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 3.57
          Mean value_function loss: 100.5640
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.3004
                       Mean reward: 411.81
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 88.5515
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.93s
                      Time elapsed: 00:15:02
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 914/1500 [0m                      

                       Computation: 106449 steps/s (collection: 0.820s, learning 0.104s)
             Mean action noise std: 3.57
          Mean value_function loss: 104.3581
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.3129
                       Mean reward: 402.46
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 90.5951
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.92s
                      Time elapsed: 00:15:03
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 915/1500 [0m                      

                       Computation: 106462 steps/s (collection: 0.820s, learning 0.103s)
             Mean action noise std: 3.58
          Mean value_function loss: 97.5271
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.3191
                       Mean reward: 442.23
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 88.6460
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.92s
                      Time elapsed: 00:15:04
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 916/1500 [0m                      

                       Computation: 104436 steps/s (collection: 0.842s, learning 0.100s)
             Mean action noise std: 3.58
          Mean value_function loss: 92.4636
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.3219
                       Mean reward: 481.51
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 89.8958
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.94s
                      Time elapsed: 00:15:05
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 917/1500 [0m                      

                       Computation: 108767 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 3.58
          Mean value_function loss: 92.4345
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.3232
                       Mean reward: 425.35
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.3215
    Episode_Reward/rotating_object: 90.5768
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.90s
                      Time elapsed: 00:15:06
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 918/1500 [0m                      

                       Computation: 108505 steps/s (collection: 0.791s, learning 0.115s)
             Mean action noise std: 3.58
          Mean value_function loss: 99.8976
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 20.3254
                       Mean reward: 448.24
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 89.7226
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.91s
                      Time elapsed: 00:15:07
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 919/1500 [0m                      

                       Computation: 112494 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 3.59
          Mean value_function loss: 99.7477
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.3351
                       Mean reward: 421.29
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 90.7014
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.87s
                      Time elapsed: 00:15:08
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 920/1500 [0m                      

                       Computation: 115266 steps/s (collection: 0.762s, learning 0.091s)
             Mean action noise std: 3.59
          Mean value_function loss: 84.2875
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 20.3397
                       Mean reward: 479.40
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 88.9106
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.85s
                      Time elapsed: 00:15:09
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 921/1500 [0m                      

                       Computation: 112506 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 3.59
          Mean value_function loss: 103.2451
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.3420
                       Mean reward: 446.40
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 85.9063
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.87s
                      Time elapsed: 00:15:10
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 922/1500 [0m                      

                       Computation: 105382 steps/s (collection: 0.831s, learning 0.102s)
             Mean action noise std: 3.60
          Mean value_function loss: 87.9343
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.3444
                       Mean reward: 449.88
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 92.6272
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.93s
                      Time elapsed: 00:15:11
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 923/1500 [0m                      

                       Computation: 105829 steps/s (collection: 0.830s, learning 0.099s)
             Mean action noise std: 3.60
          Mean value_function loss: 99.8446
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3474
                       Mean reward: 432.48
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.3246
    Episode_Reward/rotating_object: 90.5995
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.93s
                      Time elapsed: 00:15:11
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 924/1500 [0m                      

                       Computation: 106988 steps/s (collection: 0.813s, learning 0.106s)
             Mean action noise std: 3.60
          Mean value_function loss: 113.3365
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.3475
                       Mean reward: 446.10
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 92.9989
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.92s
                      Time elapsed: 00:15:12
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 925/1500 [0m                      

                       Computation: 107053 steps/s (collection: 0.821s, learning 0.097s)
             Mean action noise std: 3.60
          Mean value_function loss: 99.6131
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.3535
                       Mean reward: 478.88
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 0.3060
    Episode_Reward/rotating_object: 91.2747
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.92s
                      Time elapsed: 00:15:13
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 926/1500 [0m                      

                       Computation: 102483 steps/s (collection: 0.838s, learning 0.121s)
             Mean action noise std: 3.61
          Mean value_function loss: 104.5184
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.3556
                       Mean reward: 454.16
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 88.1563
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.96s
                      Time elapsed: 00:15:14
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 927/1500 [0m                      

                       Computation: 105491 steps/s (collection: 0.821s, learning 0.110s)
             Mean action noise std: 3.61
          Mean value_function loss: 106.6313
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.3549
                       Mean reward: 465.78
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 92.1759
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.93s
                      Time elapsed: 00:15:15
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 928/1500 [0m                      

                       Computation: 104142 steps/s (collection: 0.828s, learning 0.116s)
             Mean action noise std: 3.62
          Mean value_function loss: 104.4245
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.3617
                       Mean reward: 448.45
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 91.6535
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.94s
                      Time elapsed: 00:15:16
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 929/1500 [0m                      

                       Computation: 105467 steps/s (collection: 0.821s, learning 0.112s)
             Mean action noise std: 3.62
          Mean value_function loss: 103.1735
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.3830
                       Mean reward: 444.72
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 0.3093
    Episode_Reward/rotating_object: 89.5149
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.93s
                      Time elapsed: 00:15:17
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 930/1500 [0m                      

                       Computation: 107219 steps/s (collection: 0.814s, learning 0.103s)
             Mean action noise std: 3.63
          Mean value_function loss: 93.5848
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.3919
                       Mean reward: 452.53
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 90.0192
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.92s
                      Time elapsed: 00:15:18
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 931/1500 [0m                      

                       Computation: 110253 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 3.63
          Mean value_function loss: 90.9458
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.4027
                       Mean reward: 465.16
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 94.6517
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.89s
                      Time elapsed: 00:15:19
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 932/1500 [0m                      

                       Computation: 114135 steps/s (collection: 0.758s, learning 0.103s)
             Mean action noise std: 3.63
          Mean value_function loss: 87.7963
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.4151
                       Mean reward: 514.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 96.6417
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.86s
                      Time elapsed: 00:15:20
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 933/1500 [0m                      

                       Computation: 110666 steps/s (collection: 0.790s, learning 0.099s)
             Mean action noise std: 3.64
          Mean value_function loss: 97.4800
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.4253
                       Mean reward: 453.92
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 93.7267
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.89s
                      Time elapsed: 00:15:21
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 934/1500 [0m                      

                       Computation: 115630 steps/s (collection: 0.752s, learning 0.098s)
             Mean action noise std: 3.65
          Mean value_function loss: 93.7865
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4402
                       Mean reward: 487.85
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.3094
    Episode_Reward/rotating_object: 92.0487
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.85s
                      Time elapsed: 00:15:21
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 935/1500 [0m                      

                       Computation: 111620 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 3.65
          Mean value_function loss: 106.8076
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.4592
                       Mean reward: 465.63
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 91.5330
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.88s
                      Time elapsed: 00:15:22
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 936/1500 [0m                      

                       Computation: 112424 steps/s (collection: 0.776s, learning 0.098s)
             Mean action noise std: 3.66
          Mean value_function loss: 93.7126
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4673
                       Mean reward: 467.49
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.3027
    Episode_Reward/rotating_object: 89.1793
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.87s
                      Time elapsed: 00:15:23
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 937/1500 [0m                      

                       Computation: 111957 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 3.67
          Mean value_function loss: 89.8558
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4762
                       Mean reward: 434.97
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.3085
    Episode_Reward/rotating_object: 93.2046
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.88s
                      Time elapsed: 00:15:24
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 938/1500 [0m                      

                       Computation: 107718 steps/s (collection: 0.811s, learning 0.102s)
             Mean action noise std: 3.67
          Mean value_function loss: 95.6984
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.4842
                       Mean reward: 464.52
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 92.0059
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.91s
                      Time elapsed: 00:15:25
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 939/1500 [0m                      

                       Computation: 105678 steps/s (collection: 0.826s, learning 0.105s)
             Mean action noise std: 3.67
          Mean value_function loss: 98.7849
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.4858
                       Mean reward: 467.58
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 92.7837
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.93s
                      Time elapsed: 00:15:26
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 940/1500 [0m                      

                       Computation: 107882 steps/s (collection: 0.807s, learning 0.105s)
             Mean action noise std: 3.68
          Mean value_function loss: 84.5080
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.4955
                       Mean reward: 466.46
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.3024
    Episode_Reward/rotating_object: 94.0443
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.91s
                      Time elapsed: 00:15:27
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 941/1500 [0m                      

                       Computation: 104317 steps/s (collection: 0.837s, learning 0.105s)
             Mean action noise std: 3.68
          Mean value_function loss: 87.6691
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.5042
                       Mean reward: 505.98
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 95.0318
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.94s
                      Time elapsed: 00:15:28
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 942/1500 [0m                      

                       Computation: 107397 steps/s (collection: 0.806s, learning 0.109s)
             Mean action noise std: 3.69
          Mean value_function loss: 92.9853
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.5093
                       Mean reward: 447.05
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.3044
    Episode_Reward/rotating_object: 88.7229
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.92s
                      Time elapsed: 00:15:29
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 943/1500 [0m                      

                       Computation: 107622 steps/s (collection: 0.814s, learning 0.100s)
             Mean action noise std: 3.69
          Mean value_function loss: 91.9238
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.5179
                       Mean reward: 462.17
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.3042
    Episode_Reward/rotating_object: 89.9476
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.91s
                      Time elapsed: 00:15:30
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 944/1500 [0m                      

                       Computation: 108349 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 3.70
          Mean value_function loss: 92.0099
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 20.5303
                       Mean reward: 480.73
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 92.1661
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.91s
                      Time elapsed: 00:15:31
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 945/1500 [0m                      

                       Computation: 106257 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 3.70
          Mean value_function loss: 89.9419
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.5446
                       Mean reward: 490.00
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 90.8204
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.93s
                      Time elapsed: 00:15:31
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 946/1500 [0m                      

                       Computation: 104387 steps/s (collection: 0.829s, learning 0.113s)
             Mean action noise std: 3.71
          Mean value_function loss: 96.5449
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.5526
                       Mean reward: 439.43
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 91.5187
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.94s
                      Time elapsed: 00:15:32
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 947/1500 [0m                      

                       Computation: 108593 steps/s (collection: 0.802s, learning 0.103s)
             Mean action noise std: 3.71
          Mean value_function loss: 93.7096
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.5569
                       Mean reward: 492.76
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 92.0329
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.91s
                      Time elapsed: 00:15:33
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 948/1500 [0m                      

                       Computation: 111500 steps/s (collection: 0.787s, learning 0.094s)
             Mean action noise std: 3.71
          Mean value_function loss: 98.5124
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.5608
                       Mean reward: 471.81
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 94.7818
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.88s
                      Time elapsed: 00:15:34
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 949/1500 [0m                      

                       Computation: 103888 steps/s (collection: 0.829s, learning 0.117s)
             Mean action noise std: 3.72
          Mean value_function loss: 97.5176
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.5698
                       Mean reward: 505.90
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 95.5288
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.95s
                      Time elapsed: 00:15:35
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 950/1500 [0m                      

                       Computation: 102391 steps/s (collection: 0.845s, learning 0.115s)
             Mean action noise std: 3.72
          Mean value_function loss: 101.4065
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 20.5778
                       Mean reward: 447.95
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 91.7132
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.96s
                      Time elapsed: 00:15:36
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 951/1500 [0m                      

                       Computation: 104477 steps/s (collection: 0.828s, learning 0.113s)
             Mean action noise std: 3.73
          Mean value_function loss: 96.1776
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.5947
                       Mean reward: 452.27
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 90.9003
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.94s
                      Time elapsed: 00:15:37
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 952/1500 [0m                      

                       Computation: 107194 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 3.73
          Mean value_function loss: 96.7480
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.6078
                       Mean reward: 439.05
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.3156
    Episode_Reward/rotating_object: 91.6775
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.92s
                      Time elapsed: 00:15:38
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 953/1500 [0m                      

                       Computation: 105010 steps/s (collection: 0.825s, learning 0.111s)
             Mean action noise std: 3.73
          Mean value_function loss: 90.2888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.6096
                       Mean reward: 429.27
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 91.6359
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.94s
                      Time elapsed: 00:15:39
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 954/1500 [0m                      

                       Computation: 107217 steps/s (collection: 0.817s, learning 0.100s)
             Mean action noise std: 3.74
          Mean value_function loss: 102.4858
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.6198
                       Mean reward: 418.28
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 93.2354
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.92s
                      Time elapsed: 00:15:40
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 955/1500 [0m                      

                       Computation: 101936 steps/s (collection: 0.863s, learning 0.102s)
             Mean action noise std: 3.74
          Mean value_function loss: 91.2455
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.6258
                       Mean reward: 467.79
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 89.1604
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.96s
                      Time elapsed: 00:15:41
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 956/1500 [0m                      

                       Computation: 110100 steps/s (collection: 0.784s, learning 0.109s)
             Mean action noise std: 3.75
          Mean value_function loss: 120.7333
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.6339
                       Mean reward: 450.67
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 90.4972
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.89s
                      Time elapsed: 00:15:42
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 957/1500 [0m                      

                       Computation: 105794 steps/s (collection: 0.813s, learning 0.116s)
             Mean action noise std: 3.75
          Mean value_function loss: 104.4995
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.6481
                       Mean reward: 459.26
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 92.4589
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.93s
                      Time elapsed: 00:15:43
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 958/1500 [0m                      

                       Computation: 107240 steps/s (collection: 0.818s, learning 0.098s)
             Mean action noise std: 3.76
          Mean value_function loss: 100.3749
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.6597
                       Mean reward: 457.16
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 96.6368
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.92s
                      Time elapsed: 00:15:44
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 959/1500 [0m                      

                       Computation: 104294 steps/s (collection: 0.828s, learning 0.115s)
             Mean action noise std: 3.76
          Mean value_function loss: 108.4557
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.6737
                       Mean reward: 457.12
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 89.8378
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.94s
                      Time elapsed: 00:15:44
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 960/1500 [0m                      

                       Computation: 105815 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 3.77
          Mean value_function loss: 104.9883
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6876
                       Mean reward: 434.42
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 91.9483
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.93s
                      Time elapsed: 00:15:45
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 961/1500 [0m                      

                       Computation: 109121 steps/s (collection: 0.797s, learning 0.104s)
             Mean action noise std: 3.78
          Mean value_function loss: 104.1683
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.7058
                       Mean reward: 490.32
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 92.5912
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.90s
                      Time elapsed: 00:15:46
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 962/1500 [0m                      

                       Computation: 113022 steps/s (collection: 0.773s, learning 0.097s)
             Mean action noise std: 3.78
          Mean value_function loss: 102.3290
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.7146
                       Mean reward: 465.00
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 93.2682
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.87s
                      Time elapsed: 00:15:47
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 963/1500 [0m                      

                       Computation: 112143 steps/s (collection: 0.775s, learning 0.101s)
             Mean action noise std: 3.78
          Mean value_function loss: 97.6100
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.7168
                       Mean reward: 483.86
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 93.3626
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.88s
                      Time elapsed: 00:15:48
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 964/1500 [0m                      

                       Computation: 114897 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 3.78
          Mean value_function loss: 94.8062
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.7180
                       Mean reward: 477.46
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 90.1811
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.86s
                      Time elapsed: 00:15:49
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 965/1500 [0m                      

                       Computation: 112456 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 3.79
          Mean value_function loss: 90.5017
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.7286
                       Mean reward: 412.11
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 0.3079
    Episode_Reward/rotating_object: 92.2660
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.87s
                      Time elapsed: 00:15:50
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 966/1500 [0m                      

                       Computation: 109855 steps/s (collection: 0.787s, learning 0.108s)
             Mean action noise std: 3.79
          Mean value_function loss: 106.1444
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.7443
                       Mean reward: 433.70
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.3085
    Episode_Reward/rotating_object: 91.9201
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.89s
                      Time elapsed: 00:15:51
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 967/1500 [0m                      

                       Computation: 105816 steps/s (collection: 0.828s, learning 0.101s)
             Mean action noise std: 3.80
          Mean value_function loss: 103.9790
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.7551
                       Mean reward: 455.41
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 91.8827
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.93s
                      Time elapsed: 00:15:52
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 968/1500 [0m                      

                       Computation: 111448 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 3.81
          Mean value_function loss: 101.6102
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.7637
                       Mean reward: 464.60
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 0.3059
    Episode_Reward/rotating_object: 89.3887
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.88s
                      Time elapsed: 00:15:52
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 969/1500 [0m                      

                       Computation: 108787 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 3.81
          Mean value_function loss: 107.5950
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.7664
                       Mean reward: 478.13
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.3087
    Episode_Reward/rotating_object: 89.4130
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.90s
                      Time elapsed: 00:15:53
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 970/1500 [0m                      

                       Computation: 105157 steps/s (collection: 0.835s, learning 0.099s)
             Mean action noise std: 3.82
          Mean value_function loss: 95.0898
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.7803
                       Mean reward: 444.54
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 0.3051
    Episode_Reward/rotating_object: 89.2219
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.93s
                      Time elapsed: 00:15:54
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 971/1500 [0m                      

                       Computation: 104810 steps/s (collection: 0.832s, learning 0.106s)
             Mean action noise std: 3.82
          Mean value_function loss: 117.6966
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 20.7911
                       Mean reward: 483.24
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.3027
    Episode_Reward/rotating_object: 91.1176
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.94s
                      Time elapsed: 00:15:55
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 972/1500 [0m                      

                       Computation: 106787 steps/s (collection: 0.816s, learning 0.105s)
             Mean action noise std: 3.82
          Mean value_function loss: 111.2284
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.7927
                       Mean reward: 438.28
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.3088
    Episode_Reward/rotating_object: 90.0761
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.92s
                      Time elapsed: 00:15:56
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 973/1500 [0m                      

                       Computation: 102454 steps/s (collection: 0.834s, learning 0.126s)
             Mean action noise std: 3.83
          Mean value_function loss: 99.2058
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.7954
                       Mean reward: 485.17
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 93.5965
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.96s
                      Time elapsed: 00:15:57
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 974/1500 [0m                      

                       Computation: 104736 steps/s (collection: 0.829s, learning 0.110s)
             Mean action noise std: 3.83
          Mean value_function loss: 108.0723
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.8048
                       Mean reward: 469.61
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.3094
    Episode_Reward/rotating_object: 90.7974
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.94s
                      Time elapsed: 00:15:58
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 975/1500 [0m                      

                       Computation: 109912 steps/s (collection: 0.776s, learning 0.119s)
             Mean action noise std: 3.84
          Mean value_function loss: 92.3967
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.8101
                       Mean reward: 428.15
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 90.7623
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.89s
                      Time elapsed: 00:15:59
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 976/1500 [0m                      

                       Computation: 108605 steps/s (collection: 0.802s, learning 0.104s)
             Mean action noise std: 3.84
          Mean value_function loss: 102.6290
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.8127
                       Mean reward: 458.54
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 0.3041
    Episode_Reward/rotating_object: 94.5359
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.91s
                      Time elapsed: 00:16:00
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 977/1500 [0m                      

                       Computation: 103889 steps/s (collection: 0.839s, learning 0.107s)
             Mean action noise std: 3.84
          Mean value_function loss: 98.0196
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.8268
                       Mean reward: 428.61
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.3115
    Episode_Reward/rotating_object: 93.5431
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.95s
                      Time elapsed: 00:16:01
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 978/1500 [0m                      

                       Computation: 106104 steps/s (collection: 0.824s, learning 0.102s)
             Mean action noise std: 3.85
          Mean value_function loss: 90.8213
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.8357
                       Mean reward: 504.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3138
    Episode_Reward/rotating_object: 100.1853
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.93s
                      Time elapsed: 00:16:02
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 979/1500 [0m                      

                       Computation: 109260 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 3.85
          Mean value_function loss: 96.2233
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.8422
                       Mean reward: 491.67
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.3073
    Episode_Reward/rotating_object: 94.1374
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.90s
                      Time elapsed: 00:16:03
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 980/1500 [0m                      

                       Computation: 112659 steps/s (collection: 0.773s, learning 0.100s)
             Mean action noise std: 3.85
          Mean value_function loss: 90.9797
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.8425
                       Mean reward: 482.22
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 94.6269
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.87s
                      Time elapsed: 00:16:03
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 981/1500 [0m                      

                       Computation: 111521 steps/s (collection: 0.765s, learning 0.116s)
             Mean action noise std: 3.85
          Mean value_function loss: 100.9106
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.8393
                       Mean reward: 491.17
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 93.0345
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.88s
                      Time elapsed: 00:16:04
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 982/1500 [0m                      

                       Computation: 111009 steps/s (collection: 0.784s, learning 0.101s)
             Mean action noise std: 3.85
          Mean value_function loss: 87.4300
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.8339
                       Mean reward: 480.99
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3049
    Episode_Reward/rotating_object: 93.8096
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.89s
                      Time elapsed: 00:16:05
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 983/1500 [0m                      

                       Computation: 113879 steps/s (collection: 0.757s, learning 0.107s)
             Mean action noise std: 3.86
          Mean value_function loss: 101.1321
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 20.8343
                       Mean reward: 430.52
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.3031
    Episode_Reward/rotating_object: 90.7999
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.86s
                      Time elapsed: 00:16:06
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 984/1500 [0m                      

                       Computation: 111412 steps/s (collection: 0.773s, learning 0.109s)
             Mean action noise std: 3.86
          Mean value_function loss: 91.2550
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.8438
                       Mean reward: 470.43
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.3011
    Episode_Reward/rotating_object: 91.6691
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.88s
                      Time elapsed: 00:16:07
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 985/1500 [0m                      

                       Computation: 112063 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 3.87
          Mean value_function loss: 91.9809
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.8596
                       Mean reward: 472.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 96.4529
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.88s
                      Time elapsed: 00:16:08
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 986/1500 [0m                      

                       Computation: 108438 steps/s (collection: 0.792s, learning 0.115s)
             Mean action noise std: 3.87
          Mean value_function loss: 91.8525
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.8672
                       Mean reward: 467.03
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 95.7222
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.91s
                      Time elapsed: 00:16:09
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 987/1500 [0m                      

                       Computation: 111541 steps/s (collection: 0.779s, learning 0.103s)
             Mean action noise std: 3.87
          Mean value_function loss: 89.4213
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.8724
                       Mean reward: 428.64
               Mean episode length: 242.14
    Episode_Reward/reaching_object: 0.3025
    Episode_Reward/rotating_object: 92.2766
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.88s
                      Time elapsed: 00:16:10
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 988/1500 [0m                      

                       Computation: 114828 steps/s (collection: 0.760s, learning 0.097s)
             Mean action noise std: 3.88
          Mean value_function loss: 89.9628
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.8790
                       Mean reward: 468.71
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.3081
    Episode_Reward/rotating_object: 97.0430
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.86s
                      Time elapsed: 00:16:11
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 989/1500 [0m                      

                       Computation: 108761 steps/s (collection: 0.810s, learning 0.094s)
             Mean action noise std: 3.88
          Mean value_function loss: 98.3841
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 20.8934
                       Mean reward: 484.53
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 96.8915
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.90s
                      Time elapsed: 00:16:11
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 990/1500 [0m                      

                       Computation: 108623 steps/s (collection: 0.800s, learning 0.105s)
             Mean action noise std: 3.89
          Mean value_function loss: 100.9385
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 20.9005
                       Mean reward: 491.65
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.3019
    Episode_Reward/rotating_object: 94.5912
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.90s
                      Time elapsed: 00:16:12
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 991/1500 [0m                      

                       Computation: 110269 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 3.89
          Mean value_function loss: 90.9181
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 20.9015
                       Mean reward: 447.20
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 0.3085
    Episode_Reward/rotating_object: 97.5106
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.89s
                      Time elapsed: 00:16:13
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 992/1500 [0m                      

                       Computation: 112144 steps/s (collection: 0.778s, learning 0.099s)
             Mean action noise std: 3.89
          Mean value_function loss: 90.9690
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.9022
                       Mean reward: 477.64
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.3075
    Episode_Reward/rotating_object: 95.0796
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.88s
                      Time elapsed: 00:16:14
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 993/1500 [0m                      

                       Computation: 107250 steps/s (collection: 0.814s, learning 0.103s)
             Mean action noise std: 3.89
          Mean value_function loss: 98.3654
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.9031
                       Mean reward: 446.23
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.3094
    Episode_Reward/rotating_object: 91.7525
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.92s
                      Time elapsed: 00:16:15
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 994/1500 [0m                      

                       Computation: 112503 steps/s (collection: 0.782s, learning 0.092s)
             Mean action noise std: 3.90
          Mean value_function loss: 100.1460
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.9154
                       Mean reward: 467.45
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 96.3573
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.87s
                      Time elapsed: 00:16:16
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 995/1500 [0m                      

                       Computation: 114773 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 3.90
          Mean value_function loss: 103.7547
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.9288
                       Mean reward: 485.30
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 96.9184
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.86s
                      Time elapsed: 00:16:17
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 996/1500 [0m                      

                       Computation: 108399 steps/s (collection: 0.810s, learning 0.097s)
             Mean action noise std: 3.90
          Mean value_function loss: 84.7852
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.9314
                       Mean reward: 514.45
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 97.6649
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.91s
                      Time elapsed: 00:16:18
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 997/1500 [0m                      

                       Computation: 114292 steps/s (collection: 0.769s, learning 0.091s)
             Mean action noise std: 3.91
          Mean value_function loss: 84.7550
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.9381
                       Mean reward: 472.71
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.3069
    Episode_Reward/rotating_object: 94.8768
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.86s
                      Time elapsed: 00:16:19
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 998/1500 [0m                      

                       Computation: 112398 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 3.91
          Mean value_function loss: 81.0832
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.9451
                       Mean reward: 443.30
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 94.0215
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.87s
                      Time elapsed: 00:16:19
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 999/1500 [0m                      

                       Computation: 113236 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 3.92
          Mean value_function loss: 92.5353
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.9516
                       Mean reward: 483.71
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 100.8912
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.87s
                      Time elapsed: 00:16:20
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1000/1500 [0m                     

                       Computation: 37033 steps/s (collection: 2.549s, learning 0.105s)
             Mean action noise std: 3.92
          Mean value_function loss: 92.9917
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.9542
                       Mean reward: 522.60
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.3071
    Episode_Reward/rotating_object: 99.2109
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.65s
                      Time elapsed: 00:16:23
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1001/1500 [0m                     

                       Computation: 35927 steps/s (collection: 2.620s, learning 0.117s)
             Mean action noise std: 3.92
          Mean value_function loss: 83.0299
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.9584
                       Mean reward: 477.67
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 0.3005
    Episode_Reward/rotating_object: 94.2939
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 2.74s
                      Time elapsed: 00:16:26
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1002/1500 [0m                     

                       Computation: 34228 steps/s (collection: 2.745s, learning 0.127s)
             Mean action noise std: 3.92
          Mean value_function loss: 96.8886
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 20.9613
                       Mean reward: 492.62
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.3048
    Episode_Reward/rotating_object: 94.3118
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 2.87s
                      Time elapsed: 00:16:29
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1003/1500 [0m                     

                       Computation: 34585 steps/s (collection: 2.709s, learning 0.133s)
             Mean action noise std: 3.93
          Mean value_function loss: 101.2435
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.9612
                       Mean reward: 455.60
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 95.1140
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 2.84s
                      Time elapsed: 00:16:31
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1004/1500 [0m                     

                       Computation: 33656 steps/s (collection: 2.778s, learning 0.143s)
             Mean action noise std: 3.93
          Mean value_function loss: 104.2423
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.9668
                       Mean reward: 436.29
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 0.3012
    Episode_Reward/rotating_object: 92.9993
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 2.92s
                      Time elapsed: 00:16:34
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1005/1500 [0m                     

                       Computation: 34644 steps/s (collection: 2.707s, learning 0.130s)
             Mean action noise std: 3.94
          Mean value_function loss: 101.4167
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.9732
                       Mean reward: 501.70
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 97.3086
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 2.84s
                      Time elapsed: 00:16:37
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1006/1500 [0m                     

                       Computation: 34692 steps/s (collection: 2.718s, learning 0.116s)
             Mean action noise std: 3.94
          Mean value_function loss: 86.5090
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.9826
                       Mean reward: 524.02
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 96.1120
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 2.83s
                      Time elapsed: 00:16:40
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1007/1500 [0m                     

                       Computation: 32953 steps/s (collection: 2.866s, learning 0.117s)
             Mean action noise std: 3.94
          Mean value_function loss: 95.4952
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.9883
                       Mean reward: 495.63
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.3050
    Episode_Reward/rotating_object: 95.8417
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 2.98s
                      Time elapsed: 00:16:43
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1008/1500 [0m                     

                       Computation: 28670 steps/s (collection: 3.328s, learning 0.101s)
             Mean action noise std: 3.95
          Mean value_function loss: 91.5224
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 20.9944
                       Mean reward: 519.25
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 101.5255
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.43s
                      Time elapsed: 00:16:46
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1009/1500 [0m                     

                       Computation: 118820 steps/s (collection: 0.732s, learning 0.095s)
             Mean action noise std: 3.95
          Mean value_function loss: 92.3808
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.0008
                       Mean reward: 456.30
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 95.1899
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.83s
                      Time elapsed: 00:16:47
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1010/1500 [0m                     

                       Computation: 120266 steps/s (collection: 0.724s, learning 0.093s)
             Mean action noise std: 3.95
          Mean value_function loss: 104.6721
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 20.9998
                       Mean reward: 471.26
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 98.8443
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.82s
                      Time elapsed: 00:16:48
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1011/1500 [0m                     

                       Computation: 109911 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 3.95
          Mean value_function loss: 93.9981
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.0012
                       Mean reward: 527.06
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 97.7244
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.89s
                      Time elapsed: 00:16:49
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1012/1500 [0m                     

                       Computation: 109667 steps/s (collection: 0.790s, learning 0.106s)
             Mean action noise std: 3.96
          Mean value_function loss: 93.9351
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.0036
                       Mean reward: 464.88
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 97.8995
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.90s
                      Time elapsed: 00:16:50
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1013/1500 [0m                     

                       Computation: 103598 steps/s (collection: 0.844s, learning 0.105s)
             Mean action noise std: 3.96
          Mean value_function loss: 93.1584
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.0074
                       Mean reward: 462.56
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 94.0379
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.95s
                      Time elapsed: 00:16:51
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1014/1500 [0m                     

                       Computation: 105079 steps/s (collection: 0.838s, learning 0.098s)
             Mean action noise std: 3.96
          Mean value_function loss: 91.2951
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.0098
                       Mean reward: 461.49
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 95.9536
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.94s
                      Time elapsed: 00:16:52
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1015/1500 [0m                     

                       Computation: 106882 steps/s (collection: 0.817s, learning 0.103s)
             Mean action noise std: 3.97
          Mean value_function loss: 95.9677
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.0115
                       Mean reward: 468.45
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 94.0337
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.92s
                      Time elapsed: 00:16:53
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1016/1500 [0m                     

                       Computation: 105553 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 3.97
          Mean value_function loss: 97.8991
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.0191
                       Mean reward: 487.50
               Mean episode length: 241.67
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 96.9658
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.93s
                      Time elapsed: 00:16:54
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1017/1500 [0m                     

                       Computation: 106911 steps/s (collection: 0.795s, learning 0.124s)
             Mean action noise std: 3.97
          Mean value_function loss: 88.1101
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.0173
                       Mean reward: 524.13
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 98.1544
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.92s
                      Time elapsed: 00:16:54
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1018/1500 [0m                     

                       Computation: 101133 steps/s (collection: 0.863s, learning 0.109s)
             Mean action noise std: 3.98
          Mean value_function loss: 102.2353
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.0239
                       Mean reward: 403.86
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.3095
    Episode_Reward/rotating_object: 93.6076
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.97s
                      Time elapsed: 00:16:55
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1019/1500 [0m                     

                       Computation: 98446 steps/s (collection: 0.876s, learning 0.122s)
             Mean action noise std: 3.98
          Mean value_function loss: 104.5734
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.0302
                       Mean reward: 444.76
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 94.4362
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.00s
                      Time elapsed: 00:16:56
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1020/1500 [0m                     

                       Computation: 103324 steps/s (collection: 0.843s, learning 0.108s)
             Mean action noise std: 3.98
          Mean value_function loss: 106.0027
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.0318
                       Mean reward: 518.22
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 98.8124
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.95s
                      Time elapsed: 00:16:57
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1021/1500 [0m                     

                       Computation: 106296 steps/s (collection: 0.826s, learning 0.099s)
             Mean action noise std: 3.98
          Mean value_function loss: 106.0202
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.0366
                       Mean reward: 504.06
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 99.4997
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.92s
                      Time elapsed: 00:16:58
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1022/1500 [0m                     

                       Computation: 109703 steps/s (collection: 0.793s, learning 0.103s)
             Mean action noise std: 3.99
          Mean value_function loss: 96.9064
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.0423
                       Mean reward: 485.28
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 95.6312
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.90s
                      Time elapsed: 00:16:59
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1023/1500 [0m                     

                       Computation: 105780 steps/s (collection: 0.828s, learning 0.101s)
             Mean action noise std: 3.99
          Mean value_function loss: 109.6143
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.0486
                       Mean reward: 473.05
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 102.4753
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.93s
                      Time elapsed: 00:17:00
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1024/1500 [0m                     

                       Computation: 103179 steps/s (collection: 0.845s, learning 0.108s)
             Mean action noise std: 3.99
          Mean value_function loss: 101.7669
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.0580
                       Mean reward: 444.17
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 92.9786
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.95s
                      Time elapsed: 00:17:01
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1025/1500 [0m                     

                       Computation: 103717 steps/s (collection: 0.841s, learning 0.107s)
             Mean action noise std: 4.00
          Mean value_function loss: 102.7172
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.0678
                       Mean reward: 504.39
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 99.7717
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.95s
                      Time elapsed: 00:17:02
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1026/1500 [0m                     

                       Computation: 106007 steps/s (collection: 0.824s, learning 0.104s)
             Mean action noise std: 4.00
          Mean value_function loss: 103.6716
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.0721
                       Mean reward: 471.94
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 93.7277
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.93s
                      Time elapsed: 00:17:03
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1027/1500 [0m                     

                       Computation: 99299 steps/s (collection: 0.884s, learning 0.106s)
             Mean action noise std: 4.00
          Mean value_function loss: 91.6381
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.0753
                       Mean reward: 487.06
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 98.5809
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.99s
                      Time elapsed: 00:17:04
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1028/1500 [0m                     

                       Computation: 97842 steps/s (collection: 0.905s, learning 0.100s)
             Mean action noise std: 4.01
          Mean value_function loss: 94.8511
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.0804
                       Mean reward: 503.45
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 93.7472
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.00s
                      Time elapsed: 00:17:05
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1029/1500 [0m                     

                       Computation: 105206 steps/s (collection: 0.832s, learning 0.102s)
             Mean action noise std: 4.01
          Mean value_function loss: 93.3980
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.0886
                       Mean reward: 523.11
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.3106
    Episode_Reward/rotating_object: 97.6987
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.93s
                      Time elapsed: 00:17:06
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1030/1500 [0m                     

                       Computation: 108008 steps/s (collection: 0.808s, learning 0.103s)
             Mean action noise std: 4.02
          Mean value_function loss: 107.2363
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.0978
                       Mean reward: 506.46
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 98.5234
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.91s
                      Time elapsed: 00:17:07
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1031/1500 [0m                     

                       Computation: 107691 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 4.03
          Mean value_function loss: 102.4064
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 21.1084
                       Mean reward: 498.82
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 0.3120
    Episode_Reward/rotating_object: 97.1652
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.91s
                      Time elapsed: 00:17:08
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1032/1500 [0m                     

                       Computation: 112248 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 4.03
          Mean value_function loss: 98.4933
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.1121
                       Mean reward: 509.75
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 0.3153
    Episode_Reward/rotating_object: 95.6265
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.88s
                      Time elapsed: 00:17:09
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1033/1500 [0m                     

                       Computation: 112330 steps/s (collection: 0.771s, learning 0.104s)
             Mean action noise std: 4.03
          Mean value_function loss: 101.1545
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.1127
                       Mean reward: 454.86
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 96.6107
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.88s
                      Time elapsed: 00:17:09
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1034/1500 [0m                     

                       Computation: 114228 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 4.03
          Mean value_function loss: 99.5940
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.1196
                       Mean reward: 494.68
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 98.6177
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.86s
                      Time elapsed: 00:17:10
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1035/1500 [0m                     

                       Computation: 114036 steps/s (collection: 0.762s, learning 0.100s)
             Mean action noise std: 4.04
          Mean value_function loss: 100.5739
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.1299
                       Mean reward: 508.27
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3167
    Episode_Reward/rotating_object: 97.9861
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.86s
                      Time elapsed: 00:17:11
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1036/1500 [0m                     

                       Computation: 108765 steps/s (collection: 0.793s, learning 0.111s)
             Mean action noise std: 4.04
          Mean value_function loss: 99.3845
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.1364
                       Mean reward: 478.29
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 100.8175
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.90s
                      Time elapsed: 00:17:12
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1037/1500 [0m                     

                       Computation: 114619 steps/s (collection: 0.760s, learning 0.098s)
             Mean action noise std: 4.04
          Mean value_function loss: 91.5030
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.1370
                       Mean reward: 483.09
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 95.3217
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.86s
                      Time elapsed: 00:17:13
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1038/1500 [0m                     

                       Computation: 113057 steps/s (collection: 0.764s, learning 0.106s)
             Mean action noise std: 4.05
          Mean value_function loss: 88.5464
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.1413
                       Mean reward: 484.25
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 92.3722
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.87s
                      Time elapsed: 00:17:14
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1039/1500 [0m                     

                       Computation: 115128 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 4.05
          Mean value_function loss: 94.7051
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.1457
                       Mean reward: 491.44
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 97.9193
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.85s
                      Time elapsed: 00:17:15
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1040/1500 [0m                     

                       Computation: 117610 steps/s (collection: 0.735s, learning 0.101s)
             Mean action noise std: 4.05
          Mean value_function loss: 96.7874
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.1470
                       Mean reward: 521.64
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.3203
    Episode_Reward/rotating_object: 102.9604
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.84s
                      Time elapsed: 00:17:16
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1041/1500 [0m                     

                       Computation: 111175 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 4.05
          Mean value_function loss: 100.6661
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.1461
                       Mean reward: 489.49
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 94.8632
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.88s
                      Time elapsed: 00:17:16
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1042/1500 [0m                     

                       Computation: 116127 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 4.06
          Mean value_function loss: 100.5537
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 21.1501
                       Mean reward: 478.16
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 95.1510
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.85s
                      Time elapsed: 00:17:17
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1043/1500 [0m                     

                       Computation: 111617 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 4.06
          Mean value_function loss: 101.7574
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.1529
                       Mean reward: 507.69
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 100.3545
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.88s
                      Time elapsed: 00:17:18
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1044/1500 [0m                     

                       Computation: 106283 steps/s (collection: 0.816s, learning 0.109s)
             Mean action noise std: 4.06
          Mean value_function loss: 90.6006
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.1573
                       Mean reward: 506.52
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.3269
    Episode_Reward/rotating_object: 99.8554
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.92s
                      Time elapsed: 00:17:19
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1045/1500 [0m                     

                       Computation: 110350 steps/s (collection: 0.797s, learning 0.094s)
             Mean action noise std: 4.07
          Mean value_function loss: 102.9941
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.1646
                       Mean reward: 509.07
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 96.2714
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.89s
                      Time elapsed: 00:17:20
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1046/1500 [0m                     

                       Computation: 105471 steps/s (collection: 0.805s, learning 0.127s)
             Mean action noise std: 4.07
          Mean value_function loss: 94.6433
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.1738
                       Mean reward: 496.92
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 98.3097
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.93s
                      Time elapsed: 00:17:21
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1047/1500 [0m                     

                       Computation: 109040 steps/s (collection: 0.802s, learning 0.100s)
             Mean action noise std: 4.07
          Mean value_function loss: 85.4760
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.1850
                       Mean reward: 488.97
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 97.5369
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.90s
                      Time elapsed: 00:17:22
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1048/1500 [0m                     

                       Computation: 109586 steps/s (collection: 0.787s, learning 0.110s)
             Mean action noise std: 4.07
          Mean value_function loss: 91.4315
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.1895
                       Mean reward: 497.03
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.3221
    Episode_Reward/rotating_object: 102.6308
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.90s
                      Time elapsed: 00:17:23
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1049/1500 [0m                     

                       Computation: 113663 steps/s (collection: 0.761s, learning 0.104s)
             Mean action noise std: 4.08
          Mean value_function loss: 100.5307
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.1943
                       Mean reward: 537.94
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 101.5462
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.86s
                      Time elapsed: 00:17:24
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1050/1500 [0m                     

                       Computation: 91568 steps/s (collection: 0.888s, learning 0.186s)
             Mean action noise std: 4.08
          Mean value_function loss: 94.8340
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.2029
                       Mean reward: 505.69
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 100.2609
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.07s
                      Time elapsed: 00:17:25
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1051/1500 [0m                     

                       Computation: 89173 steps/s (collection: 0.951s, learning 0.152s)
             Mean action noise std: 4.09
          Mean value_function loss: 103.5566
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 21.2091
                       Mean reward: 470.48
               Mean episode length: 237.11
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 97.3250
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 1.10s
                      Time elapsed: 00:17:26
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1052/1500 [0m                     

                       Computation: 103408 steps/s (collection: 0.832s, learning 0.118s)
             Mean action noise std: 4.09
          Mean value_function loss: 112.8742
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.2201
                       Mean reward: 474.13
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 98.6114
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.95s
                      Time elapsed: 00:17:27
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1053/1500 [0m                     

                       Computation: 103060 steps/s (collection: 0.811s, learning 0.143s)
             Mean action noise std: 4.10
          Mean value_function loss: 91.8590
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.2343
                       Mean reward: 520.00
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 98.9225
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.95s
                      Time elapsed: 00:17:28
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1054/1500 [0m                     

                       Computation: 110585 steps/s (collection: 0.790s, learning 0.099s)
             Mean action noise std: 4.10
          Mean value_function loss: 100.6209
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.2453
                       Mean reward: 511.15
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 97.1430
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.89s
                      Time elapsed: 00:17:29
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1055/1500 [0m                     

                       Computation: 106346 steps/s (collection: 0.802s, learning 0.122s)
             Mean action noise std: 4.10
          Mean value_function loss: 94.3699
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.2502
                       Mean reward: 446.52
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 93.1214
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.92s
                      Time elapsed: 00:17:29
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1056/1500 [0m                     

                       Computation: 101532 steps/s (collection: 0.809s, learning 0.159s)
             Mean action noise std: 4.10
          Mean value_function loss: 105.1946
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 21.2559
                       Mean reward: 508.69
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 101.8413
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.97s
                      Time elapsed: 00:17:30
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1057/1500 [0m                     

                       Computation: 109274 steps/s (collection: 0.792s, learning 0.108s)
             Mean action noise std: 4.11
          Mean value_function loss: 94.6463
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.2652
                       Mean reward: 500.71
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 98.2336
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.90s
                      Time elapsed: 00:17:31
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1058/1500 [0m                     

                       Computation: 97304 steps/s (collection: 0.860s, learning 0.150s)
             Mean action noise std: 4.12
          Mean value_function loss: 97.7740
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.2757
                       Mean reward: 531.50
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 103.2418
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.01s
                      Time elapsed: 00:17:32
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1059/1500 [0m                     

                       Computation: 102594 steps/s (collection: 0.858s, learning 0.100s)
             Mean action noise std: 4.12
          Mean value_function loss: 95.6223
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 21.2824
                       Mean reward: 510.87
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 102.5444
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.96s
                      Time elapsed: 00:17:33
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1060/1500 [0m                     

                       Computation: 108048 steps/s (collection: 0.818s, learning 0.092s)
             Mean action noise std: 4.12
          Mean value_function loss: 98.8707
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.2808
                       Mean reward: 473.54
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 98.0202
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.91s
                      Time elapsed: 00:17:34
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1061/1500 [0m                     

                       Computation: 109174 steps/s (collection: 0.783s, learning 0.118s)
             Mean action noise std: 4.12
          Mean value_function loss: 106.7771
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.2812
                       Mean reward: 505.81
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 98.6202
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.90s
                      Time elapsed: 00:17:35
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1062/1500 [0m                     

                       Computation: 109732 steps/s (collection: 0.780s, learning 0.116s)
             Mean action noise std: 4.13
          Mean value_function loss: 107.4700
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.2925
                       Mean reward: 546.24
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 102.7533
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.90s
                      Time elapsed: 00:17:36
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1063/1500 [0m                     

                       Computation: 102350 steps/s (collection: 0.799s, learning 0.162s)
             Mean action noise std: 4.13
          Mean value_function loss: 101.6959
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.2971
                       Mean reward: 508.83
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3234
    Episode_Reward/rotating_object: 101.8887
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.96s
                      Time elapsed: 00:17:37
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1064/1500 [0m                     

                       Computation: 104793 steps/s (collection: 0.805s, learning 0.133s)
             Mean action noise std: 4.14
          Mean value_function loss: 104.2117
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.3127
                       Mean reward: 500.87
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 101.4264
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.94s
                      Time elapsed: 00:17:38
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1065/1500 [0m                     

                       Computation: 74693 steps/s (collection: 1.133s, learning 0.183s)
             Mean action noise std: 4.14
          Mean value_function loss: 96.4749
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.3192
                       Mean reward: 510.59
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 103.4017
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 1.32s
                      Time elapsed: 00:17:39
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1066/1500 [0m                     

                       Computation: 97211 steps/s (collection: 0.910s, learning 0.102s)
             Mean action noise std: 4.14
          Mean value_function loss: 94.5804
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.3269
                       Mean reward: 469.45
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 95.7030
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 1.01s
                      Time elapsed: 00:17:40
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1067/1500 [0m                     

                       Computation: 93178 steps/s (collection: 0.947s, learning 0.108s)
             Mean action noise std: 4.15
          Mean value_function loss: 96.8977
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.3335
                       Mean reward: 507.31
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 100.7788
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 1.06s
                      Time elapsed: 00:17:41
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1068/1500 [0m                     

                       Computation: 100652 steps/s (collection: 0.834s, learning 0.143s)
             Mean action noise std: 4.15
          Mean value_function loss: 91.5217
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.3374
                       Mean reward: 513.99
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 94.6076
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.98s
                      Time elapsed: 00:17:42
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1069/1500 [0m                     

                       Computation: 86585 steps/s (collection: 0.960s, learning 0.175s)
             Mean action noise std: 4.15
          Mean value_function loss: 78.8440
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.3397
                       Mean reward: 515.94
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 98.0539
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 1.14s
                      Time elapsed: 00:17:43
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1070/1500 [0m                     

                       Computation: 95880 steps/s (collection: 0.853s, learning 0.172s)
             Mean action noise std: 4.15
          Mean value_function loss: 94.8978
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.3387
                       Mean reward: 474.75
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 95.7099
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.03s
                      Time elapsed: 00:17:44
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1071/1500 [0m                     

                       Computation: 92840 steps/s (collection: 0.900s, learning 0.159s)
             Mean action noise std: 4.16
          Mean value_function loss: 101.2706
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.3417
                       Mean reward: 468.11
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 94.7101
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 1.06s
                      Time elapsed: 00:17:45
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1072/1500 [0m                     

                       Computation: 94846 steps/s (collection: 0.861s, learning 0.175s)
             Mean action noise std: 4.16
          Mean value_function loss: 101.8921
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.3539
                       Mean reward: 486.27
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 100.4649
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.04s
                      Time elapsed: 00:17:46
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1073/1500 [0m                     

                       Computation: 116715 steps/s (collection: 0.745s, learning 0.097s)
             Mean action noise std: 4.16
          Mean value_function loss: 101.6078
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 21.3638
                       Mean reward: 490.53
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 95.4720
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.84s
                      Time elapsed: 00:17:47
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1074/1500 [0m                     

                       Computation: 118073 steps/s (collection: 0.736s, learning 0.096s)
             Mean action noise std: 4.16
          Mean value_function loss: 96.7832
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.3657
                       Mean reward: 523.71
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 97.4989
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.83s
                      Time elapsed: 00:17:48
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1075/1500 [0m                     

                       Computation: 114159 steps/s (collection: 0.773s, learning 0.088s)
             Mean action noise std: 4.17
          Mean value_function loss: 100.3281
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.3665
                       Mean reward: 505.59
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 100.0974
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.86s
                      Time elapsed: 00:17:49
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1076/1500 [0m                     

                       Computation: 105000 steps/s (collection: 0.828s, learning 0.108s)
             Mean action noise std: 4.17
          Mean value_function loss: 99.3146
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.3700
                       Mean reward: 457.69
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 98.5207
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.94s
                      Time elapsed: 00:17:50
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1077/1500 [0m                     

                       Computation: 114829 steps/s (collection: 0.756s, learning 0.101s)
             Mean action noise std: 4.17
          Mean value_function loss: 99.2150
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.3741
                       Mean reward: 465.01
               Mean episode length: 234.95
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 98.3452
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.86s
                      Time elapsed: 00:17:51
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1078/1500 [0m                     

                       Computation: 114579 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 4.18
          Mean value_function loss: 88.6238
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.3792
                       Mean reward: 483.74
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 101.0081
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.86s
                      Time elapsed: 00:17:52
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1079/1500 [0m                     

                       Computation: 118801 steps/s (collection: 0.720s, learning 0.108s)
             Mean action noise std: 4.18
          Mean value_function loss: 81.4144
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.3863
                       Mean reward: 482.18
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 100.0143
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.83s
                      Time elapsed: 00:17:52
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1080/1500 [0m                     

                       Computation: 120407 steps/s (collection: 0.714s, learning 0.102s)
             Mean action noise std: 4.18
          Mean value_function loss: 92.8573
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.3894
                       Mean reward: 511.48
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 99.7275
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.82s
                      Time elapsed: 00:17:53
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1081/1500 [0m                     

                       Computation: 91808 steps/s (collection: 0.927s, learning 0.144s)
             Mean action noise std: 4.19
          Mean value_function loss: 104.5006
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.3951
                       Mean reward: 544.45
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 100.6138
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.07s
                      Time elapsed: 00:17:54
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1082/1500 [0m                     

                       Computation: 98479 steps/s (collection: 0.867s, learning 0.131s)
             Mean action noise std: 4.19
          Mean value_function loss: 101.8677
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.3999
                       Mean reward: 500.41
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 97.6118
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.00s
                      Time elapsed: 00:17:55
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1083/1500 [0m                     

                       Computation: 113659 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 4.20
          Mean value_function loss: 94.3870
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.4089
                       Mean reward: 465.98
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 99.1681
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.86s
                      Time elapsed: 00:17:56
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1084/1500 [0m                     

                       Computation: 116276 steps/s (collection: 0.751s, learning 0.095s)
             Mean action noise std: 4.20
          Mean value_function loss: 85.1921
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.4212
                       Mean reward: 461.47
               Mean episode length: 242.31
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 99.2918
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.85s
                      Time elapsed: 00:17:57
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1085/1500 [0m                     

                       Computation: 118726 steps/s (collection: 0.736s, learning 0.092s)
             Mean action noise std: 4.20
          Mean value_function loss: 91.7411
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 21.4275
                       Mean reward: 454.32
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.3036
    Episode_Reward/rotating_object: 97.8800
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.83s
                      Time elapsed: 00:17:58
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1086/1500 [0m                     

                       Computation: 116710 steps/s (collection: 0.746s, learning 0.097s)
             Mean action noise std: 4.21
          Mean value_function loss: 96.1295
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.4310
                       Mean reward: 475.60
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.3081
    Episode_Reward/rotating_object: 96.6518
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.84s
                      Time elapsed: 00:17:59
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1087/1500 [0m                     

                       Computation: 117074 steps/s (collection: 0.730s, learning 0.109s)
             Mean action noise std: 4.21
          Mean value_function loss: 96.1696
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.4363
                       Mean reward: 496.19
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.3103
    Episode_Reward/rotating_object: 99.4955
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.84s
                      Time elapsed: 00:18:00
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1088/1500 [0m                     

                       Computation: 117308 steps/s (collection: 0.732s, learning 0.106s)
             Mean action noise std: 4.22
          Mean value_function loss: 90.9959
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.4405
                       Mean reward: 509.68
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 99.2890
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.84s
                      Time elapsed: 00:18:00
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1089/1500 [0m                     

                       Computation: 117747 steps/s (collection: 0.740s, learning 0.095s)
             Mean action noise std: 4.22
          Mean value_function loss: 87.6658
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.4477
                       Mean reward: 502.26
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 98.8887
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.83s
                      Time elapsed: 00:18:01
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1090/1500 [0m                     

                       Computation: 119373 steps/s (collection: 0.735s, learning 0.088s)
             Mean action noise std: 4.23
          Mean value_function loss: 101.4066
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.4567
                       Mean reward: 527.09
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 98.7044
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.82s
                      Time elapsed: 00:18:02
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1091/1500 [0m                     

                       Computation: 119542 steps/s (collection: 0.731s, learning 0.091s)
             Mean action noise std: 4.23
          Mean value_function loss: 86.0547
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.4594
                       Mean reward: 490.30
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 98.0020
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.82s
                      Time elapsed: 00:18:03
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1092/1500 [0m                     

                       Computation: 118456 steps/s (collection: 0.741s, learning 0.089s)
             Mean action noise std: 4.23
          Mean value_function loss: 101.2626
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.4620
                       Mean reward: 540.11
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.3158
    Episode_Reward/rotating_object: 100.2460
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.83s
                      Time elapsed: 00:18:04
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1093/1500 [0m                     

                       Computation: 119390 steps/s (collection: 0.730s, learning 0.094s)
             Mean action noise std: 4.23
          Mean value_function loss: 108.1578
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.4616
                       Mean reward: 490.96
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 95.6504
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.82s
                      Time elapsed: 00:18:05
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1094/1500 [0m                     

                       Computation: 118385 steps/s (collection: 0.741s, learning 0.090s)
             Mean action noise std: 4.24
          Mean value_function loss: 96.4363
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.4677
                       Mean reward: 513.14
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.3077
    Episode_Reward/rotating_object: 99.7775
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.83s
                      Time elapsed: 00:18:05
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1095/1500 [0m                     

                       Computation: 119756 steps/s (collection: 0.731s, learning 0.090s)
             Mean action noise std: 4.24
          Mean value_function loss: 96.3148
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 21.4702
                       Mean reward: 505.32
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 95.7973
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.82s
                      Time elapsed: 00:18:06
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1096/1500 [0m                     

                       Computation: 115342 steps/s (collection: 0.744s, learning 0.109s)
             Mean action noise std: 4.24
          Mean value_function loss: 101.5998
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.4707
                       Mean reward: 526.64
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 100.9071
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.85s
                      Time elapsed: 00:18:07
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1097/1500 [0m                     

                       Computation: 115393 steps/s (collection: 0.741s, learning 0.111s)
             Mean action noise std: 4.25
          Mean value_function loss: 88.2887
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.4784
                       Mean reward: 451.05
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 97.0517
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.85s
                      Time elapsed: 00:18:08
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1098/1500 [0m                     

                       Computation: 114391 steps/s (collection: 0.751s, learning 0.109s)
             Mean action noise std: 4.25
          Mean value_function loss: 98.8403
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.4827
                       Mean reward: 532.92
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 105.5032
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.86s
                      Time elapsed: 00:18:09
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1099/1500 [0m                     

                       Computation: 117939 steps/s (collection: 0.721s, learning 0.112s)
             Mean action noise std: 4.25
          Mean value_function loss: 97.2542
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.4896
                       Mean reward: 503.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 97.6075
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.83s
                      Time elapsed: 00:18:10
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1100/1500 [0m                     

                       Computation: 116851 steps/s (collection: 0.728s, learning 0.113s)
             Mean action noise std: 4.25
          Mean value_function loss: 87.6358
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.4917
                       Mean reward: 530.21
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 103.5756
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.84s
                      Time elapsed: 00:18:10
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1101/1500 [0m                     

                       Computation: 116027 steps/s (collection: 0.740s, learning 0.107s)
             Mean action noise std: 4.25
          Mean value_function loss: 88.6210
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.4946
                       Mean reward: 493.73
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 95.7862
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.85s
                      Time elapsed: 00:18:11
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1102/1500 [0m                     

                       Computation: 114290 steps/s (collection: 0.755s, learning 0.105s)
             Mean action noise std: 4.26
          Mean value_function loss: 98.0541
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.4989
                       Mean reward: 512.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3190
    Episode_Reward/rotating_object: 101.3230
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.86s
                      Time elapsed: 00:18:12
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1103/1500 [0m                     

                       Computation: 115513 steps/s (collection: 0.745s, learning 0.106s)
             Mean action noise std: 4.26
          Mean value_function loss: 96.1835
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.5026
                       Mean reward: 511.02
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 104.3342
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.85s
                      Time elapsed: 00:18:13
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1104/1500 [0m                     

                       Computation: 117699 steps/s (collection: 0.736s, learning 0.099s)
             Mean action noise std: 4.27
          Mean value_function loss: 96.7735
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.5121
                       Mean reward: 527.93
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 102.6441
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.84s
                      Time elapsed: 00:18:14
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1105/1500 [0m                     

                       Computation: 118288 steps/s (collection: 0.725s, learning 0.106s)
             Mean action noise std: 4.27
          Mean value_function loss: 86.4383
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.5250
                       Mean reward: 499.23
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 100.7735
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.83s
                      Time elapsed: 00:18:15
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1106/1500 [0m                     

                       Computation: 118047 steps/s (collection: 0.733s, learning 0.100s)
             Mean action noise std: 4.28
          Mean value_function loss: 96.1357
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.5379
                       Mean reward: 462.21
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 103.5733
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.83s
                      Time elapsed: 00:18:16
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1107/1500 [0m                     

                       Computation: 120770 steps/s (collection: 0.719s, learning 0.095s)
             Mean action noise std: 4.28
          Mean value_function loss: 91.2950
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 21.5475
                       Mean reward: 477.87
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.3110
    Episode_Reward/rotating_object: 95.4898
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.81s
                      Time elapsed: 00:18:16
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1108/1500 [0m                     

                       Computation: 121151 steps/s (collection: 0.723s, learning 0.088s)
             Mean action noise std: 4.28
          Mean value_function loss: 93.5011
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.5524
                       Mean reward: 486.70
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 99.6141
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.81s
                      Time elapsed: 00:18:17
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1109/1500 [0m                     

                       Computation: 115008 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 4.29
          Mean value_function loss: 88.1741
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.5576
                       Mean reward: 521.99
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 101.6934
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.85s
                      Time elapsed: 00:18:18
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1110/1500 [0m                     

                       Computation: 117668 steps/s (collection: 0.733s, learning 0.102s)
             Mean action noise std: 4.29
          Mean value_function loss: 89.5143
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.5635
                       Mean reward: 520.77
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3183
    Episode_Reward/rotating_object: 100.2028
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.84s
                      Time elapsed: 00:18:19
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1111/1500 [0m                     

                       Computation: 120054 steps/s (collection: 0.728s, learning 0.091s)
             Mean action noise std: 4.30
          Mean value_function loss: 84.1340
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.5704
                       Mean reward: 481.49
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 102.7232
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.82s
                      Time elapsed: 00:18:20
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1112/1500 [0m                     

                       Computation: 120622 steps/s (collection: 0.725s, learning 0.090s)
             Mean action noise std: 4.30
          Mean value_function loss: 90.8036
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 21.5827
                       Mean reward: 531.40
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.3273
    Episode_Reward/rotating_object: 104.2528
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.81s
                      Time elapsed: 00:18:20
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1113/1500 [0m                     

                       Computation: 121020 steps/s (collection: 0.721s, learning 0.091s)
             Mean action noise std: 4.31
          Mean value_function loss: 99.1224
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.5919
                       Mean reward: 506.14
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 102.3384
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.81s
                      Time elapsed: 00:18:21
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1114/1500 [0m                     

                       Computation: 119366 steps/s (collection: 0.738s, learning 0.085s)
             Mean action noise std: 4.31
          Mean value_function loss: 94.0908
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.5990
                       Mean reward: 473.03
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 101.9159
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.82s
                      Time elapsed: 00:18:22
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1115/1500 [0m                     

                       Computation: 119704 steps/s (collection: 0.732s, learning 0.090s)
             Mean action noise std: 4.31
          Mean value_function loss: 81.4225
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 21.6020
                       Mean reward: 543.85
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 101.5374
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.82s
                      Time elapsed: 00:18:23
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1116/1500 [0m                     

                       Computation: 114704 steps/s (collection: 0.762s, learning 0.095s)
             Mean action noise std: 4.32
          Mean value_function loss: 94.1319
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.6137
                       Mean reward: 529.64
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 103.7125
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.86s
                      Time elapsed: 00:18:24
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1117/1500 [0m                     

                       Computation: 113777 steps/s (collection: 0.771s, learning 0.093s)
             Mean action noise std: 4.32
          Mean value_function loss: 93.0194
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.6192
                       Mean reward: 546.45
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 105.7259
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.86s
                      Time elapsed: 00:18:25
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1118/1500 [0m                     

                       Computation: 111266 steps/s (collection: 0.762s, learning 0.121s)
             Mean action noise std: 4.32
          Mean value_function loss: 81.2576
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 21.6216
                       Mean reward: 535.40
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 104.2797
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.88s
                      Time elapsed: 00:18:26
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1119/1500 [0m                     

                       Computation: 112768 steps/s (collection: 0.775s, learning 0.097s)
             Mean action noise std: 4.33
          Mean value_function loss: 89.6627
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.6237
                       Mean reward: 514.69
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 98.4260
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.87s
                      Time elapsed: 00:18:26
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1120/1500 [0m                     

                       Computation: 116888 steps/s (collection: 0.744s, learning 0.097s)
             Mean action noise std: 4.33
          Mean value_function loss: 82.6513
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.6350
                       Mean reward: 484.81
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 102.2631
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.84s
                      Time elapsed: 00:18:27
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1121/1500 [0m                     

                       Computation: 118808 steps/s (collection: 0.739s, learning 0.088s)
             Mean action noise std: 4.34
          Mean value_function loss: 87.9119
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.6532
                       Mean reward: 491.66
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 102.6054
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.83s
                      Time elapsed: 00:18:28
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1122/1500 [0m                     

                       Computation: 120788 steps/s (collection: 0.724s, learning 0.090s)
             Mean action noise std: 4.34
          Mean value_function loss: 91.1601
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.6626
                       Mean reward: 471.73
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 0.3084
    Episode_Reward/rotating_object: 96.7926
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.81s
                      Time elapsed: 00:18:29
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1123/1500 [0m                     

                       Computation: 119985 steps/s (collection: 0.727s, learning 0.092s)
             Mean action noise std: 4.35
          Mean value_function loss: 94.1021
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.6603
                       Mean reward: 514.20
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 101.9067
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.82s
                      Time elapsed: 00:18:30
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1124/1500 [0m                     

                       Computation: 118800 steps/s (collection: 0.724s, learning 0.104s)
             Mean action noise std: 4.35
          Mean value_function loss: 91.4896
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.6678
                       Mean reward: 535.88
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 104.2014
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.83s
                      Time elapsed: 00:18:31
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1125/1500 [0m                     

                       Computation: 117142 steps/s (collection: 0.742s, learning 0.097s)
             Mean action noise std: 4.36
          Mean value_function loss: 97.0632
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.6771
                       Mean reward: 536.40
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 100.6374
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.84s
                      Time elapsed: 00:18:31
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1126/1500 [0m                     

                       Computation: 120569 steps/s (collection: 0.727s, learning 0.088s)
             Mean action noise std: 4.36
          Mean value_function loss: 90.8365
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.6872
                       Mean reward: 508.11
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 104.1320
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.82s
                      Time elapsed: 00:18:32
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1127/1500 [0m                     

                       Computation: 118112 steps/s (collection: 0.739s, learning 0.094s)
             Mean action noise std: 4.36
          Mean value_function loss: 92.4204
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.6923
                       Mean reward: 524.26
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 99.5747
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.83s
                      Time elapsed: 00:18:33
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1128/1500 [0m                     

                       Computation: 119639 steps/s (collection: 0.729s, learning 0.093s)
             Mean action noise std: 4.36
          Mean value_function loss: 79.5710
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.6924
                       Mean reward: 478.75
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 100.3037
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.82s
                      Time elapsed: 00:18:34
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1129/1500 [0m                     

                       Computation: 118737 steps/s (collection: 0.727s, learning 0.101s)
             Mean action noise std: 4.37
          Mean value_function loss: 89.9427
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.6911
                       Mean reward: 478.85
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 0.3251
    Episode_Reward/rotating_object: 99.8634
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.83s
                      Time elapsed: 00:18:35
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1130/1500 [0m                     

                       Computation: 117422 steps/s (collection: 0.745s, learning 0.093s)
             Mean action noise std: 4.37
          Mean value_function loss: 101.9367
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.6914
                       Mean reward: 521.29
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.3248
    Episode_Reward/rotating_object: 102.4697
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.84s
                      Time elapsed: 00:18:36
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1131/1500 [0m                     

                       Computation: 120556 steps/s (collection: 0.726s, learning 0.089s)
             Mean action noise std: 4.37
          Mean value_function loss: 89.0124
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.6964
                       Mean reward: 546.19
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.3270
    Episode_Reward/rotating_object: 103.0328
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.82s
                      Time elapsed: 00:18:36
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1132/1500 [0m                     

                       Computation: 119963 steps/s (collection: 0.726s, learning 0.094s)
             Mean action noise std: 4.38
          Mean value_function loss: 80.5202
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.6972
                       Mean reward: 498.69
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.3225
    Episode_Reward/rotating_object: 97.2437
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.82s
                      Time elapsed: 00:18:37
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1133/1500 [0m                     

                       Computation: 114592 steps/s (collection: 0.747s, learning 0.111s)
             Mean action noise std: 4.38
          Mean value_function loss: 96.7027
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.7010
                       Mean reward: 523.01
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 98.9857
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.86s
                      Time elapsed: 00:18:38
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1134/1500 [0m                     

                       Computation: 109182 steps/s (collection: 0.789s, learning 0.111s)
             Mean action noise std: 4.38
          Mean value_function loss: 100.0395
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.7094
                       Mean reward: 509.31
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.3265
    Episode_Reward/rotating_object: 100.3568
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.90s
                      Time elapsed: 00:18:39
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1135/1500 [0m                     

                       Computation: 116483 steps/s (collection: 0.737s, learning 0.107s)
             Mean action noise std: 4.39
          Mean value_function loss: 93.9095
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.7143
                       Mean reward: 520.84
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.3227
    Episode_Reward/rotating_object: 103.5206
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.84s
                      Time elapsed: 00:18:40
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1136/1500 [0m                     

                       Computation: 114338 steps/s (collection: 0.751s, learning 0.109s)
             Mean action noise std: 4.39
          Mean value_function loss: 101.1923
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.7140
                       Mean reward: 552.85
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.3289
    Episode_Reward/rotating_object: 107.1031
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.86s
                      Time elapsed: 00:18:41
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1137/1500 [0m                     

                       Computation: 119707 steps/s (collection: 0.724s, learning 0.098s)
             Mean action noise std: 4.39
          Mean value_function loss: 97.8957
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.7193
                       Mean reward: 504.76
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 103.4461
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.82s
                      Time elapsed: 00:18:41
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1138/1500 [0m                     

                       Computation: 115334 steps/s (collection: 0.726s, learning 0.126s)
             Mean action noise std: 4.40
          Mean value_function loss: 89.7418
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.7282
                       Mean reward: 499.24
               Mean episode length: 246.15
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 100.2457
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.85s
                      Time elapsed: 00:18:42
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1139/1500 [0m                     

                       Computation: 115395 steps/s (collection: 0.745s, learning 0.107s)
             Mean action noise std: 4.40
          Mean value_function loss: 85.5798
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.7366
                       Mean reward: 467.49
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 98.3718
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.85s
                      Time elapsed: 00:18:43
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1140/1500 [0m                     

                       Computation: 115256 steps/s (collection: 0.747s, learning 0.106s)
             Mean action noise std: 4.41
          Mean value_function loss: 82.2959
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.7517
                       Mean reward: 504.60
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.3232
    Episode_Reward/rotating_object: 101.6435
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.85s
                      Time elapsed: 00:18:44
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1141/1500 [0m                     

                       Computation: 117010 steps/s (collection: 0.740s, learning 0.100s)
             Mean action noise std: 4.41
          Mean value_function loss: 85.3559
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.7583
                       Mean reward: 543.60
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 105.5435
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.84s
                      Time elapsed: 00:18:45
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1142/1500 [0m                     

                       Computation: 117337 steps/s (collection: 0.744s, learning 0.094s)
             Mean action noise std: 4.41
          Mean value_function loss: 87.6009
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.7600
                       Mean reward: 515.85
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.3264
    Episode_Reward/rotating_object: 105.0275
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.84s
                      Time elapsed: 00:18:46
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1143/1500 [0m                     

                       Computation: 120362 steps/s (collection: 0.727s, learning 0.090s)
             Mean action noise std: 4.41
          Mean value_function loss: 86.4496
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.7622
                       Mean reward: 466.03
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.3212
    Episode_Reward/rotating_object: 102.2387
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.82s
                      Time elapsed: 00:18:46
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1144/1500 [0m                     

                       Computation: 113940 steps/s (collection: 0.752s, learning 0.111s)
             Mean action noise std: 4.42
          Mean value_function loss: 94.2626
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.7655
                       Mean reward: 526.79
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 101.0828
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.86s
                      Time elapsed: 00:18:47
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1145/1500 [0m                     

                       Computation: 118520 steps/s (collection: 0.736s, learning 0.093s)
             Mean action noise std: 4.43
          Mean value_function loss: 101.8089
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.7776
                       Mean reward: 531.92
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 100.1062
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.83s
                      Time elapsed: 00:18:48
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1146/1500 [0m                     

                       Computation: 114676 steps/s (collection: 0.766s, learning 0.092s)
             Mean action noise std: 4.43
          Mean value_function loss: 103.9731
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.7889
                       Mean reward: 467.28
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 97.1814
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.86s
                      Time elapsed: 00:18:49
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1147/1500 [0m                     

                       Computation: 117051 steps/s (collection: 0.743s, learning 0.097s)
             Mean action noise std: 4.43
          Mean value_function loss: 100.8598
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 21.7910
                       Mean reward: 515.83
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 0.3149
    Episode_Reward/rotating_object: 102.0605
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.84s
                      Time elapsed: 00:18:50
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1148/1500 [0m                     

                       Computation: 117072 steps/s (collection: 0.752s, learning 0.088s)
             Mean action noise std: 4.44
          Mean value_function loss: 107.5640
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.7961
                       Mean reward: 504.69
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.3174
    Episode_Reward/rotating_object: 98.3067
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.84s
                      Time elapsed: 00:18:51
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1149/1500 [0m                     

                       Computation: 118605 steps/s (collection: 0.732s, learning 0.097s)
             Mean action noise std: 4.44
          Mean value_function loss: 93.3439
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.8051
                       Mean reward: 481.09
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 0.3226
    Episode_Reward/rotating_object: 101.0424
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.83s
                      Time elapsed: 00:18:52
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1150/1500 [0m                     

                       Computation: 119436 steps/s (collection: 0.732s, learning 0.091s)
             Mean action noise std: 4.45
          Mean value_function loss: 104.5433
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.8181
                       Mean reward: 511.07
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 101.9451
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.82s
                      Time elapsed: 00:18:52
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1151/1500 [0m                     

                       Computation: 118237 steps/s (collection: 0.741s, learning 0.090s)
             Mean action noise std: 4.46
          Mean value_function loss: 93.2789
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.8359
                       Mean reward: 480.75
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 100.0215
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.83s
                      Time elapsed: 00:18:53
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1152/1500 [0m                     

                       Computation: 117888 steps/s (collection: 0.744s, learning 0.090s)
             Mean action noise std: 4.46
          Mean value_function loss: 96.5078
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.8469
                       Mean reward: 509.47
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 0.3207
    Episode_Reward/rotating_object: 99.5730
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.83s
                      Time elapsed: 00:18:54
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1153/1500 [0m                     

                       Computation: 116248 steps/s (collection: 0.748s, learning 0.098s)
             Mean action noise std: 4.46
          Mean value_function loss: 92.8616
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.8549
                       Mean reward: 520.00
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 0.3247
    Episode_Reward/rotating_object: 101.6575
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.85s
                      Time elapsed: 00:18:55
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1154/1500 [0m                     

                       Computation: 116455 steps/s (collection: 0.742s, learning 0.102s)
             Mean action noise std: 4.47
          Mean value_function loss: 88.4948
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.8625
                       Mean reward: 459.21
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 91.9255
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.84s
                      Time elapsed: 00:18:56
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1155/1500 [0m                     

                       Computation: 117509 steps/s (collection: 0.734s, learning 0.103s)
             Mean action noise std: 4.47
          Mean value_function loss: 105.5815
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.8692
                       Mean reward: 493.54
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 98.4461
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.84s
                      Time elapsed: 00:18:57
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1156/1500 [0m                     

                       Computation: 118755 steps/s (collection: 0.739s, learning 0.089s)
             Mean action noise std: 4.48
          Mean value_function loss: 99.3221
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.8710
                       Mean reward: 492.84
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 97.9839
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.83s
                      Time elapsed: 00:18:57
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1157/1500 [0m                     

                       Computation: 119838 steps/s (collection: 0.728s, learning 0.093s)
             Mean action noise std: 4.48
          Mean value_function loss: 102.0007
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.8707
                       Mean reward: 462.17
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 95.5645
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.82s
                      Time elapsed: 00:18:58
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1158/1500 [0m                     

                       Computation: 117861 steps/s (collection: 0.746s, learning 0.088s)
             Mean action noise std: 4.48
          Mean value_function loss: 98.6343
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.8759
                       Mean reward: 452.99
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 94.5342
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.83s
                      Time elapsed: 00:18:59
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1159/1500 [0m                     

                       Computation: 110860 steps/s (collection: 0.777s, learning 0.110s)
             Mean action noise std: 4.48
          Mean value_function loss: 93.0138
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.8828
                       Mean reward: 528.50
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.3238
    Episode_Reward/rotating_object: 100.2463
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.89s
                      Time elapsed: 00:19:00
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1160/1500 [0m                     

                       Computation: 117607 steps/s (collection: 0.743s, learning 0.093s)
             Mean action noise std: 4.49
          Mean value_function loss: 89.0234
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.8871
                       Mean reward: 454.90
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 97.9785
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.84s
                      Time elapsed: 00:19:01
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1161/1500 [0m                     

                       Computation: 119178 steps/s (collection: 0.724s, learning 0.101s)
             Mean action noise std: 4.49
          Mean value_function loss: 90.4677
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.8965
                       Mean reward: 493.18
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 100.1529
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.82s
                      Time elapsed: 00:19:02
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1162/1500 [0m                     

                       Computation: 118894 steps/s (collection: 0.738s, learning 0.089s)
             Mean action noise std: 4.50
          Mean value_function loss: 86.7927
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.9125
                       Mean reward: 487.90
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 96.0209
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.83s
                      Time elapsed: 00:19:02
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1163/1500 [0m                     

                       Computation: 115550 steps/s (collection: 0.748s, learning 0.103s)
             Mean action noise std: 4.51
          Mean value_function loss: 83.1289
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.9180
                       Mean reward: 477.90
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 100.2318
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.85s
                      Time elapsed: 00:19:03
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1164/1500 [0m                     

                       Computation: 116977 steps/s (collection: 0.748s, learning 0.092s)
             Mean action noise std: 4.51
          Mean value_function loss: 84.1680
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.9289
                       Mean reward: 476.91
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 98.2891
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.84s
                      Time elapsed: 00:19:04
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1165/1500 [0m                     

                       Computation: 118097 steps/s (collection: 0.741s, learning 0.091s)
             Mean action noise std: 4.52
          Mean value_function loss: 99.6837
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.9388
                       Mean reward: 519.16
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 102.6272
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.83s
                      Time elapsed: 00:19:05
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1166/1500 [0m                     

                       Computation: 119755 steps/s (collection: 0.724s, learning 0.097s)
             Mean action noise std: 4.52
          Mean value_function loss: 100.5257
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.9425
                       Mean reward: 495.21
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 102.2090
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.82s
                      Time elapsed: 00:19:06
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1167/1500 [0m                     

                       Computation: 116451 steps/s (collection: 0.750s, learning 0.094s)
             Mean action noise std: 4.53
          Mean value_function loss: 93.3393
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.9456
                       Mean reward: 494.44
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 99.3731
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.84s
                      Time elapsed: 00:19:07
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1168/1500 [0m                     

                       Computation: 116039 steps/s (collection: 0.748s, learning 0.100s)
             Mean action noise std: 4.53
          Mean value_function loss: 95.1941
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.9557
                       Mean reward: 487.98
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 0.3236
    Episode_Reward/rotating_object: 99.5288
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.85s
                      Time elapsed: 00:19:07
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1169/1500 [0m                     

                       Computation: 115478 steps/s (collection: 0.745s, learning 0.106s)
             Mean action noise std: 4.53
          Mean value_function loss: 96.5886
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.9630
                       Mean reward: 500.47
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 102.4399
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.85s
                      Time elapsed: 00:19:08
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1170/1500 [0m                     

                       Computation: 113690 steps/s (collection: 0.759s, learning 0.106s)
             Mean action noise std: 4.54
          Mean value_function loss: 89.0573
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.9722
                       Mean reward: 521.66
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.3257
    Episode_Reward/rotating_object: 101.3197
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.86s
                      Time elapsed: 00:19:09
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1171/1500 [0m                     

                       Computation: 115711 steps/s (collection: 0.752s, learning 0.097s)
             Mean action noise std: 4.54
          Mean value_function loss: 95.4862
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.9752
                       Mean reward: 505.49
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 105.0633
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 0.85s
                      Time elapsed: 00:19:10
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1172/1500 [0m                     

                       Computation: 117694 steps/s (collection: 0.736s, learning 0.099s)
             Mean action noise std: 4.54
          Mean value_function loss: 90.0018
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 21.9778
                       Mean reward: 463.00
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 96.7603
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.84s
                      Time elapsed: 00:19:11
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1173/1500 [0m                     

                       Computation: 111923 steps/s (collection: 0.765s, learning 0.114s)
             Mean action noise std: 4.55
          Mean value_function loss: 82.1049
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.9828
                       Mean reward: 505.50
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 99.7386
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.88s
                      Time elapsed: 00:19:12
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1174/1500 [0m                     

                       Computation: 114835 steps/s (collection: 0.755s, learning 0.101s)
             Mean action noise std: 4.55
          Mean value_function loss: 90.1483
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.9907
                       Mean reward: 517.04
               Mean episode length: 242.89
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 93.0801
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.86s
                      Time elapsed: 00:19:13
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1175/1500 [0m                     

                       Computation: 116088 steps/s (collection: 0.737s, learning 0.110s)
             Mean action noise std: 4.56
          Mean value_function loss: 102.3741
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.9975
                       Mean reward: 514.03
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 107.2561
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.85s
                      Time elapsed: 00:19:13
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1176/1500 [0m                     

                       Computation: 112644 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 4.56
          Mean value_function loss: 102.5104
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 22.0040
                       Mean reward: 543.75
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.3243
    Episode_Reward/rotating_object: 104.3788
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.87s
                      Time elapsed: 00:19:14
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1177/1500 [0m                     

                       Computation: 116368 steps/s (collection: 0.738s, learning 0.107s)
             Mean action noise std: 4.56
          Mean value_function loss: 90.6124
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.0086
                       Mean reward: 507.13
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 103.0256
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.84s
                      Time elapsed: 00:19:15
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1178/1500 [0m                     

                       Computation: 115074 steps/s (collection: 0.757s, learning 0.097s)
             Mean action noise std: 4.56
          Mean value_function loss: 92.3557
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.0164
                       Mean reward: 522.96
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 105.1706
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.85s
                      Time elapsed: 00:19:16
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1179/1500 [0m                     

                       Computation: 114398 steps/s (collection: 0.754s, learning 0.105s)
             Mean action noise std: 4.57
          Mean value_function loss: 89.3056
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.0225
                       Mean reward: 500.74
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 103.1999
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.86s
                      Time elapsed: 00:19:17
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1180/1500 [0m                     

                       Computation: 118765 steps/s (collection: 0.733s, learning 0.094s)
             Mean action noise std: 4.57
          Mean value_function loss: 88.6435
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.0273
                       Mean reward: 523.77
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 105.1519
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.83s
                      Time elapsed: 00:19:18
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1181/1500 [0m                     

                       Computation: 116009 steps/s (collection: 0.756s, learning 0.091s)
             Mean action noise std: 4.58
          Mean value_function loss: 83.9630
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.0358
                       Mean reward: 540.18
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 103.5351
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.85s
                      Time elapsed: 00:19:19
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1182/1500 [0m                     

                       Computation: 116954 steps/s (collection: 0.745s, learning 0.096s)
             Mean action noise std: 4.58
          Mean value_function loss: 83.3890
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 22.0465
                       Mean reward: 462.65
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 98.9689
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.84s
                      Time elapsed: 00:19:19
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1183/1500 [0m                     

                       Computation: 115392 steps/s (collection: 0.743s, learning 0.109s)
             Mean action noise std: 4.59
          Mean value_function loss: 78.9152
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.0580
                       Mean reward: 485.72
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 101.0318
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.85s
                      Time elapsed: 00:19:20
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1184/1500 [0m                     

                       Computation: 111750 steps/s (collection: 0.781s, learning 0.099s)
             Mean action noise std: 4.59
          Mean value_function loss: 80.5840
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.0629
                       Mean reward: 541.95
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 108.8086
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.88s
                      Time elapsed: 00:19:21
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1185/1500 [0m                     

                       Computation: 121066 steps/s (collection: 0.721s, learning 0.091s)
             Mean action noise std: 4.59
          Mean value_function loss: 80.3679
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 22.0672
                       Mean reward: 543.53
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 104.0582
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.81s
                      Time elapsed: 00:19:22
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1186/1500 [0m                     

                       Computation: 115074 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 4.59
          Mean value_function loss: 84.1096
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.0675
                       Mean reward: 498.62
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 101.3109
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.85s
                      Time elapsed: 00:19:23
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1187/1500 [0m                     

                       Computation: 114714 steps/s (collection: 0.762s, learning 0.095s)
             Mean action noise std: 4.60
          Mean value_function loss: 94.8660
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.0757
                       Mean reward: 550.02
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.3280
    Episode_Reward/rotating_object: 103.7247
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.86s
                      Time elapsed: 00:19:24
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1188/1500 [0m                     

                       Computation: 115192 steps/s (collection: 0.758s, learning 0.095s)
             Mean action noise std: 4.61
          Mean value_function loss: 89.8208
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.0885
                       Mean reward: 506.14
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.3265
    Episode_Reward/rotating_object: 107.4568
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.85s
                      Time elapsed: 00:19:24
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1189/1500 [0m                     

                       Computation: 115595 steps/s (collection: 0.753s, learning 0.098s)
             Mean action noise std: 4.61
          Mean value_function loss: 94.8525
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.0949
                       Mean reward: 513.66
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 104.4209
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.85s
                      Time elapsed: 00:19:25
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1190/1500 [0m                     

                       Computation: 115699 steps/s (collection: 0.756s, learning 0.094s)
             Mean action noise std: 4.61
          Mean value_function loss: 87.1719
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.0898
                       Mean reward: 541.08
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 0.3166
    Episode_Reward/rotating_object: 106.0319
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.85s
                      Time elapsed: 00:19:26
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1191/1500 [0m                     

                       Computation: 115401 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 4.61
          Mean value_function loss: 87.7572
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 22.0877
                       Mean reward: 567.10
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 106.9362
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.85s
                      Time elapsed: 00:19:27
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1192/1500 [0m                     

                       Computation: 113891 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 4.61
          Mean value_function loss: 80.9299
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.0864
                       Mean reward: 563.60
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.3239
    Episode_Reward/rotating_object: 108.3954
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.86s
                      Time elapsed: 00:19:28
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1193/1500 [0m                     

                       Computation: 116944 steps/s (collection: 0.744s, learning 0.096s)
             Mean action noise std: 4.62
          Mean value_function loss: 76.2356
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 22.0806
                       Mean reward: 506.45
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 105.3600
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.84s
                      Time elapsed: 00:19:29
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1194/1500 [0m                     

                       Computation: 115529 steps/s (collection: 0.750s, learning 0.101s)
             Mean action noise std: 4.62
          Mean value_function loss: 86.1031
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.0802
                       Mean reward: 539.79
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 104.4358
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 0.85s
                      Time elapsed: 00:19:30
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1195/1500 [0m                     

                       Computation: 119385 steps/s (collection: 0.732s, learning 0.091s)
             Mean action noise std: 4.62
          Mean value_function loss: 84.0302
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.0779
                       Mean reward: 507.24
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.3222
    Episode_Reward/rotating_object: 101.6407
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.82s
                      Time elapsed: 00:19:30
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1196/1500 [0m                     

                       Computation: 116190 steps/s (collection: 0.755s, learning 0.091s)
             Mean action noise std: 4.62
          Mean value_function loss: 88.1002
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.0784
                       Mean reward: 531.85
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 104.7212
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.85s
                      Time elapsed: 00:19:31
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1197/1500 [0m                     

                       Computation: 112632 steps/s (collection: 0.763s, learning 0.110s)
             Mean action noise std: 4.63
          Mean value_function loss: 94.2130
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.0837
                       Mean reward: 499.37
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 105.6723
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.87s
                      Time elapsed: 00:19:32
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1198/1500 [0m                     

                       Computation: 115532 steps/s (collection: 0.763s, learning 0.088s)
             Mean action noise std: 4.63
          Mean value_function loss: 80.4885
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 22.0908
                       Mean reward: 509.99
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3189
    Episode_Reward/rotating_object: 102.1640
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.85s
                      Time elapsed: 00:19:33
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1199/1500 [0m                     

                       Computation: 116134 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 4.63
          Mean value_function loss: 82.3549
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.0926
                       Mean reward: 556.52
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 0.3231
    Episode_Reward/rotating_object: 105.1639
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.85s
                      Time elapsed: 00:19:34
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1200/1500 [0m                     

                       Computation: 116641 steps/s (collection: 0.744s, learning 0.098s)
             Mean action noise std: 4.64
          Mean value_function loss: 85.7668
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.0981
                       Mean reward: 527.69
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.3244
    Episode_Reward/rotating_object: 103.2603
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.84s
                      Time elapsed: 00:19:35
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1201/1500 [0m                     

                       Computation: 117957 steps/s (collection: 0.739s, learning 0.094s)
             Mean action noise std: 4.64
          Mean value_function loss: 88.2511
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.1105
                       Mean reward: 546.60
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.3208
    Episode_Reward/rotating_object: 104.9252
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.83s
                      Time elapsed: 00:19:36
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1202/1500 [0m                     

                       Computation: 116058 steps/s (collection: 0.756s, learning 0.091s)
             Mean action noise std: 4.65
          Mean value_function loss: 85.1720
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.1234
                       Mean reward: 547.91
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 107.8302
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.85s
                      Time elapsed: 00:19:36
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1203/1500 [0m                     

                       Computation: 118149 steps/s (collection: 0.743s, learning 0.089s)
             Mean action noise std: 4.65
          Mean value_function loss: 87.0910
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.1302
                       Mean reward: 491.07
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.3242
    Episode_Reward/rotating_object: 104.5439
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.83s
                      Time elapsed: 00:19:37
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1204/1500 [0m                     

                       Computation: 109685 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 4.66
          Mean value_function loss: 85.7957
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.1392
                       Mean reward: 554.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3262
    Episode_Reward/rotating_object: 105.8398
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.90s
                      Time elapsed: 00:19:38
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1205/1500 [0m                     

                       Computation: 118323 steps/s (collection: 0.743s, learning 0.088s)
             Mean action noise std: 4.66
          Mean value_function loss: 82.5704
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.1526
                       Mean reward: 535.09
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.3215
    Episode_Reward/rotating_object: 107.1819
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.83s
                      Time elapsed: 00:19:39
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1206/1500 [0m                     

                       Computation: 114674 steps/s (collection: 0.751s, learning 0.107s)
             Mean action noise std: 4.67
          Mean value_function loss: 79.7588
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.1595
                       Mean reward: 535.72
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.3255
    Episode_Reward/rotating_object: 107.8330
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.86s
                      Time elapsed: 00:19:40
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1207/1500 [0m                     

                       Computation: 112698 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 4.67
          Mean value_function loss: 97.8887
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.1605
                       Mean reward: 546.44
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.3249
    Episode_Reward/rotating_object: 104.6914
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.87s
                      Time elapsed: 00:19:41
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1208/1500 [0m                     

                       Computation: 116416 steps/s (collection: 0.750s, learning 0.095s)
             Mean action noise std: 4.67
          Mean value_function loss: 85.9705
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.1604
                       Mean reward: 533.69
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 102.9921
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.84s
                      Time elapsed: 00:19:41
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1209/1500 [0m                     

                       Computation: 115352 steps/s (collection: 0.742s, learning 0.111s)
             Mean action noise std: 4.68
          Mean value_function loss: 79.5653
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.1613
                       Mean reward: 516.95
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 106.5298
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.85s
                      Time elapsed: 00:19:42
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1210/1500 [0m                     

                       Computation: 115266 steps/s (collection: 0.744s, learning 0.109s)
             Mean action noise std: 4.68
          Mean value_function loss: 75.3896
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.1682
                       Mean reward: 515.39
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 104.7655
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.85s
                      Time elapsed: 00:19:43
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1211/1500 [0m                     

                       Computation: 115489 steps/s (collection: 0.742s, learning 0.109s)
             Mean action noise std: 4.69
          Mean value_function loss: 77.8255
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.1793
                       Mean reward: 529.06
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 106.3752
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.85s
                      Time elapsed: 00:19:44
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1212/1500 [0m                     

                       Computation: 112925 steps/s (collection: 0.769s, learning 0.101s)
             Mean action noise std: 4.69
          Mean value_function loss: 67.1418
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.1943
                       Mean reward: 494.01
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 106.8363
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 0.87s
                      Time elapsed: 00:19:45
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1213/1500 [0m                     

                       Computation: 115864 steps/s (collection: 0.739s, learning 0.110s)
             Mean action noise std: 4.70
          Mean value_function loss: 86.7173
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.2102
                       Mean reward: 532.07
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 104.6720
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.85s
                      Time elapsed: 00:19:46
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1214/1500 [0m                     

                       Computation: 115259 steps/s (collection: 0.749s, learning 0.104s)
             Mean action noise std: 4.70
          Mean value_function loss: 76.4424
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.2210
                       Mean reward: 498.67
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 102.6711
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.85s
                      Time elapsed: 00:19:47
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1215/1500 [0m                     

                       Computation: 118050 steps/s (collection: 0.739s, learning 0.094s)
             Mean action noise std: 4.71
          Mean value_function loss: 83.2758
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.2271
                       Mean reward: 524.75
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 104.7567
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.83s
                      Time elapsed: 00:19:47
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1216/1500 [0m                     

                       Computation: 118796 steps/s (collection: 0.728s, learning 0.100s)
             Mean action noise std: 4.71
          Mean value_function loss: 86.0766
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.2300
                       Mean reward: 549.43
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.3235
    Episode_Reward/rotating_object: 101.8705
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.83s
                      Time elapsed: 00:19:48
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1217/1500 [0m                     

                       Computation: 115454 steps/s (collection: 0.758s, learning 0.093s)
             Mean action noise std: 4.71
          Mean value_function loss: 80.5315
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.2344
                       Mean reward: 542.75
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.3223
    Episode_Reward/rotating_object: 108.9818
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 0.85s
                      Time elapsed: 00:19:49
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1218/1500 [0m                     

                       Computation: 117685 steps/s (collection: 0.745s, learning 0.090s)
             Mean action noise std: 4.72
          Mean value_function loss: 104.1280
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.2446
                       Mean reward: 515.53
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 105.3366
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 0.84s
                      Time elapsed: 00:19:50
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1219/1500 [0m                     

                       Computation: 116703 steps/s (collection: 0.748s, learning 0.095s)
             Mean action noise std: 4.72
          Mean value_function loss: 87.9589
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2525
                       Mean reward: 550.18
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.3218
    Episode_Reward/rotating_object: 107.2751
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.84s
                      Time elapsed: 00:19:51
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1220/1500 [0m                     

                       Computation: 116112 steps/s (collection: 0.749s, learning 0.098s)
             Mean action noise std: 4.73
          Mean value_function loss: 88.9006
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 22.2609
                       Mean reward: 556.13
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 109.8015
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.85s
                      Time elapsed: 00:19:52
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1221/1500 [0m                     

                       Computation: 118041 steps/s (collection: 0.731s, learning 0.102s)
             Mean action noise std: 4.73
          Mean value_function loss: 82.7215
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 22.2636
                       Mean reward: 551.68
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 108.5366
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.83s
                      Time elapsed: 00:19:52
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1222/1500 [0m                     

                       Computation: 117176 steps/s (collection: 0.739s, learning 0.100s)
             Mean action noise std: 4.73
          Mean value_function loss: 88.6298
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.2668
                       Mean reward: 546.33
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 105.8871
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 0.84s
                      Time elapsed: 00:19:53
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1223/1500 [0m                     

                       Computation: 116489 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 4.74
          Mean value_function loss: 91.8279
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.2699
                       Mean reward: 562.07
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 108.6515
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.84s
                      Time elapsed: 00:19:54
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1224/1500 [0m                     

                       Computation: 116437 steps/s (collection: 0.746s, learning 0.098s)
             Mean action noise std: 4.74
          Mean value_function loss: 74.7810
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.2750
                       Mean reward: 502.91
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 105.2825
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.84s
                      Time elapsed: 00:19:55
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1225/1500 [0m                     

                       Computation: 117045 steps/s (collection: 0.752s, learning 0.088s)
             Mean action noise std: 4.74
          Mean value_function loss: 70.5247
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.2835
                       Mean reward: 566.60
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 106.8548
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.84s
                      Time elapsed: 00:19:56
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1226/1500 [0m                     

                       Computation: 118872 steps/s (collection: 0.724s, learning 0.103s)
             Mean action noise std: 4.75
          Mean value_function loss: 86.7348
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 22.2920
                       Mean reward: 528.86
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 109.8947
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.83s
                      Time elapsed: 00:19:57
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1227/1500 [0m                     

                       Computation: 118242 steps/s (collection: 0.729s, learning 0.103s)
             Mean action noise std: 4.75
          Mean value_function loss: 80.0014
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.2929
                       Mean reward: 558.19
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 106.1633
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.83s
                      Time elapsed: 00:19:58
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1228/1500 [0m                     

                       Computation: 117043 steps/s (collection: 0.747s, learning 0.093s)
             Mean action noise std: 4.75
          Mean value_function loss: 81.8443
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2951
                       Mean reward: 490.03
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 105.3704
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.84s
                      Time elapsed: 00:19:58
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1229/1500 [0m                     

                       Computation: 118674 steps/s (collection: 0.738s, learning 0.091s)
             Mean action noise std: 4.76
          Mean value_function loss: 94.5783
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.2986
                       Mean reward: 552.09
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 107.5538
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.83s
                      Time elapsed: 00:19:59
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1230/1500 [0m                     

                       Computation: 117065 steps/s (collection: 0.744s, learning 0.096s)
             Mean action noise std: 4.76
          Mean value_function loss: 83.6597
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.3063
                       Mean reward: 515.03
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 106.6168
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.84s
                      Time elapsed: 00:20:00
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1231/1500 [0m                     

                       Computation: 113669 steps/s (collection: 0.763s, learning 0.102s)
             Mean action noise std: 4.77
          Mean value_function loss: 98.5952
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.3128
                       Mean reward: 552.96
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.3234
    Episode_Reward/rotating_object: 109.1585
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.86s
                      Time elapsed: 00:20:01
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1232/1500 [0m                     

                       Computation: 117215 steps/s (collection: 0.741s, learning 0.097s)
             Mean action noise std: 4.77
          Mean value_function loss: 81.9245
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.3264
                       Mean reward: 527.76
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 110.1891
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.84s
                      Time elapsed: 00:20:02
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1233/1500 [0m                     

                       Computation: 116017 steps/s (collection: 0.755s, learning 0.092s)
             Mean action noise std: 4.77
          Mean value_function loss: 84.8933
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 22.3338
                       Mean reward: 555.40
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 110.0866
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.85s
                      Time elapsed: 00:20:03
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1234/1500 [0m                     

                       Computation: 118042 steps/s (collection: 0.737s, learning 0.096s)
             Mean action noise std: 4.78
          Mean value_function loss: 86.6870
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.3371
                       Mean reward: 496.10
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3214
    Episode_Reward/rotating_object: 104.2170
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.83s
                      Time elapsed: 00:20:03
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1235/1500 [0m                     

                       Computation: 113309 steps/s (collection: 0.765s, learning 0.103s)
             Mean action noise std: 4.78
          Mean value_function loss: 74.1578
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.3418
                       Mean reward: 550.98
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 107.8816
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.87s
                      Time elapsed: 00:20:04
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1236/1500 [0m                     

                       Computation: 114659 steps/s (collection: 0.764s, learning 0.094s)
             Mean action noise std: 4.79
          Mean value_function loss: 80.6369
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.3497
                       Mean reward: 535.61
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 106.3245
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.86s
                      Time elapsed: 00:20:05
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1237/1500 [0m                     

                       Computation: 117917 steps/s (collection: 0.747s, learning 0.087s)
             Mean action noise std: 4.79
          Mean value_function loss: 81.6442
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.3599
                       Mean reward: 525.86
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3197
    Episode_Reward/rotating_object: 105.2402
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.83s
                      Time elapsed: 00:20:06
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1238/1500 [0m                     

                       Computation: 117273 steps/s (collection: 0.746s, learning 0.093s)
             Mean action noise std: 4.80
          Mean value_function loss: 87.0899
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.3662
                       Mean reward: 539.66
               Mean episode length: 247.63
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 106.5147
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.84s
                      Time elapsed: 00:20:07
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1239/1500 [0m                     

                       Computation: 115706 steps/s (collection: 0.748s, learning 0.102s)
             Mean action noise std: 4.80
          Mean value_function loss: 83.9061
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.3702
                       Mean reward: 539.21
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 106.9002
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.85s
                      Time elapsed: 00:20:08
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1240/1500 [0m                     

                       Computation: 113898 steps/s (collection: 0.762s, learning 0.101s)
             Mean action noise std: 4.81
          Mean value_function loss: 85.6210
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.3846
                       Mean reward: 533.99
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 107.9421
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.86s
                      Time elapsed: 00:20:09
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1241/1500 [0m                     

                       Computation: 117280 steps/s (collection: 0.747s, learning 0.092s)
             Mean action noise std: 4.81
          Mean value_function loss: 85.2831
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.3881
                       Mean reward: 539.64
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.3219
    Episode_Reward/rotating_object: 106.7941
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.84s
                      Time elapsed: 00:20:09
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1242/1500 [0m                     

                       Computation: 117713 steps/s (collection: 0.749s, learning 0.087s)
             Mean action noise std: 4.81
          Mean value_function loss: 87.5645
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.3922
                       Mean reward: 509.25
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 106.8816
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.84s
                      Time elapsed: 00:20:10
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1243/1500 [0m                     

                       Computation: 114801 steps/s (collection: 0.753s, learning 0.104s)
             Mean action noise std: 4.82
          Mean value_function loss: 85.4254
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4041
                       Mean reward: 539.74
               Mean episode length: 238.40
    Episode_Reward/reaching_object: 0.3191
    Episode_Reward/rotating_object: 106.3640
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.86s
                      Time elapsed: 00:20:11
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1244/1500 [0m                     

                       Computation: 115982 steps/s (collection: 0.751s, learning 0.097s)
             Mean action noise std: 4.83
          Mean value_function loss: 88.3975
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.4146
                       Mean reward: 540.12
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 106.5601
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.85s
                      Time elapsed: 00:20:12
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1245/1500 [0m                     

                       Computation: 119915 steps/s (collection: 0.721s, learning 0.099s)
             Mean action noise std: 4.83
          Mean value_function loss: 79.3978
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.4214
                       Mean reward: 539.88
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.3202
    Episode_Reward/rotating_object: 110.1843
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.82s
                      Time elapsed: 00:20:13
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1246/1500 [0m                     

                       Computation: 114910 steps/s (collection: 0.743s, learning 0.112s)
             Mean action noise std: 4.83
          Mean value_function loss: 87.6723
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 22.4262
                       Mean reward: 575.48
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 110.4491
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.86s
                      Time elapsed: 00:20:14
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1247/1500 [0m                     

                       Computation: 116201 steps/s (collection: 0.739s, learning 0.107s)
             Mean action noise std: 4.83
          Mean value_function loss: 86.3727
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4298
                       Mean reward: 574.38
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.3138
    Episode_Reward/rotating_object: 107.3307
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.85s
                      Time elapsed: 00:20:14
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1248/1500 [0m                     

                       Computation: 114848 steps/s (collection: 0.765s, learning 0.091s)
             Mean action noise std: 4.84
          Mean value_function loss: 87.9437
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.4388
                       Mean reward: 544.27
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 110.8577
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.86s
                      Time elapsed: 00:20:15
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1249/1500 [0m                     

                       Computation: 116234 steps/s (collection: 0.740s, learning 0.106s)
             Mean action noise std: 4.84
          Mean value_function loss: 85.4683
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4489
                       Mean reward: 541.44
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.3176
    Episode_Reward/rotating_object: 109.9762
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.85s
                      Time elapsed: 00:20:16
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1250/1500 [0m                     

                       Computation: 115399 steps/s (collection: 0.750s, learning 0.102s)
             Mean action noise std: 4.85
          Mean value_function loss: 82.5621
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.4546
                       Mean reward: 530.68
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.3122
    Episode_Reward/rotating_object: 104.8282
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.85s
                      Time elapsed: 00:20:17
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1251/1500 [0m                     

                       Computation: 113539 steps/s (collection: 0.759s, learning 0.107s)
             Mean action noise std: 4.85
          Mean value_function loss: 85.6478
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.4594
                       Mean reward: 570.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 110.1164
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.87s
                      Time elapsed: 00:20:18
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1252/1500 [0m                     

                       Computation: 113668 steps/s (collection: 0.756s, learning 0.109s)
             Mean action noise std: 4.85
          Mean value_function loss: 90.6401
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.4619
                       Mean reward: 539.56
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 101.1555
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.86s
                      Time elapsed: 00:20:19
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1253/1500 [0m                     

                       Computation: 111641 steps/s (collection: 0.768s, learning 0.113s)
             Mean action noise std: 4.86
          Mean value_function loss: 100.2791
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4725
                       Mean reward: 546.88
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 107.7736
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.88s
                      Time elapsed: 00:20:20
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1254/1500 [0m                     

                       Computation: 116650 steps/s (collection: 0.742s, learning 0.101s)
             Mean action noise std: 4.86
          Mean value_function loss: 84.9058
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.4861
                       Mean reward: 555.25
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 108.0215
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.84s
                      Time elapsed: 00:20:20
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1255/1500 [0m                     

                       Computation: 116101 steps/s (collection: 0.743s, learning 0.104s)
             Mean action noise std: 4.87
          Mean value_function loss: 96.7514
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.4977
                       Mean reward: 588.65
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.3211
    Episode_Reward/rotating_object: 108.9808
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.85s
                      Time elapsed: 00:20:21
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1256/1500 [0m                     

                       Computation: 115089 steps/s (collection: 0.752s, learning 0.103s)
             Mean action noise std: 4.87
          Mean value_function loss: 85.2820
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.5012
                       Mean reward: 548.47
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 108.5779
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.85s
                      Time elapsed: 00:20:22
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1257/1500 [0m                     

                       Computation: 116194 steps/s (collection: 0.751s, learning 0.095s)
             Mean action noise std: 4.87
          Mean value_function loss: 78.1230
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 22.5035
                       Mean reward: 593.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 109.1195
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.85s
                      Time elapsed: 00:20:23
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1258/1500 [0m                     

                       Computation: 117543 steps/s (collection: 0.748s, learning 0.089s)
             Mean action noise std: 4.88
          Mean value_function loss: 104.6359
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.5138
                       Mean reward: 508.70
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 105.4564
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.84s
                      Time elapsed: 00:20:24
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1259/1500 [0m                     

                       Computation: 116576 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 4.89
          Mean value_function loss: 95.1989
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.5305
                       Mean reward: 581.09
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 106.8250
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.84s
                      Time elapsed: 00:20:25
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1260/1500 [0m                     

                       Computation: 115836 steps/s (collection: 0.753s, learning 0.095s)
             Mean action noise std: 4.90
          Mean value_function loss: 96.3251
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.5476
                       Mean reward: 543.26
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 108.8405
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.85s
                      Time elapsed: 00:20:25
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1261/1500 [0m                     

                       Computation: 117330 steps/s (collection: 0.749s, learning 0.089s)
             Mean action noise std: 4.90
          Mean value_function loss: 90.0330
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.5531
                       Mean reward: 500.58
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 103.4024
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.84s
                      Time elapsed: 00:20:26
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1262/1500 [0m                     

                       Computation: 118697 steps/s (collection: 0.732s, learning 0.097s)
             Mean action noise std: 4.90
          Mean value_function loss: 92.4597
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.5557
                       Mean reward: 558.30
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 107.6510
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.83s
                      Time elapsed: 00:20:27
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1263/1500 [0m                     

                       Computation: 112579 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 4.91
          Mean value_function loss: 97.0411
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.5637
                       Mean reward: 568.82
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.3133
    Episode_Reward/rotating_object: 108.3156
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.87s
                      Time elapsed: 00:20:28
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1264/1500 [0m                     

                       Computation: 115333 steps/s (collection: 0.760s, learning 0.093s)
             Mean action noise std: 4.91
          Mean value_function loss: 97.2968
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.5732
                       Mean reward: 551.47
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 105.6311
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.85s
                      Time elapsed: 00:20:29
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1265/1500 [0m                     

                       Computation: 118523 steps/s (collection: 0.739s, learning 0.091s)
             Mean action noise std: 4.91
          Mean value_function loss: 98.5902
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 22.5762
                       Mean reward: 521.00
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 106.9576
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.83s
                      Time elapsed: 00:20:30
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1266/1500 [0m                     

                       Computation: 116544 steps/s (collection: 0.750s, learning 0.093s)
             Mean action noise std: 4.91
          Mean value_function loss: 97.4885
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.5743
                       Mean reward: 502.40
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.3141
    Episode_Reward/rotating_object: 106.9159
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.84s
                      Time elapsed: 00:20:31
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1267/1500 [0m                     

                       Computation: 118629 steps/s (collection: 0.739s, learning 0.090s)
             Mean action noise std: 4.91
          Mean value_function loss: 86.7117
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5688
                       Mean reward: 465.27
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.3030
    Episode_Reward/rotating_object: 100.3451
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.83s
                      Time elapsed: 00:20:31
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1268/1500 [0m                     

                       Computation: 115960 steps/s (collection: 0.750s, learning 0.098s)
             Mean action noise std: 4.91
          Mean value_function loss: 86.9563
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.5652
                       Mean reward: 532.29
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 103.9960
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.85s
                      Time elapsed: 00:20:32
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1269/1500 [0m                     

                       Computation: 118971 steps/s (collection: 0.738s, learning 0.089s)
             Mean action noise std: 4.92
          Mean value_function loss: 83.1036
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.5713
                       Mean reward: 550.01
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 104.7779
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.83s
                      Time elapsed: 00:20:33
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1270/1500 [0m                     

                       Computation: 115163 steps/s (collection: 0.760s, learning 0.094s)
             Mean action noise std: 4.92
          Mean value_function loss: 88.6358
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.5747
                       Mean reward: 533.49
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 109.7995
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 0.85s
                      Time elapsed: 00:20:34
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1271/1500 [0m                     

                       Computation: 113573 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 4.92
          Mean value_function loss: 100.3663
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5751
                       Mean reward: 500.41
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 104.0905
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.87s
                      Time elapsed: 00:20:35
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1272/1500 [0m                     

                       Computation: 115561 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 4.93
          Mean value_function loss: 100.3354
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.5774
                       Mean reward: 490.16
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 103.9184
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.85s
                      Time elapsed: 00:20:36
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1273/1500 [0m                     

                       Computation: 118555 steps/s (collection: 0.728s, learning 0.101s)
             Mean action noise std: 4.93
          Mean value_function loss: 94.4605
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.5759
                       Mean reward: 529.18
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 103.2944
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.83s
                      Time elapsed: 00:20:36
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1274/1500 [0m                     

                       Computation: 112811 steps/s (collection: 0.773s, learning 0.098s)
             Mean action noise std: 4.93
          Mean value_function loss: 91.6524
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.5769
                       Mean reward: 519.81
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.3101
    Episode_Reward/rotating_object: 102.8877
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.87s
                      Time elapsed: 00:20:37
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1275/1500 [0m                     

                       Computation: 115350 steps/s (collection: 0.761s, learning 0.092s)
             Mean action noise std: 4.93
          Mean value_function loss: 91.4325
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.5863
                       Mean reward: 563.24
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 105.3009
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.85s
                      Time elapsed: 00:20:38
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1276/1500 [0m                     

                       Computation: 112437 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 4.94
          Mean value_function loss: 87.4968
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.5942
                       Mean reward: 545.47
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 107.1288
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.87s
                      Time elapsed: 00:20:39
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1277/1500 [0m                     

                       Computation: 119903 steps/s (collection: 0.730s, learning 0.090s)
             Mean action noise std: 4.95
          Mean value_function loss: 82.4554
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 22.6039
                       Mean reward: 534.56
               Mean episode length: 240.16
    Episode_Reward/reaching_object: 0.3186
    Episode_Reward/rotating_object: 112.4383
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 0.82s
                      Time elapsed: 00:20:40
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1278/1500 [0m                     

                       Computation: 115980 steps/s (collection: 0.756s, learning 0.092s)
             Mean action noise std: 4.95
          Mean value_function loss: 84.7372
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.6128
                       Mean reward: 503.44
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.3087
    Episode_Reward/rotating_object: 102.4839
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.85s
                      Time elapsed: 00:20:41
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1279/1500 [0m                     

                       Computation: 115397 steps/s (collection: 0.736s, learning 0.116s)
             Mean action noise std: 4.95
          Mean value_function loss: 76.8945
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.6159
                       Mean reward: 545.14
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 108.5153
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 0.85s
                      Time elapsed: 00:20:42
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1280/1500 [0m                     

                       Computation: 113001 steps/s (collection: 0.755s, learning 0.115s)
             Mean action noise std: 4.96
          Mean value_function loss: 69.1630
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.6184
                       Mean reward: 557.63
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3058
    Episode_Reward/rotating_object: 103.5746
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 0.87s
                      Time elapsed: 00:20:42
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1281/1500 [0m                     

                       Computation: 111594 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 4.96
          Mean value_function loss: 78.7214
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 22.6256
                       Mean reward: 553.57
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 109.5481
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.88s
                      Time elapsed: 00:20:43
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1282/1500 [0m                     

                       Computation: 112740 steps/s (collection: 0.765s, learning 0.107s)
             Mean action noise std: 4.96
          Mean value_function loss: 77.8435
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.6350
                       Mean reward: 503.43
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 0.3106
    Episode_Reward/rotating_object: 106.8674
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.87s
                      Time elapsed: 00:20:44
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1283/1500 [0m                     

                       Computation: 110280 steps/s (collection: 0.776s, learning 0.115s)
             Mean action noise std: 4.97
          Mean value_function loss: 82.7463
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.6415
                       Mean reward: 551.64
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 112.3127
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.89s
                      Time elapsed: 00:20:45
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1284/1500 [0m                     

                       Computation: 114922 steps/s (collection: 0.752s, learning 0.104s)
             Mean action noise std: 4.97
          Mean value_function loss: 77.7191
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 22.6511
                       Mean reward: 553.24
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 106.9403
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.86s
                      Time elapsed: 00:20:46
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1285/1500 [0m                     

                       Computation: 117324 steps/s (collection: 0.749s, learning 0.089s)
             Mean action noise std: 4.98
          Mean value_function loss: 86.9057
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.6592
                       Mean reward: 570.91
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 109.5131
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 0.84s
                      Time elapsed: 00:20:47
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1286/1500 [0m                     

                       Computation: 113259 steps/s (collection: 0.753s, learning 0.115s)
             Mean action noise std: 4.98
          Mean value_function loss: 81.0966
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.6680
                       Mean reward: 566.33
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.3067
    Episode_Reward/rotating_object: 108.6977
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.87s
                      Time elapsed: 00:20:48
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1287/1500 [0m                     

                       Computation: 117675 steps/s (collection: 0.742s, learning 0.093s)
             Mean action noise std: 4.99
          Mean value_function loss: 78.6421
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.6783
                       Mean reward: 549.54
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 109.5343
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.84s
                      Time elapsed: 00:20:48
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1288/1500 [0m                     

                       Computation: 114465 steps/s (collection: 0.760s, learning 0.099s)
             Mean action noise std: 4.99
          Mean value_function loss: 84.5704
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.6847
                       Mean reward: 547.23
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 109.8638
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.86s
                      Time elapsed: 00:20:49
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1289/1500 [0m                     

                       Computation: 116981 steps/s (collection: 0.750s, learning 0.090s)
             Mean action noise std: 4.99
          Mean value_function loss: 78.5183
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.6864
                       Mean reward: 549.50
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 112.6663
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.84s
                      Time elapsed: 00:20:50
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1290/1500 [0m                     

                       Computation: 115468 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 4.99
          Mean value_function loss: 79.6893
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.6881
                       Mean reward: 518.19
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 0.3081
    Episode_Reward/rotating_object: 105.2376
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.85s
                      Time elapsed: 00:20:51
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1291/1500 [0m                     

                       Computation: 115741 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 5.00
          Mean value_function loss: 80.5003
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.6935
                       Mean reward: 560.56
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.3082
    Episode_Reward/rotating_object: 106.2293
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.85s
                      Time elapsed: 00:20:52
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1292/1500 [0m                     

                       Computation: 114464 steps/s (collection: 0.750s, learning 0.109s)
             Mean action noise std: 5.00
          Mean value_function loss: 72.7446
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.7017
                       Mean reward: 543.14
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 109.4651
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.86s
                      Time elapsed: 00:20:53
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1293/1500 [0m                     

                       Computation: 118753 steps/s (collection: 0.732s, learning 0.096s)
             Mean action noise std: 5.01
          Mean value_function loss: 84.4425
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.7091
                       Mean reward: 511.32
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 105.7638
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.83s
                      Time elapsed: 00:20:54
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1294/1500 [0m                     

                       Computation: 113858 steps/s (collection: 0.761s, learning 0.103s)
             Mean action noise std: 5.01
          Mean value_function loss: 84.5674
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.7151
                       Mean reward: 576.94
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 108.4619
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.86s
                      Time elapsed: 00:20:54
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1295/1500 [0m                     

                       Computation: 115396 steps/s (collection: 0.743s, learning 0.109s)
             Mean action noise std: 5.02
          Mean value_function loss: 84.5157
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.7266
                       Mean reward: 554.19
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 108.6296
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 0.85s
                      Time elapsed: 00:20:55
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1296/1500 [0m                     

                       Computation: 117116 steps/s (collection: 0.748s, learning 0.091s)
             Mean action noise std: 5.02
          Mean value_function loss: 82.3122
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 22.7377
                       Mean reward: 553.78
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.3200
    Episode_Reward/rotating_object: 111.0242
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 0.84s
                      Time elapsed: 00:20:56
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1297/1500 [0m                     

                       Computation: 116764 steps/s (collection: 0.748s, learning 0.094s)
             Mean action noise std: 5.03
          Mean value_function loss: 87.3349
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.7442
                       Mean reward: 542.11
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 112.1007
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 0.84s
                      Time elapsed: 00:20:57
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1298/1500 [0m                     

                       Computation: 118114 steps/s (collection: 0.740s, learning 0.093s)
             Mean action noise std: 5.03
          Mean value_function loss: 72.7576
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.7461
                       Mean reward: 556.06
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.3068
    Episode_Reward/rotating_object: 107.6296
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 0.83s
                      Time elapsed: 00:20:58
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1299/1500 [0m                     

                       Computation: 114128 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 5.03
          Mean value_function loss: 76.2080
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7508
                       Mean reward: 510.54
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 105.0364
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 0.86s
                      Time elapsed: 00:20:59
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1300/1500 [0m                     

                       Computation: 115063 steps/s (collection: 0.759s, learning 0.095s)
             Mean action noise std: 5.04
          Mean value_function loss: 79.9028
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.7591
                       Mean reward: 556.18
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.3136
    Episode_Reward/rotating_object: 108.6824
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.85s
                      Time elapsed: 00:21:00
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1301/1500 [0m                     

                       Computation: 117186 steps/s (collection: 0.742s, learning 0.097s)
             Mean action noise std: 5.05
          Mean value_function loss: 80.4847
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.7748
                       Mean reward: 545.24
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.3139
    Episode_Reward/rotating_object: 111.0562
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 0.84s
                      Time elapsed: 00:21:00
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1302/1500 [0m                     

                       Computation: 115057 steps/s (collection: 0.760s, learning 0.094s)
             Mean action noise std: 5.05
          Mean value_function loss: 64.7237
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.7882
                       Mean reward: 506.02
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.3073
    Episode_Reward/rotating_object: 106.4891
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.85s
                      Time elapsed: 00:21:01
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1303/1500 [0m                     

                       Computation: 116359 steps/s (collection: 0.759s, learning 0.086s)
             Mean action noise std: 5.05
          Mean value_function loss: 81.6291
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.7919
                       Mean reward: 521.39
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.3080
    Episode_Reward/rotating_object: 109.2642
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.84s
                      Time elapsed: 00:21:02
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1304/1500 [0m                     

                       Computation: 116512 steps/s (collection: 0.753s, learning 0.091s)
             Mean action noise std: 5.06
          Mean value_function loss: 70.0291
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.7982
                       Mean reward: 552.35
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.3093
    Episode_Reward/rotating_object: 107.8076
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.84s
                      Time elapsed: 00:21:03
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1305/1500 [0m                     

                       Computation: 114927 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 5.06
          Mean value_function loss: 83.1518
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.7995
                       Mean reward: 583.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 110.3972
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.86s
                      Time elapsed: 00:21:04
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1306/1500 [0m                     

                       Computation: 117540 steps/s (collection: 0.746s, learning 0.090s)
             Mean action noise std: 5.07
          Mean value_function loss: 78.3669
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.8037
                       Mean reward: 500.84
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 0.3032
    Episode_Reward/rotating_object: 103.4572
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.84s
                      Time elapsed: 00:21:05
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1307/1500 [0m                     

                       Computation: 114403 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 5.07
          Mean value_function loss: 77.0627
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.8080
                       Mean reward: 551.42
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 109.8677
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.86s
                      Time elapsed: 00:21:05
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1308/1500 [0m                     

                       Computation: 116141 steps/s (collection: 0.744s, learning 0.102s)
             Mean action noise std: 5.08
          Mean value_function loss: 78.5300
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.8157
                       Mean reward: 528.80
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.3093
    Episode_Reward/rotating_object: 105.7792
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.85s
                      Time elapsed: 00:21:06
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1309/1500 [0m                     

                       Computation: 113463 steps/s (collection: 0.766s, learning 0.100s)
             Mean action noise std: 5.08
          Mean value_function loss: 87.4437
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.8207
                       Mean reward: 529.53
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 107.4896
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.87s
                      Time elapsed: 00:21:07
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1310/1500 [0m                     

                       Computation: 117470 steps/s (collection: 0.741s, learning 0.096s)
             Mean action noise std: 5.09
          Mean value_function loss: 79.7997
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.8308
                       Mean reward: 508.75
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 104.9835
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.84s
                      Time elapsed: 00:21:08
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1311/1500 [0m                     

                       Computation: 115536 steps/s (collection: 0.756s, learning 0.095s)
             Mean action noise std: 5.09
          Mean value_function loss: 86.5653
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.8370
                       Mean reward: 510.18
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 110.3981
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.85s
                      Time elapsed: 00:21:09
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1312/1500 [0m                     

                       Computation: 114402 steps/s (collection: 0.757s, learning 0.102s)
             Mean action noise std: 5.10
          Mean value_function loss: 82.2249
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.8435
                       Mean reward: 556.09
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.3111
    Episode_Reward/rotating_object: 107.4451
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.86s
                      Time elapsed: 00:21:10
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1313/1500 [0m                     

                       Computation: 117393 steps/s (collection: 0.744s, learning 0.094s)
             Mean action noise std: 5.10
          Mean value_function loss: 83.2909
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 22.8475
                       Mean reward: 555.02
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 110.4337
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.84s
                      Time elapsed: 00:21:11
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1314/1500 [0m                     

                       Computation: 114786 steps/s (collection: 0.757s, learning 0.100s)
             Mean action noise std: 5.11
          Mean value_function loss: 84.5795
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.8563
                       Mean reward: 540.20
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 110.5762
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.86s
                      Time elapsed: 00:21:11
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1315/1500 [0m                     

                       Computation: 115960 steps/s (collection: 0.749s, learning 0.099s)
             Mean action noise std: 5.11
          Mean value_function loss: 91.3183
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.8623
                       Mean reward: 571.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3203
    Episode_Reward/rotating_object: 112.2418
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.85s
                      Time elapsed: 00:21:12
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1316/1500 [0m                     

                       Computation: 116148 steps/s (collection: 0.735s, learning 0.112s)
             Mean action noise std: 5.12
          Mean value_function loss: 88.6114
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 22.8679
                       Mean reward: 532.53
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.3168
    Episode_Reward/rotating_object: 109.1123
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.85s
                      Time elapsed: 00:21:13
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1317/1500 [0m                     

                       Computation: 114592 steps/s (collection: 0.745s, learning 0.113s)
             Mean action noise std: 5.12
          Mean value_function loss: 93.6048
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.8718
                       Mean reward: 552.91
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 107.9899
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.86s
                      Time elapsed: 00:21:14
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1318/1500 [0m                     

                       Computation: 114513 steps/s (collection: 0.752s, learning 0.106s)
             Mean action noise std: 5.12
          Mean value_function loss: 84.8963
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.8786
                       Mean reward: 567.76
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 107.0854
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.86s
                      Time elapsed: 00:21:15
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1319/1500 [0m                     

                       Computation: 114594 steps/s (collection: 0.752s, learning 0.106s)
             Mean action noise std: 5.13
          Mean value_function loss: 78.8379
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.8929
                       Mean reward: 546.70
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 110.9671
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.86s
                      Time elapsed: 00:21:16
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1320/1500 [0m                     

                       Computation: 113372 steps/s (collection: 0.760s, learning 0.107s)
             Mean action noise std: 5.14
          Mean value_function loss: 92.0664
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 22.9053
                       Mean reward: 579.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3206
    Episode_Reward/rotating_object: 108.2370
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.87s
                      Time elapsed: 00:21:17
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1321/1500 [0m                     

                       Computation: 115934 steps/s (collection: 0.750s, learning 0.098s)
             Mean action noise std: 5.14
          Mean value_function loss: 90.4030
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.9113
                       Mean reward: 499.72
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 0.3160
    Episode_Reward/rotating_object: 103.5707
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.85s
                      Time elapsed: 00:21:17
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1322/1500 [0m                     

                       Computation: 111393 steps/s (collection: 0.753s, learning 0.129s)
             Mean action noise std: 5.14
          Mean value_function loss: 88.0574
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.9139
                       Mean reward: 574.29
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 111.7168
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.88s
                      Time elapsed: 00:21:18
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1323/1500 [0m                     

                       Computation: 114187 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 5.15
          Mean value_function loss: 85.7815
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.9198
                       Mean reward: 527.73
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 106.1533
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.86s
                      Time elapsed: 00:21:19
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1324/1500 [0m                     

                       Computation: 112428 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 5.15
          Mean value_function loss: 80.0837
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9256
                       Mean reward: 552.73
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.3263
    Episode_Reward/rotating_object: 113.5419
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.87s
                      Time elapsed: 00:21:20
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1325/1500 [0m                     

                       Computation: 113092 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 5.15
          Mean value_function loss: 88.8653
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.9279
                       Mean reward: 537.47
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 105.0222
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.87s
                      Time elapsed: 00:21:21
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1326/1500 [0m                     

                       Computation: 112837 steps/s (collection: 0.760s, learning 0.112s)
             Mean action noise std: 5.16
          Mean value_function loss: 85.4475
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.9308
                       Mean reward: 572.38
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 109.7146
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.87s
                      Time elapsed: 00:21:22
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1327/1500 [0m                     

                       Computation: 115258 steps/s (collection: 0.762s, learning 0.091s)
             Mean action noise std: 5.16
          Mean value_function loss: 89.7497
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.9348
                       Mean reward: 538.97
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.3188
    Episode_Reward/rotating_object: 109.5392
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.85s
                      Time elapsed: 00:21:23
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1328/1500 [0m                     

                       Computation: 115566 steps/s (collection: 0.750s, learning 0.101s)
             Mean action noise std: 5.16
          Mean value_function loss: 90.8473
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.9368
                       Mean reward: 492.04
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 107.5150
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.85s
                      Time elapsed: 00:21:23
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1329/1500 [0m                     

                       Computation: 116985 steps/s (collection: 0.739s, learning 0.101s)
             Mean action noise std: 5.17
          Mean value_function loss: 84.9635
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.9419
                       Mean reward: 541.77
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.3210
    Episode_Reward/rotating_object: 109.4406
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.84s
                      Time elapsed: 00:21:24
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1330/1500 [0m                     

                       Computation: 116353 steps/s (collection: 0.755s, learning 0.090s)
             Mean action noise std: 5.17
          Mean value_function loss: 80.3144
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 22.9456
                       Mean reward: 564.12
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.3220
    Episode_Reward/rotating_object: 113.5614
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.84s
                      Time elapsed: 00:21:25
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1331/1500 [0m                     

                       Computation: 116919 steps/s (collection: 0.752s, learning 0.089s)
             Mean action noise std: 5.17
          Mean value_function loss: 74.3935
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.9471
                       Mean reward: 527.07
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 104.7523
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.84s
                      Time elapsed: 00:21:26
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1332/1500 [0m                     

                       Computation: 114565 steps/s (collection: 0.766s, learning 0.093s)
             Mean action noise std: 5.18
          Mean value_function loss: 78.3064
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.9508
                       Mean reward: 580.72
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.3216
    Episode_Reward/rotating_object: 111.3177
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.86s
                      Time elapsed: 00:21:27
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1333/1500 [0m                     

                       Computation: 48832 steps/s (collection: 1.919s, learning 0.095s)
             Mean action noise std: 5.18
          Mean value_function loss: 82.0653
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9566
                       Mean reward: 555.78
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 106.4330
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.01s
                      Time elapsed: 00:21:29
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1334/1500 [0m                     

                       Computation: 36157 steps/s (collection: 2.607s, learning 0.112s)
             Mean action noise std: 5.19
          Mean value_function loss: 78.6840
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.9646
                       Mean reward: 477.12
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 104.9967
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 2.72s
                      Time elapsed: 00:21:32
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1335/1500 [0m                     

                       Computation: 34146 steps/s (collection: 2.758s, learning 0.121s)
             Mean action noise std: 5.19
          Mean value_function loss: 80.0457
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 22.9779
                       Mean reward: 532.11
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 106.7465
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 2.88s
                      Time elapsed: 00:21:34
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1336/1500 [0m                     

                       Computation: 33846 steps/s (collection: 2.775s, learning 0.130s)
             Mean action noise std: 5.19
          Mean value_function loss: 77.3247
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 22.9802
                       Mean reward: 554.25
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 109.5784
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 2.90s
                      Time elapsed: 00:21:37
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1337/1500 [0m                     

                       Computation: 33962 steps/s (collection: 2.775s, learning 0.120s)
             Mean action noise std: 5.20
          Mean value_function loss: 77.7976
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.9840
                       Mean reward: 557.14
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 109.0349
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 2.89s
                      Time elapsed: 00:21:40
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1338/1500 [0m                     

                       Computation: 33340 steps/s (collection: 2.823s, learning 0.125s)
             Mean action noise std: 5.20
          Mean value_function loss: 77.9987
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9933
                       Mean reward: 543.17
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 108.3520
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 2.95s
                      Time elapsed: 00:21:43
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1339/1500 [0m                     

                       Computation: 32091 steps/s (collection: 2.849s, learning 0.214s)
             Mean action noise std: 5.21
          Mean value_function loss: 76.6034
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 23.0020
                       Mean reward: 563.81
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 110.4263
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.06s
                      Time elapsed: 00:21:46
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1340/1500 [0m                     

                       Computation: 30001 steps/s (collection: 3.159s, learning 0.118s)
             Mean action noise std: 5.21
          Mean value_function loss: 86.7394
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.0044
                       Mean reward: 582.83
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 109.7461
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.28s
                      Time elapsed: 00:21:50
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1341/1500 [0m                     

                       Computation: 21382 steps/s (collection: 4.456s, learning 0.142s)
             Mean action noise std: 5.21
          Mean value_function loss: 76.4398
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.0076
                       Mean reward: 507.32
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 0.3159
    Episode_Reward/rotating_object: 108.6759
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.60s
                      Time elapsed: 00:21:54
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1342/1500 [0m                     

                       Computation: 113513 steps/s (collection: 0.778s, learning 0.088s)
             Mean action noise std: 5.22
          Mean value_function loss: 82.5461
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.0089
                       Mean reward: 554.57
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 111.6515
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.87s
                      Time elapsed: 00:21:55
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1343/1500 [0m                     

                       Computation: 115501 steps/s (collection: 0.732s, learning 0.119s)
             Mean action noise std: 5.22
          Mean value_function loss: 78.3255
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.0131
                       Mean reward: 542.99
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.3121
    Episode_Reward/rotating_object: 108.1789
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.85s
                      Time elapsed: 00:21:56
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1344/1500 [0m                     

                       Computation: 118110 steps/s (collection: 0.730s, learning 0.102s)
             Mean action noise std: 5.22
          Mean value_function loss: 67.5931
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.0175
                       Mean reward: 542.67
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 111.0221
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.83s
                      Time elapsed: 00:21:57
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1345/1500 [0m                     

                       Computation: 112531 steps/s (collection: 0.768s, learning 0.106s)
             Mean action noise std: 5.23
          Mean value_function loss: 84.7892
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.0239
                       Mean reward: 518.36
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 108.3569
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.87s
                      Time elapsed: 00:21:58
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1346/1500 [0m                     

                       Computation: 117100 steps/s (collection: 0.749s, learning 0.091s)
             Mean action noise std: 5.23
          Mean value_function loss: 90.3035
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.0199
                       Mean reward: 552.56
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 111.6222
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.84s
                      Time elapsed: 00:21:58
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1347/1500 [0m                     

                       Computation: 116075 steps/s (collection: 0.746s, learning 0.101s)
             Mean action noise std: 5.23
          Mean value_function loss: 83.7246
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 23.0241
                       Mean reward: 537.90
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 0.3146
    Episode_Reward/rotating_object: 108.4549
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.85s
                      Time elapsed: 00:21:59
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1348/1500 [0m                     

                       Computation: 108979 steps/s (collection: 0.794s, learning 0.108s)
             Mean action noise std: 5.24
          Mean value_function loss: 91.7801
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.0369
                       Mean reward: 537.95
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 107.3261
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.90s
                      Time elapsed: 00:22:00
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1349/1500 [0m                     

                       Computation: 113826 steps/s (collection: 0.747s, learning 0.117s)
             Mean action noise std: 5.25
          Mean value_function loss: 88.7407
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.0484
                       Mean reward: 555.20
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.3144
    Episode_Reward/rotating_object: 108.8158
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.86s
                      Time elapsed: 00:22:01
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1350/1500 [0m                     

                       Computation: 113396 steps/s (collection: 0.755s, learning 0.112s)
             Mean action noise std: 5.25
          Mean value_function loss: 85.4351
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 23.0595
                       Mean reward: 552.35
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 111.2881
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.87s
                      Time elapsed: 00:22:02
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1351/1500 [0m                     

                       Computation: 114940 steps/s (collection: 0.751s, learning 0.105s)
             Mean action noise std: 5.25
          Mean value_function loss: 85.1064
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.0633
                       Mean reward: 507.16
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 109.9095
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.86s
                      Time elapsed: 00:22:03
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1352/1500 [0m                     

                       Computation: 114845 steps/s (collection: 0.750s, learning 0.106s)
             Mean action noise std: 5.26
          Mean value_function loss: 73.9265
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.0664
                       Mean reward: 563.96
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 109.1497
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.86s
                      Time elapsed: 00:22:04
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1353/1500 [0m                     

                       Computation: 106893 steps/s (collection: 0.813s, learning 0.107s)
             Mean action noise std: 5.27
          Mean value_function loss: 82.2396
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.0710
                       Mean reward: 571.33
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 109.7751
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.92s
                      Time elapsed: 00:22:04
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1354/1500 [0m                     

                       Computation: 111299 steps/s (collection: 0.782s, learning 0.101s)
             Mean action noise std: 5.27
          Mean value_function loss: 85.4712
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 23.0762
                       Mean reward: 548.30
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.3177
    Episode_Reward/rotating_object: 109.5898
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.88s
                      Time elapsed: 00:22:05
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1355/1500 [0m                     

                       Computation: 108451 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 5.27
          Mean value_function loss: 98.2447
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.0776
                       Mean reward: 542.82
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 0.3106
    Episode_Reward/rotating_object: 108.8087
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.91s
                      Time elapsed: 00:22:06
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1356/1500 [0m                     

                       Computation: 109899 steps/s (collection: 0.768s, learning 0.126s)
             Mean action noise std: 5.28
          Mean value_function loss: 95.0080
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.0849
                       Mean reward: 521.85
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 108.7579
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.89s
                      Time elapsed: 00:22:07
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1357/1500 [0m                     

                       Computation: 110623 steps/s (collection: 0.781s, learning 0.108s)
             Mean action noise std: 5.28
          Mean value_function loss: 92.9810
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.0893
                       Mean reward: 557.37
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.3078
    Episode_Reward/rotating_object: 104.3095
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.89s
                      Time elapsed: 00:22:08
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1358/1500 [0m                     

                       Computation: 103302 steps/s (collection: 0.777s, learning 0.175s)
             Mean action noise std: 5.28
          Mean value_function loss: 104.3745
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.0943
                       Mean reward: 510.56
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 106.1176
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.95s
                      Time elapsed: 00:22:09
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1359/1500 [0m                     

                       Computation: 102314 steps/s (collection: 0.810s, learning 0.151s)
             Mean action noise std: 5.28
          Mean value_function loss: 96.0248
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.0966
                       Mean reward: 556.35
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.3151
    Episode_Reward/rotating_object: 110.5941
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.96s
                      Time elapsed: 00:22:10
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1360/1500 [0m                     

                       Computation: 102980 steps/s (collection: 0.795s, learning 0.159s)
             Mean action noise std: 5.29
          Mean value_function loss: 99.9763
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.0957
                       Mean reward: 515.65
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 0.3082
    Episode_Reward/rotating_object: 104.4367
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.95s
                      Time elapsed: 00:22:11
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1361/1500 [0m                     

                       Computation: 98577 steps/s (collection: 0.800s, learning 0.197s)
             Mean action noise std: 5.29
          Mean value_function loss: 97.9989
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.1012
                       Mean reward: 540.28
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.3135
    Episode_Reward/rotating_object: 107.2175
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.00s
                      Time elapsed: 00:22:12
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1362/1500 [0m                     

                       Computation: 105788 steps/s (collection: 0.785s, learning 0.144s)
             Mean action noise std: 5.30
          Mean value_function loss: 85.3495
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.1067
                       Mean reward: 554.96
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.3155
    Episode_Reward/rotating_object: 110.7150
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.93s
                      Time elapsed: 00:22:13
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1363/1500 [0m                     

                       Computation: 105751 steps/s (collection: 0.811s, learning 0.118s)
             Mean action noise std: 5.30
          Mean value_function loss: 87.9780
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.1096
                       Mean reward: 516.69
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 105.6387
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.93s
                      Time elapsed: 00:22:14
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1364/1500 [0m                     

                       Computation: 115135 steps/s (collection: 0.753s, learning 0.101s)
             Mean action noise std: 5.31
          Mean value_function loss: 91.7523
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.1111
                       Mean reward: 544.40
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.3182
    Episode_Reward/rotating_object: 110.4576
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.85s
                      Time elapsed: 00:22:15
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1365/1500 [0m                     

                       Computation: 109798 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 5.31
          Mean value_function loss: 95.0011
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 23.1139
                       Mean reward: 556.49
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.3192
    Episode_Reward/rotating_object: 109.4459
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.90s
                      Time elapsed: 00:22:16
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1366/1500 [0m                     

                       Computation: 102200 steps/s (collection: 0.813s, learning 0.149s)
             Mean action noise std: 5.31
          Mean value_function loss: 94.1605
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.1163
                       Mean reward: 546.82
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 104.2075
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.96s
                      Time elapsed: 00:22:17
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1367/1500 [0m                     

                       Computation: 102617 steps/s (collection: 0.826s, learning 0.132s)
             Mean action noise std: 5.31
          Mean value_function loss: 95.6216
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.1196
                       Mean reward: 537.07
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 109.6402
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.96s
                      Time elapsed: 00:22:17
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1368/1500 [0m                     

                       Computation: 103412 steps/s (collection: 0.810s, learning 0.141s)
             Mean action noise std: 5.32
          Mean value_function loss: 100.9780
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.1256
                       Mean reward: 518.68
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.3152
    Episode_Reward/rotating_object: 108.6753
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.95s
                      Time elapsed: 00:22:18
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1369/1500 [0m                     

                       Computation: 100870 steps/s (collection: 0.829s, learning 0.146s)
             Mean action noise std: 5.32
          Mean value_function loss: 79.2309
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.1410
                       Mean reward: 570.66
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 104.1697
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.97s
                      Time elapsed: 00:22:19
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1370/1500 [0m                     

                       Computation: 97846 steps/s (collection: 0.857s, learning 0.148s)
             Mean action noise std: 5.33
          Mean value_function loss: 86.6005
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.1557
                       Mean reward: 531.94
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.3161
    Episode_Reward/rotating_object: 107.1755
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.00s
                      Time elapsed: 00:22:20
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1371/1500 [0m                     

                       Computation: 108894 steps/s (collection: 0.783s, learning 0.120s)
             Mean action noise std: 5.33
          Mean value_function loss: 89.8593
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.1668
                       Mean reward: 543.77
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 0.3181
    Episode_Reward/rotating_object: 108.1555
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.90s
                      Time elapsed: 00:22:21
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1372/1500 [0m                     

                       Computation: 107389 steps/s (collection: 0.823s, learning 0.093s)
             Mean action noise std: 5.34
          Mean value_function loss: 82.6774
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 23.1729
                       Mean reward: 558.03
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 108.9884
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.92s
                      Time elapsed: 00:22:22
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1373/1500 [0m                     

                       Computation: 103100 steps/s (collection: 0.834s, learning 0.119s)
             Mean action noise std: 5.34
          Mean value_function loss: 95.4379
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.1782
                       Mean reward: 524.47
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.3193
    Episode_Reward/rotating_object: 110.0100
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.95s
                      Time elapsed: 00:22:23
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1374/1500 [0m                     

                       Computation: 99321 steps/s (collection: 0.878s, learning 0.112s)
             Mean action noise std: 5.34
          Mean value_function loss: 86.3173
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.1836
                       Mean reward: 563.69
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.3194
    Episode_Reward/rotating_object: 110.5902
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.99s
                      Time elapsed: 00:22:24
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1375/1500 [0m                     

                       Computation: 105624 steps/s (collection: 0.820s, learning 0.111s)
             Mean action noise std: 5.35
          Mean value_function loss: 84.9930
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.1876
                       Mean reward: 557.15
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 105.8026
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.93s
                      Time elapsed: 00:22:25
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1376/1500 [0m                     

                       Computation: 96541 steps/s (collection: 0.862s, learning 0.156s)
             Mean action noise std: 5.35
          Mean value_function loss: 72.6227
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.1854
                       Mean reward: 529.99
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 111.1279
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.02s
                      Time elapsed: 00:22:26
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1377/1500 [0m                     

                       Computation: 91788 steps/s (collection: 0.969s, learning 0.102s)
             Mean action noise std: 5.36
          Mean value_function loss: 72.0130
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.1903
                       Mean reward: 529.23
               Mean episode length: 240.85
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 108.3295
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.07s
                      Time elapsed: 00:22:27
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1378/1500 [0m                     

                       Computation: 97528 steps/s (collection: 0.863s, learning 0.145s)
             Mean action noise std: 5.36
          Mean value_function loss: 78.2673
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.1933
                       Mean reward: 577.58
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.3157
    Episode_Reward/rotating_object: 112.4908
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.01s
                      Time elapsed: 00:22:28
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1379/1500 [0m                     

                       Computation: 102690 steps/s (collection: 0.847s, learning 0.110s)
             Mean action noise std: 5.37
          Mean value_function loss: 71.3343
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.1978
                       Mean reward: 537.47
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 112.9135
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.96s
                      Time elapsed: 00:22:29
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1380/1500 [0m                     

                       Computation: 96084 steps/s (collection: 0.866s, learning 0.157s)
             Mean action noise std: 5.37
          Mean value_function loss: 76.8765
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.2065
                       Mean reward: 532.17
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.3147
    Episode_Reward/rotating_object: 111.2303
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.02s
                      Time elapsed: 00:22:30
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1381/1500 [0m                     

                       Computation: 94823 steps/s (collection: 0.901s, learning 0.136s)
             Mean action noise std: 5.37
          Mean value_function loss: 65.9554
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.2096
                       Mean reward: 564.07
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 114.3924
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.04s
                      Time elapsed: 00:22:31
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1382/1500 [0m                     

                       Computation: 94163 steps/s (collection: 0.893s, learning 0.151s)
             Mean action noise std: 5.38
          Mean value_function loss: 69.0054
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.2106
                       Mean reward: 559.18
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.3103
    Episode_Reward/rotating_object: 111.3920
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.04s
                      Time elapsed: 00:22:32
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1383/1500 [0m                     

                       Computation: 100113 steps/s (collection: 0.844s, learning 0.138s)
             Mean action noise std: 5.38
          Mean value_function loss: 69.6345
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.2169
                       Mean reward: 582.33
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 112.2861
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.98s
                      Time elapsed: 00:22:33
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1384/1500 [0m                     

                       Computation: 104755 steps/s (collection: 0.785s, learning 0.154s)
             Mean action noise std: 5.38
          Mean value_function loss: 78.5800
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.2249
                       Mean reward: 581.13
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.3065
    Episode_Reward/rotating_object: 110.6623
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.94s
                      Time elapsed: 00:22:34
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1385/1500 [0m                     

                       Computation: 101334 steps/s (collection: 0.858s, learning 0.112s)
             Mean action noise std: 5.39
          Mean value_function loss: 78.9772
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.2346
                       Mean reward: 560.84
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.3096
    Episode_Reward/rotating_object: 110.8064
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.97s
                      Time elapsed: 00:22:35
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1386/1500 [0m                     

                       Computation: 106637 steps/s (collection: 0.799s, learning 0.123s)
             Mean action noise std: 5.39
          Mean value_function loss: 78.2493
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.2316
                       Mean reward: 554.22
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.3014
    Episode_Reward/rotating_object: 109.3608
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.92s
                      Time elapsed: 00:22:36
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1387/1500 [0m                     

                       Computation: 101722 steps/s (collection: 0.795s, learning 0.171s)
             Mean action noise std: 5.39
          Mean value_function loss: 78.2446
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.2309
                       Mean reward: 538.58
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3055
    Episode_Reward/rotating_object: 111.4216
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.97s
                      Time elapsed: 00:22:37
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1388/1500 [0m                     

                       Computation: 102525 steps/s (collection: 0.809s, learning 0.150s)
             Mean action noise std: 5.40
          Mean value_function loss: 75.6847
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.2399
                       Mean reward: 542.80
               Mean episode length: 247.46
    Episode_Reward/reaching_object: 0.3044
    Episode_Reward/rotating_object: 111.7275
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.96s
                      Time elapsed: 00:22:38
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1389/1500 [0m                     

                       Computation: 96690 steps/s (collection: 0.820s, learning 0.197s)
             Mean action noise std: 5.40
          Mean value_function loss: 80.0966
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.2445
                       Mean reward: 545.83
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3057
    Episode_Reward/rotating_object: 111.3674
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 1.02s
                      Time elapsed: 00:22:39
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1390/1500 [0m                     

                       Computation: 106349 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 5.40
          Mean value_function loss: 76.0757
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 23.2506
                       Mean reward: 558.45
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 115.5280
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.92s
                      Time elapsed: 00:22:40
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1391/1500 [0m                     

                       Computation: 100834 steps/s (collection: 0.804s, learning 0.171s)
             Mean action noise std: 5.41
          Mean value_function loss: 69.1396
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 23.2580
                       Mean reward: 586.01
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3079
    Episode_Reward/rotating_object: 113.8490
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.97s
                      Time elapsed: 00:22:41
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1392/1500 [0m                     

                       Computation: 99822 steps/s (collection: 0.847s, learning 0.138s)
             Mean action noise std: 5.41
          Mean value_function loss: 71.7413
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.2632
                       Mean reward: 557.14
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 114.1534
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.98s
                      Time elapsed: 00:22:42
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1393/1500 [0m                     

                       Computation: 95058 steps/s (collection: 0.880s, learning 0.155s)
             Mean action noise std: 5.41
          Mean value_function loss: 68.6143
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.2707
                       Mean reward: 586.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 113.0526
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.03s
                      Time elapsed: 00:22:43
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1394/1500 [0m                     

                       Computation: 103054 steps/s (collection: 0.822s, learning 0.132s)
             Mean action noise std: 5.42
          Mean value_function loss: 73.6383
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.2774
                       Mean reward: 516.51
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 0.3013
    Episode_Reward/rotating_object: 108.3710
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.95s
                      Time elapsed: 00:22:44
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1395/1500 [0m                     

                       Computation: 85785 steps/s (collection: 0.963s, learning 0.183s)
             Mean action noise std: 5.42
          Mean value_function loss: 73.8685
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.2824
                       Mean reward: 569.90
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.3058
    Episode_Reward/rotating_object: 113.2854
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.15s
                      Time elapsed: 00:22:45
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1396/1500 [0m                     

                       Computation: 92013 steps/s (collection: 0.889s, learning 0.180s)
             Mean action noise std: 5.43
          Mean value_function loss: 83.6882
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.2898
                       Mean reward: 592.12
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.3094
    Episode_Reward/rotating_object: 113.2612
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.07s
                      Time elapsed: 00:22:46
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1397/1500 [0m                     

                       Computation: 105955 steps/s (collection: 0.818s, learning 0.110s)
             Mean action noise std: 5.43
          Mean value_function loss: 81.6226
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.2980
                       Mean reward: 531.46
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.3030
    Episode_Reward/rotating_object: 107.0931
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.93s
                      Time elapsed: 00:22:47
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1398/1500 [0m                     

                       Computation: 94441 steps/s (collection: 0.831s, learning 0.210s)
             Mean action noise std: 5.44
          Mean value_function loss: 71.9483
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 23.3091
                       Mean reward: 596.13
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.3081
    Episode_Reward/rotating_object: 111.2079
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.04s
                      Time elapsed: 00:22:48
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1399/1500 [0m                     

                       Computation: 97418 steps/s (collection: 0.841s, learning 0.169s)
             Mean action noise std: 5.44
          Mean value_function loss: 80.7040
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.3126
                       Mean reward: 545.77
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 111.8537
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.01s
                      Time elapsed: 00:22:49
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1400/1500 [0m                     

                       Computation: 96423 steps/s (collection: 0.888s, learning 0.132s)
             Mean action noise std: 5.44
          Mean value_function loss: 81.9163
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.3162
                       Mean reward: 581.80
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.3074
    Episode_Reward/rotating_object: 113.5069
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.02s
                      Time elapsed: 00:22:50
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1401/1500 [0m                     

                       Computation: 101142 steps/s (collection: 0.819s, learning 0.153s)
             Mean action noise std: 5.45
          Mean value_function loss: 72.4321
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.3255
                       Mean reward: 567.49
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 112.1824
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.97s
                      Time elapsed: 00:22:51
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1402/1500 [0m                     

                       Computation: 97098 steps/s (collection: 0.860s, learning 0.152s)
             Mean action noise std: 5.46
          Mean value_function loss: 72.4856
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 23.3384
                       Mean reward: 581.79
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 116.0387
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.01s
                      Time elapsed: 00:22:52
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1403/1500 [0m                     

                       Computation: 99358 steps/s (collection: 0.817s, learning 0.173s)
             Mean action noise std: 5.46
          Mean value_function loss: 74.4158
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 23.3455
                       Mean reward: 587.96
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 111.5073
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.99s
                      Time elapsed: 00:22:53
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1404/1500 [0m                     

                       Computation: 101715 steps/s (collection: 0.837s, learning 0.130s)
             Mean action noise std: 5.46
          Mean value_function loss: 77.7925
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.3491
                       Mean reward: 583.80
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.3171
    Episode_Reward/rotating_object: 113.1426
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.97s
                      Time elapsed: 00:22:54
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1405/1500 [0m                     

                       Computation: 103711 steps/s (collection: 0.791s, learning 0.157s)
             Mean action noise std: 5.47
          Mean value_function loss: 72.7439
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.3531
                       Mean reward: 550.04
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.3162
    Episode_Reward/rotating_object: 111.8107
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.95s
                      Time elapsed: 00:22:55
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1406/1500 [0m                     

                       Computation: 102466 steps/s (collection: 0.819s, learning 0.141s)
             Mean action noise std: 5.47
          Mean value_function loss: 76.6796
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.3607
                       Mean reward: 563.07
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.3175
    Episode_Reward/rotating_object: 110.9047
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.96s
                      Time elapsed: 00:22:56
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1407/1500 [0m                     

                       Computation: 107403 steps/s (collection: 0.798s, learning 0.117s)
             Mean action noise std: 5.48
          Mean value_function loss: 73.2144
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.3701
                       Mean reward: 565.62
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 110.5627
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.92s
                      Time elapsed: 00:22:57
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1408/1500 [0m                     

                       Computation: 105027 steps/s (collection: 0.802s, learning 0.134s)
             Mean action noise std: 5.48
          Mean value_function loss: 72.4448
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.3724
                       Mean reward: 590.85
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 0.3125
    Episode_Reward/rotating_object: 113.5207
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.94s
                      Time elapsed: 00:22:58
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1409/1500 [0m                     

                       Computation: 106611 steps/s (collection: 0.776s, learning 0.146s)
             Mean action noise std: 5.48
          Mean value_function loss: 69.2615
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 23.3742
                       Mean reward: 595.14
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 111.7939
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.92s
                      Time elapsed: 00:22:59
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1410/1500 [0m                     

                       Computation: 105490 steps/s (collection: 0.822s, learning 0.110s)
             Mean action noise std: 5.49
          Mean value_function loss: 83.1308
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.3819
                       Mean reward: 531.18
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.3131
    Episode_Reward/rotating_object: 109.3279
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.93s
                      Time elapsed: 00:23:00
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1411/1500 [0m                     

                       Computation: 114205 steps/s (collection: 0.759s, learning 0.102s)
             Mean action noise std: 5.49
          Mean value_function loss: 73.4248
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.3881
                       Mean reward: 561.49
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.3137
    Episode_Reward/rotating_object: 112.3644
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.86s
                      Time elapsed: 00:23:00
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1412/1500 [0m                     

                       Computation: 106052 steps/s (collection: 0.827s, learning 0.100s)
             Mean action noise std: 5.50
          Mean value_function loss: 86.6959
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.3916
                       Mean reward: 557.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3195
    Episode_Reward/rotating_object: 113.7214
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.93s
                      Time elapsed: 00:23:01
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1413/1500 [0m                     

                       Computation: 104527 steps/s (collection: 0.798s, learning 0.142s)
             Mean action noise std: 5.50
          Mean value_function loss: 77.4167
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.4025
                       Mean reward: 544.96
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.3164
    Episode_Reward/rotating_object: 109.9832
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.94s
                      Time elapsed: 00:23:02
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1414/1500 [0m                     

                       Computation: 105210 steps/s (collection: 0.809s, learning 0.125s)
             Mean action noise std: 5.50
          Mean value_function loss: 71.8540
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 23.4015
                       Mean reward: 545.66
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.3201
    Episode_Reward/rotating_object: 114.2491
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.93s
                      Time elapsed: 00:23:03
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1415/1500 [0m                     

                       Computation: 106121 steps/s (collection: 0.810s, learning 0.116s)
             Mean action noise std: 5.51
          Mean value_function loss: 80.3538
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.4020
                       Mean reward: 579.95
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.3173
    Episode_Reward/rotating_object: 115.4057
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.93s
                      Time elapsed: 00:23:04
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1416/1500 [0m                     

                       Computation: 111206 steps/s (collection: 0.771s, learning 0.113s)
             Mean action noise std: 5.51
          Mean value_function loss: 81.5748
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.4101
                       Mean reward: 526.28
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 110.7737
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.88s
                      Time elapsed: 00:23:05
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1417/1500 [0m                     

                       Computation: 118493 steps/s (collection: 0.743s, learning 0.086s)
             Mean action noise std: 5.52
          Mean value_function loss: 79.7433
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.4168
                       Mean reward: 568.78
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.3154
    Episode_Reward/rotating_object: 111.4437
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.83s
                      Time elapsed: 00:23:06
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1418/1500 [0m                     

                       Computation: 113107 steps/s (collection: 0.746s, learning 0.123s)
             Mean action noise std: 5.52
          Mean value_function loss: 68.7089
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.4218
                       Mean reward: 563.92
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 0.3127
    Episode_Reward/rotating_object: 113.6355
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.87s
                      Time elapsed: 00:23:07
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1419/1500 [0m                     

                       Computation: 114859 steps/s (collection: 0.749s, learning 0.107s)
             Mean action noise std: 5.52
          Mean value_function loss: 67.5952
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.4242
                       Mean reward: 576.87
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.3120
    Episode_Reward/rotating_object: 113.5362
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.86s
                      Time elapsed: 00:23:08
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1420/1500 [0m                     

                       Computation: 109910 steps/s (collection: 0.805s, learning 0.089s)
             Mean action noise std: 5.53
          Mean value_function loss: 70.5366
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.4223
                       Mean reward: 566.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3145
    Episode_Reward/rotating_object: 114.3706
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.89s
                      Time elapsed: 00:23:09
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1421/1500 [0m                     

                       Computation: 111543 steps/s (collection: 0.787s, learning 0.094s)
             Mean action noise std: 5.53
          Mean value_function loss: 84.8659
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.4248
                       Mean reward: 578.40
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 0.3142
    Episode_Reward/rotating_object: 115.1732
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.88s
                      Time elapsed: 00:23:09
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1422/1500 [0m                     

                       Computation: 113737 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 5.54
          Mean value_function loss: 73.6815
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.4345
                       Mean reward: 556.53
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.3109
    Episode_Reward/rotating_object: 112.7390
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.86s
                      Time elapsed: 00:23:10
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1423/1500 [0m                     

                       Computation: 112265 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 5.55
          Mean value_function loss: 80.2111
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.4472
                       Mean reward: 549.28
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 0.3130
    Episode_Reward/rotating_object: 114.8993
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.88s
                      Time elapsed: 00:23:11
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1424/1500 [0m                     

                       Computation: 105529 steps/s (collection: 0.796s, learning 0.136s)
             Mean action noise std: 5.55
          Mean value_function loss: 70.6247
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.4517
                       Mean reward: 557.31
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.3079
    Episode_Reward/rotating_object: 111.1622
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.93s
                      Time elapsed: 00:23:12
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1425/1500 [0m                     

                       Computation: 99137 steps/s (collection: 0.825s, learning 0.166s)
             Mean action noise std: 5.55
          Mean value_function loss: 67.3317
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.4618
                       Mean reward: 608.11
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 119.6983
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.99s
                      Time elapsed: 00:23:13
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1426/1500 [0m                     

                       Computation: 102738 steps/s (collection: 0.860s, learning 0.097s)
             Mean action noise std: 5.56
          Mean value_function loss: 69.1375
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.4724
                       Mean reward: 550.33
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.3115
    Episode_Reward/rotating_object: 111.8548
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.96s
                      Time elapsed: 00:23:14
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1427/1500 [0m                     

                       Computation: 110418 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 5.56
          Mean value_function loss: 74.5920
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.4781
                       Mean reward: 547.67
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 111.2433
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.89s
                      Time elapsed: 00:23:15
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1428/1500 [0m                     

                       Computation: 114371 steps/s (collection: 0.761s, learning 0.098s)
             Mean action noise std: 5.57
          Mean value_function loss: 74.9932
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.4852
                       Mean reward: 584.68
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 0.3099
    Episode_Reward/rotating_object: 112.8249
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.86s
                      Time elapsed: 00:23:16
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1429/1500 [0m                     

                       Computation: 105997 steps/s (collection: 0.755s, learning 0.172s)
             Mean action noise std: 5.57
          Mean value_function loss: 73.6119
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.4940
                       Mean reward: 527.64
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.3090
    Episode_Reward/rotating_object: 113.3933
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.93s
                      Time elapsed: 00:23:17
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1430/1500 [0m                     

                       Computation: 110912 steps/s (collection: 0.774s, learning 0.113s)
             Mean action noise std: 5.58
          Mean value_function loss: 78.3535
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.5020
                       Mean reward: 597.04
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 114.0931
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.89s
                      Time elapsed: 00:23:18
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1431/1500 [0m                     

                       Computation: 118995 steps/s (collection: 0.730s, learning 0.097s)
             Mean action noise std: 5.58
          Mean value_function loss: 80.6001
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.5086
                       Mean reward: 581.59
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.3107
    Episode_Reward/rotating_object: 114.4433
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.83s
                      Time elapsed: 00:23:18
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1432/1500 [0m                     

                       Computation: 114021 steps/s (collection: 0.767s, learning 0.095s)
             Mean action noise std: 5.58
          Mean value_function loss: 69.6318
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.5090
                       Mean reward: 567.06
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.3116
    Episode_Reward/rotating_object: 113.6361
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.86s
                      Time elapsed: 00:23:19
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1433/1500 [0m                     

                       Computation: 106499 steps/s (collection: 0.816s, learning 0.107s)
             Mean action noise std: 5.59
          Mean value_function loss: 81.8131
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.5118
                       Mean reward: 590.29
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 112.4456
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.92s
                      Time elapsed: 00:23:20
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1434/1500 [0m                     

                       Computation: 102439 steps/s (collection: 0.842s, learning 0.118s)
             Mean action noise std: 5.59
          Mean value_function loss: 85.5327
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5166
                       Mean reward: 572.14
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.3066
    Episode_Reward/rotating_object: 110.2219
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.96s
                      Time elapsed: 00:23:21
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1435/1500 [0m                     

                       Computation: 97570 steps/s (collection: 0.808s, learning 0.199s)
             Mean action noise std: 5.60
          Mean value_function loss: 68.7661
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.5190
                       Mean reward: 562.38
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 114.9778
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.01s
                      Time elapsed: 00:23:22
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1436/1500 [0m                     

                       Computation: 102239 steps/s (collection: 0.868s, learning 0.093s)
             Mean action noise std: 5.60
          Mean value_function loss: 74.3083
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.5220
                       Mean reward: 552.97
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.3165
    Episode_Reward/rotating_object: 113.8296
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.96s
                      Time elapsed: 00:23:23
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1437/1500 [0m                     

                       Computation: 104604 steps/s (collection: 0.806s, learning 0.134s)
             Mean action noise std: 5.60
          Mean value_function loss: 78.5618
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.5211
                       Mean reward: 563.44
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.3196
    Episode_Reward/rotating_object: 114.3386
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0021
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.94s
                      Time elapsed: 00:23:24
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1438/1500 [0m                     

                       Computation: 101703 steps/s (collection: 0.863s, learning 0.104s)
             Mean action noise std: 5.61
          Mean value_function loss: 76.3906
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.5233
                       Mean reward: 565.75
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.3122
    Episode_Reward/rotating_object: 111.6572
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.97s
                      Time elapsed: 00:23:25
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1439/1500 [0m                     

                       Computation: 112272 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 5.61
          Mean value_function loss: 66.2074
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 23.5276
                       Mean reward: 579.11
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 116.0428
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.88s
                      Time elapsed: 00:23:26
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1440/1500 [0m                     

                       Computation: 108637 steps/s (collection: 0.774s, learning 0.131s)
             Mean action noise std: 5.61
          Mean value_function loss: 71.2767
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.5290
                       Mean reward: 584.49
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.3198
    Episode_Reward/rotating_object: 116.8224
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.90s
                      Time elapsed: 00:23:27
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1441/1500 [0m                     

                       Computation: 100463 steps/s (collection: 0.795s, learning 0.184s)
             Mean action noise std: 5.62
          Mean value_function loss: 80.0839
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 23.5352
                       Mean reward: 585.21
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.3205
    Episode_Reward/rotating_object: 116.5886
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.98s
                      Time elapsed: 00:23:28
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1442/1500 [0m                     

                       Computation: 108332 steps/s (collection: 0.774s, learning 0.134s)
             Mean action noise std: 5.62
          Mean value_function loss: 88.2754
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.5389
                       Mean reward: 574.67
               Mean episode length: 244.51
    Episode_Reward/reaching_object: 0.3119
    Episode_Reward/rotating_object: 113.1386
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.91s
                      Time elapsed: 00:23:29
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1443/1500 [0m                     

                       Computation: 105767 steps/s (collection: 0.814s, learning 0.116s)
             Mean action noise std: 5.63
          Mean value_function loss: 77.2810
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.5457
                       Mean reward: 602.97
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.3187
    Episode_Reward/rotating_object: 114.2678
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.93s
                      Time elapsed: 00:23:30
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1444/1500 [0m                     

                       Computation: 114526 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 5.63
          Mean value_function loss: 83.2613
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 23.5571
                       Mean reward: 553.55
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 112.9681
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.86s
                      Time elapsed: 00:23:31
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1445/1500 [0m                     

                       Computation: 114847 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 5.64
          Mean value_function loss: 85.8055
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.5653
                       Mean reward: 564.01
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 0.3185
    Episode_Reward/rotating_object: 109.9226
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.86s
                      Time elapsed: 00:23:31
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1446/1500 [0m                     

                       Computation: 100365 steps/s (collection: 0.819s, learning 0.160s)
             Mean action noise std: 5.64
          Mean value_function loss: 96.0948
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.5731
                       Mean reward: 593.90
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 0.3233
    Episode_Reward/rotating_object: 115.4447
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.98s
                      Time elapsed: 00:23:32
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1447/1500 [0m                     

                       Computation: 101565 steps/s (collection: 0.880s, learning 0.088s)
             Mean action noise std: 5.64
          Mean value_function loss: 104.7827
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5744
                       Mean reward: 544.01
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 111.0759
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.97s
                      Time elapsed: 00:23:33
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1448/1500 [0m                     

                       Computation: 104978 steps/s (collection: 0.834s, learning 0.103s)
             Mean action noise std: 5.64
          Mean value_function loss: 98.0334
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.5768
                       Mean reward: 588.37
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.3217
    Episode_Reward/rotating_object: 113.1818
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.94s
                      Time elapsed: 00:23:34
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1449/1500 [0m                     

                       Computation: 111262 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 5.65
          Mean value_function loss: 81.0600
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.5819
                       Mean reward: 575.88
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.3245
    Episode_Reward/rotating_object: 112.0131
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.88s
                      Time elapsed: 00:23:35
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1450/1500 [0m                     

                       Computation: 105395 steps/s (collection: 0.782s, learning 0.151s)
             Mean action noise std: 5.65
          Mean value_function loss: 87.9657
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.5827
                       Mean reward: 581.26
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 112.3621
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.93s
                      Time elapsed: 00:23:36
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1451/1500 [0m                     

                       Computation: 104687 steps/s (collection: 0.787s, learning 0.152s)
             Mean action noise std: 5.65
          Mean value_function loss: 81.5144
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.5826
                       Mean reward: 566.51
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 0.3275
    Episode_Reward/rotating_object: 108.5505
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.94s
                      Time elapsed: 00:23:37
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1452/1500 [0m                     

                       Computation: 101719 steps/s (collection: 0.799s, learning 0.168s)
             Mean action noise std: 5.65
          Mean value_function loss: 89.6220
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.5865
                       Mean reward: 556.19
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.3239
    Episode_Reward/rotating_object: 110.0295
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.97s
                      Time elapsed: 00:23:38
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1453/1500 [0m                     

                       Computation: 103407 steps/s (collection: 0.814s, learning 0.137s)
             Mean action noise std: 5.66
          Mean value_function loss: 79.4439
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.5932
                       Mean reward: 555.11
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.3296
    Episode_Reward/rotating_object: 112.1558
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.95s
                      Time elapsed: 00:23:39
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1454/1500 [0m                     

                       Computation: 110164 steps/s (collection: 0.766s, learning 0.126s)
             Mean action noise std: 5.66
          Mean value_function loss: 74.3838
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.6024
                       Mean reward: 544.25
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.3261
    Episode_Reward/rotating_object: 108.7394
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.89s
                      Time elapsed: 00:23:40
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1455/1500 [0m                     

                       Computation: 114479 steps/s (collection: 0.737s, learning 0.122s)
             Mean action noise std: 5.67
          Mean value_function loss: 75.7777
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.6132
                       Mean reward: 580.07
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.3229
    Episode_Reward/rotating_object: 113.3206
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.86s
                      Time elapsed: 00:23:41
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1456/1500 [0m                     

                       Computation: 112061 steps/s (collection: 0.774s, learning 0.103s)
             Mean action noise std: 5.67
          Mean value_function loss: 82.3711
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.6242
                       Mean reward: 580.32
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.3266
    Episode_Reward/rotating_object: 114.5385
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.88s
                      Time elapsed: 00:23:42
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1457/1500 [0m                     

                       Computation: 104034 steps/s (collection: 0.769s, learning 0.176s)
             Mean action noise std: 5.68
          Mean value_function loss: 79.6022
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.6284
                       Mean reward: 572.16
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 0.3172
    Episode_Reward/rotating_object: 107.9536
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.94s
                      Time elapsed: 00:23:43
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1458/1500 [0m                     

                       Computation: 101480 steps/s (collection: 0.848s, learning 0.121s)
             Mean action noise std: 5.68
          Mean value_function loss: 81.1130
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.6337
                       Mean reward: 614.97
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.3227
    Episode_Reward/rotating_object: 114.0513
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.97s
                      Time elapsed: 00:23:43
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1459/1500 [0m                     

                       Computation: 112501 steps/s (collection: 0.783s, learning 0.091s)
             Mean action noise std: 5.69
          Mean value_function loss: 80.1609
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.6419
                       Mean reward: 573.96
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.3150
    Episode_Reward/rotating_object: 115.5643
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.87s
                      Time elapsed: 00:23:44
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1460/1500 [0m                     

                       Computation: 101218 steps/s (collection: 0.802s, learning 0.170s)
             Mean action noise std: 5.69
          Mean value_function loss: 90.7828
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.6471
                       Mean reward: 611.25
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.3169
    Episode_Reward/rotating_object: 115.0578
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.97s
                      Time elapsed: 00:23:45
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1461/1500 [0m                     

                       Computation: 104790 steps/s (collection: 0.825s, learning 0.113s)
             Mean action noise std: 5.70
          Mean value_function loss: 97.6430
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 23.6511
                       Mean reward: 576.13
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.3170
    Episode_Reward/rotating_object: 114.1189
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.94s
                      Time elapsed: 00:23:46
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1462/1500 [0m                     

                       Computation: 87496 steps/s (collection: 0.946s, learning 0.178s)
             Mean action noise std: 5.70
          Mean value_function loss: 86.0745
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.6567
                       Mean reward: 561.93
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 0.3083
    Episode_Reward/rotating_object: 112.5254
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.12s
                      Time elapsed: 00:23:47
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1463/1500 [0m                     

                       Computation: 94876 steps/s (collection: 0.902s, learning 0.134s)
             Mean action noise std: 5.70
          Mean value_function loss: 98.8238
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.6640
                       Mean reward: 561.55
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.3086
    Episode_Reward/rotating_object: 110.5840
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.04s
                      Time elapsed: 00:23:48
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1464/1500 [0m                     

                       Computation: 103842 steps/s (collection: 0.819s, learning 0.128s)
             Mean action noise std: 5.71
          Mean value_function loss: 93.4635
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6735
                       Mean reward: 564.86
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.3105
    Episode_Reward/rotating_object: 111.8473
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.95s
                      Time elapsed: 00:23:49
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1465/1500 [0m                     

                       Computation: 107163 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 5.72
          Mean value_function loss: 90.3949
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 23.6850
                       Mean reward: 554.02
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.3124
    Episode_Reward/rotating_object: 114.3676
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.92s
                      Time elapsed: 00:23:50
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1466/1500 [0m                     

                       Computation: 108794 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 5.72
          Mean value_function loss: 88.5731
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.6946
                       Mean reward: 549.65
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 0.3074
    Episode_Reward/rotating_object: 108.9175
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.90s
                      Time elapsed: 00:23:51
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1467/1500 [0m                     

                       Computation: 111964 steps/s (collection: 0.748s, learning 0.130s)
             Mean action noise std: 5.72
          Mean value_function loss: 85.8503
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.6998
                       Mean reward: 511.21
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 0.2990
    Episode_Reward/rotating_object: 102.6129
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.88s
                      Time elapsed: 00:23:52
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1468/1500 [0m                     

                       Computation: 100960 steps/s (collection: 0.856s, learning 0.118s)
             Mean action noise std: 5.73
          Mean value_function loss: 92.6639
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.7022
                       Mean reward: 581.85
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.3140
    Episode_Reward/rotating_object: 115.1869
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.97s
                      Time elapsed: 00:23:53
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1469/1500 [0m                     

                       Computation: 105936 steps/s (collection: 0.779s, learning 0.149s)
             Mean action noise std: 5.73
          Mean value_function loss: 87.7975
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 23.7036
                       Mean reward: 523.66
               Mean episode length: 236.53
    Episode_Reward/reaching_object: 0.3037
    Episode_Reward/rotating_object: 104.5536
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.93s
                      Time elapsed: 00:23:54
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1470/1500 [0m                     

                       Computation: 112757 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 5.73
          Mean value_function loss: 77.9268
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.7058
                       Mean reward: 551.23
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 110.9741
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.87s
                      Time elapsed: 00:23:55
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1471/1500 [0m                     

                       Computation: 113532 steps/s (collection: 0.753s, learning 0.113s)
             Mean action noise std: 5.73
          Mean value_function loss: 78.9583
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.7060
                       Mean reward: 518.58
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3128
    Episode_Reward/rotating_object: 112.0306
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.87s
                      Time elapsed: 00:23:56
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1472/1500 [0m                     

                       Computation: 110240 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 5.73
          Mean value_function loss: 85.8609
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.7064
                       Mean reward: 558.45
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.3118
    Episode_Reward/rotating_object: 113.0576
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.89s
                      Time elapsed: 00:23:57
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1473/1500 [0m                     

                       Computation: 110508 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 5.73
          Mean value_function loss: 76.0617
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.7024
                       Mean reward: 574.59
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.3113
    Episode_Reward/rotating_object: 113.3936
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.89s
                      Time elapsed: 00:23:57
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1474/1500 [0m                     

                       Computation: 109778 steps/s (collection: 0.788s, learning 0.107s)
             Mean action noise std: 5.74
          Mean value_function loss: 82.9127
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.7037
                       Mean reward: 572.46
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.3102
    Episode_Reward/rotating_object: 112.4043
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.90s
                      Time elapsed: 00:23:58
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1475/1500 [0m                     

                       Computation: 109464 steps/s (collection: 0.775s, learning 0.123s)
             Mean action noise std: 5.75
          Mean value_function loss: 91.5915
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.7095
                       Mean reward: 583.64
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.3104
    Episode_Reward/rotating_object: 111.2331
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.90s
                      Time elapsed: 00:23:59
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1476/1500 [0m                     

                       Computation: 111815 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 5.75
          Mean value_function loss: 89.8278
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.7132
                       Mean reward: 533.01
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.3042
    Episode_Reward/rotating_object: 107.7501
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.88s
                      Time elapsed: 00:24:00
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1477/1500 [0m                     

                       Computation: 114498 steps/s (collection: 0.758s, learning 0.100s)
             Mean action noise std: 5.75
          Mean value_function loss: 83.6975
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.7135
                       Mean reward: 566.86
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.3112
    Episode_Reward/rotating_object: 110.3056
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.86s
                      Time elapsed: 00:24:01
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1478/1500 [0m                     

                       Computation: 107711 steps/s (collection: 0.777s, learning 0.136s)
             Mean action noise std: 5.75
          Mean value_function loss: 88.1028
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.7147
                       Mean reward: 571.46
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.3076
    Episode_Reward/rotating_object: 113.9353
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.91s
                      Time elapsed: 00:24:02
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1479/1500 [0m                     

                       Computation: 108100 steps/s (collection: 0.745s, learning 0.164s)
             Mean action noise std: 5.76
          Mean value_function loss: 89.7786
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.7237
                       Mean reward: 566.77
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.3098
    Episode_Reward/rotating_object: 114.0673
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.91s
                      Time elapsed: 00:24:03
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1480/1500 [0m                     

                       Computation: 113905 steps/s (collection: 0.745s, learning 0.118s)
             Mean action noise std: 5.76
          Mean value_function loss: 87.8952
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.7352
                       Mean reward: 572.98
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 114.9958
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.86s
                      Time elapsed: 00:24:04
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1481/1500 [0m                     

                       Computation: 115590 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 5.77
          Mean value_function loss: 84.6462
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.7415
                       Mean reward: 527.71
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 110.2467
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.85s
                      Time elapsed: 00:24:05
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1482/1500 [0m                     

                       Computation: 111701 steps/s (collection: 0.773s, learning 0.107s)
             Mean action noise std: 5.77
          Mean value_function loss: 88.5956
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.7469
                       Mean reward: 531.56
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.3132
    Episode_Reward/rotating_object: 112.3365
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.88s
                      Time elapsed: 00:24:05
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1483/1500 [0m                     

                       Computation: 114676 steps/s (collection: 0.755s, learning 0.102s)
             Mean action noise std: 5.78
          Mean value_function loss: 91.3060
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.7535
                       Mean reward: 550.06
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 113.9779
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.86s
                      Time elapsed: 00:24:06
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1484/1500 [0m                     

                       Computation: 111767 steps/s (collection: 0.785s, learning 0.095s)
             Mean action noise std: 5.78
          Mean value_function loss: 81.2924
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.7580
                       Mean reward: 565.70
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 113.1309
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.88s
                      Time elapsed: 00:24:07
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1485/1500 [0m                     

                       Computation: 114093 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 5.78
          Mean value_function loss: 81.0904
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.7585
                       Mean reward: 590.63
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.3097
    Episode_Reward/rotating_object: 113.1345
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.86s
                      Time elapsed: 00:24:08
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1486/1500 [0m                     

                       Computation: 106534 steps/s (collection: 0.786s, learning 0.137s)
             Mean action noise std: 5.78
          Mean value_function loss: 72.9630
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.7620
                       Mean reward: 561.09
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.3092
    Episode_Reward/rotating_object: 111.5522
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.92s
                      Time elapsed: 00:24:09
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1487/1500 [0m                     

                       Computation: 111513 steps/s (collection: 0.769s, learning 0.113s)
             Mean action noise std: 5.79
          Mean value_function loss: 72.0356
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.7708
                       Mean reward: 589.63
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.3143
    Episode_Reward/rotating_object: 113.8245
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.88s
                      Time elapsed: 00:24:10
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1488/1500 [0m                     

                       Computation: 100354 steps/s (collection: 0.744s, learning 0.236s)
             Mean action noise std: 5.79
          Mean value_function loss: 80.9122
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.7784
                       Mean reward: 576.80
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.3126
    Episode_Reward/rotating_object: 114.4222
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.98s
                      Time elapsed: 00:24:11
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1489/1500 [0m                     

                       Computation: 105159 steps/s (collection: 0.769s, learning 0.166s)
             Mean action noise std: 5.80
          Mean value_function loss: 78.0822
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 23.7802
                       Mean reward: 557.66
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.3075
    Episode_Reward/rotating_object: 110.6811
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.93s
                      Time elapsed: 00:24:12
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1490/1500 [0m                     

                       Computation: 109778 steps/s (collection: 0.758s, learning 0.138s)
             Mean action noise std: 5.80
          Mean value_function loss: 85.4300
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.7829
                       Mean reward: 580.73
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.3122
    Episode_Reward/rotating_object: 114.5530
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.90s
                      Time elapsed: 00:24:13
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1491/1500 [0m                     

                       Computation: 112429 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 5.81
          Mean value_function loss: 76.2320
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 23.7942
                       Mean reward: 593.44
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.3108
    Episode_Reward/rotating_object: 116.0295
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.87s
                      Time elapsed: 00:24:14
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1492/1500 [0m                     

                       Computation: 111483 steps/s (collection: 0.768s, learning 0.114s)
             Mean action noise std: 5.81
          Mean value_function loss: 75.8491
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.8075
                       Mean reward: 576.82
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.3091
    Episode_Reward/rotating_object: 114.0336
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.88s
                      Time elapsed: 00:24:14
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1493/1500 [0m                     

                       Computation: 110707 steps/s (collection: 0.791s, learning 0.097s)
             Mean action noise std: 5.81
          Mean value_function loss: 81.2392
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.8109
                       Mean reward: 560.21
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.3114
    Episode_Reward/rotating_object: 112.5179
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.89s
                      Time elapsed: 00:24:15
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1494/1500 [0m                     

                       Computation: 111356 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 5.82
          Mean value_function loss: 83.2346
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.8131
                       Mean reward: 591.66
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 0.3180
    Episode_Reward/rotating_object: 115.8595
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.88s
                      Time elapsed: 00:24:16
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1495/1500 [0m                     

                       Computation: 113677 steps/s (collection: 0.768s, learning 0.096s)
             Mean action noise std: 5.82
          Mean value_function loss: 73.3003
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.8199
                       Mean reward: 537.82
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.3129
    Episode_Reward/rotating_object: 115.3544
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.86s
                      Time elapsed: 00:24:17
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1496/1500 [0m                     

                       Computation: 111624 steps/s (collection: 0.753s, learning 0.128s)
             Mean action noise std: 5.83
          Mean value_function loss: 79.2594
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.8277
                       Mean reward: 611.05
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.3199
    Episode_Reward/rotating_object: 118.6254
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.88s
                      Time elapsed: 00:24:18
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1497/1500 [0m                     

                       Computation: 112173 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 5.83
          Mean value_function loss: 70.2821
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 23.8381
                       Mean reward: 574.76
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 0.3148
    Episode_Reward/rotating_object: 115.1785
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.88s
                      Time elapsed: 00:24:19
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1498/1500 [0m                     

                       Computation: 103253 steps/s (collection: 0.746s, learning 0.206s)
             Mean action noise std: 5.84
          Mean value_function loss: 74.4448
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.8452
                       Mean reward: 557.46
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.3117
    Episode_Reward/rotating_object: 115.3341
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.95s
                      Time elapsed: 00:24:20
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1499/1500 [0m                     

                       Computation: 106572 steps/s (collection: 0.760s, learning 0.162s)
             Mean action noise std: 5.84
          Mean value_function loss: 71.6196
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 23.8484
                       Mean reward: 598.37
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.3074
    Episode_Reward/rotating_object: 113.5522
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.92s
                      Time elapsed: 00:24:21
                               ETA: 00:00:00

